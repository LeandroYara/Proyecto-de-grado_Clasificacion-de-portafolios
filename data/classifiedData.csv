,Title,Abstract,Category
0,Constructive Alignment by Portfolio Exams for an Advanced Control Master Module ★,"The paper shows how the useful paradigm of constructive alignment can be applied in the design of higher education modules on control engineering. As learning outcomes in master programs probably involve all taxonomy levels, the examination process also needs more complexity than possible in written exams. How these constraints could be met by portfolio exams is shown by an example of a master module at an University of Applied Sciences. With the help of an internal matrix structure, it is possible to give feedback to the students during the term by competency graphs as visualizations. Moreover, this structure also allows to derive exam analytics by tensor decomposition methods. © 2022 The Authors. Constructive Alignment by Portfolio Exams for an Advanced Control Master Module ★ competency graphs; Constructive alignment; exam analysis; master modules; portfolio exams; tensor decomposition Advanced control; Competency graph; Constructive alignments; Control engineering; Education module; Exam analyse; High educations; Master module; Portfolio exam; Tensor decomposition; Engineering education",Monitoring and control
1,Realising the promises of artificial intelligence in manufacturing by enhancing CRISP-DM,"To support manufacturing firms in realising the value of Artificial Intelligence (AI), we embarked on a six-year process of research and practice to enhance the popular and widely used CRISP-DM methodology. We extend CRISP-DM into a continuous, active, and iterative life-cycle of AI solutions by adding the phase of ‘Operation and Maintenance’ as well as embedding a task-based framework for linking tasks to skills. Our key findings relate to the difficult trade-offs and hidden costs of operating and maintaining AI solutions and managing AI drift, as well as ensuring the presence of domain, data science, and data engineering competence throughout the CRISP-DM phases. Further, we show how data engineering is an essential but often neglected part of the AI workflow, provide novel insights into the trajectory of involvement of the three competences, and illustrate how the enhanced CRISP-DM methodology can be used as a management tool in AI projects. © 2023 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group. Realising the promises of artificial intelligence in manufacturing by enhancing CRISP-DM Artificial intelligence; CRISP-DM; machine learning; manufacturing; production Cost engineering; Economic and social effects; Engineering education; Industrial research; Iterative methods; Life cycle; Project management; CRISP-DM; Data engineering; Embeddings; Engineering competences; Hidden costs; Machine-learning; Manufacturing firms; Operations and maintenance; Task-based; Trade off; Machine learning",Capacity management
2,Application of Geographic Information Systems in Impact Evaluation and Geospatial Portfolio Analysis of Transport Projects,"New technologies and global datasets enable transport projects to be assessed more effectively and efficiently. Geospatial data are available retrospectively and remotely, which is particularly useful for evaluators working in countries with constraints on their access to data, including those caused by COVID-19 pandemic. Another advantage is that data quality is the same and comparable across countries. This paper reports the results of geospatial portfolio analysis and economic impact analysis with geospatial data. It generated several interesting findings. For example, the contribution of projects carried out by the Asian Development Bank (ADB) became visible, as about 290 million people live along the road alignments of ADB projects and are therefore potential beneficiaries. Their presence and greater economic activity are indicated by increases in the radiance of nighttime light. The data also demonstrated which countries need to make additional efforts to reduce CO2 emissions from the project areas. Two levels of impact analysis using nighttime light were carried out, which measured local benefits of economic growth. The first assessed a national highway project in Armenia. The contribution of the project to economic growth was more than 2.5% per year. The second found that 33 transportation projects had made an average annual contribution to economic growth of 5%. The authors compared the impact assessed by nighttime light with conventional economic analysis using economic internal rates of return, measuring benefits enjoyed by road users and administrators, and observed a positive correlation between the two. © National Academy of Sciences: Transportation Research Board 2022. Application of Geographic Information Systems in Impact Evaluation and Geospatial Portfolio Analysis of Transport Projects data and data science; economic impacts; geographic information systems; geospatial data visualization; sustainability and resilience; transport economics and finance; transportation and society; transportation and sustainability; transportation in the developing countries COVID-19; Data visualization; Economic analysis; Economic and social effects; Geographic information systems; Highway planning; Information systems; Information use; Roads and streets; Sustainable development; Data and data science; Economic impacts; Geo-spatial data visualizations; Night time lights; Sustainability and resilience; Transport economics; Transport finance; Transportation and society; Transportation and sustainability; Transportation in the developing country; Developing countries",Value management
3,Optimization algorithms and investment portfolio analytics with machine learning techniques under time-varying liquidity constraints,"Purpose: This paper aims to examine from commodity portfolio managers’ perspective the performance of liquidity adjusted risk modeling in assessing the market risk parameters of a large commodity portfolio and in obtaining efficient and coherent portfolios under different market circumstances. Design/methodology/approach: The implemented market risk modeling algorithm and investment portfolio analytics using reinforcement machine learning techniques can simultaneously handle risk-return characteristics of commodity investments under regular and crisis market settings besides considering the particular effects of the time-varying liquidity constraints of the multiple-asset commodity portfolios. Findings: In particular, the paper implements a robust machine learning method to commodity optimal portfolio selection and within a liquidity-adjusted value-at-risk (LVaR) framework. In addition, the paper explains how the adapted LVaR modeling algorithms can be used by a commodity trading unit in a dynamic asset allocation framework for estimating risk exposure, assessing risk reduction alternates and creating efficient and coherent market portfolios. Originality/value: The optimization parameters subject to meaningful operational and financial constraints, investment portfolio analytics and empirical results can have important practical uses and applications for commodity portfolio managers particularly in the wake of the 2007–2009 global financial crisis. In addition, the recommended reinforcement machine learning optimization algorithms can aid in solving some real-world dilemmas under stressed and adverse market conditions (e.g. illiquidity, switching in correlations factors signs, nonlinear and non-normal distribution of assets’ returns) and can have key applications in machine learning, expert systems, smart financial functions, internet of things (IoT) and financial technology (FinTech) in big data ecosystems. © 2021, Emerald Publishing Limited. Optimization algorithms and investment portfolio analytics with machine learning techniques under time-varying liquidity constraints Analytics; Artificial intelligence; Business analysis; Commodity; Finance; Liquidity risk; Liquidity-adjusted value-at-risk; Machine learning; Optimization; Portfolio analysis; Portfolio management; Risk analysis; Stress testing ",Strategic alignment
4,"Harnessing the power of synthetic data in healthcare: innovation, application, and privacy","Data-driven decision-making in modern healthcare underpins innovation and predictive analytics in public health and clinical research. Synthetic data has shown promise in finance and economics to improve risk assessment, portfolio optimization, and algorithmic trading. However, higher stakes, potential liabilities, and healthcare practitioner distrust make clinical use of synthetic data difficult. This paper explores the potential benefits and limitations of synthetic data in the healthcare analytics context. We begin with real-world healthcare applications of synthetic data that informs government policy, enhance data privacy, and augment datasets for predictive analytics. We then preview future applications of synthetic data in the emergent field of digital twin technology. We explore the issues of data quality and data bias in synthetic data, which can limit applicability across different applications in the clinical context, and privacy concerns stemming from data misuse and risk of re-identification. Finally, we evaluate the role of regulatory agencies in promoting transparency and accountability and propose strategies for risk mitigation such as Differential Privacy (DP) and a dataset chain of custody to maintain data integrity, traceability, and accountability. Synthetic data can improve healthcare, but measures to protect patient well-being and maintain ethical standards are key to promote responsible use. © 2023, Springer Nature Limited. Harnessing the power of synthetic data in healthcare: innovation, application, and privacy  Data privacy; Decision making; Financial data processing; Health care; Predictive analytics; Risk assessment; Algorithmic trading; Clinical research; Data driven decision; Decisions makings; Health research; Portfolio optimization; Potential liability; Power; Risks assessments; Synthetic data; adult; article; custodial care; data integrity; data privacy; data quality; digital twin; government; human; mitigation; privacy; wellbeing; Clinical research",Strategic alignment
5,Board 144: Interdisciplinary & International Research Experiences in Bioinspired Science & Technology,"Modern industry and startups, particularly in high-tech sectors, show significant growth of cross-disciplinary, cross-cultural, and cross-boundary work needs. Some cross-disciplinary areas with particular demand, now and for the future, are found at the intersection between engineering and the life sciences. Engineers increasingly need competencies in life science areas that intersect with their engineering disciplines. Engineers also must meet high-tech industry requirements of working cross-culturally, communicating effectively with all teams across the enterprise, and effectively using time and project management skills. For STEM-specific roles, young engineers are required to have data science understanding, statistics knowledge, and computational capability especially if working with big data. In response, higher education institutions (HEI) have started matching such industry needs. HEIs are initiating having students work across boundaries of sector, discipline, and identity. Students are being prepared for intersectoral collaboration and multiple career pathways in a workforce that will change more rapidly in coming years. Students are enabled to join multidisciplinary teams with people who approach problems with different methods and knowledge, and to solve problems in diverse groups in terms of culture, race/ethnicity/nationality, gender or socioeconomic status. The International Research Experience for Students (IRES) program of NSF contributes to development of a diverse, globally engaged higher education workforce with world-class skills. Within this program, Virginia Tech (VT) in the United States, with support from the American Society for Engineering Education (ASEE), has developed a transdisciplinary, international education program in the area of bioinspired engineering. IRES graduate-level scholars from multiple fields study autonomous mobility in complex natural environments, through observation and analysis of flying/gliding rainforest animals, with the help of an interdisciplinary mentor network in Brunei, South Korea, and Singapore. So far, a single cohort of students from engineering, biology, and biophysics has completed the program in 2022. The first results show students undergo a transformative process through their interdisciplinary and international research experience in the areas of biology and engineering. The program implements a novel paradigm to bring scholars international research experiences in STEM, especially in bio-oriented environments, in line with the need for solving transdisciplinary challenges through global collaboration and the trend towards personalized learning. To achieve this, the program introduces the following key elements to international research experiences and combines them in a novel manner: (i) focus on research-related professional development experiences that are well integrated into the IRES scholars' thesis research, (ii) leveraging of a combination of natural and academic resources in a variety of sociocultural settings, (iii) a mentor network offering students the opportunity to work with multiple mentors, selected according to students' needs at different program stages, (iv) a modular, customizable approach to paths and schedules for individualized research experiences, (v) pervasive use of a social media many-to-many communication model to ensure coherence within each student cohort and its scientific and social communities, and (vi) constant engagement of students and mentors to accomplish professional development goals. By doing so, the program will further international collaboration, intercultural understanding, and exchange in bioinspired research. © American Society for Engineering Education, 2023. Board 144: Interdisciplinary & International Research Experiences in Bioinspired Science & Technology  Engineering education; Engineers; Human resource management; International cooperation; Project management; Boundary works; Cross-disciplinary; Engineering disciplines; High tech; International researches; Life-sciences; Professional development; Research experience; Research experiences for students; Science technologies; Students",Risk management
6,Exploring Data Analysis and Visualization Techniques for Project Tracking: Insights from the ITC,"Data analysis has emerged as a cornerstone in facilitating informed decision-making across myriad fields, in particular in software development and project management. This integrative practice proves instrumental in enhancing operational efficiency, cutting expenditures, mitigating potential risks, and delivering superior results, all while sustaining structured organization and robust control. This paper presents ITC, a synergistic platform architected to streamline multi-organizational and multi-workspace collaboration for project management and technical documentation. ITC serves as a powerful tool, equipping users with the capability to swiftly establish and manage workspaces and documentation, thereby fostering the derivation of invaluable insights pivotal to both technical and business-oriented decisions. ITC boasts a plethora of features, from support for a diverse range of technologies and languages, synchronization of data, and customizable templates to reusable libraries and task automation, including data extraction, validation, and document automation. This paper also delves into the predictive analytics aspect of the ITC platform. It demonstrates how ITC harnesses predictive data models, such as Random Forest Regression, to anticipate project outcomes and risks, enhancing decision-making in project management. This feature plays a critical role in the strategic allocation of resources, optimizing project timelines, and promoting overall project success. In an effort to substantiate the efficacy and usability of ITC, we have also incorporated the results and feedback garnered from a comprehensive user assessment conducted in 2022. The feedback suggests promising potential for the platform’s application, setting the stage for further development and refinement. The insights provided in this paper not only underline the successful implementation of the ITC platform but also she would light on the transformative impact of predictive analytics in information systems. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG. Exploring Data Analysis and Visualization Techniques for Project Tracking: Insights from the ITC Data Analytics; Machine Learning; Project Management Computer software reusability; Data Analytics; Data visualization; Decision making; Information management; Machine learning; Predictive analytics; Project management; Regression analysis; Robust control; Software design; Data analysis techniques; Data analytics; Decisions makings; Informed decision; Machine-learning; Myriad fields; Project tracking; Software development management; Software project management; Visualization technique; Feedback",Strategic alignment
7,Creating customer value from data: foundations and archetypes of analytics-based services,"The digital transformation offers new opportunities for organizations to expand their existing service portfolio in order to achieve competitive advantages. A popular way to create new customer value is the offer of analytics-based services (ABS)—services that apply analytical methods to data to empower customers to make better decisions and to solve complex problems. However, research still lacks to provide a profound conceptualization of this novel service type. Similarly, actionable insights on how to purposefully establish ABS in the market to enrich the service portfolio remain scarce. We perform a cluster analysis of 105 ABS and triangulate it with a revelatory case study to identify four generic ABS archetypes and to unveil their specific service objectives and characteristics. We also isolate essential factors that shape decision-making regarding the choice of adequate archetypes and subsequent transitions between them. The detailed characterization of different ABS types contributes to a more profound theorizing process on ABS as well as provides a systematization for strategic opportunities to enrich service portfolios in practice. © 2021, The Author(s). Creating customer value from data: foundations and archetypes of analytics-based services Analytics-based services; Archetypes; Cluster analysis; L8; M13; O3; Service portfolio ",Strategic alignment
8,Benchmarking of construction projects performance for comparative assessment and performance improvement: a statistical quantitative approach,"Purpose: The exact process of construction projects performance assessment and benchmarking still remains subjective relying on qualitative techniques, which does not allow stakeholders to address the issues and the drawbacks of their respective projects as effectively as possible for performance improvement purposes. Hence, this research aims to establish a unified project performance score (PPS) for assessing and comparing projects performance. Design/methodology/approach: Data were collected from Construction Industry Institute (CII) members and through University of Wisconsin active research projects. Exploratory data analysis was done to investigate the calculated performance metrics and the collected data characteristics. Data were converted into six performance metrics which were used as the independent variables in creating the PPS model. Logistic regression model was developed to generate the unified PPS equation in order to explain the variables that significantly affect construction projects successful post-completion performance. The PPS model was then applied on the collected dataset to benchmark projects in terms of project delivery systems, compensation types and project types in order to showcase the PPS capabilities and possible applications. Findings: The model revealed that construction cost and schedule growth are the most important metrics in assessing projects performance, while RFIs’ processing time and change orders per million dollars were the features with the least effect on the PPS value. The authors found that integrated project delivery (IPD) and target value (TV) projects outperformed all other project delivery and compensation types. While, industrial projects showed the worst performance, as compared to commercial or institutional projects. Originality/value: The PPS model can be used to assess the performance of any pool of executed projects, and introducing a novel addition to the field of construction business analytics which is a supplementary tool to successful decision making and performance improvement. Additionally, the bidding selection system can be revolutionized from a cost-based to a performance based one using the PPS model to improve the outcomes of the buyout process. © 2023, Emerald Publishing Limited. Benchmarking of construction projects performance for comparative assessment and performance improvement: a statistical quantitative approach Benchmarking methodology; Construction analytics; Construction management; Project management; Strategic management Benchmarking; Construction industry; Decision making; Logistic regression; Benchmarking methodology; Comparative assessment; Comparative performance; Construction analytic; Construction management; Construction projects; Performance; Performance metrices; Project performance; Strategic management; Project management",Strategic alignment
9,Trends in the thematic landscape of HR analytics research: a structural topic modeling approach,"Purpose: The growth of the global labor force and business analytics has significantly impacted human resource management (HRM). Human resource (HR) analytics is an emerging field that creates value for employees and organizations. By examining the existing studies on HR analytics, the paper systematically reviews the literature to identify active research areas and establish a roadmap for future studies in HR analytics. Design/methodology/approach: A portfolio of 503 articles collected from the Scopus database was reviewed. The study has adopted a Latent Dirichlet allocation (LDA) topic modeling approach to identify significant themes in the literature. Findings: The HR analytics research domain is classified into four categories: HR functions, statistical techniques, organizational outcomes and employee characteristics. The study has also developed a framework for organizations adopting HR analytics. Linking HR with blockchain technology, explainable artificial intelligence and Metaverse are the areas identified for future researchers. Practical implications: The framework will assist practitioners in identifying statistical techniques for optimizing various HR functions. The paper discovers that by implementing HR analytics, HR managers and business partners can run reports, make dashboards and visualizations and make evidence-based decision-making. Originality/value: The previous studies have not applied any machine learning techniques to identify the topics in the extant literature. The paper has applied machine learning tools, making the review more robust and providing an exhaustive understanding of the domain. © 2023, Emerald Publishing Limited. Trends in the thematic landscape of HR analytics research: a structural topic modeling approach Content analysis; Human resource analytics; Topic modeling ",Strategic alignment
10,New Technique for Stock Trend Analysis - Volume-weighted Squared Moving Average Convergence & Divergence,"In computational intelligence, Gerald Appel designed MACD, short for Moving Average Convergence /Divergence in the 1970s, a popular trading indicator used in the business data analysis of stock prices to predict future trends. While it is easy to read, MACD has two distinct disadvantages, the time lagging problem and the fake signals problem, resulting in delays in buying or selling signals and decisions. Besides, three parameters input are required for the calculation model, which is not user-friendly for new learners. This study proposes a new methodology - Volume Square-Weighted Moving Average Convergence & Divergence (VSWMACD). It aims to improve MACD performance and apply various evaluation tools to verify the enhancements. Five datasets with 200 stocks from Hong Kong Stock Market in each have been applied to the testing. The outcome shows that compared to MACD, the average Return On Investment of VSWMACD increased by around 15%, and the average Maximum Drawdown decreased by about 5%. VSWMACD is proven to reduce fake signals while earning a higher return with a lower risk than MACD. A better portfolio management can be formed. © 2023 IEEE. New Technique for Stock Trend Analysis - Volume-weighted Squared Moving Average Convergence & Divergence Business analytics; Business informatics; Computational Intelligence; Data information and knowledge Artificial intelligence; Commerce; Financial data processing; Investments; Business analytics; Business data; Business informatics; Convergence/divergence; Data information and knowledge; Data informations; Moving averages; Stock price; Trend analysis; Weighted moving averages; Financial markets",Monitoring and control
11,"Leveraging RDF Graphs, Similarity Metrics and Network Analysis for Business Process Management","The paper reports on an early iteration of a Design Science effort for defining a business process analytics method. The method hybridizes explicitly engineered knowledge and implicit knowledge, as it streamlines the following ingredients: BPMN modeling, semantic linking and the transformation of models into RDF graphs, natural language processing and network analysis applied on the resulting graph repository. The engineered knowledge comes in the form of a BPMN implementation that can transform diagrams into RDF, whereas the implicit knowledge is derived from analytic measures (similarity metrics, network analysis) that further annotate the graphs obtained from models, enabling richer semantic queries and filtering possibilities for Business Process Management use cases. The originating problem context consists of contract management and project management scenarios from which use cases will be exemplified. The proposed method is deployable as an orchestration of tools: the BEE-UP modeling environment, GraphDB for storage and Python libraries (rdflib, nltk, networkx) for processing the graphs and running the annotating analytics. © 2023 CEUR-WS. All rights reserved. Leveraging RDF Graphs, Similarity Metrics and Network Analysis for Business Process Management BEE-UP; business process analytics; knowledge representation; network analysis; similarity metrics Enterprise resource management; Graphic methods; Iterative methods; Knowledge management; Modeling languages; Natural language processing systems; Project management; Resource Description Framework (RDF); Semantics; BEE-UP; Business Process; Business process analytic; Graph similarity; Implicit knowledge; Knowledge-representation; Process analytics; Process management; RDF graph; Similarity metrics; Knowledge representation",Strategic alignment
12,Enterprise Financial Project Management by Visual Analytics Tools,"Companies of all sizes and lines of business are already turning data into real value in many ways. All industries are covered: these are social networks, Internet companies, industrial enterprises, fast food chains, banks, industrialists, hotels, and many others. Scientific research reveals ways to increase productivity through data-driven decision-making. However, the human brain is designed in such a way that graphical display allows one to perceive and understand some features of quantitative data, and some quantitative tasks can be best performed. The article reveals a set of tasks for managing financial projects and the resulting visual analytics methods. Applying a systematic approach to managing financial projects and solving problems of visualization of complex data analytics is considered. The technological sequence of the visual solution of project management tasks for managers is given, and visual analytics methods are proposed to determine the necessary intervention in a problem situation that arises in the project management process. The importance of using modern analytical software is noted, which allows visualizing financial analytics using various models quickly or by combining them. Also, the article highlights the main advantages of using analytical software in financial project management and gives recommendations on using five strategies for effective visual analytics of enterprise financial project management. © 2022 AESSRA. All Rights Reserved. Enterprise Financial Project Management by Visual Analytics Tools Enterprise; financial project; management; visual analytics ",Strategic alignment
13,Corporate Renewable Procurement Analytics,"Corporate decarbonization goals have increased rapidly in the last few years. The procurement of renewable power is a core strategy used by companies to meet these goals, increasingly in a dynamic manner that addresses the risks associated with uncertain prices and supply intermittency, among others. This section discusses the interplay between data and decision analytics in this rapidly evolving area by considering the construction of a dynamic portfolio of power purchase agreements, which are popular long-term contracts signed by corporations, to meet a future renewable procurement target. It analyzes a stylized setting to provide insight into the effect on decisions of the joint evolution of uncertainties. It also discusses how forecasts, stochastic processes, and deterministic models can be used to obtain procurement policies in practical settings. These elements have a rich history in operations management but have received limited attention for renewable power procurement. Emphasis is placed on how a traditional rolling planning model based on forecasts can be adapted to this procurement setting, as well as where a recent rolling planning technique based on information relaxations can add value. © 2023 S. Nadarajah. Corporate Renewable Procurement Analytics climate targets; Markov decision processes; optimization; power purchase agreements; reinforcement learning; renewable procurement Data Analytics; Markov processes; Stochastic models; Stochastic systems; Climate targets; Core strategy; Corporates; Decarbonisation; Markov Decision Processes; Optimisations; Power purchase agreement; Reinforcement learnings; Renewable Power; Renewable procurement; Reinforcement learning",Capacity management
14,"Project Management Methodology in the Practice of Evidence-Based Development, Evidence-Based Policy","It is analyzed: how the principles of evidence-based management, evidence-based policy based on big data analysis are integrated with project management methodology (ISO 21500: 2021). The results of changes in the management practices of state projects and programs are presented. The tasks of the competence transition from big-data to smart-data for data-management and evidence-based policy are formulated. The practice of implementing the methodology of evidence-based management, evidence-based policy is associated with the implementation of the concept of “Open Data”, the National Data Management System in Russia, the development of the technological infrastructure of big data and digital data analytics services. Our research under the RFBR grant No. 19-29-14016 “Methodology for Big Data Analysis and Its Integration into Professional Development Programs for Executives” revealed how data analysis and evidence-based management principles are used in project management activities for state development programs. At the strategic management level: strategic performance indicators, infrastructure and data sources are defined. At the level of project management activities, sources of alternative data analysis are determined based on feedback from citizens (public policy). At the level of project life cycle management, it is determined which data sets will indicate the dynamics of value development and the effectiveness of products in the portfolio of projects of state development programs. At every level of evidence-based project management, this requires data competencies from leaders. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG. Project Management Methodology in the Practice of Evidence-Based Development, Evidence-Based Policy big data analysis methodology; evidence-based development; evidence-based policy; impact investing; Project management methodology Advanced Analytics; Data Analytics; Data handling; Information management; Life cycle; Open Data; Project management; Big data analyse methodology; Development programmes; Evidence-based; Evidence-based development; Evidence-based managements; Evidence-based policy; Impact investing; Management activities; Project management methodology; Big data",Strategic alignment
15,Fault analysis of truck drivelines using big data methods,"In times of steadily increasing product complexity, broadening technology portfolios and accelerated product development cycles, failure analysis is becoming more and more complex. To support business experts in analyzing defects and deriving corrective measures, mass data analyses can provide valuable input. Descriptive, statistical analysis and the use of machine learning often yield insights into defect relationships more quickly than an experience-based approach. The risk of a population in the field can be estimated via the detected fault correlations and, based on this, measures for predictive maintenance can be taken. In this article MAN Truck & Bus SE’s way to establish a data driven fault analysis process with focus on driveline topics is described. The fault elimination process can be triggered by ""Schichtlinien"" or Weibull estimations exceeding limits and in the following insight generation and data provisioning is build up in three layers. Starting with dashboards to enable data self-service and continuous KPI tracking, advance statistical analytics to drill down on root-because hypotheses and a machine learning approach to tackle multicausal problems by evaluation of feature importance, partial dependency plots and SHAP values. All of this will be shown with some examples of usage. Based on the experience gained with big data, it is possible to derive the prerequisites that should be in place in order to generate a high level of benefit. Comprehensive, welldocumented product lifecycle data should be made available via a powerful data infrastructure such as a cloud-based data lake. The basis for this is a cross-company data organization and management with centralized data engineering. The full benefit of Big Data unfolds through close networking of domain know-how and analysis methodology know-how. A cross-company data community has proven its worth in networking data experts. © 2023 The Authors. Fault analysis of truck drivelines using big data methods  Data Analytics; Defects; Life cycle; Machine learning; Predictive analytics; Risk assessment; Risk perception; Technology transfer; Trucks; Corrective measures; Drivelines; Fault analysis; Fault correlation; Machine-learning; Mass data; Predictive maintenance; Product's complexity; Product-development cycles; Technology portfolios; Big data",Strategic alignment
16,Data-driven approaches can harness crop diversity to address heterogeneous needs for breeding products,"This perspective describes the opportunities and challenges of data-driven approaches for crop diversity management (genebanks and breeding) in the context of agricultural research for sustainable development in the Global South. Data-driven approaches build on larger volumes of data and flexible analyses that link different datasets across domains and disciplines. This can lead to more information-rich management of crop diversity, which can address the complex interactions between crop diversity, production environments, and socioeconomic heterogeneity and help to deliver more suitable portfolios of crop diversity to users with highly diverse demands. We describe recent efforts that illustrate the potential of data-driven approaches for crop diversity management. A continued investment in this area should fill remaining gaps and seize opportunities, including i) supporting genebanks to play a more active role in linking with farmers using data-driven approaches; ii) designing low-cost, appropriate technologies for phenotyping; iii) generating more and better gender and socioeconomic data; iv) designing information products to facilitate decision-making; and v) building more capacity in data science. Broad, well-coordinated policies and investments are needed to avoid fragmentation of such capacities and achieve coherence between domains and disciplines so that crop diversity management systems can become more effective in delivering benefits to farmers, consumers, and other users of crop diversity. Copyright © 2023 the Author(s). Data-driven approaches can harness crop diversity to address heterogeneous needs for breeding products gender; genebanks; genotype by environment interactions; plant breeding; socioeconomic heterogeneity Agriculture; Crops, Agricultural; Plant Breeding; agricultural worker; article; consumer; crop; data science; decision making; female; gender; genotype; genotype environment interaction; human; investment; male; nonhuman; phenotype; plant breeding; agriculture; genetics",Value management
17,Data-Driven Approach to Risk Identification for Major Transportation Projects: A Common Risk Breakdown Structure,"Identifying and evaluating risks is one of the most essential steps in risk management in construction projects. When technical and managerial complexity increases in major transportation projects, this becomes even more important. Currently, project teams are assumed to identify risks mostly based on their experience and expertise. It is a major issue that some state departments of transportation (DOT) project teams lack the risk management experience. This study proposes using a data-driven approach to unify and summarize existing risk documents to create a comprehensive risk breakdown structure (RBS). As a preliminary risk identification framework, a consolidated RBS were developed, using content analysis of public risk reports by various DOTs. Then, comparison was made between the developed RBS with 70 US transportation projects' risk registers. Natural language processing techniques, bidirectional encoder representations from transformers, were employed to calculate semantic text similarity to determine what percentage of risks are covered by generic RBS. The results showed that 70 generic risk templates cover almost 81&#x0025; of the identified risks in the database of 70 major projects which is about 6000 individual risks. Project parties can use these results to discuss and identify context-specific risks as a starting point. The study also determined the interactions between risk items based on their co-occurrence using historical data. Research findings revealed the importance of considering interdependencies between risks in future studies. IEEE Data-Driven Approach to Risk Identification for Major Transportation Projects: A Common Risk Breakdown Structure Bit error rate; Data analytics; Natural language processing; natural language processing; Registers; risk breakdown structure; Risk management; risk management; Schedules; Semantics; Transportation; transportation projects Bit error rate; Data Analytics; Human resource management; Information management; Natural language processing systems; Project management; Risk analysis; Risk assessment; Risk management; Risk perception; Bit-error rate; Data analytics; Language processing; Natural language processing; Natural languages; Register; Risk breakdown structures; Risks management; Schedule; Transportation projects; Semantics",Risk management
18,Inductive Representation Learning on Dynamic Stock Co-Movement Graphs for Stock Predictions,"Co-movement among individual firms’ stock prices can reflect complex inter-firm relationships. This paper proposes a novel method to leverage such relationships for stock price predictions by adopting inductive graph representation learning on dynamic stock graphs constructed based on historical stock price co-movement. To learn node representations from such dynamic graphs for better stock predictions, we propose the hybrid-attention dynamic graph neural network, an inductive graph representation learning method. We also extended mini-batch gradient descent to inductive representation learning on dynamic stock graphs so that the model can update parameters over mini-batch stock graphs with higher training efficiency. Extensive experiments on stocks from different markets and trading simulations demonstrate that the proposed method significantly improves stock predictions. The proposed method can have important implications for the management of financial portfolios and investment risk. © 2022 INFORMS. Inductive Representation Learning on Dynamic Stock Co-Movement Graphs for Stock Predictions business intelligence; deep learning; graph representation learning; predictive models Commerce; Financial markets; Forecasting; Gradient methods; Graph neural networks; Graphic methods; Investments; Learning systems; Business-intelligence; Comovement; Deep learning; Dynamic graph; Graph representation; Graph representation learning; ON dynamics; Predictive models; Stock predictions; Stock price; Deep learning",Strategic alignment
19,"Successful innovation and the alignment of knowledge workers at the executive, management, and technical specialist levels","This paper focuses on the critical roles of knowledge workers when a firm pursues a major innovation project. In this context, we consider knowledge workers as those who contribute to a firm's performance at the executive, management, and technical specialist levels. Technical specialists include persons with advanced skills in engineering, analytics, statistics, science, and economics. By analyzing a series of case studies and personal interviews, we demonstrate that alignment (i.e., coordination, integration, and collaboration) among these knowledge workers is critical for the success of an innovation project. The paper concludes with a discussion of the responsibilities of knowledge workers at the executive, management, and technical specialist levels to ensure the necessary alignment occurs for successful innovation. © 2022 Production and Operations Management Society. Successful innovation and the alignment of knowledge workers at the executive, management, and technical specialist levels innovation; knowledge management; knowledge worker; organizational alignment; project management ",Strategic alignment
20,Exploring the Effects of Data-Driven Hospital Operations on Operational Performance from the Resource Orchestration Theory Perspective,"In the big data era, managing data-driven hospital operations have become one of the most important tasks for healthcare executives, increasing responsiveness to exceptional disruptions such as those caused by the COVID-19 pandemic. However, they are still facing the challenges of how best to orchestrate the digital medical resources for improving operational performance such as cost, delivery, and quality. Therefore, drawing upon resource orchestration theory, this article investigates how hospitals orchestrate data-driven culture (DDC) and digital technology orientation (DTO) to develop big data analytics capability (BDAC) for operational performance improvement. Survey data were collected from 105 hospitals in China and analyzed using structural equation modeling and ordinary least square regression. The results show that DDC has a significant positive impact on DTO. More interestingly, there is no significant interaction effect between DDC and DTO, indicating that DDC and DTO affect BDAC independently, and not synergistically. The results further reveal that BDAC fully mediates the DTO-operational performance relationship. The findings offer useful and timely guidance on how healthcare executives can manage data-driven hospital operations to improve operational performance during and post the COVID-19 pandemic. © 1988-2012 IEEE. Exploring the Effects of Data-Driven Hospital Operations on Operational Performance from the Resource Orchestration Theory Perspective Big data analytics capability (BDAC); COVID-19 pandemic; data-driven culture (DDC); digital technology orientation (DTO); hospital operations; resource orchestration theory (ROT) Big data; Data Analytics; Decision making; Decision theory; Health care; Hospitals; Big data analytic capability; COVID-19 pandemic; Data analytics; Data driven; Data-driven culture; Decisions makings; Digital technologies; Digital technology orientation; Hospital operations; Medical services; Pandemic; Portfolio; Resource orchestration theory; Technology orientation; COVID-19",Strategic alignment
21,Automated Fixing Cost Estimation of Photovoltaic System Failures for the Creation of a Decision Support System,"The increasing focus on sustainability connected to extended lifetimes with guaranteed high energy yields in the photovoltaic (PV) sector brings new challenges in operating and maintaining PV systems. Additionally, as the PV Terawatt Age has been reached in 2022 and a sustained exponential PV deployment growth is expected, operation and maintenance (O&M) companies have an ever-increasing portfolio of PV plants to operate while maintaining cost competitive. Therefore, a shift from manual fault detection and solution toward smart and automated alternatives is required to keep downtime due to faults at a minimum while ensuring that PV systems can operate safely and efficiently. This work extends past developments of an automated O&M decision support system (DSS) by focusing on the automation of selecting and prioritizing solutions in case of PV system event/failure appearance. Thereby, the TRUST-PV Solution Matrix is added to the published TRUST-PV Risk Matrix, providing a standardized list of solutions for all events and failures which can appear in a PV system. Furthermore, the Cost Priority Number, a prioritization methodology to assess technical failures and their economic impact in energy systems, is improved by integrating automated fixing cost calculations into the DSS. © 2023 The Authors. Solar RRL published by Wiley-VCH GmbH. Automated Fixing Cost Estimation of Photovoltaic System Failures for the Creation of a Decision Support System data analytics; fault detection; photovoltaic (PV) system maintenance; photovoltaics; solutions Artificial intelligence; Automation; Cost estimating; Data Analytics; Fault detection; Maintenance; Matrix algebra; Cost estimations; Data analytics; Extended lifetime; Faults detection; Operations and maintenance; Photovoltaic  system maintenance; Photovoltaic systems; Photovoltaics; System failures; System maintenance; Decision support systems",Strategic alignment
22,Digital Twins of Complex Projects,"The Digital Twin (DT) concept has rapidly gained acceptance. More recently it has become clear that just as a DTs support product, they can also represent the activities needed to design, execute, and manage projects. DTs bring a remarkable potential to bring complex projects to market successfully and to support after-market phases including training, maintenance, repair and retirement. Project Design is a method that brings DTs to project management based on realistic and reliable project models, forecasts, and ongoing instrumentation. In Project Design, the digital twin represents not only the products, services, and processes being created, but also the project teams and their activities. In other words, the project itself is recognized as a system. Digital models are extended to include people and organization in addition to product and process. Feedback and feedforward with automated flows are a critical characteristic of DTs leading to better attention, decisions, and actions by teams. Three cases are shown which demonstrate Project Design with digital models, digital projections, and digital shadows of complex projects. These cases show a collaborative environment in which teams build models which capture the project as a sociotechnical system. The models integrate three fundamental contributing architectures: products, processes, and organization (PPO). While building the model, a view emerges of the relationships amongst these three which, in turn, promotes shared awareness of the project across teams. The model-building likewise shapes mental models as teams explore the impact of changes, variation in assumptions, architectural options, and other real-world execution parameters. An analytics engine, in this case an agent-based simulator, generates project forecasts which act as digital projections. The forecasts are more realistic than classic methods, as the simulations include the project’s uncertain demands, behaviors, feasibility, and coordination in dynamic interaction. A wide range of feasible project variants with schedules, costs, quality, and utilization are generated by simulation and compared to targets. Teams rapidly assess trade-offs, risks, what-if scenarios and contingencies. The model is adjusted: teams expanded or reduced, dependencies changed, activities added or removed, roles and responsibilities tuned, concurrency increased, worksites changed, etc. The project teams learn quickly how changes in their own roles, commitments, and priorities systemically impact the project results. As the project proceeds, estimates to completion including alternate paths forward are rapidly and easily analyzed. A long-lasting benefit of Project Design is that the DT is used over the project lifetime. A digital shadow evolves with the actual project as refinements and contingencies arise, immediately yielding new forecasts of quality, schedule and cost. Instrumentation of scope, interfaces, and teamwork brings significant new feedback to maintain alignment of the project model with the actual project. The model also acts as digital thread across the model’s connected PPO and across changes in the project over time, promoting persistence for practical leverage of information in future projects. Recent research advances in instrumentation and analytics, including placement of non-intrusive sensors across project elements and teamwork, rapidly reveal the health of the project and chances for success. Taking advantage of these techniques, new insights are yielded on teamwork as innovation and complex problem-solving across various industrial and government project domains. © The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG 2023. Digital Twins of Complex Projects Agent-based simulation; Digital Twin; Instrumented engineering teamwork; Model-based project management; Project design ",Strategic alignment
23,IEEE Technology and Engineering Management Society Body of Knowledge (TEMSBOK),"IEEE Technology and Engineering Management Society Body of Knowledge (TEMSBOK) IEEE TEMS Board of Directors-approved body of knowledge dedicated to technology and engineering management The IEEE Technology and Engineering Management Society Body of Knowledge (TEMSBOK) establishes a set of common practices for technology and engineering management, acts as a reference for entrepreneurs, establishes a basis for future official certifications, and summarizes the literature on the management field in order to publish reference documentation for new initiatives. The editors have used a template approach with authors that instructed them on how to introduce their manuscript, how to organize the technology and area fundamentals, the managing approach, techniques and benefits, realistic examples that show the application of concepts, recommended best use (focusing on how to identify the most adequate approach to typical cases), with a summary and conclusion of each section, plus a list of references for further study. The book is structured according to the following area knowledge chapters: business analysis, technology adoption, innovation, entrepreneurship, project management, digital disruption, digital transformation of industry, data science and management, and ethics and legal issues. Specific topics covered include: Market requirement analysis, business analysis for governance planning, financial analysis, evaluation and control, and risk analysis of market opportunities Leading and managing working groups, optimizing group creation and evolution, enterprise agile governance, and leading agile organizations and working groups Marketing plans for new products and services, risk analysis and challenges for entrepreneurs, and procurement and collaboration Projects, portfolios and programs, economic constraints and roles, integration management and control of change, and project plan structure The IEEE Technology and Engineering Management Society Body of Knowledge (TEMSBOK) will appeal to engineers, graduates, and professionals who wish to prepare for challenges in initiatives using new technologies, as well as managers who are responsible for conducting business involving technology and engineering. © 2024 by The Institute of Electrical and Electronics Engineers, Inc. All rights reserved. IEEE Technology and Engineering Management Society Body of Knowledge (TEMSBOK)  ",Risk management
24,The development of a competence framework for artificial intelligence professionals using probabilistic topic modelling,"Purpose: The current gap between the required and available artificial intelligence (AI) professionals poses significant challenges for organisations and academia. Organisations are challenged to identify and secure the appropriate AI competencies. Simultaneously, academia is challenged to design, offer and quickly scale academic programmes in line with industry needs and train new generations of AI professionals. Therefore, identifying and structuring AI competencies is necessary to effectively overcome the AI competence shortage. Design/methodology/approach: A probabilistic topic model was applied to explore the AI competence categories empirically. The authors analysed 1159 AI-related online job ads published on LinkedIn. Findings: The authors identified five predominant competence categories: (1) Data Science, (2) AI Software Development, (3) AI Product Development and Management, (4) AI Client Servicing, and (5) AI Research. These five competence categories were summarised under the developed AI competence framework. Originality/value: The AI competence framework contributes to clarifying and structuring the diverse AI landscape. These findings have the potential to aid various stakeholders involved in the process of training, recruiting and selecting AI professionals. They may guide organisations in constructing a complementary portfolio of AI competencies by helping users match the right competence requirements with an organisation's needs and business objectives. Similarly, they can support academia in designing academic programmes aligned with industry needs. Furthermore, while focusing on AI, this study contributes to the research stream of information technology (IT) competencies. © 2023, Emerald Publishing Limited. The development of a competence framework for artificial intelligence professionals using probabilistic topic modelling Artificial intelligence; Competence framework; Online job ads; Probabilistic topic model ",Value management
25,Modern Metrics (MM): Software size estimation using function points for artificial intelligence and data analytics applications and finding the effort modifiers of the functional units using indian software industry,"SPM, Software Project Management is an engineering process for scheduling, developing, testing, and maintaining a software system. The size of a software is one of the important attributes in determining all the activities of SPM. Price, time, and effort are the basic factors for developing software in a successful and profitable manner. The fast explosions in technological development of software systems attracted all the fields that existed in the universe. The modern software system uses many programming languages, domains, operating systems, technologies, development cycles, topologies, etc. So, the industry is expecting a new, versatile, and highly updated sizing technique for modern software. Modern Metrics (MM) is a new anticipated size estimation technique for present software systems with updated functional values and metrics using Function Point (FP). This article analyzed the functional unit metric values (effort modifiers) of the MM with real world applications. MM is a novel method for determining the solution to the SPM challenges. © 2023, Taru Publications. All rights reserved. Modern Metrics (MM): Software size estimation using function points for artificial intelligence and data analytics applications and finding the effort modifiers of the functional units using indian software industry Modern metrics; Modern metrics size; Software complexity factors; Software effort; Software project management; Software schedule; Software size estimation ",Strategic alignment
26,A Comparison of ResidentCompleted and Preceptor-Completed Formative Workplace-Based Assessments in a CompetencyBased Medical Education Program,"BACKGROUND AND OBJECTIVES: In competency-based medical education (CBME), should resident self-assessments be included in the array of evidence upon which summative progress decisions are made? We examined the congruence between self-assessments and preceptor assessments of residents using assessment data collected in a 2-year Canadian family medicine residency program that uses programmatic assessment as part of their approach to CBME. METHODS: This was a retrospective observational cohort study using a learning analytics approach. The data source was archived formative workplace-based assessment forms (fieldnotes) stored in an online portfolio by family medicine residents and preceptors. Data came from three academic teaching sites over 3 academic years (2015-2016, 2016-2017, 2017-2018), and were analyzed in aggregate using nonparametric tests to evaluate differences in progress levels selected both within and between groups. RESULTS: In aggregate, first-year residents’ self-reported progress was consistent with that indicated by preceptors. Progress level rating on fieldnotes improved over training in both groups. Second-year residents tended to assign themselves higher ratings on self-entered assessments compared with those assigned by preceptors; however, the effect sizes associated with these findings were small. CONCLUSIONS: Although we found differences in the progress level selected between preceptor-entered and resident-entered fieldnotes, small effect sizes suggest these differences may have little practical significance. Reasonable consistency between resident self-assessments and preceptor assessments suggests that benefits of guided self-assessment (eg, support of self-regulated learning, program efficacy monitoring) remain appealing despite potential risks. © 2022, Society of Teachers of Family Medicine. All rights reserved. A Comparison of ResidentCompleted and Preceptor-Completed Formative Workplace-Based Assessments in a CompetencyBased Medical Education Program  Canada; Clinical Competence; Cohort Studies; Humans; Internship and Residency; Workplace; adult; article; Canadian; cohort analysis; drug efficacy; effect size; family medicine; female; human; human experiment; learning; male; medical education; nonparametric test; program efficacy; resident; retrospective study; risk assessment; self evaluation; workplace; Canada; clinical competence",Risk management
27,Effect of Big Data and Analytics on Managing Projects,"The rapid pace of technological advancement necessitates constant adaptation. As a rapidly evolving field, project management has little choice but to take use of technological breakthroughs to stay relevant and fresh. Using big data analytics, businesses and project managers alike can reap the benefits of this technology. Big data analytics is definitely useful for influencing the future of project management, as outlined in this article’s preliminary comments. A new era of 21st-century living appears to be upon us, and project management as a profession appears to be ready to embrace it. Big data collects and stores enormous amounts of data that are becoming increasingly difficult to manage and analyze. The potential benefits and competitive edge of this new technology are motivating the majority of businesses to invest in big data analytics. Structured or unstructured, large amounts of heterogeneous data are processed and managed as ”Big Data” in the enterprise. This includes both structured and unstructured data. Analytic methods and technologies are heavily employed in the management and analysis of large and complex data sets for use in a wide range of applications that enhance the performance of a business. This paper analyzes the impact of big data and business analysis on project management. A literature review is followed by primary and secondary data analysis, which includes interviews and surveys for architectural analysis, in an exploratory study. Qualitative and quantitative data are the norm in all studies in this study. The present study is descriptive in nature, as the goal is to examine the impact of big data and business analysis on project management. Below mentioned impacts have been analyzed and concluded that big data analytics helps to reduce the project complexities, reduces the project cost and enhances the project risk management. © 2023, ISCSA. All rights reserved. Effect of Big Data and Analytics on Managing Projects analytics, project management; and disruptive technologies; Big data; digital disruption; digital transformation ",Monitoring and control
28,Hot Off the Press: Predicting Intraday Risk and Liquidity with News Analytics,"We examine the relation between news arrival intensity, volatility and volume at an intraday frequency using a global dataset. The analysis is based on news analytics platforms that use natural language processing to perform entity recognition, classification by topic and sentiment analysis. We introduce our own news arrival intensity metric, which is simple and intuitive, and present compelling evidence that intraday volume and volatility forecasts can be improved using these metrics. For stocks traded in the USA and Europe, we use Refinitiv's news analytics dataset based on the news written in English. We present strong and robust out-of-sample performance of our model in these markets. The results for the USA and Europe suggest that it is possible to extract stronger signals from news articles written in languages that are native to each market. For this reason, we extend our model to stocks traded in Japan using a news analytics dataset provided by FTRI/Alexandria. This dataset is based on articles in Japanese published by Nikkei. Our model successfully harnesses markedly strong predictive power of news in this application. In particular, our out-of-sample analysis for Japanese stocks shows that about 80% ∼ 90% of the stocks would have benefitted from the use of our news arrival intensity metrics. Our results also suggest a spillover effect within sectors: Volatility in stock i tends to increase if other companies in the same sector experience an increase in news arrival intensity. We demonstrate that the output of the model is economically and statistically significant and remains robust over time even in the presence of outlying data points. Our model can be applied to optimal trade execution both at the stock and at the portfolio level. © 2023 selection and editorial matter OptiRisk Systems; individual chapters, the contributors. Hot Off the Press: Predicting Intraday Risk and Liquidity with News Analytics  ",Risk management
29,Practicable optimization for portfolios that contain nonfungible tokens,"Non-fungible tokens (NFT) constitute a novel asset class that has the potential to diversify portfolios. Scant research supports that hypothesis at a collection level, yet it remains an open question how to leverage the potential in practice. Owing to their non-fungible nature, liquidity of the asset that leads to a mathematically optimal portfolio does not always exist. This letter introduces a practicable portfolio optimization strategy for NFTs based on machine learning, more specifically robust hierarchical risk parity. When applied to portfolios that contain high valued NFT collections, the latter's inclusion into the portfolio is shown to improve overall portfolio return. © 2023 Elsevier Inc. Practicable optimization for portfolios that contain nonfungible tokens Analytics; Hierarchical risk parity; Non-fungible tokens; Portfolio optimization ",Strategic alignment
30,Advancement in business analytics tools for higher financial performance,"The relentless growth of data in financial markets has boosted the demand for more advanced analytical tools to facilitate and improve financial planning. The ability to constructively use this data is limited for managers and investors without the proper theoretical support. Within this context, there is an unmet demand for combining analytical finance methods with business analytics topics to inform better investment decisions. Advancement in Business Analytics Tools for Higher Financial Performance explores the financial applications of business analytics tools that can help financial managers and investors to better understand financial theory and improve institutional investment practices. This book explores the value extraction process using more accurate financial data via business analytical tools to help investors and portfolio managers develop more modern financial planning processes. Covering topics such as financial markets, investment analysis, and statistical tools, this book is ideal for accountants, data analysts, researchers, students, business professionals, academicians, and more. © 2023 by IGI Global. All rights reserved. Advancement in business analytics tools for higher financial performance  ",Strategic alignment
31,Insight-Driven Digital Engineering - A Key Enabler Driving Operational Intelligence in the Energy Industry,"The energy industry is undergoing fundamental change with industry 4.0 enabling intelligent operations, production optimization, prescriptive maintenance and efficiency boosting, embedding insights, hyper-automation and enhance agility across production assets with the objectives of increasing productivity, delivering cost savings, and improving superior customer experience. Leveraging digital solution accelerators, APIs and network gateways connectivity for event data analytics, these digital native products can expedite development on energy assets with infrastructure as code - the single source of truth. Adopting insight-driven digital engineering is crucial for the energy industry to remain resilient, profitable, and sustainable in meeting net-zero carbon emission in the energy transition. There is urgent need for the energy industry to partner with the digital native companies and integrators to co-create solutions in driving digital enablement across oil and gas, offshore developments, LNGs, renewables, hydrogen, and utilities. The energy industry does not need to reinvent the will, as the technology exist. What is needed is collaborations of ideas and application of disruptive use cases across the digital and energy industry to pivot sustainable novel solutions for the ecosystem. Deployments of autonomous systems with AI for remote operations optimization, IoT connectivity for pervasive platform across assets, digital twin for visibility of entire assets, edge computing for resilience and low cost of operations, data analytic platform for prescriptive maintenance, and the protection of critical infrastructural assets in an ecosystem of vulnerabilities, are key processes for net-zero carbon emission and sustainability in the energy industry. The energy and digital tech leaders must collaborate to revolutionize the energy industry with insight-driven digital engineering in this age of digital disruptions. Designing for speed in an EPC project without compromising quality relies on digital information from concept and FEED, to inform a digital engineering solution platform, which have resulted in enabling Energy Operators get to a final investment decision up to 45% faster. A typical use case is where an EPC company begun the process of reducing their engineering software and technology portfolio from over 175 distinct products down to less than 32. The benefits include expedited evaluation of concepts through process simulation software, automated plant layout and piping designs through solution accelerators enablement, and coordinated estimates tied to the engineering information. With over 200 digital accelerator models deployed in the first year of an energy operating asset, the solution correctly identified 49 major early warnings, a value of $16.8 million saving in ROI was achieved. Thus, with support for the entire concept to FEED process now enabled with integrated digital design, layout and estimating software, EPCs can explore and optimize options for both offshore greenfield and brownfield projects quickly and efficiently with solution accelerators, providing Energy Operators the highest value from their investments. Copyright © 2023, Society of Petroleum Engineers. Insight-Driven Digital Engineering - A Key Enabler Driving Operational Intelligence in the Energy Industry  Carbon; Computer software; Costs; Ecosystems; Gasoline; Investments; Offshore oil well production; Profitability; Carbon emissions; Data analytics; Digital engineering; Digital natives; Energy; Energy industry; Energy operators; Fundamental changes; Intelligent operations; Zero carbons; Data Analytics",Financial management
32,"Little rewards, big changes: Using exercise analytics to motivate sustainable changes in physical activity","Even using simple techniques like taking the stairs, many individuals struggle to maintain the motivation to be physically active. Health gamification systems can aid this goal by providing points earned through exercise that are redeemable for tangible extrinsic rewards. Using self-determination theory, we conduct research on one such system and investigate rewards’ effectiveness to promote exercise considering reward value, redemption frequency patterns, and fitness levels. We find that rewards do significantly increase activity levels, and this effect is larger for advanced users who redeem multiple times for higher value rewards. We close by offering future research avenues and advice to optimize reward portfolios. © 2019 Elsevier B.V. Little rewards, big changes: Using exercise analytics to motivate sustainable changes in physical activity Fitness stage; Health gamification system; Optimize reward effectiveness; Redemption patterns; Reward portfolio design; Tangible extrinsic rewards Big changes; Extrinsic rewards; Fitness stage; Gamification; Health gamification system; Optimize reward effectiveness; Physical activity; Redemption pattern; Reward portfolio design; Tangible extrinsic reward; Health",Risk management
33,The role of big data analytics and organizational agility in improving organizational performance of business processing organizations,"Purpose: Business processing organizations are continuously focusing on customer knowledge management (CKM) due to the competitive business environment. CKM is being recognized as an essential source for improving organizational performance (OP). This study focuses on understanding CKM and its impact on OP. It also explores the moderating role of big data analytics capability (BDAC) on OP. Moreover, the mediating role of operational and strategic agility on OP was empirically tested. Design/methodology/approach: Positivist research doctrine has been deployed and data was collected using structured survey using cross-sectional approach. The data were collected from 392 employees working in business processing software houses in the emerging market of Pakistan. Structural equation modeling (SEM) was deployed for the estimation of theoretical model. Findings: The study's findings indicate that CKM has no significant impact on OP; although the presence of BDAC moderates the relationship significantly. Moreover, the study recommends that CKM and BDAC to be tested in the project environment, considering organization's operational and technical capabilities. Research limitations/implications: The study proclaims that BDAC can be helpful for organizations to improve their capabilities and output. Likewise, enhancing BDAC reduces failure rates of the projects. Originality/value: This study provides a critical theoretical and practical contribution to project management in business processing organizations. Big data analytics can be of value for diagnostic, predictive and prescriptive analysis in the project management context. © 2023, Emerald Publishing Limited. The role of big data analytics and organizational agility in improving organizational performance of business processing organizations BPOs; Customer knowledge management; Operational agility; Project performance; Strategic agility ",Strategic alignment
34,Heterogeneities among credit risk parameter distributions: the modality defines the best estimation method,"Comparative studies investigating the estimation accuracy of statistical methods often arrive at different conclusions. Therefore, it remains unclear which method is best suited for a particular estimation task. While this problem exists in many areas of predictive analytics, it has particular relevance in the banking sector owing to regulatory requirements regarding transparency and quality of estimation methods. For the estimation of the relevant credit risk parameter loss given default (LGD), we find that the different results can be attributed to the modality type of the respective LGD distribution. Specifically, we use cluster analysis to identify heterogeneities among the LGD distributions of loan portfolios of 16 European countries with 32,851 defaulted loans. The analysis leads to three clusters, whose distributions essentially differ in their modality type. For each modality type, we empirically determine the accuracy of 20 estimation methods, including traditional regression and advanced machine learning. We show that the specific modality type is crucial for the best method. The results are not limited to the banking sector, because the present distribution type-dependent recommendation for method selection, which is based on cluster analysis, can also be applied to parameter estimation problems in all areas of predictive analytics. © 2022, The Author(s). Heterogeneities among credit risk parameter distributions: the modality defines the best estimation method Global Credit Data; LGD Distributions; Machine Learning; Parameter Estimation; Risk Management ",Strategic alignment
35,Neural networks meet least squares Monte Carlo at internal model data,"In August 2020 we published “Comprehensive Internal Model Data for Three Portfolios” as an outcome of our work for the committee “Actuarial Data Science” of the German Actuarial Association. The data sets include realistic cash-flow models outputs used for proxy modelling of life and health insurers. Using these data, we implement the hitherto most promising model in proxy modeling consisting of ensembles of feed-forward neural networks and compare the results with the least squares Monte Carlo (LSMC) polynomial regression. To date, the latter represents—to our best knowledge—the most accurate proxy function productively in use by insurance companies. An additional goal of this publication is a more precise description of “Comprehensive Internal Model Data for Three Portfolios” for other researchers, practitioners and regulators interested in developing solvency capital requirement (SCR) proxy models. © 2022, The Author(s). Neural networks meet least squares Monte Carlo at internal model data Cash flow projection models; Insurance risk management; Least squares Monte Carlo; Machine learning; Neural networks; Proxy modeling; Risk-neutral valuation; Solvency II ",Risk management
36,Ex Post Project Risk Assessment: Method and Empirical Study,"Project risk is an important part of managing large projects of any sort. This study contributes to the state of knowledge in project risk management by introducing a data-driven approach to measure risk identification performance using historical data. In the early phases of a project, the identification and assessment of risk is based largely on experience and expert judgment. As a project moves through its life cycle, these identified risks and the assessment of them evolve. Some risks become issues, some are mitigated, and some are retired as no longer important. This study investigated the quality of early risk registers and risk assessments on large transportation projects and compared them to how the identified risks evolved on historical projects. The investigation involved the use of textual analysis of archival risk register documents. Finite-state automation methods akin to Markov chain models were used to track the changes in risk attributes on large infrastructure projects as the projects matured. The objective was to be better able to anticipate how project risks will change as projects move forward and to be better able to forecast changes to the risk register from ex ante to ex post conditions. Results from 11 major US transportation projects suggested that, on average, fewer than 65% of ex ante identified risks ultimately occurred in projects and were mitigated, while more than 35% did not occur and were retired. In addition, more than half of the risks emerged during project execution when new information became available. Based on the categorization of risk management styles, we find that identifying risks early in the project life cycle is necessary, but not sufficient to ensure successful project delivery. A project team with positive doer behavior (i.e., actively monitoring and identifying risks during project execution) performed better in delivering projects on time and within budget. © 2022 American Society of Civil Engineers. Ex Post Project Risk Assessment: Method and Empirical Study Data analytics; Ex post risk assessment; Risk register; Transportation projects Budget control; Data Analytics; Life cycle; Markov processes; Project management; Risk management; Data analytics; Ex antes; Ex post risk assessment; Project execution; Project risk; Project risk assessment; Risk assessment methods; Risk registers; Risks assessments; Transportation projects; Risk assessment",Risk management
37,Deterministic and Probabilistic Risk Management Approaches in Construction Projects: A Systematic Literature Review and Comparative Analysis,"Risks and uncertainties are inevitable in construction projects and can drastically change the expected outcome, negatively impacting the project’s success. However, risk management (RM) is still conducted in a manual, largely ineffective, and experience-based fashion, hindering automation and knowledge transfer in projects. The construction industry is benefitting from the recent Industry 4.0 revolution and the advancements in data science branches, such as artificial intelligence (AI), for the digitalization and optimization of processes. Data-driven methods, e.g., AI and machine learning algorithms, Bayesian inference, and fuzzy logic, are being widely explored as possible solutions to RM domain shortcomings. These methods use deterministic or probabilistic risk reasoning approaches, the first of which proposes a fixed predicted value, and the latter embraces the notion of uncertainty, causal dependencies, and inferences between variables affecting projects’ risk in the predicted value. This research used a systematic literature review method with the objective of investigating and comparatively analyzing the main deterministic and probabilistic methods applied to construction RM in respect of scope, primary applications, advantages, disadvantages, limitations, and proven accuracy. The findings established recommendations for optimum AI-based frameworks for different management levels—enterprise, project, and operational—for large or small data sets. © 2023 by the authors. Deterministic and Probabilistic Risk Management Approaches in Construction Projects: A Systematic Literature Review and Comparative Analysis artificial intelligence; construction industry; machine learning algorithms; project management; risk management ",Strategic alignment
38,Investment decisions and passive portfolio construction utilizing patent analytics: A multi-case study on COVID-19 treatment technologies,"The question of which stocks to buy remains at the heart of every investment decision. New, non-priced information sources and rationales linking such information to financial theory are constantly investigated. Patent metrics are often relied upon to capture technological change. Prior research detected positive corporate performance implications when relating corporate patent metrics to corporate financial performance indicators like stock prices. However, largely anecdotal evidence from the corporate sector on building investment strategies based on such patent metrics exists. In this multi-case study comprising COVID-19 treatment technologies, we select investment targets from a pool of companies previously identified based on patent analytics via patent portfolio sizes, average patent qualities, and patent portfolio strengths. Casting our thematic investment choice into passively constructed portfolios and benchmarking the portfolios' performance against the sector and three established global benchmarks, we find that relying on patent information in the selection process results in substantial outperformance compared to these global benchmarks. This finding holds true in the short- and mid-term. These findings provide ample opportunities for future research and may guide investment, managerial, and policy decision-makers in crafting promising investment strategies. © 2023 Investment decisions and passive portfolio construction utilizing patent analytics: A multi-case study on COVID-19 treatment technologies Alternative beta/data; COVID-19; Investment decisions; Patent analytics; Portfolio construction ",Strategic alignment
39,A resource orchestration perspective of organizational big data analytics adoption: evidence from supply chain planning,"Purpose: This paper investigated the organizational adoption of big data analytics (BDA) in the context of supply chain planning (SCP) to conceptualize how resources are orchestrated for organizational BDA adoption and to elucidate how resources and capabilities intervene with the resource management process during BDA adoption. Design/methodology/approach: This research elaborated on the resource orchestration theory and technology innovation adoption literature to she would light on BDA adoption with multiple case studies. Findings: A framework for the resource orchestration process in BDA adoption is presented. The authors associated the development and deployment of relevant individual, technological and organizational resources and capabilities with the phases of organizational BDA adoption and implementation. The authors highlighted that organizational BDA adoption can be initiated before consolidating the full resource portfolio. Resource acquisition, capability development and internalization of competences can take place alongside BDA adoption through structured processes and governance mechanisms. Practical implications: A relevant discussion identifying the capability gap and provides insight into potential paths of organizational BDA adoption is presented. Social implications: The authors call for attention from policymakers and academics to reflect on the changes in the expected capabilities of supply chain planners to facilitate industry-wide BDA transition. Originality/value: This study opens the black box of organizational BDA adoption by emphasizing and scrutinizing the role of resource management actions. © 2023, Jinou Xu and Margherita Emma Paola Pero. A resource orchestration perspective of organizational big data analytics adoption: evidence from supply chain planning Big data analytics; Case study; Resource orchestration theory; Supply chain planning; Technology adoption; Theory testing ",Strategic alignment
40,Upper funnel ad effectiveness and seasonality in consumer durable goods,"Brands usually invest in a portfolio of ad products for brand consideration and conversion. To gauge performance, brands often use ad-attributed metrics to compare return on advertising spend (ROAS) across different ad channels. There are two shortcomings with this approach. First, it relies on a predetermined attribution window (eg 1 day, 14 days, 30 days).1 The resulting ROAS does not only favour lower funnel ads, such as paid search, but could change with different attribution models. Secondly, attributed performance metrics usually do not consider seasonality, and organic consumer demand changes over the course of the year, which could bias the estimates of the effectiveness of ads. This could result in mistakenly attributing high organic demand to campaign performance. These issues are addressed with a seasonal autoregressive integrated moving average with x/exogenous variables (SARIMAX) model, accounting for seasonality and brands’ past performance. This analysis compares ad efficacy on total retail metrics, regardless of attribution methods. This method is applied to Amazon Ads for 15 brands in US consumer durables. While the lower-funnel ad product (Sponsored Products) sees a lot more current usage, higher-funnel ad products such as Sponsored Brands, demand side platform (DSP) display, and Streaming TV Video are all found to have higher efficacy. Brands should evaluate their ad-product performance at regular intervals to avoid under-utilising high-performing ad products. Future research is also encouraged to replicate this methodology in different verticals and locales for generalisability. © 2023, Henry Stewart Publications. All rights reserved. Upper funnel ad effectiveness and seasonality in consumer durable goods Amazon Ads; brands; consumer durables; digital advertising; marketing analytics; ROAS; time series model ",Strategic alignment
41,A Stacked Model for Approving Bank Loans,"Financial institutions in the United States provide a wide range of products and services but interest earned on customers' credit card balances is their primary source of revenue. That way, they may profit from the income collected on the money they lend out. The profitability of a bank is very sensitive to the performance of its loan portfolio, namely to the amount to which borrowers are meeting their repayment obligations. The bank's NPAs may be lowered if it can identify potential loan defaulters in advance. Therefore, it is crucial to investigate this occurrence. Modern studies have demonstrated that there are a plethora of approaches to the question of preventing loan default. However, it is crucial to investigate the characteristics of the various approaches and compare them in order to make the most accurate forecasts possible and so maximize earnings. The topic of forecasting loan defaulters is studied using the Logistic regression model, a key method in predictive analytics. The Kaggle data is used for analytics and modelling. Several models of hr have been run, and various performance metrics have been calculated. Sensitivity and specificity are two examples of performance metrics used to evaluate the models. It has been proved conclusively that the model yields varying outcomes. The model is slightly better because it takes into account factors (client observable traits like age, intent, good credit, sum, timeframe, etc.) that should be considered to accurately determine the likelihood of insolvency on loan in addition to information about the customer's checking account (which shows the customer's wealth). Thus, the proper clients to be addressed for loan giving may be simply recognised using a lr technique, which evaluates the chance of default on credit. A bank should not only look for wealthy borrowers, the model suggests; there are plenty of other factors to consider when making credit decisions. To put our plan into action, we have chosen to work with the Flask framework. Categorizes the desired value in-page using HTML and CSS to connect our models to variables. Given that it is a user-friendly website. An advantage of this framework is that it makes it simple for users to make sense of the values that have been categorised. Users may learn the status of an illness at a preliminary phase by using our website.  © 2023 IEEE. A Stacked Model for Approving Bank Loans Component; Loan; Outlier; Overfitting; Prediction; Transform Logistic regression; Predictive analytics; Sales; Websites; Bank loans; Component; Credit cards; Customer credits; Financial institution; Loan; Outlier; Overfitting; Performance metrices; Product and services; Profitability",Strategic alignment
42,Pricing and CEOs: why top executives need to get involved,"Purpose: Despite its increased adoption by small, medium and large firms, pricing continues to be ignored in the C-suite. C-suite executives have minimal understanding of what pricing can do and how it impacts a firm’s performance. After two years in the COVID-19 pandemic and the resulting economic crisis, consultants agree that the next wave of strategies and business models will require the development of strategic pricing capabilities, including analytics and software. Design/methodology/approach: The authors conducted 49 interviews with CXOs, VPs of pricing and CEOs of pricing software vendors to understand how the best-performing companies use pricing to drive profits and select pricing technologies. Then, supported by the Professional Pricing Society, the world’s largest organization dedicated to pricing, the authors conducted a 2020 survey of 540 pricing professionals to understand the perceptions of pricing in the C-suite and how top executives prioritize pricing investments. The authors complemented their own research with analysis of publicly available data, analyst presentations and public comments by CEOs on pricing. Findings: The authors propose a portfolio of 15 activities to include in the CEO’s strategic agenda and 10 actions to get started with in the short term. The next normal will not be based on business-as-usual. For the next three to five years, developing strategic pricing capabilities will give firms a competitive advantage over those who continue to neglect this hidden gem. Originality/value: In the context of the accelerating economic recovery, the authors address one of the most pressing priority for the C-suite. The authors focus on a series of actions and activities that the C-suite can take to accelerate recovery and focus on profitable growth. © 2021, Emerald Publishing Limited. Pricing and CEOs: why top executives need to get involved C-suite; CEO; COVID-19; Pricing; Pricing strategy ",Strategic alignment
43,How greater data access will make civil engineering and construction more productive,"Remote-working and other innovative technology developed during the Covid-19 pandemic is now helping the civil engineering and wider construction industry to save money and create new high-value, data-driven jobs. In the UK this is being driven forward by bodies such as the Construction Data Trust and Project Data Analytics Task Force. This paper explores how data has empowered the construction sector during Covid-19 and the role of the industry in harnessing this work for future collective benefit in a post-pandemic recovery. © 2022 ICE Publishing: All rights reserved. How greater data access will make civil engineering and construction more productive construction; data; project management; UN SDG 13: Climate action; UN SDG 8: Decent work and economic growth United Kingdom; Data Analytics; Construction data; Data access; Data analytics; Data driven; Innovative technology; New high; Project data; Remote working; Task force; Value data; civil engineering; construction industry; COVID-19; economic regeneration; project management; recovery plan; telecommuting; Construction industry",Capacity management
44,Management Model and Capture of Benefits Integrated into the Practice of Project Management,"An attempt has been made to address the difficulty of identifying and measuring the benefits derived from investment projects and capturing capital gains for an organization, focusing on developing and implementing a management model and realizing benefits for a leading company in its activity sector. Thus, the objective is to understand how it is possible to achieve the expected benefits of an investment project: A model characterized as generalist was developed (applied to all areas of the company), with the objective of optimizing the realization of benefits, measuring them and thus create value for the organization. Among the methods used, we highlight, in a first phase, the research of some existing Frameworks, which later enabled the development of a proposed framework, validated internally using the existing Business Intelligence platform. Subsequently, based on a satisfaction questionnaire about the framework proposed to users, data related to its development and implementation were collected, with the aim of understanding its acceptance among the users and employees of the company. With the data from this questionnaire, an artifact was developed: a PowerBI dashboard that reflects the benefits identified and captured. In summary, the artifact made it possible to identify, measure, and achieve the benefits generated by the project in question, but also to motivate its use in other existing investment projects, by adapting it to each of the other ones. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. Management Model and Capture of Benefits Integrated into the Practice of Project Management Benefits management; Dashboard; Framework; Investment project management; Realization of benefits Investments; Benefits management; Business Intelligence platform; Capital gains; Dashboard; Framework; Investment programmes; Investment project management; Management Model; Realization of benefit; User data; Project management",Financial management
45,A NEW REALITY: The Close Interaction Between Economic Theory and Practice,"After the global recession of 2008–2009, the signs of a new reality are becoming more apparent in public practice: stagnation in the economy, inhibition of integration interactions, increased nationalism, environmental crisis, etc. The mass of accumulated problems from previous changes has become redundant for the modern economy, which means that it is necessary to focus the opportunities, that economic theory possesses, on the consistent resolution of such problems and the creation of conditions for further development. However, the new reality highlights the dangerous gap between economic theory and practice, which alienates it from participating effectively in that process. The authors explore many specific manifestations of the gap between economic theory and practice, drawing on the principles of a systemic approach, revealing the contradictions that arise here, and attracting interdisciplinary opportunities. At the same time, these manifestations are considered in the space-time continuum of interaction between economic theory and practice in the conditions of the new reality. The authors are focused on the issue concerning the degree of conformity of fundamental studies in economic theory to the needs in the development of state economic policy basis, in the field of investment analytics, risk assessment, and management, corporate governance, project management, project financing, human capital, spatial economy. The results obtained by the authors represent one of the advanced research directions for social and economic studies resulting in the novelty and polemical nature of theses, conclusions, and recommendations of this chapter. © 2023 by Information Age Publishing. A NEW REALITY: The Close Interaction Between Economic Theory and Practice  ",Strategic alignment
46,Seismic risk prioritisation schemes for reinforced concrete bridge portfolios,"A significant portion of the existing bridge inventory in Italy is decades old, requiring continuous maintenance and safety assessment approaches. Recent collapses of existing reinforced concrete bridges have piqued public interest, placing pressure on management agencies to define methodologies with which to prioritise asset maintenance and to effectively utilise their limited resources. When looking for decision variables to perform this prioritisation, seismic risk assessment metrics, such as average annual losses (AAL), are an appealing choice. However, obtaining this metric for a large bridge inventory is technically challenging and requires large amounts of information that are seldom available, promoting the development of practical approaches that can predict the relative priority of assets within a portfolio, based on processing simple indicators with acceptable accuracy. In this research, a case study of 617 bridges from the Italian road network was assessed considering state-of-the-art approaches to calculate total losses. The results were explored with data science techniques, identifying the main features that drive the relative importance of bridges in terms of AAL and using them as guidance to calibrate a simplified methodology, based on the recent Italian Guidelines for Bridge Safety Assessment. The proposed AAL-based modifications demonstrate a notable improvement in the definition of bridge assessment priorities, as well as providing further resolution in the classification for more efficient decision making. © 2023 Informa UK Limited, trading as Taylor & Francis Group. Seismic risk prioritisation schemes for reinforced concrete bridge portfolios Bridges; machine learning; prioritization; regional seismic risk; road networks; transportation infrastructure ",Risk management
47,Digital transformation from data science: A source of organizational agility,"In this chapter, concepts of data science definition and market business agility with strong support of digital transformation paradigm are discussed. The intention is to provide the reader with a consistent, proposedly not finished view of this powerful, actual, and potential association, which presents a new fashion for entrepreneurship, leading also to organizational changes which imply, in their way, a redefinition of professional works, technology application, risk and project management, and strategic planning. The chapter describes these concepts, develops their relationships, and leaves an ""open door"" for the reader to perceive not only the conceptual application for competitive scenario description, but also as an associative and integrative methodology to be used, beyond other perspectives, to appreciate the evolution of the context developed by the chapter itself, updating it in the time ahead. In the final part of the text, study cases are discussed as to affirm chapter objectives, findings, and proposed integrative base. Copyright © 2023, IGI Global. Digital transformation from data science: A source of organizational agility  ",Risk management
48,The emotional and social side of analytics professionals: an exploratory study of the behavioral profile of data scientists and data analysts,"Purpose: Analytics technologies are profoundly changing the way in which organizations generate economic and social value from data. Consequently, the professional roles of data scientists and data analysts are in high demand in the labor market. Although the technical competencies expected for these roles are well known, their behavioral competencies have not been thoroughly investigated. Drawing on the competency-based theoretical framework, this study aims to address this gap, providing evidence of the emotional, social and cognitive competencies that data scientists and data analysts most frequently demonstrate when they effectively perform their jobs, and identifying those competencies that distinguish them. Design/methodology/approach: This study is exploratory in nature and adopts the competency-based methodology through the analysis of in-depth behavioral event interviews collected from a sample of 24 Italian data scientists and data analysts. Findings: The findings empirically enrich the extant literature on the intangible dimensions of human capital that are relevant in analytics roles. Specifically, the results show that, in comparison to data analysts, data scientists more frequently use certain competencies related to self-awareness, teamwork, networking, flexibility, system thinking and lateral thinking. Research limitations/implications: The study was conducted in a small sample and in a specific geographical area, and this may reduce the analytic generalizability of the findings. Practical implications: The skills shortages that characterize these roles need to be addressed in a way that also considers the intangible dimensions of human capital. Educational institutions can design better curricula for entry-level data scientists and analysts who encompass the development of behavioral competencies. Organizations can effectively orient the recruitment and the training processes toward the most relevant competencies for those analytics roles. Originality/value: This exploratory study advances our understanding of the competencies required by professionals who mostly contribute to the performance of data science teams. This article proposes a competency framework that can be adopted to assess a broader portfolio of the behaviors of big data professionals. © 2020, Sara Bonesso, Fabrizio Gerli and Elena Bruni. The emotional and social side of analytics professionals: an exploratory study of the behavioral profile of data scientists and data analysts Behavioral competencies; Behavioral event interview; Big data; Competency-based approach; Data analysts; Data analytics jobs; Data scientists; Emotional intelligence; Soft skills ",Strategic alignment
49,Solar Photovoltaic Modules’ Performance Reliability and Degradation Analysis—A Review,"The current geometric increase in the global deployment of solar photovoltaic (PV) modules, both at utility-scale and residential roof-top systems, is majorly attributed to its affordability, scalability, long-term warranty and, most importantly, the continuous reduction in the levelized cost of electricity (LCOE) of solar PV in numerous countries. In addition, PV deployment is expected to continue this growth trend as energy portfolio globally shifts towards cleaner energy technologies. However, irrespective of the PV module type/material and component technology, the modules are exposed to a wide range of environmental conditions during outdoor deployment. Oftentimes, these environmental conditions are extreme for the modules and subject them to harsh chemical, photo-chemical and thermo-mechanical stress. Asides from manufacturing defects, these conditions contribute immensely to PV module’s aging rate, defects and degradation. Therefore, in recent times, there has been various investigations into PV reliability and degradation mechanisms. These studies do not only provide insight on how PV module’s performance degrades over time, but more importantly, they serve as meaningful input information for future developments in PV technologies, as well as performance prediction for better financial modelling. In view of this, prompt and efficient detection and classification of degradation modes and mechanisms due to manufacturing imperfections and field conditions are of great importance towards minimizing potential failure and associated risks. In the literature, several methods, ranging from visual inspection, electrical parameter measurements (EPM), imaging methods, and most recently data-driven techniques have been proposed and utilized to measure or characterize PV module degradation signatures and mechanisms/pathways. In this paper, we present a critical review of recent studies whereby solar PV systems performance reliability and degradation were analyzed. The aim is to make cogent contributions to the state-of-the-art, identify various critical issues and propose thoughtful ideas for future studies particularly in the area of data-driven analytics. In contrast with statistical and visual inspection approaches that tend to be time consuming and require huge human expertise, data-driven analytic methods including machine learning (ML) and deep learning (DL) models have impressive computational capacities to process voluminous data, with vast features, with reduced computation time. Thus, they can be deployed for assessing module performance in laboratories, manufacturing, and field deployments. With the huge size of PV modules’ installations especially in utility scale systems, coupled with the voluminous datasets generated in terms of EPM and imaging data features, ML and DL can learn irregular patterns and make conclusions in the prediction, diagnosis and classification of PV degradation signatures, with reduced computation time. Analysis and comparison of different models proposed for solar PV degradation are critically reviewed, in terms of the methodologies, characterization techniques, datasets, feature extraction mechanisms, accelerated testing procedures and classification procedures. Finally, we briefly highlight research gaps and summarize some recommendations for the future studies. © 2022 by the authors. Solar Photovoltaic Modules’ Performance Reliability and Degradation Analysis—A Review characterization; data-driven analytics; degradation; machine learning; photovoltaics Deep learning; Defects; Electric power transmission networks; Environmental technology; Learning systems; Reliability analysis; Solar concentrators; Solar energy; Solar panels; Solar power generation; Characterization; Data driven; Data-driven analytic; Machine-learning; Module performance; Performance degradation; Performance reliability; Photovoltaic modules; Photovoltaics; Solar photovoltaic modules; Degradation",Monitoring and control
50,Bioprocessing 4.0 in biomanufacturing: paving the way for sustainable bioeconomy,"The past decade has been envisaged as a period of unprecedented growth and development in the bioprocessing industry due to the increasing prominence of manufacturing bioproducts encompassing day-to-day life. Bioprocesses are the heart of biotechnology and represent the most dynamic constituent for conceptualizing the bioeconomy as it has the potential to tackle the most burgeoning problems such as climatic adversity, global population growth, reduced ecosystem resilience. The promising amalgamation of digitalization, biologicalization, and biomanufacturing paved the way for an emerging concept of “bio-intelligent value addition” or more prominently Bioprocessing 4.0 that enables the transformation in the landscape of biomanufacturing. Despite its positive credentials, the technology is facing technical, organizational, economical, and likely some unforeseen challenges that must be resolved for its successful implementation for hailing the sustainability development goals (SDGs) of bioeconomy. Though the road of bioeconomy is quite arduous, the continuous demand for bioproducts and their timely delivery at a faster rate necessitates the culture of sharing knowledge, digitalization, automation, and development of flexible modular and podular facility footprints to accelerate biomanufacturing. Therefore, it is worth summarizing the major portfolios of Bioprocessing 4.0 such as conception of biofoundry, bioprocess intensification strategies, process and data analytics, software and automation, and its synergistic correlation with bioeconomy. Thus, the present article advocates about the technological glance of Bioprocessing 4.0 along with technical challenges and future research priorities for sparking the glory of this industrial landscape for enshrining the bioeconomy. Graphical abstract: [Figure not available: see fulltext.] © 2023, Jiangnan University. Bioprocessing 4.0 in biomanufacturing: paving the way for sustainable bioeconomy Bioeconomy; Biofoundry; Downstream processing; Industry 4.0; Process intensification; Upstream processing Abstracting; Data Analytics; Industrial research; Population statistics; Bio-manufacturing; Bio-processing industries; Biofoundry; Bioprocesses; Bioprocessing; Bioproducts; Downstream-processing; Growth and development; Process intensification; Upstream processing; Industry 4.0",Change Management
51,Strategic Resource Allocation in Project Management: A Fusion of ERM and Financial Insights in the Financial Sector,"In the realm of the financial sector, the efficient allocation of resources within project management is paramount for success. This paper presents a pioneering approach that fuses the principles of Enterprise Risk Management (ERM) with financial insights to optimize strategic resource allocation. With project complexities and risk mitigation at the forefront, this innovative approach not only aligns skilled staff resources but also harmonizes financial objectives within the dynamic financial landscape. Navigating the financial sector requires a delicate balance of expertise, regulatory compliance, and strategic vision. The integration of ERM and financial insights offers a unique perspective that anticipates challenges, aligns resources with objectives, and ensures risk-aware project execution. Through a detailed methodology, this paper illuminates the process of weaving financial considerations into resource assessment and allocation strategies, empowered by data analytics and technological advancements. As the mobile application development process becomes more complex, it has become increasingly difficult to manage different tasks and resources. The traditional project management methodology is ineffective and time-consuming for small or short projects. This paper introduces a new project management methodology based on ERM model. ERM is an abbreviation of ""Expected Results Method."" It was developed by Shep Hyken and Rod Dirks in 2002 when they found there was a lack of accuracy in the traditional PERT method. This paper suggests a new method that enables organizations to accurately measure their projects by employing an ERM approach as opposed to PERT which allocates team members based on individual time schedules rather than considering tasks that need to be completed together. © 2023 ThinkBiotech LLC. All rights reserved. Strategic Resource Allocation in Project Management: A Fusion of ERM and Financial Insights in the Financial Sector Capital structure; Consumer goods; ERM; Firm growth; Project management Asset management; Data Analytics; Finance; Regulatory compliance; Resource allocation; Risk management; Capital structure; Consumer Goods; Efficient allocations; Enterprise risk management; Financial sectors; Firm growth; Management IS; Project management methodology; Resources allocation; Strategic resource; Article; capital; commercial phenomena; data analysis; financial management; human; management; organization; resource allocation; risk management; staff; Project management",Strategic alignment
52,Handbook of Price Impact Modeling,"Handbook of Price Impact Modeling provides practitioners and students with a mathematical framework grounded in academic references to apply price impact models to quantitative trading and portfolio management. Automated trading is now the dominant form of trading across all frequencies. Furthermore, trading algorithms’ rise introduces new questions professionals must answer, for instance: • How do stock prices react to a trading strategy? • How to scale a portfolio considering its trading costs and liquidity risk? • How to measure and improve trading algorithms while avoiding biases? Price impact models answer these novel questions at the forefront of quantitative finance. Hence, practitioners and students use the Handbook as a comprehensive, modern view of systematic trading. For financial institutions, the Handbook’s framework aims to minimize the firm’s price impact, measure market liquidity risk, and provide a unified, succinct view of the firm’s trading activity to the C-suite via analytics and tactical research. The Handbook’s focus on applications and everyday skillsets makes it an ideal textbook for a master’s in finance class and students joining quantitative trading desks. Using price • Build a market simulator to back test trading algorithms • Implement closed-form strategies that optimize trading signals • Measure liquidity risk and stress test portfolios for fire sales • Analyze algorithms’ performance controlling for common trading biases • Estimate price impact models using the public trading tape Finally, the reader finds a primer on the database kdb+ and its programming language q, which are standard tools for analyzing high-frequency trading data at banks and hedge funds. Authored by a finance professional, this book is a valuable resource for quantitative researchers and traders. © 2023 Kevin Thomas Webster. Handbook of Price Impact Modeling  ",Strategic alignment
53,Manpower forecasting models in the construction industry: a systematic review,"Purpose: This paper aims to make a systematic review of the manpower prediction model of the construction industry. It aims to determine the forecasting model's development trend, analyse the use limitations and applicable conditions of each forecasting model and then identify the impact indicators of the human resource forecasting model from an economic point of view. It is hoped that this study will provide insights into the selection of forecasting models for governments and groups that are dealing with human resource forecasts. Design/methodology/approach: The common search engine, Scopus, was used to retrieve construction manpower forecast-related articles for this review. Keywords such as “construction”, “building”, “labour”, “manpower” were searched. Papers that not related to the manpower prediction model of the construction industry were excluded. A total of 27 articles were obtained and rated according to the publication time, author and organisation of the article. The prediction model used in the selected paper was analysed. Findings: The number of papers focussing on the prediction of manpower in the construction industry is on the rise. Hong Kong is the region with the largest number of published papers. Different methods have different requirements for the quality of historical data. Most forecasting methods are not suitable for sudden changes in the labour market. This paper also finds that the construction output is the economic indicator with the most significant influence on the forecasting model. Research limitations/implications: The research results discuss the problem that the prediction results are not accurate due to the sudden change of data in the current prediction model. Besides, the study results take stock of the published literature and can provide an overall understanding of the forecasting methods of human resources in the construction industry. Practical implications: Through this study, decision-makers can choose a reasonable prediction model according to their situation. Decision-makers can make clear plans for future construction projects specifically when there are changes in the labour market caused by emergencies. Also, this study can help decision-makers understand the current research trend of human resources forecasting models. Originality/value: Although the human resource prediction model's effectiveness in the construction industry is affected by the dynamic change of data, the research results show that it is expected to solve the problem using artificial intelligence. No one has researched this area, and it is expected to become the focus of research in the future. © 2021, Emerald Publishing Limited. Manpower forecasting models in the construction industry: a systematic review Construction; Forecasting model; Manpower planning; Project management Artificial intelligence; Commerce; Construction industry; Decision making; Employment; Forecasting; Personnel; Search engines; Applicable conditions; Construction outputs; Construction projects; Design/methodology/approach; Economic indicators; Focus of researches; Forecasting methods; Forecasting modeling; Predictive analytics",Strategic alignment
54,Data management within new product development and collaborative engineering: a bibliometric and systemic analysis,"Purpose: Although organizations have more data than ever at their disposal, actually deriving meaningful insights and actions from them is easier said than done. In this concern, the main objective of this study is to identify trends and research opportunities regarding data management within new product development (NPD) and collaborative engineering. Design/methodology/approach: Bibliometric and systemic analyses have been carried out using the methodological procedure ProKnow-C, which provides a structured framework for the literature review. A bibliographic portfolio (BP) was consolidated with 33 papers that represent the state of art in the subject. Findings: Most recent researches within the BP indicate new trends and paradigm shifts in this area of research, tackling subjects such as the internet of things, cloud computing, big data analytics and digital twin. Research gaps include the lack of data automation and the absence of a common architecture for systems integration. However, from a general perspective of the BP, the management of experimental data is suggested as a research opportunity for future works. Although many studies have tackled data and collaboration based on computer-aided technologies environments, no study examined the management of the measured data collected during the verification and validation stages of a product. Originality/value: This work provides a fresh and relevant source of authors, journals and studies for researchers and practitioners interested in the domain of data management applied to NPD and collaborative engineering. © 2021, Emerald Publishing Limited. Data management within new product development and collaborative engineering: a bibliometric and systemic analysis Bibliometric analysis; Data management; Product development; Product lifecycle management; Systemic analysis Cloud analytics; Data Analytics; Life cycle; Product development; Bibliometrics analysis; Collaborative engineering; Design/methodology/approach; Literature reviews; New product development; Product life cycle management; Recent researches; Research opportunities; Systemic analysis; Trend shifts; Information management",Strategic alignment
55,Business Value Creation Through Project Management Based on Big Data Approach,"In the era of big data, the high level of businesses’ digitalization, and new technology development in various fields, awash companies in a flood of massive amounts of data. Dealing with that fact is non more an option. Companies have to reexamine the way they do business in order to gain benefits from big data. Consequently, they have to review their approach of managing projects in order to create the added value. For the purpose of assessing this issue, we have adopted a research approach built on two phases. In the first, we have performed a systematic literature review to spot the gaps in the current research. The results have revealed that so far, no scientific work has discussed how companies can create business value through project management in a big data context. These results have also shown significant contributions of the research community on how big data contributes to value creation in organizations. In the second, we have suggested an approach to fills the identified gap by proposing a framework that support project management process in big data environment. © 2022 International Information and Engineering Technology Association. All rights reserved. Business Value Creation Through Project Management Based on Big Data Approach big data; big data analytics; business value; project management ",Financial management
56,Bitcoin Price Forecasting and Trading: Data Analytics Approaches,"Currently, the most popular cryptocurrency is bitcoin. Predicting the future value of bitcoin can help investors to make more educated decisions and to provide authorities with a point of reference for evaluating cryptocurrency. The novelty of the proposed prediction models lies in the use of artificial intelligence to identify movement cryptocurrency prices, particularly bitcoin prices. A forecasting model that can accurately and reliably predict the market’s volatility and price variations is necessary for portfolio management and optimization in this continually expanding financial market. In this paper, we investigate a time series analysis that makes use of deep learning to investigate volatility and provide an explanation for this behavior. Our findings have managerial ramifications, such as the potential for developing a product for investors. This can help to expand upon our model by adjusting various hyperparameters to produce a more accurate model for predicting the price of cryptocurrencies. Another possible managerial implication of our findings is the potential for developing a product for investors, as it can predict the price of cryptocurrencies more accurately. The proposed models were evaluated by collecting historical bitcoin prices from 1 January 2021 to 16 June 2022. The results analysis of the GRU and MLP models revealed that the MLP model achieved highly efficient regression, at ARE = 99.15% during the training phase and ARE = 98.90% during the testing phase. These findings have the potential to significantly influence the appropriateness of asset pricing, considering the uncertainties caused by digital currencies. In addition, these findings provide instruments that contribute to establishing stability in cryptocurrency markets. By assisting asset assessments of cryptocurrencies, such as bitcoin, our models deliver high and steady success outcomes over a future prediction horizon. In general, the models described in this article offer approximately accurate estimations of the real value of the bitcoin market. Because the models enable users to assess the timing of bitcoin sales and purchases more accurately, they have the potential to influence the economy significantly when put to use by investors and traders. © 2022 by the authors. Bitcoin Price Forecasting and Trading: Data Analytics Approaches bitcoin; cryptocurrency markets; deep learning; prediction models; statistical analysis ",Value management
57,Understanding project success involving analytic-based decision support in the digital era: a focus on IC and agile project management,"Purpose: Digital transformations of business processes are on the rise and the result is a need for a better understanding of how the elements of intellectual capital (IC) play a role in achieving successful digital project outcomes. New structural capital in the form of digital technologies must be identified and understood. Evolving skills of human capital in assimilating digital elements must also be considered, while collaboration within the development process involving relational capital provides a critical integration among these IC elements. This study illustrates the importance of identifying and managing the integration of IC components within an agile project management framework that are essential to achieving success for a digital initiative. More specifically, this study describes the process by which a multinational technology-based products company successfully developed a dynamic decision support platform utilizing an agile approach to guide a project management team to better manage the company's operations. Design/methodology/approach: This study focuses on a case analysis approach of a multinational commercial and consumer products company. The paper presents existing research on the evolving state of project management for digital initiatives and focuses on agile methods. This study then delves into the case analysis that illustrates how IC played an integral role in the company successfully developing effective decision support involving an interactive dashboard using agile Project Management (PM), which enabled the project management team to better manage resources. Findings: An examination at the case level illustrates that effective management and integration of IC has positive effects on project outcomes. While a balanced approach is evident as a requirement, the unique characteristics of the agile project management approach entails greater emphasis on select elements to adapt to a more dynamic development process. Originality/value: This work depicts the complexities in providing analytic-based decision support in an agile/flexible project management scenario. This work adds to existing research by illustrating elements within IC categories and the elements' interdependencies that play an essential role in achieving success in this more flexible project environment. © 2023, Emerald Publishing Limited. Understanding project success involving analytic-based decision support in the digital era: a focus on IC and agile project management Agile; Analytics; Dashboard; Data engineering; Decision support; Project management ",Financial management
58,E-Portfolio as a Component of the Information and Analytical System of Scientific Staff Training,"In the conditions of extremely dangerous human life (COVID-19 pandemic, martial law, etc.) the urgent tasks of science and education are the preservation of values, the creation of E-communications for adaptive and comprehensive support of scientists. The goal of the research is to intensify the problems of information analytics in the system of scientific staff training (SST) and to develop a methodology for building an information-analytical system (IAS), which will ensure the integrity of the process of intellectual potential formation. This is aimed at the rationality of the IAS, in particular, its component – E-Portfolio. This will make it possible to convert the accumulated information into electronic form and facilitate the transition to the creation of fundamentally new types of information resources. They can be combined into digital collections, which in the projection of the educational space become a system complex of digital scientific and educational resources – part of the IAS. When force-majeure makes radical changes in the education system and causes a certain failure in all spheres of scientific activity, the E-Portfolio itself becomes a reasonable and well-reasoned decision. Its development as an open component of the IAS for SST is using methods of adequate definition of structure based on of systematic, competency, transdisciplinary and diagnostic approaches. The components of the structure are combined into meaningful blocks based – on conceptual, organizational, socio-psychological, and diagnostic. This allows for establishing transdisciplinary relationships between researchers in different fields of science. Thus E-Portfolio is an effective IAS-tool for distance SST and also helps with the adaptive performance of scientific tasks. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG. E-Portfolio as a Component of the Information and Analytical System of Scientific Staff Training E-Portfolio; Information-analytical system; Scientific staff training; Transdisciplinarity Analytical systems; Condition; E-portfolios; Human lives; Information and analytical systems; Information-analytical system; Martial law; Scientific staff training; Staff training; Transdisciplinarity; COVID-19",Financial management
59,"Essentials of Excel VBA, Python, and R: Volume II: Financial Derivatives, Risk Management and Machine Learning","This advanced textbook for business statistics teaches, statistical analyses and research methods utilizing business case studies and financial data with the applications of Excel VBA, Python and R. Each chapter engages the reader with sample data drawn from individual stocks, stock indices, options, and futures. Now in its second edition, it has been expanded into two volumes, each of which is devoted to specific parts of the business analytics curriculum. To reflect the current age of data science and machine learning, the used applications have been updated from Minitab and SAS to Python and R, so that readers will be better prepared for the current industry. This second volume is designed for advanced courses in financial derivatives, risk management, and machine learning and financial management. In this volume we extensively use Excel, Python, and R to analyze the above-mentioned topics. It is also a comprehensive reference for active statistical finance scholars and business analysts who are looking to upgrade their toolkits. Readers can look to the first volume for dedicated content on financial statistics, and portfolio analysis. © The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG 2023. Essentials of Excel VBA, Python, and R: Volume II: Financial Derivatives, Risk Management and Machine Learning Business Analytics; Business Mathematics; Mathematical Finance; Probability and Statistics in Computer Science; Python; Quantitative Finance; R; Statistical Finance ",Monitoring and control
60,Mapping business analytics skillsets with industries: empirical evidence from online job advertisements,"As a large accumulation of data is captured and contained, organisations find that the invaluable information can be used to improve company performance, leverage competitive advantages, and create business values. Using business analytics (BA) job advertisements collected from a recruiting website, this study identified knowledge domains and skillsets of BA professionals. Additionally, it examined the relative importance of these BA skills in different industries such as Financial and Information Technology services. The results of Text mining analysis indicate that data modelling, statistical software, visualisation, forecasting, and database are the top ranked BA technical skills. In addition, process skills such as communication, project management, and financial techniques are crucial. The association rules analysis recognises the relative importance of BA skillsets across different industries. The findings contribute to the employability and professional development of new graduates; additionally, they provide insights to BA academic curriculum design and human resources management. © 2022 The Operational Research Society. Mapping business analytics skillsets with industries: empirical evidence from online job advertisements association rules; Business analytics; capabilities; job distributions; skillsets; text mining Association rules; Competition; Curricula; Business analytics; Business value; Capability; Company performance; Competitive advantage; Information technology services; Job distribution; Knowledge domains; Skill sets; Text-mining; Project management",Strategic alignment
61,Integrating Customer Portfolio Theory and the Multiple Sources of Risk Approaches to Model Risk-Adjusted Revenue,"This study proposes new methods to formulate customers' risk-adjusted revenue (RAR) metrics applied to the financial industry. Using a customer dataset provided by a loan company, we compute RAR using benchmark approaches presented in the literature and new formulas that combine the Customer Portfolio Theory and the Multiple Sources of Revenues approaches. We validate the efficiency and originality of our formulations by implementing statistical tests to check for differences across the different RAR measures. We find that the proposed RAR models are unique and can be implemented in the industry to account for multiple sources of risk, hence providing managers with ways to improve their valuation of customers' portfolios.  Copyright © 2022 The Authors. Integrating Customer Portfolio Theory and the Multiple Sources of Risk Approaches to Model Risk-Adjusted Revenue Analytics; Customer Valuation; Data-Driven Models; Management; Risk-Adjusted Revenue Analytic; Customer risks; Customer valuation; Data-driven model; Modeling risk; Multiple source; Portfolio theories; Risk approaches; Risk-adjusted; Risk-adjusted revenue; Sales",Strategic alignment
62,Constrained optimization algorithms for the computation of investable portfolios analytics: evaluation of economic-capital parameters for performance measurement and improvement,"Purpose: This paper aims to empirically test, from a regulatory portfolio management standpoint, the application of liquidity-adjusted risk techniques in the process of getting optimum and investable economic-capital structures in the Gulf Cooperation Council financial markets, subject to applying various operational and financial optimization restrictions under crisis outlooks. Design/methodology/approach: The author implements a robust methodology to assess regulatory economic-capital allocation in a liquidity-adjusted value at risk (LVaR) context, mostly from the standpoint of investable portfolios analytics that have long- and short-sales asset allocation or for those portfolios that contain long-only asset allocation. The optimization route is accomplished by controlling the nonlinear quadratic objective risk function with certain regulatory constraints along with LVaR-GARCH-M (1,1) procedure to forecast conditional risk parameters and expected returns for multiple asset classes. Findings: The author’s conclusions emphasize that the attained investable economic-capital portfolios lie-off the efficient frontier, yet those long-only portfolios seem to lie near the efficient frontier than portfolios with long- and short-sales assets allocation. In effect, the newly observed market microstructures forms and derived deductions were not apparent in prior research studies (Al Janabi, 2013). Practical implications: The attained empirical results are quite interesting for practical portfolio optimization, within the environments of big data analytics, reinforcement machine learning, expert systems and smart financial applications. Furthermore, it is quite promising for multiple-asset portfolio management techniques, performance measurement and improvement analytics, reinforcement machine learning and operations research algorithms in financial institutions operations, above all after the consequences of the 2007–2009 financial crisis. Originality/value: While this paper builds on Al Janabi’s (2013) optimization algorithms and modeling techniques, it varies in the sense that it covers the outcomes of a multi-asset portfolio optimization method under severe event market scenarios and by allowing for both long-only and combinations of long-/short-sales multiple asset. The achieved empirical results, optimization parameters and efficient and investable economic-capital figures were not apparent in Al Janabi’s (2013) paper because the prior evaluation were performed under normal market circumstances and without bearing in mind the impacts of the 2007–2009 global financial crunch. © 2022, Emerald Publishing Limited. Constrained optimization algorithms for the computation of investable portfolios analytics: evaluation of economic-capital parameters for performance measurement and improvement Financial crisis; Financial engineering; Liquidity-adjusted value at risk; Optimization algorithms; Portfolio management analytics; Reinforcement machine-learning ",Strategic alignment
63,Verification and Validation for a Project Information Model Based on a Blockchain,"Agile project management based on minimum viable products has some benefits against the traditional waterfall method. Agile supports an early return of investment that supports circular reinvesting and makes the product more adaptable to variable social-economical environments. However, agile also presents some intrinsic issues due to its iterative approach. Project information requires an efficient record of the requirements, aims, governance not only for the investors, owners or users but also to keep evidence in future health and safety and other statutory compliance-related issues. In order to address the agile project management issues and address new safety regulations, this paper proposes a project information model (PIM) based on a distributed ledger technology (DLT) with a ranked procedure for the verification and validation (V&V) of data. Each V&V phase inserts a process of authenticity, data abstraction and analytics that adds value to the information founded on artificial intelligence (AI) and natural language processing (NLP). The underlying DLT consists of smart contracts embedded on a private Ethereum blockchain. This approach supports a decentralised approach in which every project stakeholder owns, manages and stores the data. The presented model is validated in a real scenario: University College London—Real Estate—Pearl Project. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. Verification and Validation for a Project Information Model Based on a Blockchain Artificial intelligence; Blockchain; Information model; Real estate; Smart contracts ",Risk management
64,Accelerating and Scaling Data Products with Serverless,"Managing a comprehensive data products portfolio with scaling capabilities is one important task for an organization-wide analytics team. Those data products can be broken down into components that use serverless offerings from cloud service providers, allowing a team of modest size to manage company-wide analytics and data science solutions while improving productivity and promoting data-driven decisions. This work describes an architecture and tools used to speed up and manage data offerings, including data visualization, pipelines, models, and APIs. Considerations of component design include re-usability, integration, and maintainability, which are discussed along with their impact to team productivity. The components described are: Data ingestion using a containerized solution as a fundamental layer for all the applications, including its execution, orchestration, and monitoring. This solution is combined with traditional pipelines in order to enrich the data available; APIs for data and model serving using containerized solutions as a building block for data products that are powered by machine learning models, and for serving a unified data ontology; Data Visualization in the form of containerized web apps that provides fast solutions for data explorations, model predictions, visualization, and user insights. The architectures of three data products are then described as aggregations of the distinct building blocks (components) that were developed and how those can be repurposed for different applications. This includes continuous integration and delivery as well as pairing of each solution with the corresponding products from cloud service providers (e.g. Google Cloud Platform) in order to provide some real world examples. The scaling of each solution is discussed, as well as lessons learned and pitfalls we have encountered regarding security, usability, and maintenance. The proposed components and architecture allowed a team of 7 members to cater for analytics solutions for a section (equivalent to approx. 1500 employees), which provides a clear picture of the potential of serverless in rapid prototyping and empowering effective teams. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG. Accelerating and Scaling Data Products with Serverless Analytics; Architecture; Containerized; Data pipelines; Data products; Data science; Machine learning; Real-world; Serverless; Visualization; Web apps Architecture; Containers; Data Analytics; Data Science; Distributed database systems; Information management; Machine learning; Pipelines; Visualization; Analytic; Cloud service providers; Containerized; Data pipelines; Data products; Machine-learning; Real-world; Scalings; Serverless; Web App; Data visualization",Strategic alignment
65,Video Surveillance Architecture from the Cloud to the Edge,"This paper examines the use of digital video in public safety and surveillance systems. Traditionally video recordings are used by law enforcement to review events retrospectively and for evidential purposes in the pursuance of criminal prosecution. We also examine how, due to the proliferation of cameras around cities, human operators are challenged to monitor these data feeds in real-time and how the emergence of AI and computer vision solutions can process this data. Computer vision can enable the move from a purely reactive to a predictive, real-time analysis platform. As camera numbers and the resolution and framerate of cameras grow, existing network infrastructure frequently causes challenges provisioning low latency, high bandwidth networking to private or public cloud infrastructure for evidential storage. These technical challenges can provide issues for law enforcement providing a data chain of custody to ensure its admissibility during court proceedings. Emerging technologies offer solutions to overcome these challenges: the use of emerging edge compute capabilities, including the use of on-camera and mobile edge compute nodes providing compute capabilities closer to the data source and new software paradigms, including CI/CD methodologies, and the use of micro-services and containerization to manage and deliver applications across the portfolio of devices, at the edge of the network. Using Amazon Web Services as an example, we review how cloud providers are now overcoming challenges in delivering real time video analytics solutions in the classical cloud model, and how they are enabling services and platforms closer to the edge, while delivering the cloud computing experience of scalability and manageability across different edges of the network. © 2022, ISCA. All rights reserved. Video Surveillance Architecture from the Cloud to the Edge Amazon web services; AWS; CCTV; cloud management; edge; PaaS; SaaS; Surveillance; video analysis ",Risk management
66,Responsible Knowledge Management in Energy Data Ecosystems,"This paper analyzes the challenges and requirements of establishing energy data ecosystems (EDEs) as data-driven infrastructures that overcome the limitations of currently fragmented energy applications. It proposes a new data-and knowledge-driven approach for management and process-ing. This approach aims to extend the analytics services portfolio of various energy stakeholders and achieve two-way flows of electricity and information for optimized generation, distribution, and electricity consumption. The approach is based on semantic technologies to create knowledge-based systems that will aid machines in integrating and processing resources contextually and intelligently. Thus, a paradigm shift in the energy data value chain is proposed towards transparency and the responsible management of data and knowledge exchanged by the various stakeholders of an energy data space. The approach can contribute to innovative energy management and the adoption of new business models in future energy data spaces. © 2022 by the authors. Licensee MDPI, Basel, Switzerland. Responsible Knowledge Management in Energy Data Ecosystems big data analytic; data exchange; data integration systems; energy big data; knowledge graphs; semantic interoperability Big data; Data integration; Ecosystems; Interoperability; Knowledge based systems; Knowledge management; Semantics; Data driven; Data integration system; Data space; Energy; Energy applications; Energy big data; Energy data; Knowledge graphs; Paper analysis; Semantic interoperability; Electronic data interchange",Strategic alignment
67,Enhanced Field Sustainable Oil Production Rate Forecast Using Integrated Asset Operating Model for Production Efficiency Improvement,"As hydrocarbon fields are maturing, field sustainable oil production rate (FSOPR) assurance is a challenge for operators to deliver the demands while adhering reservoir management guidelines after accounting for all well & facility downtimes and system inefficiencies. As part of digital transformation journey, ADNOC Onshore has embarked on an initiative to automate FSOPR forecast while eliminating inefficiencies in current process, standardization, leveraging robust data integration and analytics. FSOPR forecast is probably the most complex task performed by oil companies as involved the orchestration of all support and technical departments from the field to the terminal. Effective FSOPR estimation is an integrated effort from reservoir/petroleum engineering, drilling/well services, operations, oil movement, planning, maintenance, engineering and reliability teams. The assurance process requires a structured and integrated approach of data gathering, review, and simulations to produce a reliable forecasts. Before, data/process integration and reporting was time-consuming representing a real challenge as it was managed through manual data-loading in spreadsheets & emails with low visibility to all stakeholders. Now FSOPR is estimated using up-to-date well/network models, including contribution from newly drilled & well reactivation plans, well-performance deterioration and production restoration from field activities. Similarly planned/unplanned losses from reservoir management (RM), field activities & facility maintenance jobs are optimized through the solution, offering integration with reliability models. The automated web-page solution leverages business process engine enabled by intelligent data integration from various workflow elements and allows all stakeholders to collaborate in one portal to have on single version of the truth. The data abstraction layer retrieves information and presented in the solution from legacy systems, production modelling workflows and SAP. The base plan & optimization cases are tracked by process health KPIs and approved using Business process management workflow. FSOPR automation system has been successfully piloted in one ADNOC Onshore assets. This automation was an enabler for better planning and scheduling of maintenance and RM activities and also provides assurance of monthly oil quota achievement by highlighting early potential threats that can hinder the realization of the FSOPR plan, thus corrective action can be taken on time considering best RM practices. The whole process of data gathering, planning, activity scheduling, optimization and approval has been reduced by 60% along with a more rigorous and errorless process. The added value besides enhancing efficiency, it has minimized well downtime and production deferment avoidance of 1-2%. This automation paved the path for FSOPR assurance process standardization across company portfolio of assets. The initiative is aligned with ADNOC digital transformation roadmap to leverage industry 4.0 technologies and digitalization of key upstream business processes in a consistent, integrated and uniform manner. This paper talks about transformation, FSOPR elements, low code solution & integration architecture, analytics, change adoption and benefits realization. Copyright © 2023, Society of Petroleum Engineers. Enhanced Field Sustainable Oil Production Rate Forecast Using Integrated Asset Operating Model for Production Efficiency Improvement  Automation; Data Analytics; Data integration; Deterioration; Engines; Enterprise resource management; Forecasting; Gasoline; Infill drilling; Maintenance; Oil field development; Oil wells; Petroleum reservoir engineering; Reservoir management; Websites; Workflow management; Business Process; Data gathering; Digital transformation; Efficiency improvement; Field activities; Oil-production rates; Operating models; Process standardizations; Production efficiency; Work-flows; Efficiency",Strategic alignment
68,Commercial office portfolio risks during the COVID pandemic and the future beyond – a survey of stakeholders in India,"Purpose: This paper investigates the dynamic nature of risk in pre-, during- and post-COVID duration. It investigates how commercial office portfolio stakeholders in India perceived risk during the COVID pandemic, their risk response and mitigation strategies, and emerging structural changes that would impact the commercial office portfolio (COP) in the post-COVID period. Design/methodology/approach: A qualitative and applied research method is adopted for the study. Through purposive sampling, commercial office portfolio stakeholders were selected and interviewed using a semi-structured questionnaire having two parts. In the first part, risk attributes were accessed on the Likert scale and in the second part there were open-ended questions. Findings: The uncertainty during the COVID period increased the risk perception significantly. There was a sense of urgency to retain the tenants, preserve the headline rentals and keep the properties operational. COP managers were forthcoming to offer rent deferments, common area maintenance discounts and upgrades in the physical office in form of touchless equipment, better air filters, etc. Post-pandemic there would be extensive use of technology and data for facility management and space utilization analytics; mainstreaming of hybrid working and flexible office spaces; increased certification of buildings; adoption of ESG and sustainability norms; and better-designed buildings with a focus on EHS and wellbeing. Practical implications: Identifying structural changes in the post-pandemic period will help the COP managers to align their portfolios to the emerging office market requirements. Originality/value: This study helps in developing an understanding of the dynamic nature of the risk across pre-, during- and post-COVID periods. And risk responses and mitigation strategies adopted during the COVID period in an emerging market. © 2023, Emerald Publishing Limited. Commercial office portfolio risks during the COVID pandemic and the future beyond – a survey of stakeholders in India Commercial office portfolio; COVID; India; Post-COVID changes; Risk ",Risk management
69,IT Controlling: From IT cost and activity allocation to smart controlling,"IT controlling is established as a tool for controlling information technology. The job description of the IT controller has changed only moderately over a long period of time. It was mainly associated with IT budgeting, IT portfolio management, IT cost planning, accounting and controlling. However, digitalization has brought movement in goals, contents and methods. New topics such as digital strategy management, cloud controlling, data science, etc. are being discussed. The task profile is changing away from pure IT cost analysis to the management of the digitization strategy with a focus on strategic IT portfolio management. Some voices are already talking about “smart controlling” or “digital controlling”. This book presents an IT controlling concept for the digital age and explains the relevant methods in a practical way. © The Editor(s) (if applicable) and The Author(s), under exclusive licence to Springer Fachmedien Wiesbaden GmbH, part of Springer Nature 2023. IT Controlling: From IT cost and activity allocation to smart controlling Earned Value Analysis; IT balanced scorecard; IT controlling; IT cost accounting; IT Governance; IT Management; IT performance accounting; IT project controlling ",Monitoring and control
70,Investigation of gender differences in familiar portfolio choice,"The prevailing assumption holds that investors include in their portfolios securities that they know well, are located near their place of residence, or align with their fields of interest. This article analyse familiarity in investment through gender perspective and their fields of interest. Women and men field of interest is defined by enabling online magazines’ article’s themes. The aim of this paper is to investigate gender-based behavioural differences in investment decisions – i.e. to define women’s and men’s fields of interests and value investment portfolios. Portfolios differ according to whether they are formed from securities that are consistent with women’s fields of interest, men’s fields of interest or both women’s and men’s fields of interest. Textual analysis was employed to identify men’s and women’s fields of interest. Investment portfolios were built using mean variance (MV) and Black–Litterman (BL) models. The analysis revealed that portfolios built from men’s fields of interests are more diversified than are portfolios built either from women’s fields of interests or from both men’s and women’s fields of interest. Analysing 12 portfolios’ efficiency revealed that women’s portfolio returns are more stable than are men’s. Moreover, the study demonstrated that time impacts investment portfolio returns to a greater extent than do gendered fields of interest. The article complements the existing knowledge about bias in investor familiarity, which results from differences in men’s and women’s fields of interest. © 2022 The Author(s). Published by Vilnius Gediminas Technical University. Investigation of gender differences in familiar portfolio choice Behavioural finance; Familiarity bias; Gender differences; Investment decisions; Investment portfolio; Investment psychology; Media analytics; Textual analysis ",Strategic alignment
71,Random walk through a stock network and predictive analysis for portfolio optimization,"Portfolio optimization is the process that aims to make a profitable asset distribution and minimize the risk of loss. Usually, portfolio optimization is performed by a human analyst through technical analysis; a process subjective to the analyst's background and confined to a reduced group of assets, limiting choices, and, consequently, the possibility of better results. This work proposes the Stock Network Portfolio Allocation (SNPA) algorithm for the automatic recommendation of a stock portfolio for the next period. SNPA especially applies to conditions where there is a possibly large number of assets to consider. It works by modeling the set of assets as a complex network, in which nodes represent assets and the edges between them are established by the correlation between their returns. The choice of assets is carried out through a random walk in the network, selecting, in the end, the assets represented by the most visited nodes. We present SNPA in two forms, differing in the way they define the transition probabilities based on return: (1) SNPAr algorithm uses the cumulative wealth over a period and (2) SNPAh is a hybrid algorithm that employs a forecasting model to estimate the asset returns. Both variants are compared to six portfolio algorithms and three market indices. Investment simulations were carried out from January 2019 to December 2020 considering three stock markets, the Brazilian market share (BRB3), the S&P 500 index (SP500), and the Russell Top 200 index (EQ200). Results show a predominance performance of the proposed algorithm in all three data sets. Specifically, the SNPAh algorithm demonstrated a cumulative wealth of 267%, 308%, and 98.8%, in the 24 months, for the markets BRB3, EQ200, and SP500, respectively. In addition to these values, which are expressive for the period, the algorithm obtained the best values for the Sharpe Ratio. © 2023 Elsevier Ltd Random walk through a stock network and predictive analysis for portfolio optimization Analytics; Portfolio optimization; Random walk; Stock network; Xgboost Commerce; Competition; Financial data processing; Financial markets; Investments; Predictive analytics; Random processes; Allocation algorithm; Analytic; Assets distributions; Condition; Portfolio optimization; Random Walk; Stock network; Stock portfolio; Technical analysis; Xgboost; Complex networks",Financial management
72,"2nd International Conference on Business Analytics for Technology and Security, ICBATS 2023","The proceedings contain 168 papers. The topics discussed include: rock-paper-scissors image classification using transfer learning; risk-informed target set analysis of nuclear power plants; deep learning and industrial Internet of things to improve smart city safety; analysis of query processing on different databases; goldonomics: cryptocurrency vs. gold; which is a better store of value in the global; the impact of implementing mind mapping technology on raising the achievement of curriculum processing unit in teaching methodologies & styles course; the nexus between digital innovation and digital entrepreneurship in the strategic transformation; agent-based modelling and simulation of crowd evacuation: case study for electric train cabin; digital platforms’ influence on project management; impact of open big data and Insurtech on business digitalization; identity threats in the metaverse and future research opportunities; and the role of big data tools and supply chain capabilities in promoting supply chain sustainability: insights using balanced scorecard approach. 2nd International Conference on Business Analytics for Technology and Security, ICBATS 2023  ",Monitoring and control
73,Special Session: Marketing Science at the Service of Innovative Startups and Vice Versa: An Abstract,"This session orchestrates participation from startup entrepreneurs, investors, academics and policy makers. Innovative startups can adopt marketing science in their decision-making processes, provide data to academics, and inspire them to co-develop new theories and models contributing to the elevation and influence of the discipline (Key et al., 2021), putting marketing science to practice (Steenkamp, 2021) and solve managerial problems (Lilien, 2011). Four presentations are made by startup founders operating in Artificial Intelligence, Financial Tech, Digital News and Advertising Tech, using marketing science to their advantage. Bottleneck removal for artificial intelligence startup decision making using marketing science. K.G. Charles-Harris. CEO, Quarrio. Carlos Pérez-Vidal1. Rising amounts of data affects the ability of startup founders to predict outcomes and make decisions pertaining to employees, strategies, consumers and markets. The Ladder of Business Intelligence framework (LOBI) enables computers to convert data from facts into information. We present how marketing science in practice orchestrates how managers and employees within AI startups can enhance cycle times from data to information, and then to action, avoiding bottlenecks with regards to operational execution both internally and with the market. Using marketing science to fix economic inequality unlocking startup access to global capital markets. Jonathan Nelson. CEO, Hackers and Founders. Marketing science-based methodology for the selection of startups floating on public capital markets. The five stages of the method are universe, content analysis, conversation, interest and acceptance. Metrics are defined by London Stock Exchange practice. Data is collected from startups seeking to float on the stock exchange and analysed with cluster analysis. An innovation diffusion aggregation analysis methodology. Tim Hill. CEO, Social Status.Carlos Pérez-Vidal1. Entrepreneurs and startups increasingly adopt social media diffusion in launch phases. Audiences are becoming elusive due to fragmentation and there is a paradigm shift in evolving user behaviour. We suggest an integrative analytics methodology that normalises cross-channel adoption to reach expected behaviour. Anonymous data is aggregated by Social Status platform and analysed with partial least squares path modelling. Quick Diversification. Deciding the scaling strategies of a digital news startup using marketing science. Joey Chung. CEO, TNL Media. Carlos Pérez-Vidal1. We conceptualise a framework for the assessment of diversification speed in digital portfolio management strategy within digital news entrepreneurship, based on the innovation diffusion and product life cycle theories. Data is collected from the verticals of a rapidly expanding Taiwan-based digital news agency seeking stability and survivability in the market adopting marketing science. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG. Special Session: Marketing Science at the Service of Innovative Startups and Vice Versa: An Abstract Advertising; Applied marketing science; Artificial intelligence; Financial tech; Martech; Social media; Startups ",Value management
74,Does a knowledge gap contribute to the performance gap? Interviews with building operators to identify how data-driven insights are interpreted,"Data-driven operation and maintenance analytics for improving building energy performance are prevalent in the literature and produce various visualizations and key performance indicators that identify hard and soft heating, ventilation, and air conditioning faults, occupancy patterns, and energy use deficiencies. However, an underlying assumption exists that the outputs of these analyses, some adopted from ASHRAE guidelines, can effectively impart actionable energy-saving insights to building operations personnel and augment the handling of faults, optimized operating controls, or building-level scheduling. This paper attempts to identify potential barriers to effective interpretation and utilization of data-driven, energy-saving insights by building operators and facility managers. Eleven operations professionals were interviewed and assessed regarding what initiatives or changes they would make in light of various outputs of multiple established data-driven approaches. Data from the interviewees’ individual building portfolio were inputted into a multi-source data analytics web application, employing fault detection and diagnosis and inverse modelling techniques, and its outputs were presented to interviewees. Only two of the 11 interviewees expressed interest in making controls changes in response, revealing a possible knowledge gap of data-driven analytics, combined with the risk-adverse nature of building operations, which may inhibit operations personnel from effectively utilizing data-driven insights as presented. © 2022 Elsevier B.V. Does a knowledge gap contribute to the performance gap? Interviews with building operators to identify how data-driven insights are interpreted Building operations; Facility managers; Interviews; Knowledge gap; Multi-source data analytics; Operators Air conditioning; Benchmarking; Buildings; Energy conservation; Fault detection; Managers; Building operations; Data analytics; Data driven; Energy-savings; Facilities Managers; Interview; Knowledge gaps; Multi-source data analytic; Multisource data; Operator; Data Analytics",Risk management
75,An Ameliorate Analysis of Cryptocurrencies to Determine the Trading Business with Deep Learning Techniques,"As the range of COVID-19 sufferers increased, many nations imposed a complete lockdown. As a result, it caused a devastating international financial disaster everywhere in the world. Technical and essential evaluation are two methods for determining future worth. Different strategies use statistics from outside the market, such as monetary conditions, hobby rates, and geopolitical events, to forecast future charge. We use technical evaluation forecasts potential charge using buying and selling statistics from the market, which includes charge and buying and selling volume, whereas other strategies use statistics from outside the market, such as monetary conditions, hobby rates, and geopolitical events. The objective of this project is to give technical and fundamental analysis using machine learning approaches. In business, AI is broadly used to remedy and optimize various problems, including marketing, credit score card fraud detection, algorithmic trading, patron service, portfolio management, and product advice primarily based totally on patron needs. Furthermore, the technology due used in this finished greater to optimize the proposed set of rules to attain the maximum correct result primarily based totally on the current valuation of cryptocurrency. This project is to use machine learning techniques to provide technical analysis. The incorporation of new technology into financial institutions has the potential to propel cryptocurrency values to time highs. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. An Ameliorate Analysis of Cryptocurrencies to Determine the Trading Business with Deep Learning Techniques Artificial intelligence; Big data; Business analytics; Decision making; Machine learning Big data; Data Analytics; Deep learning; Financial data processing; Financial markets; Investments; Learning algorithms; Learning systems; Business analytics; Condition; Decisions makings; Future worth; Geopolitical events; Learning techniques; Machine-learning; Strategy use; Technical evaluation; Use statistics; Decision making",Strategic alignment
76,All grown up? Market maturity and investment in London's purpose-built student accommodation sector,"Purpose: The UK's purpose-built student accommodation (PBSA) sector has seen significant institutional investment in recent decades. This paper unpacks contemporary trends and perspectives on the sector. It questions whether PBSA has moved from being an “alternative” to “mainstream” residential asset class, framing the analysis through the lens of market maturity. Design/methodology/approach: The methods triangulate perspectives drawn from literature on the evolution of PBSA as an asset class with illustrations of investment trends across the UK between 2005 and 2020 using data from Real Capital Analytics (RCA), combined with findings from 40 semi-structured interviews with investors and stakeholders in PBSA in the UK London is the focus of the work, whilst other regional cities are integrated for comparison. Findings: The results demonstrate that London's PBSA market is ahead of trends currently being replicated in regional cities. However, the regions currently offer greater return potential and opportunities for risk taking compared to London, where yields are compressed, and the market is considered lower risk. The concept of maturity remains useful as a framework for evaluating markets, however a more granular analysis of sectors is necessary to further understand asset classes within sectors. PBSA continues to trade at a premium across the UK; it is considered the most mature residential asset class. Practical implications: The emergence of PBSA as an asset class continues to play a developing role within the residential sector and UK investment market. Risk, value and local context remain key when integrating PBSA into institutional portfolios, and as the first to consider the UK market from a qualitative research approach, this research provides a snapshot of these influences in 2021. Originality/value: Our approach offers original insight into investment trends across the UK and is the first to focus reflections on the London market specifically. The research highlights the role of PBSA as a vanguard asset class for investors into residential, situating its growth within the framework of market maturity and drawing out market nuances from interviews. © 2021, Emerald Publishing Limited. All grown up? Market maturity and investment in London's purpose-built student accommodation sector Investment value; London; Maturity; Purpose-built student accommodation (PBSA); Risk; UK ",Strategic alignment
77,Exploring the social and technical factors in organisational AI adoption: A systematic literature review,"Embedding artificial intelligence as part of an organisation’s analytics portfolio can lead to better data-driven business insight, optimised IT systems for greater reliability, and new AI-enabled innovations. However, organisations are struggling to achieve these potential benefits. This paper reviews 45 publications across the Basket of Eight and MIS Quarterly Executive. The study aims to highlight the state-of-the-art information systems research on organisational AI adoption and how to embed AI in organisations. A combination of manual analysis and augmented AI through topic modelling was utilised to conduct the systematic literature review. The literature review confirms that an AI-supported method to conduct a literature review is efficient, but human insight is still required. From the topic modelling analysis, four underlying research themes emerged: AI to support decisionmaking and its effect on the social side, design of AI solutions, bringing value to business and humans, and lastly, the challenges of embedding AI in organisations. Furthermore, state-of-the-art research is discussed, and the requirement for a holistic sociotechnical view on how organisations can increase the adoption of AI as part of their quest to become more data-driven is highlighted. © 2023, EasyChair. All rights reserved. Exploring the social and technical factors in organisational AI adoption: A systematic literature review  Artificial intelligence; Data driven; Embeddings; IT system; Literature reviews; Organisational; Social factor; State of the art; Systematic literature review; Technical factors; Topic Modeling; Embeddings",Value management
78,Financial Portfolio Management Based on Shaped-Based Unsupervised Machine Learning: A Dynamic Time Warping Baycenter Averaging Approach to International Markets and Periods of Downside Event Risks,"Empirical evidence has shown that modern portfolio theory relating to diversification had failed investors in the recent financial crises, times when investors would hope that diversification is an effective tool to sustain portfolio performance. Almost all markets around the world declined, with varying degrees, at the 2008 financial crisis and 2020 COVID-19 market crisis. Correlation-based diversification optimized portfolios were not spared, generating significant losses. Recent research on an unsupervised machine learning method of time-series clustering using Dynamic Time Warping (DTW) as a distance measure have shown research promise as a financial portfolio diversification method and shown prospects of overcoming correlation convergence issues during periods of downside event risks. This research validates the applicability of DTW cluster diversification to achieve persistent portfolio performance in international developed markets, even across periods of market weakness. Results showed outperformance of mean and median return and Sharpe metrics of optimally weighted DTW cluster diversification, against correlation-based diversification methods. The findings will augment existing literature in the use of data science approach to portfolio diversification. Copyright 2022 With Intelligence LLC. Financial Portfolio Management Based on Shaped-Based Unsupervised Machine Learning: A Dynamic Time Warping Baycenter Averaging Approach to International Markets and Periods of Downside Event Risks  ",Strategic alignment
79,Process mining at Lufthansa CityLine: The path to process excellence,"Process mining is a big data technology, which focuses on the discovery, monitoring, and improvement of business processes, based on real data from information systems. This teaching case describes the objectives of a German airline as it introduces process mining and discusses current and future value potentials of this technology. The case is particularly useful for executive MBA courses on Strategy (the value of IT investments) or master’s-level courses on Business Process Management. This case has three main learning objectives. First, students will evaluate the capabilities of different (technological) approaches to reaching the airline’s business goals and will make a justified decision on the feasibility of implementing process mining. Second, students will analyze the airline’s approach to implementing process mining and the challenges along the way. They will derive lessons learned and discuss approaches to solving challenges. Third, students will evaluate the value potentials of process mining. This will enable the students to make well-informed decisions on technology investments and to discover how these decisions can contribute to business goals. The case is designed to be taught in two formats. In a 90-min lecture, students need to prepare short assignments for classroom discussions. In a 180-min lecture, the assignments are included as group work during the lecture, but they require the students to read the case before class. Teaching Notes, including videos and additional study material to support group work, are available to eligible lecturers upon request. © Association for Information Technology Trust 2021. Process mining at Lufthansa CityLine: The path to process excellence Business intelligence and analytics; business process management; business value and economics of IT; enterprise systems; process mining; project management ",Strategic alignment
80,The National Coordinated Citrien eHealth Program to Scale Up Telemonitoring: Protocol for a Before-and-After Evaluation Study,"Background: Sustainable implementation of telemonitoring in health care is challenging, especially if one aims to scale up telemonitoring initiatives nationwide. The National collaborative eHealth program in the Netherlands is supporting the nationwide upscaling of telemonitoring in 3 clinical domains by implementing telemonitoring in all Dutch university medical centers (UMCs). The chosen telemonitoring concepts are (1) telemonitoring solutions in the domain of cardiology, (2) telemonitoring solutions providing care from a distance in obstetrics, and (3) telemonitoring solutions monitoring vital functions in hospital wards. Objective: The aim of this study is to evaluate the upscaling of telemonitoring in Dutch university hospitals in order to gain a better knowledge of the process, methods, and outcomes of nationwide upscaling strategies. Our hypothesis is that by the completion of the Citrien program’s scale-up, telemonitoring will be operational in all UMCs but not normalized in routine care. Methods: A before-and-after study will be conducted to assess upscaling. The theoretical frameworks used are the framework for nonadoption, abandonment, scale-up, spread, and sustainability; the Normalization Process Theory; and a project management tool Project Canvas. The primary outcome of the study is the degree of normalization to which health care providers at UMCs consider telemonitoring a part of their routine practice, measured using the Normalization MeAsurement Development tool (NoMAD). Our secondary outcome is the uptake of telemonitoring at the Dutch UMCs, using management data from UMCs’ business intelligence systems query. Results: Data will be collected between May 2020 and December 2022. Results were retrieved in June 2023. UMCs’ business intelligence systems are queried for data for the secondary outcome measures. There is a risk that the UMCs will not be able to provide this management information. The laws and regulations governing telemonitoring in the Netherlands are changing, with the Electronic Data Exchange in Health Care Act (Wet elektronische gegevensuitwisseling in de zorg) and the European Health Data Space Act expected to positively influence implementation and upscaling. Conclusions: The Citrien program is a nationally coordinated change management program that is scaling up telemonitoring across contexts and settings. This study will produce original data on the uptake and upscaling of telemonitoring at Dutch UMCs. Future initiatives to implement eHealth in the health care sector may be guided by the wide range of success factors, obstacles, and experiences collected through this program. The network itself may be of great value impacting future acceleration of eHealth initiatives. ©Harm Gijsbers, Azam Nurmohamed, Tom H van de Belt, Marlies Schijven, The Citrien 2 Project Leaders and Steering Group. The National Coordinated Citrien eHealth Program to Scale Up Telemonitoring: Protocol for a Before-and-After Evaluation Study e-health; eHealth; health care; healthcare; implementation; study protocol; telemedicine; telemonitoring; upscaling ",Capacity management
81,Empirical risk assessment of maintenance costs under full-service contracts,"We provide a data-driven framework to conduct a risk assessment, including data pre-processing, exploration, and statistical modeling, on a portfolio of full-service maintenance contracts. These contracts cover all maintenance-related costs for a fixed, upfront fee during a predetermined horizon. Charging each contract a price proportional to its risk prevents adverse selection by incentivizing low risk (i.e., maintenance-light) profiles to not renege on their agreements. We borrow techniques from non-life insurance pricing and tailor them to the setting of maintenance contracts to assess the risk and estimate the expected maintenance costs under a full-service contract. We apply the framework on a portfolio of about 5000 full-service contracts of industrial equipment and show how a data-driven analysis based on contract and machine characteristics, or risk factors, supports a differentiated, risk-based break-even tariff plan. We employ generalized additive models (GAMs) to predict the risk factors’ impact on the frequency (number of) and severity (cost) of maintenance interventions. GAMs are interpretable yet flexible statistical models that capture the effect of both continuous and categorical risk factors. Our predictive models quantify the impact of the contract and machine type, service history, and machine running hours on the contract cost. We additionally utilize the predictive cost distributions of our models to augment the break-even price with the appropriate risk margins to further protect against the inherently stochastic nature of the maintenance costs. The framework shows how maintenance intervention data can set up a differentiated tariff plan. © 2022 Elsevier B.V. Empirical risk assessment of maintenance costs under full-service contracts Empirical analysis; Frequency-severity modeling; Maintenance; Predictive analytics; Risk assessment Contracts; Cost benefit analysis; Costs; Insurance; Maintenance; Risk assessment; Risk management; Risk perception; Stochastic models; Stochastic systems; Empirical analysis; Frequency-severity modeling; Full service; Maintenance contracts; Maintenance cost; Risk factors; Risks assessments; Service contract; Severity model; Statistic modeling; Predictive analytics",Risk management
82,SELF AS STORY: Meaning-Making and Identity Integration in Capstone ePortfolios,"The Master of Education program is a fully online degree that includes 5 concentrations: higher education administration, learning and instruction, eLearning and instructional design, special education, and learning analytics. It enrolls approximately 300 students. The learning sequence in the capstone course deploys the Catalyst design principles to engage students in three linked forms of meaning-making: Inquiry, Reflection and Integration. At the outset of the course, the annotated curriculum and self-assessment exercise known as the Personal Competency Model (PCM) helps students step back to examine their entire program experience and growth, to see their previous coursework as more than the sum of its parts. The PCM exercise reinforces students’ memories of the seminal authors, research, and big ideas that they have investigated during the program and helps students take stock of the capabilities they have developed. There is heightened interest among faculty now that they have viewed exemplary PoP Case Studies and Professional Portfolios. © 2018 by Taylor & Francis Group. SELF AS STORY: Meaning-Making and Identity Integration in Capstone ePortfolios  ",Strategic alignment
83,A sentiment analysis approach to the prediction of market volatility,"Prediction and quantification of future volatility and returns play an important role in financial modeling, both in portfolio optimisation and risk management. Natural language processing today allows one to process news and social media comments to detect signals of investors' confidence. We have explored the relationship between sentiment extracted from financial news and tweets and FTSE100 movements. We investigated the strength of the correlation between sentiment measures on a given day and market volatility and returns observed the next day. We found that there is evidence of correlation between sentiment and stock market movements. Moreover, the sentiment captured from news headlines could be used as a signal to predict market returns; we also found that the same does not apply for volatility. However, for the sentiment found in Twitter comments we obtained, in a surprising finding, a correlation coefficient of –0.7 (p < 0.05), which indicates a strong negative correlation between negative sentiment captured from the tweets on a given day and the volatility observed the next day. It is important to keep in mind that stock volatility rises greatly when the market collapses but not symmetrically so when it goes up (the so-called leverage effect). We developed an accurate classifier for the prediction of market volatility in response to the arrival of new information by deploying topic modeling, based on Latent Dirichlet Allocation, in order to extract feature vectors from a collection of tweets and financial news. The obtained features were used as additional input to the classifier. Thanks to the combination of sentiment and topic modeling even on modest (essentially personal) architecture our classifier achieved a directional prediction accuracy for volatility of 63%. Copyright © 2022 Deveikyte, Geman, Piccari and Provetti. A sentiment analysis approach to the prediction of market volatility news analytics; sentiment analysis; topic modeling (LDA); Twitter; volatility ",Value management
84,ESG Controversies and Stock Returns,"In the chatter of online communities and in the writings of investigative journalists, corporate controversies are revealed and discussed in real time. Many of these controversies involve violations of environmental, social and governance (ESG) standards. This paper studies the Refinitiv MarketPsych ESG Analytics, a dataset of ESG controversies extracted from real-time news articles and social media posts. Using monthly rotation models on the Russell 3000 constituents, our research demonstrates that companies associated with a higher level of controversial online chatter experience greater future volatility and stock price underperformance. After excluding companies that are the most involved in ESG controversies, and controlling for the industry, a long-only simulated portfolio achieved annualized risk-adjusted returns 46% higher than the benchmark in the period from 2006 to 2020. © 2023 selection and editorial matter OptiRisk Systems; individual chapters, the contributors. ESG Controversies and Stock Returns ESG controversies; News analytics; Russell 3000; Social media ",Risk management
85,Introducing concepts: stairs of acceptance and project speciiic reputation score. Exploring public acceptance in three Finnish construction projects via large dataset media-analytics,"The opposition to a deployed technology in large construction projects can grow step by step when transferred from a global level to local project delivery. Large construction projects with specific technology implementations put pressure on local public acceptance and community involvement. This pressure is transferred to project management, how to deal with the issue of stakeholder acceptance before, during, and after project execution. Hence, understanding public acceptance and project-specific reputation can prove beneficial. Utilized mostly in the company Market Intelligence function(MI), modem large dataset media analytics enables mining technology-related sentiments on global, regional, or local project levels. This paper measures the media sentiment towards three large Finnish construction projects. The specific interest is to investigate which stakeholder groups are visible through the editorial and social media and how these can be classified according to the level of required information or participation level. The aim is to gain a numerical value for project reputation, a concept belonging to the marketing field of studies. Relevant technology deployment indications are provided, and a stairs of acceptance concept is conceptualized to reject the project-specific public acceptance. Specific needs to increase efforts at a local project level are indicated. The means to counteract local resistance can involve the mode of project execution or social marketing. The new algorithm-based method for measuring public acceptance and the introduced stairs of acceptance concept may bring project-level benefits by providing the added focus for increasing public acceptance. © 2023,ournal of Intelligence Studies in Business.All Rights Reserved. Introducing concepts: stairs of acceptance and project speciiic reputation score. Exploring public acceptance in three Finnish construction projects via large dataset media-analytics complex project stakeholder analysis; Data-analytics; project reputation; public acceptance; sentiment analysis ",Stakeholder management
86,Innovation Process of Hydrogen Fuel Cell Storage Technology Using the Theory of Inventive Problem Solving (TRIZ),"The innovation of hydrogen fuel cells is essential for the sustainability of the future environment, which requires technical support as a medium to achieve the goal. This study proposes a strategic patent innovation framework to transform the innovation process for inventors in developing breakthrough innovations. Using the theory of inventive problem solving (TRIZ) in the framework's foundation is changing the approach, method, strategy, and tools for the inventor to be more systematic in the innovation process. Also, a case study is presented on how the framework is applied to developing hydrogen fuel cell storage tanks. Several TRIZ methods and tools are used to increase the value of the system innovation from the selected patent and solve the trimming problem to produce the breakthrough innovation from a technical perspective. Furthermore, the study shares the patent strategy from a legal standpoint in strengthening the portfolio of the new patent system. The proposed framework is recommended to be applied beyond the scope of hydrogen fuel cell technological areas, and it will be the catalyst for developing breakthrough solutions for clean energy systems. © 2023 Wiley-VCH GmbH. Innovation Process of Hydrogen Fuel Cell Storage Technology Using the Theory of Inventive Problem Solving (TRIZ) Hydrogen fuel cell; Patent analytics; Storage system; Theory of inventive problem solving (TRIZ) Fuel cells; Hydrogen storage; Patents and inventions; Breakthrough innovations; Cell-be; Cell/B.E; Hydrogen fuel cells; Innovations process; Patent analytic; Storage systems; Storage technology; Theory of inventive problem solving; Theory of inventive problem solving (TRIZ); Sustainable development",Strategic alignment
87,Location-Based Tracking and Monitoring Infrastructural Construction Works by Using Business Intelligence Tool,"Tracking and monitoring construction projects are essential tasks to make decisions on the projects. There are various techniques to collect and store data from the construction site. In the current construction sector, informatics play an important role in collecting and interpreting data from construction processes. In recent studies, various technologies such as RFID, UAV are used for surveying the construction site. However, there is a study about collecting and interpreting the collected data is still needed. In this study, the collected data is categorized by labeling with several dimensions (e.g. location, contractor) using business intelligence and data warehouse tools. Moreover, this technique allows us to construct relationships between activities and generate practical information. The major implication of this study for engineering managers is monitoring the public demands and construction processes within one framework. © 2023 American Society for Engineering Management. Location-Based Tracking and Monitoring Infrastructural Construction Works by Using Business Intelligence Tool Business Intelligence; Construction Tracking; Decision Making & Risk Management, Organization & Work System Design Organizational & Performance Assessment; Decision Support Systems; Governmental Sector; Project Monitoring; Urban Construction Artificial intelligence; Construction; Construction industry; Data acquisition; Decision making; Project management; Risk assessment; Risk management; Business-intelligence; Construction tracking; Decision making & risk management, organization & work system design organizational & performance assessment; Decision making managements; Governmental sector; Organization system; Organizational assessment; Organizational performance; Performance assessment; Project monitoring; Risk management systems; Urban construction; Work systems design; Decision support systems",Strategic alignment
88,Is artificial intelligence an enabler of supply chain resiliency post COVID-19? An exploratory state-of-the-art review for future research,"The challenging situations and disruptions that occurred due to the outbreak of the COVID-19 pandemic have created a severe need for supply chain resiliency (SCR). There has been a growing interest among researchers to investigate the resiliency in supply chain operations to overcome risks and disruptions and to achieve successful project management. The supply chain of every business requires innovative projects to accomplish competitive advantage in the market. This study was conducted to identify the significance of artificial intelligence (AI) for creating a sustainable and resilient supply chain, and also to provide optimum solutions for supply chain risk mitigation. A systematic literature review has been conducted to examine the potential research contribution or directions in the field of AI and SCR. In total, 162 articles were shortlisted from the SCOPUS database in the chosen field of research. Structural Topic Modeling (STM), a big data-based approach, was employed to generate several thematic topics of AI in SCR based on the shortlisted articles, and all topics were discussed. Furthermore, the bibliometric analysis was conducted using R-package to investigate the research trends in the area of AI in SCR. Based on the conducted review of literature, a research framework was proposed for AI in SCR that will facilitate researchers and practitioners to improve technological development in supply chain firms. The purpose is to combat sudden risks and disruptions so that project management will perform well Post COVID-19. The study will be also helpful for future researchers and practitioners to identify research directions based on existing literature covered in this paper in the field of SCR. Future research directions are proposed for AI-enabled resilient supply chain management. This study will also provide several implications for supply chain managers to achieve the required resilience in their supply chains post COVID-19 by focusing on the elements of the proposed research framework. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature. Is artificial intelligence an enabler of supply chain resiliency post COVID-19? An exploratory state-of-the-art review for future research Artificial intelligence; Big data analytics; COVID-19; Project management; Resiliency; STM; Supply chain; Text mining ",Value management
89,AI human impact: toward a model for ethical investing in AI-intensive companies,"Does AI conform to humans, or will we conform to AI? An ethical evaluation of AI-intensive companies will allow investors to knowledgeably participate in the decision. The evaluation is built from nine performance indicators that can be analyzed and scored to reflect a technology’s human-centering. The result is objective investment guidance, as well as investors empowered to act in accordance with their own values. Incorporating ethics into financial decisions is a strategy that will be recognized by participants in environmental, social, and governance investing, however, this paper argues that conventional ESG frameworks are inadequate to companies that function with AI at their core. Fully accounting for contemporary big data, predictive analytics, and machine learning requires specialized metrics customized from established AI ethics principles. With these metrics established, the larger goal is a model for humanist investing in AI-intensive companies that is intellectually robust, manageable for analysts, useful for portfolio managers, and credible for investors. © 2021 Informa UK Limited, trading as Taylor & Francis Group. AI human impact: toward a model for ethical investing in AI-intensive companies AI; AI ethics; ESG; ethical investing; finance ",Financial management
90,"Essentials of Excel VBA, Python, and R: Volume I: Financial Statistics and Portfolio Analysis","This advanced textbook for business statistics teaches, statistical analyses and research methods utilizing business case studies and financial data, with the applications of Excel VBA, Python and R. Each chapter engages the reader with sample data drawn from individual stocks, stock indices, options, and futures. Now in its second edition, it has been expanded into two volumes, each of which is devoted to specific parts of the business analytics curriculum. To reflect the current age of data science and machine learning, the used applications have been updated from Minitab and SAS to Python and R, so that readers will be better prepared for the current industry. This first volume is designed for advanced courses in financial statistics, investment analysis and portfolio management. It is also a comprehensive reference for active statistical finance scholars and business analysts who are looking to upgrade their toolkits. Readers can look to the second volume for dedicated content on financial derivatives, risk management, and machine learning. © The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG 2022. Essentials of Excel VBA, Python, and R: Volume I: Financial Statistics and Portfolio Analysis Business Analytics; Business Mathematics; Mathematical Finance; Probability and Statistics in Computer Science; Python; Quantitative Finance; R; Statistical Finance ",Monitoring and control
91,A scientometric review of construction progress monitoring studies,"Purpose: This article aims to explore valuable insights into the construction progress monitoring (CPM) research domain, which include main research topics, knowledge gaps and future research themes. For a long time, CPM has been significantly researched with increasing enthusiasm. Although a few review studies have been carried out, there is non-existence of a quantitative review study that can deliver a holistic picture of CPM. Design/methodology/approach: The science mapping-based scientometric analysis was systematically processed with 1,835 CPM-related journal articles retrieved from Scopus. The co-authorship analysis and direct citation analysis were carried out to identify the most influential researchers, countries and publishers of the knowledge domain. The co-occurrence analysis of keyword was assessed to reveal the most dominating research topics and research trend with the visual representation of the considered research domain. Findings: This study reveals seven clusters of main research topics from the keyword co-occurrence analysis. The evolution of research confirms that CPM-related research studies were mainly focused on fundamental and traditional CPM research topics before 2007. The period between 2007 and 2020 has seen a shift of research efforts towards digitalization and automation. The result suggests Building Information Modelling (BIM) as the most common, growing and influential research topic in the CPM research domain. It has been used in combination with different data acquisition technologies (e.g. photogrammetry, videogrammetry, laser scanning, Internet of Things (IoT) sensors) and data analytics approaches (e.g. machine learning and computer vision). Practical implications: This study provides the horizon of potential research in the research domain of CPM to researchers, policymakers and practitioners by availing of main research themes, current knowledge gaps and future research directions. Originality/value: This paper represents the first scientometric study depicting the state-of-the-art of the research by assessing the current knowledge domain of CPM. © 2021, Emerald Publishing Limited. A scientometric review of construction progress monitoring studies Building information modelling; Construction; Project management; Scheduling; Technology Architectural design; Data acquisition; Data Analytics; Information theory; Internet of things; Scheduling; Building Information Modelling; Co-occurrence analysis; Construction progress; Knowledge domains; Knowledge gaps; Research domains; Research topics; Scheduling; Scientometrics; Technology; Construction",Strategic alignment
92,Identifying poorly performing listed firms using data analytics,"This study presents a teaching case that analyzes the applicability of the Z-Score bankruptcy prediction model to manufacturing firms listed in Hong Kong. Although the Z-Score model has been studied extensively, there are very few studies in the context of the Hong Kong stock market. Given that the Hong Kong stock market has high retail investor participation and low liquidity, whether the Z-Score model is relevant to Hong Kong investors is an important but unanswered question. The Z-Score model predicts the bankruptcy of firms by considering financial ratios involving firm liquidity, solvency, profitability, leverage, and activity. Financial and stock return data on the manufacturing firms listed in the Hong Kong Stock Exchange from 1981 to 2020 are collected from Thomson Reuters Datastream to examine the applicability of the Z-Score model in Hong Kong. Firms are then classified into bankrupt or non-bankrupt groups based on their Z-Scores. The annual stock returns in the subsequent year are analyzed for the two groups after classification. When the Z-Score threshold is set at 0, investing in the non-bankrupt group and short-selling the bankrupt group earns an annual return of 11.99% in the subsequent year. The results are robust to alternative periods and lagged values of the Z-Score. This suggests that stock prices do not reflect all the accounting data and that investors can increase their returns using the Z-Score model. As retail investors have limited resources, it may be difficult for them to fully implement the Z-Score model for a portfolio that consists of thousands of stocks. However, they can still avoid substantial losses by not investing in firms with low Z-Scores. © The Author(s) 2023. Identifying poorly performing listed firms using data analytics data analytics; financial distress; investment return; teaching case; Z-Score model ",Strategic alignment
93,The impact of the COVID-19 crisis on global real estate capital flows,"Purpose: COVID-19 has had a significant global impact at many levels, including an impact on global real estate capital flows. This paper examines the impact of COVID-19 on global real estate capital flows over 2019–2022 to clearly articulate the extent of this impact on global real estate capital flows across regions, countries, major cities, real estate sub-sectors and by major real estate investors. Drivers of these global real estate capital flow changes are also identified. The strategic real estate investment implications of this impact are highlighted, as well as the implications going forward concerning the global real estate strategies for the real estate portfolios held by institutional investors. Design/methodology/approach: To assess the impact of COVID-19, the Real Capital Analytics (RCA) database of global real estate transactions over 2019–2022 is used to drill-out critical details on commercial real estate transactions to explore specific trends in global real estate capital flows in this period of the COVID-19 crisis. This includes real estate capital flows to specific regions, countries, cities, real estate sub-sectors as well as the role of major real estate investors. Findings: The impact of COVID-19 is clearly shown with the major decline in global real estate capital flows in 2020, with a strong recovery in 2021. Reduced levels of real estate capital flows in 2022 reflect different risk dynamics, where 2022 has seen investors move on from the COVID-19 environment. In 2022, the risk of COVID-19 for real estate has been replaced by global real estate risk factors such as inflation concerns, geopolitical tensions, economic growth concerns, increased cost of debt issues and supply chain issues. This sees COVID-19 now rated as only the 6th most important risk factor in real estate investment decision-making for real estate investors in the Americas, Europe, Middle East and Africa (EMEA) and Asia–Pacific. Practical implications: This research has clearly shown the extent of the impact of COVID-19 on global real estate capital flows, as well as identifying the drivers of these real estate capital flow changes. It highlights that real estate investors have moved on and are now prioritising new risk factors ahead of COVID-19 risk. These critical risk factors reflect more recent financial, economic and geopolitical issues, which are key issues in real estate investment decision-making going forward. Investors need to structure these new risk factors into their real estate investment decision-making for the ongoing management of their domestic and international real estate portfolios. Originality/value: This paper is the first published empirical research analysis of global real estate capital flows during the COVID-19 crisis. This research provides major insights on real estate investment decision-making during this crisis and the strategic changes seen in acquiring real estate portfolios in response to this major global crisis. The change in real estate risk priorities in 2022 as real estate investors move on from the COVID-19 environment is also identified and is clearly reflected in the 2022 global real estate capital flows. © 2023, Emerald Publishing Limited. The impact of the COVID-19 crisis on global real estate capital flows COVID-19 crisis; Drivers of change; Global real estate risk factors; Institutional investors; Real estate capital flows; Real estate investment managers ",Risk management
94,A Bayesian belief network predictive model for construction delay avoidance in the UK,"Purpose: The purpose of this research is to advise on UK construction delay strategies. Critical delay factors were identified and their interrelationships were explored; in addition, a predictive model was established upon the factors and interrelationships to calculate delay potentials. Design/methodology/approach: The critical causes were identified by a literature review, verified by an open-ended questionnaire survey and then analysed with 299 samples returned from structured questionnaire surveys. The model consisted of factors screened out by Pearson product–moment correlational coefficient, constructed by a logical reasoning process and then quantified by conducting Bayesian belief networks parameter learning. Findings: The technical aspect of construction project management was less critical while the managerial aspect became more emphasised. Project factors and client factors present relatively weak impact on construction delay, while contractor factors, contractual arrangement factors and distinctively interaction factors present relatively strong impact. Research limitations/implications: This research does not differentiate delay types, such as excusable vs non-excusable ones and compensable vs non-compensable ones. The model nodes have been tested to be critical to construction delay, but the model structure is mostly based on previous literature and logical deduction. Further research could be done to accommodate delay types and test the relationships. Originality/value: This research updates critical delay factor list for the UK construction projects, suggesting general rules for resource allocation concerning delay avoidance. Besides, this research establishes a predictive model, assisting delay avoidance strategies on a case-by-case basis. © 2021, Emerald Publishing Limited. A Bayesian belief network predictive model for construction delay avoidance in the UK Construction; Novel model; Project management; Questionnaire survey; Risk management Bayesian networks; Decision theory; Project management; Surveys; Construction delays; Construction project management; Contractual arrangements; Design/methodology/approach; Literature reviews; Open-ended questionnaire; Predictive modeling; Questionnaire surveys; Predictive analytics",Value management
95,Geological Risks and Resources-Based Portfolio Ranking and Completion Optimization by Areas: An Eagle Ford Case Study,"Asset ranking plays a critical role for the business success. Given the risk and value of asset, choosing an asset by the highest expected value is not guaranteed to be the best decision because it does not consider the uncertainty in the expected value and decision maker's changing tendency toward the risk. The first part of this paper presents an application of the modern portfolio theory to the Eagle Ford unconventional resources and shows how the assets can be prioritized by taking into account the subsurface-based expected value and the uncertainty. For the fair comparison of the asset's potential, the influence of completion parameters is excluded in the asset ranking. With the stationary completion designs across the assets, the asset is evaluated and ranked only by the subsurface quality. Having the selected asset area, the second part of this paper focuses on optimizing the completion designs. Well production trend is predicted against the varying completion designs over the selected asset area and displayed onto the axes of the completion parameters of interest. Optimizing over the selected asset makes a smaller number of data available. Some completion parameters may be fixed due to operational issues - e.g. limited range of porosity and reservoir pressure in a selected asset, with the limited lateral length and total proppant amount. A data-analytics approach with a data simulation technique has been developed. Tens of thousands additional data for the input variables (subsurface and completion) is generated by a data simulation technique. These simulation data are inputted to the statistical predictive model built for the well performance so that many data points for completion, subsurface and well performance are available. With the help of simulation data, any well performance trend analysis with key completion variables of interest with some constraints for other variables is possible. Copyright © 2023, Society of Petroleum Engineers. Geological Risks and Resources-Based Portfolio Ranking and Completion Optimization by Areas: An Eagle Ford Case Study  Decision making; Oil wells; Quality control; Resource valuation; Data simulation; Expected values; Geological risks; Portfolio ranking; Resources based; Risk-based; Simulation data; Simulation technique; Uncertainty; Well performance; Data Analytics",Monitoring and control
96,Specialized Information System for Support of the Process of Recruiting Securities,"The development technologies and functional capabilities of the information system created by the authors aimed at decision-making support of the formation of a set of securities, which enables potential investors with little experience to assess independently the effectiveness of the investment portfolio by simulating the dynamics growth of assets available on the financial market are described. The proposed information system uses visualization methods that present available tabular information in the structured form of schemes, graphs, and charts. The web-oriented solution provides an opportunity to analyze and forecast portfolios in real time based on the available types of shares of various companies. © 2023 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0). CEUR Workshop Proceedings (CEUR-WS.org) Specialized Information System for Support of the Process of Recruiting Securities data science; Information and communication technologies; mathematical methods; Python; risk; visualization Data visualization; Decision making; Financial markets; Information systems; Information use; Investments; Visualization; Decision making support; Development technology; Dynamic growth; Functional capabilities; Information and Communication Technologies; Information systems use; Investments portfolios; Mathematical method; Potential investors; Visualization method; Python",Financial management
97,Adoptation of Extended Metaheuristics Considering Risk-Allocated Portfolio Optimization,"In recent times, the soft computing criterion is competent for tackling practical ambiguities involving numerous techniques, specifically neural networks, approaches to fuzzy logic, and evolutionary computational techniques. Various soft computational-based metaheuristics for minimization of risk governing portfolio optimization by using Particle Swarm Optimization, Differential Evolution and Genetic Algorithm approach focusing on optimization of CVaR (Conditional Value at Risk) measures within various market situations established on diverse objectives and constraints have been discussed within this article. The territory of portfolio optimization is meant for the selection of the range of diversified assets present in a portfolio, thus constructing a portfolio which can be best by considering some stated principles. In the modern era, multiobjective optimization procedures have proven important within the area of business intelligence, measuring the market risk–return paradigm. VaR (Value at Risk), being a prevailing technique in ascertaining downside risk within a portfolio, has been elucidated as pth percentage of returns on a specified portfolio to plan any horizon. Another vigorous technique remains the Conditional Value at Risk to determine the labeled risk entity in a portfolio within unstable market circumstances. The suggested techniques have also proved beneficial in the selection of various financial instruments in comparison to their VaR counterparts. The obtained results depict a promising outlet for determining excellent portfolio returns. © 2023 Seventh Sense Research Group® Adoptation of Extended Metaheuristics Considering Risk-Allocated Portfolio Optimization Conditional Value at Risk(CVR); Differential evolution; Genetic algorithm; Particle swarm optimization; Value at Risk ",Risk management
98,Project portfolio risk management. Bibliometry and collaboration Scientometric domain analysis,"Purpose: ‘'Project Portfolio Risk Management” is approached through a bibliometry and collaboration networks study determining its dynamics and development as a formal domain that links Project, Risk Management and Portfolio concepts. Design/methodology/approach: To facilitate replicability, a scientometric study under a PRISMA structure is carried out: i) Identification or domain structuring, as well as keywording accuracy; ii) Screening: Search string refinement and outputs review; iii) Eligibility: Several criteria applied to a content analysis, and iv) Inclusion: Consolidation of domain analytics through bibliometry and collaboration networks. Originality and findings: Assessing the field as a formal knowledge domain is novel, contributing to a synthesis of its trends and evolution: For first time, descriptive statistics show increasing attention based on the growing citation scores, participation, H index and productivity of its main journals. Project Portfolio Selection is established as hot topic, the main authors are identified, as well as key concepts such as optimization, mathematical programming, multi-objective optimization, stochastic programming, and robust optimization. Three main research themes are obtained: Incorporation of Risk Assessment into Project Portfolio Selection problem, Risk Management as a Project Portfolio Management process, and Risk Analysis considering social and environmental issues. An accurate match is found in the contrast of the domain's behavior with some bibliometric and linguistic laws. Practical implications: Theoretical richness is achieved in the conjunction of the three terms, presenting dynamics and tendencies and thus contributing to focus related research processes on a unified field for the use of both scholars' and practitioners’ perspectives. © 2023 The Authors Project portfolio risk management. Bibliometry and collaboration Scientometric domain analysis Article classification: general review; Bibliometric study; Collaboration networks; M00; Project portfolio; Risk management; Scientometric study ",Risk management
99,ML-FaaS: Toward Exploiting the Serverless Paradigm to Facilitate Machine Learning Functions as a Service,"Serverless computing has emerged as a revolutionary model that enables the deployment of applications and services by raising the level of abstraction from the underline resources. Its main functionality is enlightened by the notion of Function-as-a-Service (FaaS) as the core means to realize efficient serverless offerings. Following the shift from traditional architectures to microservices - by attaining flexibility, productivity, portability, and performance in industrial-scale IT projects, the serverless model introduces even more fine-grained services, named 'nanoservices', which facilitate required scalability by abstracting the deployment and management of the infrastructure resources. On the application space, advances in big data analysis contribute towards extracting actionable knowledge in various application domains. In this context, approaches for big data analysis aim at exploiting the added value of serverless architectures. To this end, we are presenting an extendable and generalized approach for facilitating the provision of Machine Learning Functions-as-a-Service (MLFaaS). The proposed approach outstrips the classical atomic and standard isolated services by facilitating composite services, i.e., workflows/pipelines of ML tasks, thus enabling the realization of the complete data path functions as required by data scientists. We demonstrate the operation of the proposed approach by modeling a real-world analytics scenario as an ML workflow pipeline and evaluate its performance in terms of performance. Furthermore, we address the challenge of utilizing a function oriented service template recommendation system, by expanding the serverless functional boundaries towards a holistic Quality-of-Service (QoS)-aware service function selection approach based on Artificial Intelligence techniques. These techniques propose the optimal number of functions to be implemented in a pipeline by exploiting the importance of response time as the primary key of the application's performance.  © 2004-2012 IEEE. ML-FaaS: Toward Exploiting the Serverless Paradigm to Facilitate Machine Learning Functions as a Service Artificial intelligence; function as a service; information management; machine learning; serverless computing; service management Abstracting; Big data; Computer architecture; Costs; Data handling; Function evaluation; Machine learning; Pipelines; Project management; Quality of service; Computational modelling; Function as a service; Learning functions; Machine-learning; Performance; Serverless computing; Service management; Traditional architecture; Work-flows; Information management",Strategic alignment
100,An Implementation of Support Vector Machine Classification for Developer Academy Acceptance Prediction Model,"In order to prepare graduates with work readiness in the IT industry, specifically in mobile apps development, one of its ways is to create a Developer Academy where final year students are prepared in an intensive program for two consecutive semesters to learn the stages of mobile apps development. To ensure the quality of participants in the Developer Academy, a set of selection procedures needs to be prepared, consisting of Aptitude Test, Portfolio Showcase, and Individual Interview. The problem arises when applicants are far more than the class capacity. Hence selection procedures take a longer time. The Developer Academy registration team record showed a ratio of 1: 12, which overburdens the team when it comes to selecting the applicants. More effective procedures are needed with the help of machine learning tools to help with decision making. This study aims to produce a prediction model for developer academy applicants. Several classification algorithms such as k-nearest neighbors, support vector machine, decision tree, and random forest were analyzed. Data was collected from 527 valid applicant's data which submit complete documents based on due date, other applicants who did not submit complete documents were not included in the analysis. Preliminary findings from the study show that the Support Vector Machine algorithm performs best with an accuracy of 86% and this score was then increased by applying oversampling and kernel tricks to get an accuracy rate of 98%. Hence it can be concluded that the prediction model has excellent performance. Keywords-developer academy, artificial intelligence, machine learning, support vector machine, data science, classification  © 2021 IEEE. An Implementation of Support Vector Machine Classification for Developer Academy Acceptance Prediction Model artificial intelligence; classification; data science; developer academy; machine learning; support vector machine Data Science; Decision trees; Forecasting; Nearest neighbor search; Vectors; Aptitude tests; Decisions makings; Developer academy; IT industry; Learn+; Mobile app; Prediction modelling; Selection procedures; Support vector machine classification; Support vectors machine; Support vector machines",Value management
101,A Software Ecosystem for Project Management in BIM Environments Assisted by Artificial Intelligent Techniques,"This paper presents a new platform called BusinessRedmine for project management in BIM environments. The platform constitutes a software ecosystem that combines traditional project management techniques with soft computing techniques for decision-making in projects. The proposed platform is business intelligence tools and consist of a set of software actives that cover the different processes of project management. The platform allows the management of multiple processes such as: scope management, planning management, schedule construction, risk management, stakeholder management, communications and quality. In addition, it facilitates integration with tools for 2D and 3D design using the IFC format. The platform includes facilities for managing project files aligned with the ISO 19650 series of standards. One of the fundamental elements it provides is the application of soft computing techniques to artificial intelligence for decision-making in project cuts. We compare the algorithm FISBR proposed in BusinessRedmine systems for project evaluation with other algorithms reported in bibliography based on neural networks and genetic algorithms techniques. The algorithms selected learn from a database that contains projects already evaluated and after that classify other projects. In experiments, we applied cross validation techniques combined with Friedman test and Wilcoxon test. Finally, a comparison with different project management tools is presented. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG. A Software Ecosystem for Project Management in BIM Environments Assisted by Artificial Intelligent Techniques Artificial intelligence; BIM; Project management; Soft computing ",Risk management
102,Total quality management (TQM) implementation in the Nigerian construction industry,"Purpose: Lack of strict compliance to the principles of total quality management (TQM) by construction organizations has brought about poor quality of the finished building projects. This has been blamed for the incessant structural failure reported in Nigeria. This study appraised TQM implementation in the Nigerian construction industry, with a view to mitigating structural failure rate of construction projects. To achieve this aim, the study aims to assess the practice level of TQM and the factors hindering TQM implementation on construction projects. Design/methodology/approach: The study utilized a well-structured questionnaire and convenient sampling method in the gathering and sampling of data among construction professionals in I Am Going To state, Nigeria. Data analyses were done using, frequency, percentage, mean analytics and Pareto analysis. Findings: The study revealed that major practice of TQM principles with respect to structural failure rate are purchasing: ensuring the procurement of materials of the specified quality standard, ensuring the use of a quality improvement construction process of the organization, site management responsibility: this entails ensuring quality supervision by the project management leadership and monitoring and control of quality during the construction to guarantee firm observance quality standards. Also, the major factors hindering TQM implementation on construction projects are: inadequacy of the necessary machineries, equipment, tools and facilities for the effective execution of work on construction site; breakdown in communication and information exchange between the management and supervisory teams on site; poor attitudes and strategies toward maintenance of equipment, tools and machines; and absence of prompt salary and incentive payment. It was recommended that construction firms must require the suppliers of construction materials to strictly comply with quality specification evidence in quality certification of delivered materials to mitigate structural failure. Research limitations/implications: This study appraised TQM implementation in the construction industry of Nigeria, with emphasis on I Am Going To state. The study underscores the practice level of TQM and the key factors hindering TQM implementation on construction projects. Following the localized geographical limitation of the study area, a similar research in other part/states of Nigeria or even in other developing countries of African is necessary. Practical implications: The practices level of TQM and the factors hindering TQM implementation were identified. This will be useful in guiding construction firms, other industry's key stakeholders and regulatory agencies in bringing about a sustainable quality management system for improve profit and value maximization and avoiding incessant structural failure. Originality/value: This is one of the few studies that have assessed the practice level of TQM and the factors hindering TQM implementation on construction projects in Nigeria. This study took place in I Am Going To state with records of periodic structural failure and building collapse. © 2021, Emerald Publishing Limited. Total quality management (TQM) implementation in the Nigerian construction industry Nigeria; Project delivery; Structural failure; Total quality management; TQM practices; TQM principles Building materials; Compensation (personnel); Construction; Construction equipment; Construction industry; Developing countries; Failure (mechanical); Failure analysis; Fracture mechanics; Human resource management; Machinery; Quality control; Safety engineering; Structural integrity; Total quality management; Construction organizations; Construction professionals; Design/methodology/approach; Geographical limitations; Monitoring and control; Project management leadership; Quality management systems; Total quality managements (TQM); Project management",Value management
103,Transformation of Software Project Management in Industry 4.0,"Digital technologies create not only new opportunities, but also form new business challenges caused by the integration of design, product development and business processes (from the moment an order is placed right through to outbound logistics), personalization and adaptive response on customer demands, real-time solving of tasks and problems, openness and access to resources for remote employees, accumulating big data for analytics and adaptive management with clouds. Modern software development in Industry 4.0 is complex process, both human-intensive, technology-intensive, knowledge- intensive and innovative-intensive. The modern software industry moves unrelentingly towards new methods for managing the ever-increasing complexity of software projects. The traditional project management methodology, which contains structured and standardized forms, and is focused on a greater certainty and planning horizon of several decades, no longer allows companies to fully adapt to the new opportunities, speeds and risks of Industry 4.0. The article systematizes new conceptual approaches and forms of projects management, including adaptive management methodologies. Agile-based digital project management creates new opportunities for business: accelerating decision-making and the rapid identification of incorrect approaches, effective cooperation between technical groups and business groups, creating a more attractive environment for collaboration and co-working, reducing the time to prepare documentation and others. The transformation of software project management is associated with dramatic changes, the risk of errors when updating products, diluting responsibility for product quality, and even problems with the companies reputation. Presently it seems to be more sustainable a combined or hybrid model of software projects management, combining traditional and Agile methodologies. © 2022, Springer Nature Switzerland AG. Transformation of Software Project Management in Industry 4.0 Agile methodologies; Digital technologies; Industry 4.0; Project management; Software Agile manufacturing systems; Cloud analytics; Data Analytics; Decision making; Human resource management; Information management; Product design; Project management; Software design; Adaptive Management; Agile Methodologies; Business challenges; Business Process; Design products; Digital technologies; Outbound logistics; Product development process; Software; Software project management; Industry 4.0",Financial management
104,Project management: openings for disruption from AI and advanced analytics,"Purpose: The purpose of this essay is to illustrate how project management “pull” and AI or analytics technology “push” are likely to result in incremental and disruptive evolution of project management capabilities and practices. Design/methodology/approach: This paper is written as a critical essay reflecting the experience and reflections of the author with many ideas drawn from and extending selected items from project management, artificial intelligence (AI) and analytics literatures. Findings: Neither AI nor sophisticated analytics is likely to elicit hands on attention from project managers, other than those producing AI or analytics-based artifacts or using these tools to create their products and services. However, through the conduit of packaged software support for project management, new tools and approaches can be expected to more effectively support current activities, to streamline or eliminate activities that can be automated, to extend current capabilities with the availability of increased data, computing capacity and mathematically based algorithms and to suggest ways to reconceive how projects are done and whether they are needed. Research limitations/implications: This essay includes projections of possible, some likely and some unlikely, events and states that have not yet occurred. Although the hope and purpose are to alert readers to the possibilities of what may occur as logical extensions of current states, it is improbable that all such projections will come to pass at all or in the way described. Nonetheless, consideration of the future ranging from current trends, the interplay among intersecting trends and scenarios of future states can sharpen awareness of the effects of current choices regarding actions, decisions and plans improving the probability that the authors can move toward desired rather than undesired future states. Practical implications: Project managers not involved personally with creating AI or analytics products can avoid mastering detailed skill sets in AI and analytics, but should scan for new software features and affordances that they can use enable new levels of productivity, net benefit creation and ability to sleep well at night. Originality/value: This essay brings together AI, analytics and project management to imagine and anticipate possible directions for the evolution of the project management domain. © 2021, Emerald Publishing Limited. Project management: openings for disruption from AI and advanced analytics Agile development; Artificial intelligence; Big data; Business analytics; CRISP-DM; IS project management; Project management ",Value management
105,"Combining the best concepts of agile methods with concepts from predictive ones: applications of simulations, scenarios, and logic","In this paper, we utilise deductive analytics based on a review of the literature as well as stochastic simulation, and scenarios to perform a comparison between predictive methods like waterfall and adaptive methods such as scrum. We compare these methods using several project management performance criteria such as cost, duration, value delivered, payback (breakeven) period, predictability, project size, criticality, complexity, challenges, etc. Based on these findings, we synthesise a new plan-driven, predictive procedure that will enable it to be competitive with the adaptive/agile methods in situations where it makes sense to use a predictive method. Copyright © 2022 Inderscience Enterprises Ltd. Combining the best concepts of agile methods with concepts from predictive ones: applications of simulations, scenarios, and logic adaptive method; agile method; agile project management; breakeven point; continuous delivery; disciplined agile; hybrid methodology; iterative development; payback period; plan-driven method; predictive procedure; scrum; traditional project management; waterfall method ",Strategic alignment
106,Detecting environmental hotspots in extensive portfolios through LCA and data science: a use-case perspective,"Today, businesses need to reduce environmental impacts significantly along the entire value chain. Yet, full organisational product stewardship seems tough for extensive portfolios of several thousands of individual products varying in material and functionality, as well as production processes and locations. In addition, identifying relevant levers for improvement is more challenging with an increasing amount of influencing parameters. Moreover, while a quantification of environmental sustainability performance is required to derive sound management decisions, life cycle assessment (LCA) approaches particularly for large portfolios traditionally fail to provide effective, time efficient means of assessing more than a couple of scenarios per study. In this context, Fraunhofer IBP determined the CO2-footprint of around 24,000 individual screws in the portfolio of Würth, market leader for assembly and fastening materials, to demonstrate a data science framework for efficient scale-up of environmental sustainability assessments. Hereby, the identification of key hotspots in the portfolio along the value chain was focussed, as well as transparently displaying results and levers for improvement. This contribution builds upon proven methods and tools from LCA and data science and a modularly built approach to achieve a high degree of workflow automation. It offers practical insights into CO2-footprinting and further environmental sustainability analyses for portfolios with large amounts of individual products. © The Authors, published by EDP Sciences. Detecting environmental hotspots in extensive portfolios through LCA and data science: a use-case perspective  ",Stakeholder management
107,Risk-Based Approach to Predict the Cost Performance of Modularization in Construction Projects,"The lack of proper management of the unique modular risks hinders the construction industry from maximizing the potential benefits associated with the use of modularization. Capturing the full cost benefits of modularized processes requires careful consideration of different aspects related to material, design, technology, logistics, installation, and so forth. Although many previous research efforts provided models to assess the feasibility of modularization in construction projects, no previous study developed a risk-based approach to assess the impact of adopting modular approaches on the cost performance of construction projects. This paper fills this knowledge gap. The authors followed an interrelated multistep research methodology. First, 50 modular risks were identified based on an extensive analysis of the literature conducted in a previous research study. Second, an industry survey was conducted in which 48 construction professionals - with an average of 24.25 years experience in construction operations and 13.6 years experience in modular construction - examined and assessed the impact of modular risks on cost performance. Third, integrated statistical and mathematical techniques - including distribution fitting - were utilized to develop a predictive model that (1) maps the 50 modular risks under investigation to cost performance data collected from 56 modular construction projects, and, consequently, (2) computes the associated cost saving and/or growth. The proposed model was verified using extreme condition tests, surprise behavior test, and sensitivity analysis, and was validated by industry experts. The authors provide guidelines for using the proposed model by industry practitioners. The developed model allows for maximized use of available project information to enable a reliable a priori cost assessment for the modularization processes throughout the different project phases. This is particularly important in the case of varying conditions over the project lifecycle. The developed model helps project stakeholders identify cost enablers and barriers that are interrelated with modularization processes. Accordingly, corrective actions and mitigation plans can be established. This research provides improved practices for decision-making, risk management, and project team alignment to maximize the cost benefits of modularization in construction projects. © 2021 American Society of Civil Engineers. Risk-Based Approach to Predict the Cost Performance of Modularization in Construction Projects Decision-support tool; Modular risks; Modularization; Proactive risk management Construction industry; Costs; Decision making; Human resource management; Modular construction; Predictive analytics; Risk assessment; Risk management; Sensitivity analysis; Construction operations; Construction professionals; Construction projects; Distribution fitting; Project informations; Project stakeholders; Research methodologies; Risk based approaches; Project management",Risk management
108,Design of Brain Intelligence Framework for Water Traffic Management Integrating Multiple Spaces,"Water transportation plays an important role in China's national economic construction. Improving the capacity of water transport governance is of great significance to promote shipping development and safeguard national sovereignty. The Ministry of transport (hereinafter referred to as the ""MOT"") is the competent department of waterway transportation in China. This study constructs a brain like ability framework of waterway transportation governance integrating artificial intelligence, collective intelligence and business intelligence, and deduces its brain like development process. The research provides a theoretical reference for promoting the traditional decentralized management of the industry to move towards a new type of comprehensive management with the help of scientific and technological innovation. © Conference Proceedings of the 10th International Symposium on Project Management, China, ISPM 2022. Design of Brain Intelligence Framework for Water Traffic Management Integrating Multiple Spaces Capacity improvement; Maritime governance Business-intelligence; Capacity improvement; Collective intelligences; Decentralized management; Development process; Economic constructions; Maritime governance; Traffic management; Water transport; Water transportation; Project management",Capacity management
109,Lateral Load Behavior of Unreinforced Masonry Spandrels,"Spandrels, are usually classified as secondary elements and even though their behaviour has not received adequate focus unlike piers, they significantly affect the seismic capacity of the structure. Masonry spandrels are often damaged and the first structural components that crack within Unreinforced Masonry structures. Despite this, existing analytical methods typically consider a limit case in which the strength of spandrels is either neglected, considered to be infinitely rigid and strong or treated as rotated piers. It is clearly evident that such an assumption is not plausible. Hence, reliable predictive strength models are required. This thesis attempts to re-examine the flexural behaviour of spandrels and proposes an analytical model. The model is based on the interlocking phenomena of the joints at the end-sections of the spandrel and the contiguous masonry. The proposed analytical model is incorporated within a simplified approach to account for the influence of spandrel response on global capacity estimate of URM buildings. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG. Lateral Load Behavior of Unreinforced Masonry Spandrels Lateral load behavior; Spandrel; Storey-shear mechanism Analytical models; Piers; Predictive analytics; Project management; Structural design; Analytical method; Global capacity; Lateral loads; Seismic capacity; Strength models; Structural component; Unreinforced masonry; Unreinforced masonry structures; Masonry materials",Capacity management
110,BI Solutions as a Tool for the Management of Quality of Company's Project Activities,"The paper presents analysis of the usefulness of using business intelligence tools to maintain a company's capacity to compete in the marketplace. The potential use of an analysis tool is also discussed using the example of a power engineering firm that executes resource-intensive projects. This study took into account the key issues with implementing such programs, solutions for them, and an illustration of the Business Intelligence tool's design. Based on the findings of the study, conclusions were reached regarding the possibly beneficial impact of employing the tool within the context of project management as well as the potential for scaling the deployment of such solutions.  © 2022 IEEE. BI Solutions as a Tool for the Management of Quality of Company's Project Activities automation; business intelligence; calendar-network planning; engineering; industrial enterprise; process quality management; project management Automation; Information analysis; Quality management; Analysis tools; Business-intelligence; Calendar-network planning; Industrial enterprise; Intelligence tool; Management of qualities; Network planning; Power engineering; Process quality managements; Project activities; Project management",Capacity management
111,Key investors and their strategies in the expansion of European student housing investment,"The aim of this paper is to understand the expansion process of investment into Purpose Built Student Accommodation (PBSA) in Europe by examining transformations in student housing investment landscapes and uncovering the profiles and strategies of key investors between 2010 and 2020. Using data from Real Capital Analytics, trends in capital structures and profiles of PBSA investors are identified. Investors driving these trends are scrutinised in terms of their investment timelines, locations, hold periods and strategies of portfolio diversification. Furthermore, in-depth interviews with property analysts, PBSA investors, and developers substantiate the quantitative analysis. The empirical results show that Private Equity entered the European PBSA market, starting with the UK, when the yield premium post-GFC justified the perceived risk. Equity funds typically hold their portfolios for around five years and trade counter-cyclically with institutions such as pension funds. PBSA specialists, mainly REITs, have accumulated substantial portfolios, and the REIT structure is well-suited to the steady income which student rents should provide, but their lack of diversification leaves them vulnerable to changes in student demographics and accommodation requirements. © 2021 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group. Key investors and their strategies in the expansion of European student housing investment Europe; investment strategy; investors; PBSA; Student housing United Kingdom; capital; housing market; investment; quantitative analysis; rental sector; student; trend analysis",Strategic alignment
112,LUSTRE: An Online Data Management and Student Project Resource,"We advocate for greater emphasis in training students about data management, within the context of supporting experience in reproducible workflows. We introduce the Lancaster University STatistics REsources (LUSTRE) package, used to manage student research project data in psychology and build capacity with respect to data acumen. LUSTRE provides a safe space to engage students with open research practices—by making tangible different phases of the reproducible research pipeline, while emphasizing its value as a transferable skill. It is an open-source online data catalogue that captures key data management information about a student research project of potential relevance to data scientists. Embedded within a taught programme, it also highlights concepts and examples of data management processes. We document a portfolio of open teaching resources for LUSTRE, and consider how others can implement or adapt them to facilitate data management and open research. We discuss the role of LUSTRE as a; (a) resource and set of activities for promoting good data management practices; (b) framework to enable the delivery of key concepts in open research; (c) an online system to organize and showcase project work. © 2022 The Author(s). Published with license by Taylor & Francis Group, LLC. LUSTRE: An Online Data Management and Student Project Resource Data science; Education; Open science; Public data sharing; Statistical literacy; Teacher preparation; Transparency ",Strategic alignment
113,A FRAMEWORK FOR RANKING CRITICAL SUCCESS FACTORS OF BUSINESS INTELLIGENCE BASED ON ENTERPRISE ARCHITECTURE AND MATURITY MODEL,"Aim/Purpose The aim of this study is to identify Critical Success Factors (CSF) of Business Intelligence (BI) and provide a framework to classify CSF into layers or perspectives using an enterprise architecture approach, then rank CSF within each perspective and evaluate the importance of each perspective at different BI maturity levels as well. Background Although the implementation of the BI project has a significant impact on creating analytical and competitive capabilities, the lack of evaluation of CSF holistically is still a challenge. Moreover, the BI maturity level of the organization has not been considered in the BI implementation project. Identifying BI critical success factors and their importance can help the project team to move to a higher maturity level in the organization. Methodology First, a list of distinct CSF is identified through a literature review. Second, a framework is provided for categorizing these CSF using enterprise architecture. Interviewing is the research method used to evaluate the importance of CSF and framework layers with two questionnaires among experts. The first questionnaire was done by Analytical Hierarchy Process (AHP), a quantitative method of decision-making to calculate the weight of the CSF according to the importance of CSF in each of the framework layers. The second one was conducted to evaluate framework layers at different BI maturity levels using a Likert scale. Contribution This paper contributes to the implementation of BI projects by identifying a comprehensive list of CSF in the form of a holistic multi-layered framework and ranking the importance of CSF and layers at BI maturity levels. Findings The most important CSF in BI implementation projects include senior management support, process identification, data quality, analytics quality, hardware quality, security standards, scope management, documentation, project team skills, and customer needs transformation, which received the highest scores in framework layers. In addition, it was observed that as the organization moves to higher levels of maturity, the average importance of strategic business and security perspectives or layers increases. But the average importance of data, applications, infrastructure, and network, the project management layers in the proposed framework is the same regardless of the level of business intelligence maturity. Recommendations The results of this paper can be used by academicians and practitioners to imfor Practitioners prove BI project implementation through understanding a comprehensive list of CSF and their importance. This awareness causes us to focus on the most important CSF and have better planning to reach higher levels of maturity according to the maturity level of the organization. Future Research For future research, the interaction of critical success factors of business intelligence and framework layers can be examined with different methods. © 2022 Informing Science Institute. All rights reserved. A FRAMEWORK FOR RANKING CRITICAL SUCCESS FACTORS OF BUSINESS INTELLIGENCE BASED ON ENTERPRISE ARCHITECTURE AND MATURITY MODEL business intelligence; critical success factor; enterprise architecture; maturity model Human resource management; Information analysis; Information management; Metadata; Network architecture; Project management; Business Intelligence projects; Business-intelligence; Critical success factor; Enterprise Architecture; Enterprise architecture modeling; Implementation projects; Maturity levels; Maturity model; Project team; Success factors; Decision making",Value management
114,"Proceedings of the 2021 SIGCOMM 2021 Poster and Demo Sessions, Part of SIGCOMM 2021","The proceedings contain 30 papers. The topics discussed include: cost-effective data analytics across multiple cloud regions; non-interoperability detection for routing protocol implementations; mining social interactionsin connection traces of a campus Wi-Fi network; blockchain and games: a novel middleware for blockchain-based multiplayer games; planter: seeding trees within switches; the effect of consumer portfolio on the risk profile of cloud provider; constructing the face of network data; DNS water torture detection in the data plane; a cloud-scale per-flow backpressure system via FPGA-based heavy hitter detection; modular switch deployment in programmable forwarding planes with switch(de)composer; and SCASys: a smart congestion control algorithm selection system. Proceedings of the 2021 SIGCOMM 2021 Poster and Demo Sessions, Part of SIGCOMM 2021  ",Risk management
115,Realizing Value from Digital Transformation: Benefits Management Re-imagined,"When organizations first harnessed information technology to solve business problems, automate processes and provide information, it could take many months, even years, to deploy an application. This situation changed two decades ago when Software-as-a-Service products, hosted in the cloud, became available. Today, technology has matured sufficiently, and it is possible to install an IT platform to support a global business process in a matter of weeks and to stitch multiple applications together into a reliable and agile architectural foundation. While legacy technology still poses a major problem, technology for new investments is no longer the bottleneck that it once was.This is likely one of the reasons why it is widely acknowledged that achieving digital transformation ambitions is less about technology deployment and more about the ability of the organization to adopt it and adapt to it, and to ultimately create real business value. But here is the conundrum: while the time to implement technology has been significantly compressed, achieving the organizational changes needed to reap benefits still require months and even years to achieve.A weakness in mainstream project management literature, especially when applied to technology-enabled business investments, is the assumption that the project is complete once the software is released, stabilized, and 'accepted' by the project's sponsor. Sometimes, a window of time is allocated to 'finish' the project; an after-action or lessons learned report is produced, the project team disbands, and victory is declared. When considering the success or failure of projects, discussions usually revolve around the budget, schedule, the software and its acceptance, but generally not about the changes that the organization has experienced as a result of improved processes or enhanced capabilities or whether the expected benefits were delivered. Despite recognizing that benefits come from organizational changes, project management is still premised around scope, resources and time. This can be because benefits have not yet happened; the nature of most IT projects is that benefits only emerge many weeks and months after 'go-live.' Victory is often declared prematurely.To promote the achievement of benefits from IT investments, the concept of Benefits Management was introduced in the 1990's. Its aim was to focus on what business benefits were expected to be delivered from the investment and to accelerate the realization of these benefits from harnessing the capabilities of technology. This was achieved by identifying the organizational changes necessary to release these benefits, as well as tracking the benefits realized throughout the entire initiation-to-realization cycle. It also advocated that expected benefits should be aligned to key strategic drivers.Since the original work on Benefits Management was undertaken, the context for IT investments has changed dramatically. The initial research was focused on improving the performance outcomes from large enterprise system investments; these systems had an internal organizational focus and took considerable time to implement. Back then, how systems were built was also different, due primarily to the constraints imposed by technology, development frameworks and dominant practices. Technologies like AI and analytics pose particular challenges for managing benefits in that it is difficult to specify them prior to technology deployment. Moreover, today, technology is a competitive necessity; it is shaping business models and customers expect to engage with an organization in a digital way; and systems can extend outside the organizational boundary to ecosystem partners. Speed, innovation, and agility are critical determinants of success in a digital-first world, fundamentally changing how an organization competes. All of these changes impact the nature of projects and how they are set up and run.Although benefits management is an established concept in the project management and technology literatures, it is not well-known as an organizational practice. In this paper, we revisit the benefits management concept, discuss some of the adoption barriers, and suggest a number of ways to adapt benefits management to digital transformation programs. © 2022 PICMET. Realizing Value from Digital Transformation: Benefits Management Re-imagined  Budget control; Intelligent systems; Investments; Software as a service (SaaS); Automate process; Benefits management; Business problems; Digital transformation; IT investments; Management IS; Organizational change; Service products; Software-as-a- Service (SaaS); Technology deployment; Project management",Risk management
116,The Professional Career Path; A Structured and Transparent Guide for Career Progression and Strengthening Professional In-House Capabilities,"Through the project we built a solid professional foundation beyond the Graduate Development Program, implemented competence assurance vetted through internal & external accreditation, provided a structured and transparent development route for professional staff, provided employees with more flexibility in career growth and strengthened Petroleum Development Oman's (PDO) professional in-house capabilities. In addition, we will be able to report on overall competency levels across PDO's functions to better determine focus areas for capability development. This paper presents how a uniform Competency Framework was created across the company linked with development plans in 70:20:10 approach. In addition to the development of a structured, transparent, and auditable accreditation process and the creation of corporate competencies relevant to all roles such as HSE, leadership and continuous improvement competencies to ensure standardization across the entire organization. Furthermore, the paper will showcase the new integrated talent management process enabled by an IT solution (SAP-Success Factor) which integrates the following processes: Development & Assessment Competency Management, Coaching & Mentoring, Performance Management, High Potential Identification, Employee profile, and Succession Planning. Additionally, it will present the governance structure with an effective change management plan to ensure effective roll-out. The project resulted in a new competency framework for all functions linked to competency development plans; it also resulted in structured development and assessment processes. Automation paved the way for HR data analytics, and visualization and was key to ensuring compliance and sustainability. An effective change management process was incorporated from the beginning of the project. With the Technical Director as project sponsor, the project team obtained management steer through monthly steering committee meetings attended by Managing Director Committee (MDC) members. To ensure the project outcome would meet customer needs the team held more than 200 engagements with stakeholders over the last 2 years. We have built world-class documentation and audio-visual communication package to simplify the process of guiding employees, supervisors, and Corporate Functional Discipline Heads (CFDHs) in using the system. Within the first year of the project's launch, we had more than 60% of our employees start their self-assessments and development plans which indicates a positive adoption by the users. The SAP-Success Factors integrates all the talent management processes on one platform. Through the introduction of this world class IT system, an innovative competency development process was built that assures continuous development and gap closure, hardwired with different stakeholders' roles & responsibilities. Furthermore, an accreditation point system and external assessment was introduced for progression to senior roles to ensure consistency in professionalism across PDO. Copyright © 2022, Society of Petroleum Engineers. The Professional Career Path; A Structured and Transparent Guide for Career Progression and Strengthening Professional In-House Capabilities 70:20:10 Approach; accreditation point system; competency development; Competency framework; IT system; Professional; SAP-Success Factors Accreditation; Automation; Data Analytics; Engineers; Human resource management; Professional aspects; Project management; Social aspects; Sustainable development; Visual communication; Visualization; 70:20:10 approach; Accreditation point system; Competency development; Competency framework; Development plans; IT system; Petroleum development Oman; Professional; SAP-success factor; Success factors; Data visualization",Risk management
117,Artificial Intelligence Applications for Lending and NPA Management,"Non-performing assets (NPAs) are among the top pain points for financial institutions. In recent times an alarming increase trend in the number of bad loans and nonperforming assets attributed critical mess in the Indian banking system. Especially in Public sector banks that seize more than 70% of the total customer in India hold maximum NPAs alternatively Private sector banks having fewer NPAs that is also seen in their balance sheet which shows year-on-year growth and profit but in few cases, these become very serious. NPAs beyond a certain level is a because for concern which directly and indirectly touches almost all parts of the economy as it drastically impacts liquidity which gets blocked lending capacity of the financial sector. The lack of due attentiveness before and after loan reinforcement is the single major because for soaring the level of NPAs in the country apart from the absence of a common database and lack of timely intervention on a red flag. 'Prevention is better than cure' is an enduring maxim that applies equally well to the banking industry for combating the problem of NPAs. Financial disruptions in the banking sector aim to make them technologically robust so that every financial institution should be empowered to operate its activities in a risk-free environment and provide a cutting-edge risk management ecosystem. Disruptive technologies such as artificial intelligence (AI) provides an uncovering opportunity to the banks to formulate new modus operandi to stay ahead of the competition. The research paper focuses to understand the role of AI to propel the Indian banking system into the next generation of banking and prevent possible NP As with realtime monitoring so that they could be acknowledged before it is becoming awful for banks. In this current unprecedented situation with the pandemic, real-time monitoring of transactions through AI in the loan portfolio will provide potent shells for banks to keep credit portfolios risk-free, reeling the impact of the situation, under check. AI predictive analytics enhanced the scope of providing real-time surveillance solutions to handle uncollectible bad loans with smarter NP A management. © 2021 IEEE. Artificial Intelligence Applications for Lending and NPA Management Financial Disruptive Technology; Financial Institutions; NPA; Risk Management Artificial intelligence; Online systems; Predictive analytics; Risk management; Assets management; Banking systems; Disruptive technology; Financial disruptive technology; Financial institution; Indian banking; Non-performing asset; Real time monitoring; Risk free; Risks management; Finance",Risk management
118,Innovation in Business Intelligence Systems: The Relationship between Innovation Crowdsourcing Mechanisms and Innovation Performance,"Innovation crowdsourcing communities play a central role for companies to advance their innovation capabilities and portfolio by leveraging crowd intelligence and knowledge. However, it remains unclear how the mechanisms and structure of innovation crowdsourcing communities affect firms’ innovation performance. Based on the open innovation theory and knowledge-based view (KBV), this study develops a research model to investigate how the structure and mechanisms of innovation crowdsourcing influence firms’ knowledge management and innovation performance. The model was tested using structural equation modeling based on a dataset from the Microsoft community for business intelligence tools. The results show that both organizational and technical mechanisms of the community positively influence the community structure. The community structure positively influences knowledge acquisition, knowledge transformation, and the size and diversity of crowd participation. In turn, innovation crowdsourcing mechanisms and knowledge transformation have a strong influence on innovation performance. Copyright © 2022, IGI Global. Innovation in Business Intelligence Systems: The Relationship between Innovation Crowdsourcing Mechanisms and Innovation Performance Business Intelligence (BI); Crowdsourcing Mechanisms; Innovation Crowdsourcing Communities; Innovation Performance; Knowledge Management; Structural Equation Modeling (SEM) Information analysis; Knowledge based systems; Knowledge management; Business intelligence; Business intelligence systems; Business-intelligence; Community structures; Crowdsourcing mechanism; Innovation crowdsourcing community; Innovation performance; Knowledge transformation; Structural equation modeling; Structural equation models; Crowdsourcing",Monitoring and control
119,An analytics model for TelecoVAS customers’ basket clustering using ensemble learning approach,"Value-Added Services at a Mobile Telecommunication company provide customers with a variety of services. Value-added services generate significant revenue annually for telecommunication companies. Providing solutions that can provide customers of a telecommunication company with relevant and engaging services has become a major challenge in this field. Numerous methods have been proposed so far to analyze customer basket and provide related services. Although these methods have many applications, they still face difficulties in improving the accuracy of bids. This paper combines the X-Means algorithm, the ensemble learning system, and the N-List structure to analyze the customer portfolio of a mobile telecommunication company and provide value-added services. The X-Means algorithm is used to determine the optimal number of clusters and clustering of customers in a mobile telecommunication company. The ensemble learning algorithm is also used to assign categories to new Elder customers, and finally to the N-List structure for customer basket analysis. By simulating the proposed method and comparing it with other methods including KNN, SVM, and deep neural networks, the accuracy improved to about 7%. © 2021, The Author(s). An analytics model for TelecoVAS customers’ basket clustering using ensemble learning approach Basket analysis; Deep learning; Ensemble learning; User behavior analysis; Value added service ",Strategic alignment
120,DMISTA: Conceptual Data Model for Interactions in Support Ticket Administration,"Changing business models and dynamic markets in the globally connected world results in more and more complex system environments. The IT service infrastructure as enabler of innovative business models has to support these innovations by providing agile methods to quickly adapt to new use-cases. This underlines the need to manage the digitized environment systematically in order to foster efficiency. IT Service Management (ITSM) as a discipline evolved and now provides the framework to orchestrate the complexity in Information Technology. The activities, processes, and capabilities to maintain the portfolio are served by individuals, who interact with each other. There is an emphasized need for identifying, acquiring, organizing, storing, retrieving, and analyzing data related to human interaction processes to support finally the business processes. This paper proposes a conceptual data model to capture information about human interactions during support ticket administration (DMISTA). The presented model-structure and -requirements allow for efficient selection of appropriate data for various data science use-cases to understand and optimize business processes. The DMISTA supports different types of relationships (based on causality, joint cases, and joint activities) to enable efficient processing of specific analysis methods. The applicability of the model is shown based on a typical use-case. Copyright © 2022 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved. DMISTA: Conceptual Data Model for Interactions in Support Ticket Administration Conceptual Data Model; Data Flow Architectures; Data Mining; Enterprise Information System; IT Service Management; Requirements Engineering; Support Ticket Administration Data flow analysis; Information management; Information systems; Information use; Business models; Business Process; Conceptual data models; Data-flow architectures; Enterprise information system; IT service management; IT services; Requirement engineering; Service management; Support ticket administration; Data mining",Financial management
121,Mapping Unmanned Aerial System Data onto Building Information Modeling Parameters for Highway Construction Progress Monitoring,"The transportation infrastructure management sector lacks automated procedures that can help it find and resolve the performance deviations. The objective of this research is to illustrate the mapping of Unmanned Aerial System (UAS) collected photogrammetric data to building information modeling (BIM) parameters, and their application for automated construction progress monitoring and the generation of as-built models. The goal is to support project managers to estimate project progress during highway construction. As a part of ongoing work, this paper takes into account 4D (3D + time) data that is acquired from 3D surface digital elevation models, point clouds, LiDAR data, and orthographic photos. It maps these 4D data onto BIM parameters to create as-built models of the project at different intervals. A comparison between as-planned and as-built models using the earned value management method is employed to develop metrics that can be used for indicating cost and schedule deviations during construction. The mapping methodology introduced in this paper is illustrated using an ongoing highway construction project case study. The main contribution of this paper is the organization, processing, and integration of UAS data with BIM data structures and project management workflows. The research outcomes will assist project managers in an easy and quick identification of potential performance problems and support the project management decision-making process. © National Academy of Sciences. Mapping Unmanned Aerial System Data onto Building Information Modeling Parameters for Highway Construction Progress Monitoring 3D-4D-5D modeling; building information modeling (BIM); concrete pavement construction and rehabilitation; construction; construction information management; construction management; data and data science; data visualization; decision-making: pavement management; general; information systems and technology; infrastructure; infrastructure management and system preservation; inspection; modeling; nondestructive; pavement management systems; pavement: asset management; remote sensing; scheduling; visualization in transportation Antennas; Architectural design; Budget control; Data visualization; Information management; Information theory; Managers; Mapping; Photogrammetry; Remote sensing; Scheduling; Unmanned aerial vehicles (UAV); Visualization; 3d-4d-5d modeling; Assets management; Building information modeling; Building Information Modelling; Concrete pavement construction; Concrete pavement rehabilitation; Construction information management; Construction management; Data and data science; Decision-making: pavement management; Decisions makings; General; Information systems and technologies; Infrastructure; Infrastructure management and system preservation; Infrastructure managements; Infrastructure systems; Modeling; Non destructive; Pavement management; Pavement management systems; Pavement: asset management; Remote-sensing; Visualization in transportation; Decision making",Strategic alignment
122,Shareholder Structure of Major Technology Companies: A Graph Analytics Study during COVID-19 and Beyond,"During financial crises or other unexpected events, investors often seek to include lower-risk assets in their portfolios. Some assets are more sensitive than others to such phenomena. In the equities markets, adjustments tend to be made to the shareholdings of companies that are associated with a higher level of uncertainty. In this work, we explore the evolution of shareholder structure of various well-known companies in the technology sector during the COVID-19 pandemic and beyond. We model, as graphs, shareholder ownership data about twenty US-listed companies between 2020 and 2022. We use freely available tools to explore the bipartite interactions and generate a wide range of topologies that facilitate the identification of how shareholding structures have evolved during the pandemic. In addition, we study the role that some nodes play in the network topology and the process of change that is observed. Our findings include that (1) most investors reduced the amount invested in technology stocks during the pandemic and that these investments tended to bounce back in the post-pandemic era; (2) Vanguard Group, Inc., is the most influential investor in the network; (3) Apple has the highest market capitalization of all technology stocks for all quarters in this study, Microsoft Corp has a significantly lower market capitalization, but a significantly higher number of investors; and (4) While investors for Apple and Microsoft tend to be from London and New York, companies such as Oracle have investors from a variety of locations. © 2022 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0). Shareholder Structure of Major Technology Companies: A Graph Analytics Study during COVID-19 and Beyond Corporate Networks; Financial Structure; Network Topology; Shareholder Networks Commerce; COVID-19; Financial markets; Fruits; Investments; Shareholders; Corporate networks; Financial crisis; Financial structures; Graph-analytic; Market capitalization; Network topology; Shareholder network; Shareholdings; Technology companies; Unexpected events; Network topology",Financial management
123,An analysis of pollution Citizen Science projects from the perspective of Data Science and Open Science,"Purpose: Citizen Science – public participation in scientific projects – is becoming a global practice engaging volunteer participants, often non-scientists, with scientific research. Citizen Science is facing major challenges, such as quality and consistency, to reap open the full potential of its outputs and outcomes, including data, software and results. In this context, the principles put forth by Data Science and Open Science domains are essential for alleviating these challenges, which have been addressed at length in these domains. The purpose of this study is to explore the extent to which Citizen Science initiatives capitalise on Data Science and Open Science principles. Design/methodology/approach: The authors analysed 48 Citizen Science projects related to pollution and its effects. They compared each project against a set of Data Science and Open Science indicators, exploring how each project defines, collects, analyses and exploits data to present results and contribute to knowledge. Findings: The results indicate several shortcomings with respect to commonly accepted Data Science principles, including lack of a clear definition of research problems and limited description of data management and analysis processes, and Open Science principles, including lack of the necessary contextual information for reusing project outcomes. Originality/value: In the light of this analysis, the authors provide a set of guidelines and recommendations for better adoption of Data Science and Open Science principles in Citizen Science projects, and introduce a software tool to support this adoption, with a focus on preparation of data management plans in Citizen Science projects. © 2021, Emerald Publishing Limited. An analysis of pollution Citizen Science projects from the perspective of Data Science and Open Science Citizen Science; Data management plan; Data science; Open science; Pollution projects; Software Data Science; Open Data; Pollution; Project management; Citizen science; Data management plan; Management plans; Open science; Pollution project; Public participation; Science projects; Scientific programs; Scientific researches; Software; Information management",Risk management
124,Accelerated FDP Reservoir Studies in Challenging Brownfields Utilizing Digital Could Technologies,"PETRONAS Baronia field is a mature oil field with over 45 years of production history, located offshore Sarawak, Malaysia. It consists of several vertically stacked clastic sandstone reservoirs, namely two major reservoirs: S and V2 reservoirs. Both reservoirs have been on production since 1970's with the production strategy evolving over the years to maximize recovery. Natural depletion, infill drilling, water and gas injection, and recently Immiscible Water-Alternating-Gas (IWAG) IOR/EOR strategies have been implemented. All these elements combined with the subsurface uncertainties pose challenges to history match and to conduct probabilistic forecast studies on the dynamic models. Conventionally, the development scenarios for subsurface investigation are limited due to finite computing resources. As PETRONAS is shifting its portfolios to develop more complex and challenging fields, the need for transformation in development concept evaluation is evident. This is key for proper risk and uncertainties quantification. The notable challenges are a) limited number of development scenarios being investigated, evaluated, and compared; b) limited software licenses and infrastructure availability; c) lack of data and decisions traceability. These limitations are addressed by the PETRONAS LiveFDP digital transformation initiative commenced in 2019, through deployment of digital cloud technologies and solutions with scalable High-Performance Computing (HPC) environment. The cloud-based native and Petrotechnical applications enable remote work, ensure full data traceability and auditability, enable multi-realization ensemble analysis, and streamline the automated integration from the reservoir engineering ensemble workflow to economic analysis. Unlimited cloud computing power and licenses facilitate a broader spectrum of reservoir simulation cases to be investigated in a fast-tracked manner. The cloud HPC infrastructure has shortened the history matching cycle from 3 months to 1.5 months. The team has also observed over 5 times speed enhancement on simulation run performance using cloud computing compared to virtual machine and on-premise infrastructure. Utilizing the cloud solutions and ensemble probabilistic approach, the team has achieved over 90% of history match quality through 300 realizations per ensemble running concurrently and completed within 2 hours. The optimized IWAG injection resulted in 2% (~1MMStb) higher oil reserves with 37% less gas injection and 40% shorter injection cycles. This has improved gas sales and prioritization in the field while also monetizing the oil reserves. The ensemble analyses are then visualized using cloud-based data analytics system whereby key realizations and uncertainty parameters are further reviewed and highlighted across various disciplines collaboratively at real time. Copyright © 2022, Society of Petroleum Engineers. Accelerated FDP Reservoir Studies in Challenging Brownfields Utilizing Digital Could Technologies  C (programming language); Cloud analytics; Economic analysis; Gasoline; Infill drilling; Offshore oil well production; Offshore oil wells; Petroleum reservoir evaluation; Proven reserves; Uncertainty analysis; Brownfields; CAN technology; Cloud-based; Cloud-computing; Development scenarios; History match; Mature oil fields; Oil reserves; Performance computing; PETRONAS; Gases",Risk management
125,Assessment of process capabilities in transition to a data-driven organisation: A multidisciplinary approach,"The ability to leverage data science can generate valuable insights and actions in organisations by enhancing data-driven decision-making to find optimal solutions based on complex business parameters and data. However, only a small percentage of the organisations can successfully obtain a business value from their investments due to a lack of organisational management, alignment, and culture. Becoming a data-driven organisation requires an organisational change that should be managed and fostered from a holistic multidisciplinary perspective. Accordingly, this study seeks to address these problems by developing the Data Drivenness Process Capability Determination Model (DDPCDM) based on the ISO/IEC 330xx family of standards. The proposed model enables organisations to determine their current management capabilities, derivation of a gap analysis, and the creation of a comprehensive roadmap for improvement in a structured and standardised way. DDPCDM comprises two main dimensions: process and capability. The process dimension consists of five organisational management processes: change management, skill and talent management, strategic alignment, organisational learning, and sponsorship and portfolio management. The capability dimension embraces six levels, from incomplete to innovating. The applicability and usability of DDPCDM are also evaluated by conducting a multiple-case study in two organisations. The results reveal that the proposed model is able to evaluate the strengths and weaknesses of an organisation in adopting, managing, and fostering the transition to a data-driven organisation and providing a roadmap for continuously improving the data-drivenness of organisations. © 2021 The Authors. IET Software published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology. Assessment of process capabilities in transition to a data-driven organisation: A multidisciplinary approach  Decision making; Financial data processing; ISO Standards; Assessment of process; Capability determination; Data driven; Data driven decision; Decisions makings; Multi-disciplinary approach; Optimal solutions; Organizational management; Process capabilities; Roadmap; Investments",Strategic alignment
126,The Digitally Transformative U-WISE Software Technology,"The digitally transformative Upstream Well Integrity Surveillance Excellence (YOU-WISE) software technology was built. YOU-WISE data driven processes provide a risk-based financial optimization model inspired by IR 4.0's big data analytics. The objective of YOU-WISE software technology is to continuously optimize financial resources related to the frequency of conducting well integrity surveys. The new technology balances the calculated well integrity risk with the associated financial impact for the entire integrity surveillance program. YOU-WISE software technology application constitute a paradigm shift in the well integrity surveillance portfolio of oilfield operators. The YOU-WISE software technology development was started by analyzing thousands of historical well integrity data. The big data analytics optimization schemes embedded in YOU-WISE software technology was initially developed based on a total of 38, 104 case studies from different well and fluid types. YOU-WISE software technology runs artificial-intelligence based queries to collect health and defect data pertaining to integrity surveys. The data were conditioned for the analytics by recording health and defect time events. Then, the data were run through statistical schemes to obtain probability of health, defect, and overall probability of failure. The models' product is a risk of failure percentage specific to a survey and well type, representative of all conditions. The risk of failure percentages are used to run surveillance optimization scenarios and quantify the financial impacts from such optimization. YOU-WISE software technology continues to perform the optimization on real-time data based on new field collected data. The overall combined optimization results from applying the YOU-WISE software technology are substantial annual savings. There are other tangible benefits to this optimization in availing more crude for production by reducing well shut-in time for integrity surveys. The revamped well integrity frequencies based on the IR 4.0 optimization furnished by YOU-WISE software technology serves as an industry benchmark for proficient and fiscally-responsible asset integrity monitoring. The reliability of wells integrity is now greatly improved with the updated procedures, technologies, and integrity standards set forth by the IR 4.0 based YOU-WISE software and resulting instruction manual. Wells' production is now more efficient and sustained based on the optimized well surveillance shut-in times. Safety and integrity of the wells are now quantified and balanced via the new YOU-WISE software technology and kept at the required tolerable risk levels. Wells intact integrity strengthens environmental protection by reducing and eliminating undesirable well integrity events such as well downhole or surface leaks and the resulting aquifer and air contamination. Well integrity surveys were performed based on best oilfield practices. With the abundance of historical data, it became possible to prudently evaluate the well integrity risks and balance these risks with costs of conducting the surveys to achieve optimization. Copyright © 2022, International Petroleum Technology Conference. The Digitally Transformative YOU-WISE Software Technology  Application programs; Aquifers; Big data; Data Analytics; Finance; Gasoline; Software reliability; Data analytics; Data driven; Financial impacts; Financial optimizations; Integrity risk; Optimisations; Optimization models; Risk of failure; Risk-based; Software technology; Risk perception",Financial management
127,"Well Portfolio Optimisation: Accelerating Generation of Well Intervention Candidates with Automated Analytics and Machine Learning - A Case Study from Attaka Field, Indonesia","Advancements in technology, complemented with the abundance of static and historical data brought AI and digital automation adapted very well into the oil and gas industry. Specially to solve the challenges by the engineers in selecting well intervention candidates. In Attaka Field, a multi-layered offshore field in Indonesia, workover and well service (WOWS) have been one of the strategies to reduce production decline. With traditional workflows that absorb data from multiple unconsolidated sources and data format and resource limitation, reviewing 400+ wells that penetrates more than 200 reservoirs may take 2-3 months process with a reduced scope of review. As an addition, not all data and values are justified for the prioritization process. An intelligent automated solution termed as WEPON was developed to improve decision speed and quality in Attaka Field WOWS candidate screening. WEPON was built on top of a data science platform to ease the development, production and maintenance of the analytics engine and its data pipeline. More than 15 data sources, ranging from reservoir properties, allocated production data, up to well schematics were consumed and aggregated in this solution's flow. The main components for WEPON includes: 1. Technical analysis with analytics and ML plus multicriteria decision-making process to identify high potential completions, both produced and virgin ones 2. Adopting from the field's old workflow, feasibility checks to surface and subsurface constraints for the proposed completions 3. Diagnosing the wells and determine the right workover/ intervention opportunities 4. Calculating each well's subsurface and surface risks, and historical success rate to be integrated with the well's NPV to produce its expected value (EV) 5. Running on-demand economic analysis accessible from the solution's UI, the engine is tied into the operator's economic analysis tool that contains the currently used calculation and scheme 6. A presentation of the results on a web-based application. As the main process is triggered to be run on a weekly basis, the automation of WEPON helps to increase Attaka Field review size to the whole fields, as well as reducing 89.7% of time from 3 days to review a well to hours of run to review the whole field, enabling engineers to spend more time on high-cognitive components of the existing workflows. Moreover, it has shifted the approach to a more data-driven one leading up to smarter decisions. The implementation of this WEPON is the pilot in the Indonesian National Oil Company, PERTAMINA. This is also the first time the solution developed on a data science platform, allowing the tool to be evergreen and extensible process. This implementation is also the first one to integrate an economic analysis tool through its API.  Copyright 2022, Society of Petroleum Engineers. Well Portfolio Optimisation: Accelerating Generation of Well Intervention Candidates with Automated Analytics and Machine Learning - A Case Study from Attaka Field, Indonesia  Automation; Decision making; Engineers; Gas industry; Machine learning; Offshore oil well production; Offshore oil wells; Petroleum reservoir evaluation; Analysis tools; Case-studies; Economics analysis; Indonesia; Machine-learning; Portfolio optimization; Static datum; Well intervention; Work-flows; Workover; Economic analysis",Value management
128,Portfolio - a tool for making learning and competence development visible; [Portföljen synliggör lärandet och kompetensutvecklingen],"Portfolio used in education can be defined as a collection of documentation of performed learning activities, feedback, and progress. Currently, the documentation is electronic, hence the term e-portfolio is used. The portfolio must have a clear purpose and be aligned with the learning outcomes of the program. To be successfully implemented a portfolio must be an integral part of the education with defined tasks for both the students and the teachers. Students and teacher support in how to use the portfolio is essential especially in the beginning of the program. Learning analytics enables teachers to identify and develop support for students at risk of not achieving the outcomes. Portfolio - a tool for making learning and competence development visible; [Portföljen synliggör lärandet och kompetensutvecklingen]  Clinical Competence; Documentation; Education, Medical, Undergraduate; Educational Measurement; Humans; Learning; clinical competence; documentation; education; human; learning; medical education",Risk management
129,Multi-factor dependence modelling with specified marginals and structured association in large-scale project risk assessment,"This paper examines the high-dimensional dependence modelling problem in the context of project risk assessment. As the dimension of uncertain performance units (i.e., itemized costs and activity times) in a project increases, specifying a feasible correlation matrix and eliciting relevant pair-wise information, either from historical data or with expert judgement, becomes practically unattainable or simply not economical. This paper presents a factor-driven dependence elicitation and modelling framework with scalability to large-scale project risks. The multi-factor association model (MFAM) accounts for hierarchical relationships of multiple association factors and provides a closed-form solution to a complete and mathematically consistent correlation matrix. Augmented with the structured association (SA) technique for systematic identification of hierarchical association factors, the MFAM offers additional flexibility of utilizing the minimum information available in standardized, ubiquitous project plans (e.g., work breakdown structure, resource allocation, or risk register), while preserving the computational efficiency and the scalability to high dimensional project risks. Numerical applications and simulation experiments show that the MFAM, further combined with extended analytics (i.e., parameter calibration and optimization), provides credible risk assessments (with accuracy comparable to full-scale simulation) and further enhances the realism of dealing with high-dimensional project risks utilizing all relevant information. © 2021 Elsevier B.V. Multi-factor dependence modelling with specified marginals and structured association in large-scale project risk assessment Dependence modelling; Large-scale risks; Project management; Simulation Computational efficiency; Matrix algebra; Risk assessment; Scalability; Association models; Dependence modeling; High-dimensional; Higher-dimensional; Large-scale programs; Large-scale risk; Multi-factor; Project risk; Simulation; Structured association; Project management",Risk management
130,Transforming Insurance Business with Data Science,"This chapter introduces readers to data science practices in the insurance industry. The author discusses how these practices have transformed the industry in actuarial and underwriting operations as well as in sales and marketing. The chapter first gives an overview of data science’s role in an insurance company. Then, the data science challenges in each stage of an analytics project life cycle are described. The author draws from his experience to provide solution frameworks for each of the challenges. In the end, an example is demonstrated to showcase a complex business challenge in managing the entire customer journey and calculating customer lifetime value in a life insurance company. Ethical considerations of machine learning models are also discussed. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG. Transforming Insurance Business with Data Science Claims; Customer Journey; Data Science; Insurance; Lifetime Value; Machine Learning; Marketing; Predictive Model; Pricing; Project Management; Underwriting ",Strategic alignment
131,Reducing the Total Product Cost at the Product Design Stage,"Currently used decision support systems allow decision‐makers to evaluate the product performance, including a net present value analysis, in order to enable them to make a decision regarding whether or not to carry out a new product development project. However, these solutions are inadequate to provide simulations for verifying a possibility of reducing the total product cost through changes in the product design phase. The proposed approach provides a framework for identifying possible variants of changes in product design that can reduce the cost related to the production and after‐sales phase. This paper is concerned with using business analytics to cost estimation and simulation regarding changes in product design. The cost of a new product is estimated using analogical and parametric models that base on artificial neural networks. Relationships identified by computational intelligence are used to prepare cost estimation and simulations. A model of product development, production process, and admissible resources is described in terms of a constraint satisfaction problem that is effectively solved using constraint programming techniques. The proposed method enables the selection of a more appropriate technique to cost estimation, the identification of a set of possible changes in product design towards reducing the total product cost, and it is the framework for developing a decision support system. In this aspect, it outperforms current methods dedicated for evaluating the potential of a new product. © 2022 by the authors. Licensee MDPI, Basel, Switzerland. Reducing the Total Product Cost at the Product Design Stage Business analytics; Cost estimation; Decision support systems; New product development; Predictive and prescriptive analytics; Production planning; Project management; Systems modeling and simulation ",Strategic alignment
132,A computational approach to evaluating curricular alignment to the united nations sustainable development goals,"The United Nations (UN) considers universities to be key actors in the pursuit of the Sustainable Development Goals (SDGs). Yet, efforts to evaluate the embeddedness of the SDGs in university curricula tend to rely on manual analyses of curriculum documents for keywords contained in sustainability lexica, with little consideration for the diverse contexts of such keywords. The efficacy of these efforts, relying on expert co-elicitation in both subject-matter contexts and sustainability, suffers from drawbacks associated with keyword searches, such as limited coverage of key concepts, difficulty in extracting intended meaning and potential for greenwashing through “keyword stuffing.” This paper presents a computational technique, derived from natural language processing (NLP), which develops a sustainability lexicon of root keywords (RKs) of relative importance by adapting the Term Frequency–Inverse Document Frequency (TF-IDF) method to a corpus of sustainability documents. Identifying these RKs in module/course descriptors offers a basis for evaluating the embeddedness of sustainability in 5,773 modules in a university's curricula using classification criteria provided by the Association for the Enhancement of Sustainability in Higher Education's (AASHE). Applying this technique, our analysis of these descriptors found 286 modules (5%) to be “sustainability focused” and a further 769 modules (13%) to be “sustainability inclusive,” which appear to address SDGs 1, 17, 3, 7, and 15. Whilst this technique does not exploit machine learning methods applied to large amounts of trained data, it is, nevertheless, systemic and evolutive. It, therefore, offers an appropriate trade-off, which faculty with limited analytics skills can apply. By supplementing existing approaches to evaluating sustainability in the curriculum, the developed technique offers a contribution to benchmarking curricular alignment to the SDGs, facilitating faculty to pursue meaningful curricular enhancement, whilst complying with sustainability reporting requirements. The technique is useful for first-pass analyses of any university curriculum portfolio. Further testing and validation offer an avenue for future design-science research. Copyright © 2022 Lemarchand, McKeever, MacMahon and Owende. A computational approach to evaluating curricular alignment to the united nations sustainable development goals AASHE-STARS; curriculum; natural language processing; sustainability lexica; sustainable development goals; TF-IDF ",Strategic alignment
133,A model utilizing the artificial neural network in cost estimation of construction projects in Jordan,"Purpose: Cost estimation is one of the most significant steps in construction planning, which must be undertaken in the preliminary stages of any project; it is required for all projects to establish the project's budget. Confidence in these initial estimates is low, primarily due to the limited availability of suitable data, which leads the construction projects to frequently end up over budget. This paper investigated the efficacy of artificial neural networks (ANNs) methodologies in overcoming cost estimation problems in the early phases of the building design process. Design/methodology/approach: Cost and design data from 104 projects constructed over the past five years in Jordan were used to develop, train and test ANN models. At the detailed design stage, 53 design factors were utilized to develop the first ANN model; then the factors were reduced to 41 and were utilized to develop the second predictive model at the schematic design stage. Finally, 27 design factors available at the concept design stage were utilized for the third ANN model. Findings: The models achieved average cost estimation accuracy of 98, 98 and 97% in the detailed, schematic and concept design stages, respectively. Research limitations/implications: This paper formulated the aims and objectives to be applicable only in Jordan using historical data of building projects. Originality/value: The ANN approach introduced as a management tool is expected to provide the stakeholders in the engineering business with an indispensable tool for predicting the cost with limited data at the early stages of construction projects. © 2020, Emerald Publishing Limited. A model utilizing the artificial neural network in cost estimation of construction projects in Jordan Construction; Design; Engineering; Estimating; Project management; Technology Architectural design; Availability; Budget control; Cost engineering; Cost estimating; Predictive analytics; Project management; Building design process; Building projects; Construction planning; Construction projects; Design/methodology/approach; Engineering business; Indispensable tools; Predictive modeling; Neural networks",Monitoring and control
134,Health information exchange as a profession,"Many nations seek to implement health information exchange (HIE) networks to improve the quality of care and health outcomes. However, these efforts may be seriously hampered by the lack of available human resources with the necessary skills and competencies to develop, implement, and use HIE networks. Different types of professionals with complementary skills are needed to both maintain a region or country’s HIE network and optimally use the health data contained therein. These human resources include health information technology, health informatics, and health information management professionals, which includes HIE. These individuals possess not only the knowledge of information and communications technologies but also the knowledge of the health system, data standards and interoperability across platforms, privacy and security of health records, human factors and process engineering, project management, change management and technology adoption, information governance, and data analytics. This chapter will outline the human resources needed, their roles, competencies, and training, to put an effective HIE network in place and ensure that quality data are available for health system use and improvement. © 2023 Elsevier Inc. All rights reserved. Health information exchange as a profession certification; competencies; digital health; education; eHealth; health information professional; HIE professional; Human resources ",Financial management
135,A Comparative Study of Deep Neural Network and Statistical Models for Stock Price Prediction,"An Investment is a present-day pledge of money or an asset in the hope that it will bring future benefits. An investor can invest in fixed income securities, equities, derivatives, gold, or real estate. An investor's portfolio can contain a mixture of these assets. Financial Portfolio Optimization is a process that maximizes the return and minimizes the risk for an investor. With increasing population and commodities, finance has become a very complex and vast field. Earlier, investment options were minimal; now, with the introduction of the internet, investment opportunities know no bounds. The portfolio is a collection of assets. An 'Asset' is an entity that is convertible into cash. Assets bring future benefits. There are two types of assets: movable and immovable. Movable assets are stocks, mutual funds, etc. Immovable assets are physical properties, buildings, land, sophisticated machinery, etc. Assets can also be categorized as tangible and non-tangible. Assets such as gold, vehicle which have physical existence are 'tangible,' Assets such as patents, copyrights, trademarks, bonds, and stocks are 'non-tangible. A bond gives an investor a fixed income equal to an agreed contract. Stock gives an investor a part of the money earned in the form of dividends. Other non-tangible financial assets are the financial index, interest rate, currency, commodities, etc. The stock market is volatile and difficult to predict. The statisticians and machine learning experts have tried to forecast the stock market. This paper compares the prediction capability of both statistical and machine learning models. The Recurrent Neural Network (RNN), Convolution Neural Network(CNN), Long Short Term Memory (LSTM), and Auto-Regressive Integrated Moving Average (ARIMA) models are compared. It is observed that CNN outperforms other models for the given period of study. It is also observed that machine learning models fair better than the statistical model. © 2022 IEEE. A Comparative Study of Deep Neural Network and Statistical Models for Stock Price Prediction ARIMA; Data Analytics; Deep Neural Network; Finance; Machine Learning Commerce; Data Analytics; Deep neural networks; Financial data processing; Forecasting; Gold; Investments; Machinery; Autoregressive integrated moving average(ARIMA); Comparatives studies; Convolution neural network; Data analytics; Future benefits; Machine learning models; Machine-learning; Neural network model; Statistic modeling; Stock price prediction; Financial markets",Monitoring and control
136,Data-driven decision making with Blockchain-IoT integrated architecture: a project resource management agility perspective of industry 4.0,"Recently due to excessive competition, un-predictable market, disruptive business models and exponential growth in the complexity of technology and innovation, the existing PRM tools and techniques do not cope effectively, leading to scarcity/abundance of resources and induced overheads, which is the major challenge faced by PRM. This article discusses various key challenges faced by asset intensive EPC (Engineering, Procurement and Construction) industry related to managing their resources. Further, the challenges were modelled and analyzed using Ishikawa diagram and Pareto chart for identifying the significant causes. Considering the mentioned challenges, this article develops a Blockchain-IoT integrated architecture for providing Business Intelligence to improve agility of PRM process for the EPC industry. The developed architecture provides EPC industries with the capabilities like real-time data capture along with autonomous coordination of the resources with increased capacity of decentralization, trust-less transactions, security and transparency leading to improved process agility. This article gives a new dimension towards utilization of Blockchain blended with the boons of IoT technology and also gives a way forward to other asset-intensive industries to re-design their PRM in a more agile way. © 2021, The Society for Reliability Engineering, Quality and Operations Management (SREQOM), India and The Division of Operation and Maintenance, Lulea University of Technology, Sweden. Data-driven decision making with Blockchain-IoT integrated architecture: a project resource management agility perspective of industry 4.0 Agility; Asset intensive industry; Blockchain-IoT integrated architecture; Business intelligence; Data driven decision making; Project resource management Architecture; Blockchain; Competition; Industry 4.0; Information analysis; Information management; Integrated control; Internet of things; Natural resources management; Project management; Resource allocation; Agility; Asset intensive industry; Block-chain; Blockchain-IoT integrated architecture; Business models; Data driven decision; Data driven decision making; Decisions makings; Exponential growth; Integrated architecture; Decision making",Strategic alignment
137,Applying petri-net to construct knowledge graphs for adaptive learning diagnostics and learning recommendations,"Because of the increasing heterogeneity among students in classes and schools, determining a student’s basic learning status and ability in each subject and tailoring instruction or adapting remedial teaching to a student’s needs and characteristics have become challenging, especially for those students with learning disadvantages. According to Skinner’s behavioral learning theory (as cited in Gregory, 1987), differences in a student’s learning experiences (such as in understanding concepts) lead to considerable disparities in future learning. Drastic differences in internal cognition and concept structure may exist even among students with the same traditional learning achievements (i.e., scores) (Yu & Yu, 2006). Furthermore, the differences in concept cognition structure between experts and novices may be discoverable by analyzing similarities in students’ conceptual understandings, relationships, or psychological metrics (Brand-Gruwel et al., 2005; Hsu et al., 2012). Cognitive diagnostic models, such as the knowledge map, are useful for assessing a student’s conceptual knowledge structures and misconceptions to improve differentiated teaching, learning diagnosis, and remedial teaching (Chu et al., 2014; Hwang, Shi et al., 2011; Hwang, Wu et al., 2011; Ku et al., 2014; Lou et al., 2007; Lwo et al., 2013; Ting & Kuo, 2016). However, most such models have been constructed using only vertical hierarchical structures, wherein spotting multilateral correlations or influences between subjects or concepts proves difficult. Unlike an expert, a student cannot intuitively identify the multilateral correlations between the concepts involved a learning pathway, and analyzing novice learning is challenging and requires considerable time and effort. An adaptive mechanism for diagnosing and analyzing students’ dynamic learning behaviors and learning pathways could improve autonomous learning and remedial teaching. However, most existing mechanisms cannot offer adaptive or personalized learning content or pathways to the learner (Singhal, 2012; Ting & Kuo, 2016; Wang et al., 2019). With the advantages provided by information technologies such as big data analytics and artificial intelligence, large-scale heterogeneous data analysis, cloud computing, and Knowledge Graphs (KGs) with directionally linked data structures have been widely applied in recommendation systems to facilitate the representation of knowledge structures and to mine for new knowledge that helps to meets users’ needs (Deng et al., 2019; Fensel et al., 2020; Guan et al., 2019; Jia et al., 2018; Nickel et al., 2016; Noy et al., 2019; Yu et al., 2020). Similar to tree-structured knowledge maps, KGs are based on directional or nondirectional ontological graphs composed of concept nodes and relationship edges and employ the consensus opinions of Subject Matter Experts (SMEs) to generate consolidated KGs or generate KGs automatically from large mined data sets or text. The associated reasoning mechanism is used to make inferences based on the existing concepts and relationships among them. Among all the graph-building options, Petri-Net possess the most robust capabilities for graphical workflow presentation and pathway analysis (Peterson, 1981; Tan & Zhou, 2013). They have long been widely applied in adaptive learning and learning pathway recommendation. According to information published by the Taiwan Bureau of Foreign Trade and Taiwan Electrical and Electronic Manufacturers’ Association in 2020, the electrical and electronics industry accounted for 50.59% of Taiwan’s total export value in 2019. The foundational electricity course is an important core course for future electrical and Electronic Engineering (EE) study and can be the first obstacle for novices because of the intricate relationships among its knowledge concepts. Also, significant differences may exist in individual understandings of concept structures (Dai, 2015). To address the aforementioned challenges in learning diagnostics and remedial teaching, this study was based on learning theories such as the conditions of learning, constructivism, and scaffolding theory and utilized Petri-Net to achieve the following goals: 1. Use a Petri-Net to construct a KG visualizing a foundational electricity course. 2. Conduct a pilot study to identify personal learning pathways, interrelationships among foundational electricity concepts, and misconceptions regarding novices’ learning types. 3. Use students’ learning history to predict their learning effectiveness when studying future concepts and maximize their learning outcomes. 4. Recommend an adaptive, calibrated, personalized learning pathway for further remedial teaching and learning. In this study, we employed a three-round modified Delphi technique with 18 SMEs to identify 12 subjects, 58 concepts, and 95 corresponding interrelationships within a core foundational electricity course in EE. We utilized a Petri-Net with graphic features to construct a KG we called a Petri-Net KG. After the third round of the modified Delphi technique, all eight SMEs’ Content Validity Indexes (CVI) were 1.00. Cronbach’s α was .75. The SMEs’ opinions regarding the interrelationships of concepts exhibited good internal consistency and reliability, according to 95% confidence intervals. The correlations between concept weights were .83-.96 (p < .01). Intraclass Correlation Coefficients (ICCs) were used to confirm the consistency of SMEs’ opinions on the weights of the interrelationships between concepts. ICC (1) was .072, and ICC (2) was .75. All the SMEs’ opinions exhibited significant and strong correlations and good consistency. Using 947 students’ assessment records, learning portfolios, and learning status data (e.g. conditions of learning performance, scores), the reasoning engine was used to employ the KGs for learning transfer analysis. Preliminary exploration case studies were conducted to create Petri-Net KGs personalized for students with three different learning types (insufficiently hardworking, inadequate learning, and abnormal learning) and to determine their learning progress and status. Sato’s student-problem chart was used to classify students’ learning types. The major results and findings of this study are as follows: 1. The most important concept was circuit patterns and characteristics, affecting the learning of the subsequent 12 concepts, and the total impact was 6. Units, vector operations, and voltage— in that order— were the next most important concepts. 2. The proposed Petri-Net KG provided students with visualized learning scaffoldings that indicate the experts’ consensus cognitive structure. It also clarified any prior concepts requisite for understanding each concept. 3. By utilizing the weights of the interrelationships between concepts and their prior concepts, the reasoning engine could adaptively diagnose misconceptions and further predict students’ effectiveness in learning subsequent concepts. 4. Each learning type was associated with a unique cognitive structure. Integrating students’ learning portfolio data into the proposed Petri-Net KG enabled the reasoning engine to recommend an adaptive and personalized learning pathway. The aforementioned results have the following implications for future applications and research: 1. The visualized Petri-Net KG for the foundational electricity course could clearly depict learning types, experts’ consensus knowledge structure, and students’ personal cognitive structures. Additionally, most influential concepts were observable at a glance. Such KGs could be useful for guiding concept recognition and effectively diagnosing misconceptions. 2. The personalized learning pathways and content recommended by the Petri-Net KG and reasoning engine can be used in a self-tutoring platform. The weighted concept intercorrelations and student learning data can be analyzed to determine students’ cognitive conditions according to an expert KG. Thus, a model or student learning effectiveness can be established to strengthen the effectiveness of autonomous learning or remedial teaching. The framework of this course-level Petri-Net KG can be extended and applied to other curricula, disciplines, or educational levels to develop appropriate recommendation systems for competency development. © 2021, National Taiwan Normal University. All rights reserved. Applying petri-net to construct knowledge graphs for adaptive learning diagnostics and learning recommendations Foundational electricity; Knowledge graph; Learning diagnostic; Learning recommendation; Petri-Net ",Strategic alignment
138,Optimal Well Designs and Process Frameworks: Keys to Reducing Well Cost and Winning in any Environment,"Growing concerns for global energy transition and dwindling demand for crude oil has impacted the revenues and investment portfolios of most oil and gas companies. More than ever, hydrocarbon exploration and production costs require extra scrutiny and prudence. Drilling and Completions costs take a significant proportion of the capital expenditure and savings realized from this would improve development cost per barrel, improve project economics and yield better returns for all stakeholders. To unlock the fiscal values required to win in this economic climate and attract investments to develop oil and gas assets, a paradigm shift is needed on well delivery, well design and execution processes. Cost savings opportunities exist all through the well maturation, but the highest value can be realized in the design stage. This stage involves opportunity identification, alternatives generation, setting of value based well objectives, design, and engineering etc. This paper discusses cost saving initiatives such as advanced casing design to optimize the number of casing strings run, slim well designs, streamlined formation evaluation plan, bottom hole assembly (BHA) optimization, equipment standardization, adoption of agile methodologies to determine minimum function objectives, utilization of data-analytics tools to drive performance, application of state-of-the-art technology, extensive use of peer assists and peer reviews etc. Proper adoption of these initiatives can yield up to 30% reduction in well cost and ultimately enable oil and gas companies to compete and win in any environment. © 2022, Society of Petroleum Engineers. Optimal Well Designs and Process Frameworks: Keys to Reducing Well Cost and Winning in any Environment  Cost engineering; Cost reduction; Data Analytics; Design; Digital storage; Gas industry; Gasoline; Petroleum prospecting; Public utilities; Cost saving; Design frameworks; Energy transitions; Global energy; Hydrocarbon exploration; Investments portfolios; Oil and gas companies; Process framework; Well costs; Well design; Investments",Risk management
139,"Learning from What We Do, and Doing What We Learn: A Learning Health Care System in Action","Different models of learning health systems are emerging. At Vanderbilt University Medical Center, the Learning Health Care System (LHS) Platform was established with the goal of creating generalizable knowledge. This differentiates the LHS Platform from other efforts that have adopted a quality improvement paradigm. By supporting pragmatic trials at the intersection of research, operations, and clinical care, the LHS Platform was designed to yield evidence for advancing content and processes of care through carefully designed, rigorous study. The LHS Platform provides the necessary infrastructure and governance to leverage translational, transdisciplinary team science to inform clinical and operational decision making across the health system. The process transforms a clinical or operational question into a research question amenable to a pragmatic trial. Scientific, technical, procedural, and human infrastructure is maintained for the design and execution of individual LHS projects. This includes experienced pragmatic trialists, project management, data science inclusive of biostatistics and clinical informatics, and regulatory support. Careful attention is paid to stakeholder engagement, including health care providers and the community. Capturing lessons from each new study, the LHS Platform continues to mature with plans to integrate implementation science and to complement clinical and process outcomes with cost and value considerations. The Vanderbilt University Medical Center LHS Platform is now a pillar of the health care system and leads the evolving culture of learning from what we do and doing what we learn. © 2021 Lippincott Williams and Wilkins. All rights reserved. Learning from What We Do, and Doing What We Learn: A Learning Health Care System in Action  Academic Medical Centers; Humans; Learning Health System; Models, Organizational; Pragmatic Clinical Trials as Topic; Problem-Based Learning; Quality Improvement; Tennessee; human; nonbiological model; organization and management; problem based learning; procedures; randomized controlled trial (topic); Tennessee; total quality management; university hospital",Strategic alignment
140,A Nash bargaining solution for a multi period competitive portfolio optimization problem: Co-evolutionary approach,"This study focuses on proposing a Nash bargaining model to solve a novel multi-period competitive portfolio optimization problem for large investors in the stock market who want to maximize their terminal wealth while taking into account competitors' profits. In this study, a Competitive Portfolio Model (CPM) is developed in accordance with the Cournot competition principle for a static, non-cooperative, and non-zero-sum game with complete information. Transaction costs, risk-free assets and cash are also included to match real-world conditions. Also, three criteria including the average value at risk, the mean absolute semi-deviation, and entropy are considered to control the investment risk in the model. Moreover, due to common constraints between players (free floating shares of risky assets) in stock markets, this study falls into the category of Generalized Nash Equilibrium Problems (GNEP). Therefore, to overcome the problem, a Cooperative Co-evolutionary Algorithm (CCA) based on Particle Swarm Optimization (PSO) is customized and used. A few experimental tests and a numerical example with descriptive analytics (using real data from two large mutual funds who invest in the Iranian Stock Exchange market) are used to evaluate the feasibility of the proposed model and the efficiency of the design algorithm. After solving the model, for each time period, investors' trading strategies (trading signals and stock volume in their portfolio) are determined. The results show that the volume of transactions due to the market power of an investor has a significant effect on the terminal wealth of competitors. Also, the results of sensitivity analysis show that profit-making is inversely related to the degree of risk aversion of investors. © 2021 A Nash bargaining solution for a multi period competitive portfolio optimization problem: Co-evolutionary approach Co-evolutionary algorithm; Generalized Nash Equilibrium Problem; Multi-Period portfolio problem; Nash bargaining model Commerce; Costs; Electronic trading; Financial markets; Game theory; Investments; Numerical methods; Particle swarm optimization (PSO); Profitability; Risk assessment; Sensitivity analysis; Value engineering; Co-evolutionary approach; Coevolutionary algorithms; Generalized Nash equilibrium problems; Multi-period; Multi-period portfolio problem; Nash bargaining model; Nash bargaining solution; Optimization problems; Portfolio optimization; Terminal wealth; Computation theory",Strategic alignment
141,Asymptotic theory of principal component analysis for time series data with cautionary comments,"Principal component analysis (PCA) is a most frequently used statistical tool in almost all branches of data science. However, like many other statistical tools, there is sometimes the risk of misuse or even abuse. In this paper, we highlight possible pitfalls in using the theoretical results of PCA based on the assumption of independent data when the data are time series. For the latter, we state with proof a central limit theorem of the eigenvalues and eigenvectors (loadings), give direct and bootstrap estimation of their asymptotic covariances, and assess their efficacy via simulation. Specifically, we pay attention to the proportion of variation, which decides the number of principal components (PCs), and the loadings, which help interpret the meaning of PCs. Our findings are that while the proportion of variation is quite robust to different dependence assumptions, the inference of PC loadings requires careful attention. We initiate and conclude our investigation with an empirical example on portfolio management, in which the PC loadings play a prominent role. It is given as a paradigm of correct usage of PCA for time series data. © 2021 Royal Statistical Society. Asymptotic theory of principal component analysis for time series data with cautionary comments bootstrap; inference; limiting distribution; PCA; portfolio management; time series ",Monitoring and control
142,"Optimization of Regulatory Economic-Capital Structured Portfolios: Modeling Algorithms, Financial Data Analytics, and Reinforcement Machine Learning in Emerging Markets","This chapter examines from a regulatory portfolio management standpoint the application of liquidity-adjusted risk modeling techniques in obtaining optimal and investable economic-capital structures for the Gulf Cooperation Council (GCC) stock markets. The observed market-microstructures patterns and the obtained empirical results are quite interesting and promising for practical optimization techniques, portfolio management purposes, and operations research models in financial institutions management, particularly in the wake of the aftermaths of the 2007–2009 financial crisis. The proposed quantitative portfolio management techniques and optimization algorithms can have important uses and applications in expert systems, financial data analytics, machine learning, smart financial functions, and financial technology (FinTech) in big data environments. Likewise, it can aid in the development of regulatory technology (RegTech) for the global financial services industry. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG. Optimization of Regulatory Economic-Capital Structured Portfolios: Modeling Algorithms, Financial Data Analytics, and Reinforcement Machine Learning in Emerging Markets Al Janabi Model; Economic-Capital; Emerging Markets; Financial Crisis; Financial Data Analytics; Financial Risk Management; GARCH-in-mean; GCC Financial Markets; Liquidity Risk; Liquidity-Adjusted Value-at-Risk (LVaR); Optimization; Portfolio Management; Regulations; Reinforcement Machine Learning ",Strategic alignment
143,From Stream Flows to Cash Flows: Leveraging Evolutionary Multi-Objective Direct Policy Search to Manage Hydrologic Financial Risks,"Hydrologic variability can present severe financial challenges for organizations that rely on water for the provision of services, such as water utilities and hydropower producers. While recent decades have seen rapid growth in decision-support innovations aimed at helping utilities manage hydrologic uncertainty for multiple objectives, support for managing the related financial risks remains limited. However, the mathematical similarities between multi-objective reservoir control and financial risk management suggest that the two problems can be approached in a similar manner. This paper demonstrates the utility of Evolutionary Multi-Objective Direct Policy Search for developing adaptive policies for managing the drought-related financial risk faced by a hydropower producer. These policies dynamically balance a portfolio, consisting of snowpack-based financial hedging contracts, cash reserves, and debt, based on evolving system conditions. Performance is quantified based on four conflicting objectives, representing the classic tradeoff between “risk” and “return” in addition to decision-makers’ unique preferences toward different risk management instruments. The dynamic policies identified here significantly outperform static management formulations that are more typically employed for financial risk applications in the water resources literature. Additionally, this paper combines visual analytics and information theoretic sensitivity analysis to improve understanding about how different candidate policies achieve their comparative advantages through differences in how they adapt to real-time information. The methodology presented in this paper should be applicable to any organization subject to financial risk stemming from hydrology or other environmental variables (e.g., wind speed, insolation), including electric utilities, water utilities, agricultural producers, and renewable energy developers. © 2021. American Geophysical Union. All Rights Reserved. From Stream Flows to Cash Flows: Leveraging Evolutionary Multi-Objective Direct Policy Search to Manage Hydrologic Financial Risks direct policy search; financial risk; global sensitivity analysis; hydropower; reservoir control; water resources Decision making; Decision support systems; Electric utilities; Information theory; Reservoirs (water); Risk assessment; Risk management; Sensitivity analysis; Stream flow; Water resources; Wind; Cash flow; Decision supports; Direct policy search; Evolutionary Multi-objectives; Financial risks; Global sensitivity analysis; Rapid growth; Reservoir controls; Water utility; Waters resources; decision support system; financial system; hydrology; reservoir; streamflow; water planning; water use; Hydroelectric power",Risk management
144,Automation and Digitalization of Polymerflood Viscosity Surveillance Through Machine Learning- A Case Study from Marmul,"Since its commencement in 2010, Marmul polymer EOR has been one of the worldwide successful full field applications. One of the key success factors for the project is maintainingwellhead viscosity at the target, which has been monitored by daily selective wellhead sampling. However, daily sampling covers only 7% of the polymer injectors. Recently, a digitalization project to enhance viscosity monitoring was successfully completed. One of the outcomes is utilizing the digital data available in field to have a live viscosity of all polymer injectors using an empirical power law model along with a calibration factor. Machine learning will handle any deviation of these readings by a well-established sampling program to continually re-calibrate the model.In this paper, the approach and outcomes of this projectare shared. Two polymer injectors are selected as a demonstration of the concept and main outcomes. Statistical evaluation was used to initially select the determining process parameters such as wellhead concentration, flowrate, tubing-head pressure, and tubing-head temperature. It has been concluded that wellhead polymer concentration is highly correlated to measured wellhead viscosity. The measured viscosities in the last two years (2020 and 2021) for each well were divided into; a training set (~65%) and a test set (~35%). The training set is used to calculate the calibration factor, while the test set is used to validate model predictions. Out of 415 date points, the average viscosity of polymer injectors MMPI-1 and MMPI-2 are 20.7 and 23.1 cP, respectively. The standard deviation of the measurements of injectors MMPI-1 and MMPI-2 are 3.3 and 4.8 cP, respectively. Viscosity was correlated to wellhead concentration by a power law model with experimentally obtained constant and law's exponent. Using the training set, a tuning parameter, α, was appliedwith criteria of minimummean absolute error (MAE) for each injector. α determined of MMPI-1 and MMPI-2 is 0.915 and 0.981, respectively. The model resulted in good predictions with an average MAE of around 20%. Furthermore, the model proved to be robust and reliable to be applied for live viscosity readings of all Marmul polymer injectors. Machine learning is essential for future tuning of the model for all polymer injectors in Marmul based established program of wellhead measurements. The outcomes of this digitalization and automation step in polymerflooding has demonstrated significant, positive impact on optimization of chemicals, resources, and the overall reservoir management. This work is setting another milestone in the utilization of data analytics and digitalization of fullfield polymer EOR. Machine learning coupled with excellent metering and data streaming have shown added value to overall project management. This is more critical with the shift towards agile work environment and net zero. Significant opportunities have been already realized as an outcome of this project such as quantification of polymer overdosage, which triggered a work in progress to reduce any value-eroding polymer dosage. In Marmul, the improved surveillance of wellhead viscosity and timely optimization of polymer dosage have already positively impacted project economics, GHG and HSE. Copyright © 2022, Society of Petroleum Engineers. Automation and Digitalization of Polymerflood Viscosity Surveillance Through Machine Learning- A Case Study from Marmul  Calibration; Machine learning; Tubing; Wellheads; Absolute error; Calibration factors; Case-studies; Field application; Machine-learning; Optimisations; Polymer dosage; Power law model; Test sets; Training sets; Viscosity",Monitoring and control
145,Financial Network Connectedness and Systemic Risk During the COVID-19 Pandemic,"The COVID-19 pandemic causes a huge number of infections. The outbreak of COVID-19 has not only caused substantial healthcare impacts, but also affected the world economy and financial markets. In this paper, we study the effect of the COVID-19 pandemic on financial market connectedness and systemic risk. Specifically, we test dynamically whether the network density of pandemic networks constructed by the number of COVID-19 confirmed cases is a leading indicator of the financial network density and portfolio risk. Using rolling-window Granger-causality tests, we find strong evidence that the pandemic network density leads the financial network density and portfolio risk from February to April 2020. The findings suggest that the COVID-19 pandemic may exert significant impact on the systemic risk in financial markets. © 2021, The Author(s). Financial Network Connectedness and Systemic Risk During the COVID-19 Pandemic Financial contagion; Granger causality; Network density; Pandemic network; Risk analytics ",Risk management
146,Developing strategies to prioritise and mitigate the risks occurring in project management practices for an agile organisation - a perspective of Industry 4.0 technologies,"The traditional methods of PMP lacked the capability of dealing with fast-moving, complex and varying environments. Later, agility emerged as a concept of a faster and more flexible way of working, leading towards maximising the positive outcomes and enhancing the performance of the organisation in a highly volatile environment. There are a few risks associated with agility improvement of organisations, which have been identified and prioritised in this research for strategising their mitigation. Various risks that agile organisations encounter were identified from the literature and also from the experts' survey. The identified risks were further analysed using total-ISM and fuzzy-MICMAC analysis. Lastly, strategies were developed to mitigate the risks by leveraging Industry 4.0 technologies. Risk of replicating agile models, insufficient capability building, and intellectual property risks were identified as key risks leading to other identified risks. These risks need to be mitigated to have a flawless agile organisation. © 2022 Inderscience Enterprises Ltd. Developing strategies to prioritise and mitigate the risks occurring in project management practices for an agile organisation - a perspective of Industry 4.0 technologies agile organisation; business intelligence; Industry 4.0; project management practices; risks of agility ",Governance
147,Rule-Based Classification Based on Ant Colony Optimization: A Comprehensive Review,"The Ant Colony Optimization (ACO) algorithms have been well-studied by the Operations Research community for solving combinatorial optimization problems. A handful of researchers in the Data Science community have successfully implemented various ACO methodologies for rule-based classification. This family of ACO algorithms is referred to as AntMiner algorithms. Due to the flexibility of the framework, and the availability of alternative strategies at the modular level, a systematic review on the AntMiner algorithms can benefit the broader community of researchers and practitioners interested in highly interpretable classification techniques. In this paper, we provided a comprehensive review of each module of the AntMiner algorithms. Our motivation is to provide insight into the current practices and future research scope in the context of the rule-based classification. Our discussions address ACO methodologies, rule construction strategies, candidate selection metrics, rule quality evaluation functions, rule pruning strategies, methods to address continuous attributes, parameter selection, and experimental settings. This review also reports a summary of real-life implementations of the rule-based classifiers in diverse domains including medical, genetics, portfolio analysis, geographic information system (GIS), human-machine interaction (HMI), autonomous driving, ICT, quality, and reliability engineering. These implementations demonstrate the potential application domains that can be benefitted from the methodological contributions to the rule-based classification technique. © 2022 Sayed Kaes Maruf Hossain et al. Rule-Based Classification Based on Ant Colony Optimization: A Comprehensive Review  ",Strategic alignment
148,Machine learning and optimization based decision-support tool for seed variety selection,"Every year agribusinesses develop and market new seed varieties with traits desirable for different planting environments. When agribusinesses experiment the new varieties at different farms, data is generated about the performance of these new seed varieties. However, farmers do not have a decision support tool to process the vast amount of yield performance data to make an informed seed variety selection decision for their farm. An informed decision requires accurate estimation of yield performances of seed varieties on the targeted farmland and balancing trade-offs between the expected yield and the risk associated with the seed varieties selected to grow. This research uses a real data set provided by Syngenta—an agribusiness—to create a decision-support tool. The data set used in this research contains yield information of different soybean varieties experimented at different farms located in the Midwest of the US, as well as information on location, soil, and weather conditions prevailing in those farms. In addition to this data, we also surveyed soybean farmers to understand their preferences and current practices in choosing seed varieties to grow in their farms. We are the first to capture and document farmers’ preferences and practices in selecting and growing soybean varieties. The data collected from the survey enabled us to compare the results emerging from the proposed methodology with the status quo practices. Using the Syngenta data and survey responses, this paper proposes an analytics framework that integrates machine learning, clustering, simulation, and portfolio optimization to optimally select soybean varieties to grow at the target farm. We choose a machine learning model, which simulates the yield performance of soybean varieties under different plausible weather scenarios derived from the neighborhood of the target farm. The simulated yields are then used to estimate parameters in a portfolio optimization formulation that selects the optimal portfolio of seed varieties to grow at the target farm. The main methodological contribution of this research is in the development of an approach that integrates machine learning, clustering, simulation, and portfolio optimization to help farmers make an important decision. Specifically, we introduce a novel data-driven simulation-based approach to estimate the parameters needed to solve a portfolio optimization problem. Our analysis indicates that an average farmer will gain as much as $177,369 per year in revenue by utilizing the analytics framework introduced in this research. The methodology developed in this research can be applied to variety selection decisions for other crops and influence farming practice positively. By embracing the machine learning and analytics powered framework introduced in this paper, agribusinesses can position themselves as the innovation leader and create business value by unleashing the potential of the scientific discoveries of agronomy to offer tailored farming decision support to individual farmers. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature. Machine learning and optimization based decision-support tool for seed variety selection Data science; Food shortage; Machine learning; Portfolio optimization; Simulation and optimization; Soybean ",Strategic alignment
149,Laplacian Echo-State Networks for production analysis and forecasting in unconventional reservoirs,"Production data analysis for low permeability shale reservoirs is crucial in characterizing flow regimes and reservoir properties, and the forecasting of production is essential for portfolio and reservoir management. However, traditional methods have failed due to incorrect physics or complicated convolution from the well control history. In this research, we provide a physics-assisted analytics workflow using Laplacian Eigenmaps Coupled Echo-State Network (LEESN) to facilitate and accelerate the analysis of noisy historical production data. Pressure-rate deconvolution is an ill-posed, complex time-series problem. When using the traditional Echo-State Network (ESN), the number of training sets is less than the number of neurons. To solve this problem, we apply LEESN to first deconvolve noisy variable-pressure variable-rate histories into smooth constant-pressure rate responses. The physics-based training features and training algorithm provide additional benefits in addition to the analytic approach by honoring transient flow physics. After training, the constant-pressure rate response can be predicted and used for reservoir characterization, and the trained model could be further used for production and EUR forecasting through long-term rate predictions to the economic limit. The proposed workflow was first validated by synthetic cases where the production data were obtained through simulation. The short-term flow rate history was obtained by specifying highly variable controlling pressures. We also added artificial white Gaussian noise to mimic measured signals collected in the field and input this information into LEESN for deconvolution. The constant-pressure rate response was generated after training to determine flow regimes and properties such as permeability using a traditional transient testing specialized plot. All outcomes from the analytics approach were validated by comparison against the input data of the synthetic simulation model. The advantages of the analytics approach were maintained with a moderate variation of noisy pressure-rate signals. For production forecasting, both the trained analytics model and simulator were used to predict for an extended period, and the results indicated good agreement between the response predictions. We performed a further sensitivity analysis on important parameters such as the training scale as well as the capability of Laplacian Eigenmaps handling moderate noise in training data. The comparison between the model predictions and simulation data showed significantly increased accuracy in production estimates. The efficacy was further demonstrated from additional single-phase and multiphase synthetic and field cases. This study shows that the LEESN approach is a powerful alternative to interpret pressure-rate-time information from production data. The discussion and comparison of LEESN with other traditional production analysis and forecast methods are included as well. Deconvolved pressure-rate data greatly enhances traditional rate-transient analysis (RTA) used to characterize reservoir parameters, and the trained model enables engineers to predict future production even with noisy, highly-variable production histories. The robustness of the proposed analytics methodology is strengthened by coupling the training features with transient flow physics and provides a unique approach for production analysis and forecasting for unconventional reservoirs. © 2021 Elsevier B.V. Laplacian Echo-State Networks for production analysis and forecasting in unconventional reservoirs Deconvolution; Echo-state networks; Machine learning; Production data analysis; Production forecasting Convolution; Data handling; Gaussian noise (electronic); Information analysis; Laplace transforms; Learning systems; Petroleum reservoir engineering; Reservoir management; Sensitivity analysis; Constant pressures; Deconvolutions; Echo state networks; Laplacian eigenmap; Machine-learning; Production analysis; Production data; Production data analysis; Production forecasting; Rate response; algorithm; analytical framework; deconvolution; forecasting method; hydrocarbon reservoir; machine learning; oil production; time series; Forecasting",Monitoring and control
150,Developing a framework for evaluating construction project safety levels and optimal cost allocation to safety influential factors,"Purpose: This study aimed to explain how a framework could be developed for (1) the preliminary estimation of project safety level (PSL) in current projects, (2) the estimation of the maximum possible PSL using limited financial resources and (3) the estimation of the minimum financial resources required for reaching a specific PSL. Design/methodology/approach: The data of 95 steel structural building projects were collected via a questionnaire to evaluate the proposed framework for the Iranian construction industry. Based on unofficial local construction statistics and literature reviews, six safety influential factors (SIFs) were selected to which a cost could be assigned. The costs associated with various levels were also determined for each SIF through literature reviews and expert interviews. A multiple linear regression (MLR) model was developed as a predictive model to determine PSL for future projects based on the data of previous projects. Moreover, linear programming (LP) was applied to take modeling constraints and project conditions into account. Findings: The results demonstrated the impacts of all the factors on PSL and the model's potential for the preliminary estimation of PSL using SIFs. The results also indicated that a higher PSL could be achieved by optimizing the allocation of financial resources to each SIF. Originality/value: This study contributes to the existing body of knowledge by developing a step-by-step framework to identify an optimal safety cost allocation (OSCA) to achieve the maximum possible PSL using a limited safety budget and considering the data of similar projects. The main objective was to promote project safety, decrease construction site injuries and fatalities and help local construction industries exploit potential financial advantages. © 2021, Emerald Publishing Limited. Developing a framework for evaluating construction project safety levels and optimal cost allocation to safety influential factors Construction industry; Construction safety; Linear programming; Multiple linear regression; Project management; Project safety level; Safety budget; Safety influential factors Budget control; Construction industry; Cost accounting; Finance; Linear programming; Linear regression; Predictive analytics; Stress intensity factors; Construction projects; Construction statistics; Design/methodology/approach; Financial resources; Influential factors; Iranian construction industries; Multiple linear regression models; Predictive modeling; Accident prevention",Monitoring and control
151,Amelioration of Big Data Analytics by Employing Big Data Tools and Techniques,"This chapter describes how in the digital data era, a large volume of data became accessible to data science engineers. With the reckless growth in networking, communication, storage, and data collection capability, the Big Data science is quickly growing in each engineering and science domain. This paper aims to study many numbers of the various analytics ways and tools which might be practiced to Big Data. The important deportment in this paper is step by step process to handle the large volume and variety of data expeditiously. The rapidly evolving big data tools and Platforms have given rise to numerous technologies to influence completely different Big Data portfolio.In this paper, we debate in an elaborate manner about analyzing tools, processing tools and querying tools for Big datahese tools used for data analysis Big Data tools utilize numerous tasks, like Data capture, storage, classification, sharing, analysis, transfer, search, image, and deciding which might also apply to Big data. © 2022 by IGI Global. All rights reserved. Amelioration of Big Data Analytics by Employing Big Data Tools and Techniques  ",Stakeholder management
152,A Novel Modeling Technique for the Forecasting of Multiple-Asset Trading Volumes: Innovative Initial-Value-Problem Differential Equation Algorithms for Reinforcement Machine Learning,"Liquidity risk arises from the inability to unwind or hedge trading positions at the prevailing market prices. The risk of liquidity is a wide and complex topic as it depends on several factors and causes. While much has been written on the subject, there exists no clear-cut mathematical description of the phenomena and typical market risk modeling methods fail to identify the effect of illiquidity risk. In this paper, we do not propose a definitive one either, but we attempt to derive novel mathematical algorithms for the dynamic modeling of trading volumes during the closeout period from the perspective of multiple-Asset portfolio(s), as well as for financial entities with different subsidiary firms and multiple agents. The robust modeling techniques are based on the application of initial-value-problem differential equations technique for portfolio selection and risk management purposes. This paper provides some crucial parameters for the assessment of the trading volumes of multiple-Asset portfolio(s) during the closeout period, where the mathematical proofs for each theorem and corollary are provided. Based on the new developed econophysics theory, this paper presents for the first time a closed-form solution for key parameters for the estimation of trading volumes and liquidity risk, such as the unwinding constant, half-life, and mean lifetime and discusses how these novel parameters can be estimated and incorporated into the proposed techniques. The developed modeling algorithms are appealing in terms of theory and are promising for practical econophysics applications, particularly in developing dynamic and robust portfolio management algorithms in light of the 2007-2009 global financial crunch. In addition, they can be applied to artificial intelligence and machine learning for the policymaking process, reinforcement machine learning techniques for the Internet of Things (IoT) data analytics, expert systems in finance, FinTech, and within big data ecosystems. © 2022 Mazin A. M. Al Janabi. A Novel Modeling Technique for the Forecasting of Multiple-Asset Trading Volumes: Innovative Initial-Value-Problem Differential Equation Algorithms for Reinforcement Machine Learning  Data Analytics; Electronic trading; Expert systems; Financial markets; Initial value problems; Investments; Multi agent systems; Reinforcement learning; Risk management; Risk perception; Clear cuts; Differential equation algorithm; Econophysicss; Initial-value problem; Liquidity risk; Machine-learning; Market price; Mathematical descriptions; Modelling techniques; Trading volumes; Commerce",Strategic alignment
153,Estimation with uncertainty via conditional generative adversarial networks,"Conventional predictive Artificial Neural Networks (ANNs) commonly employ deterministic weight matrices; therefore, their prediction is a point estimate. Such a deterministic nature in ANNs causes the limitations of using ANNs for medical diagnosis, law problems, and portfolio management in which not only discovering the prediction but also the uncertainty of the prediction is essentially required. In order to address such a problem, we propose a predictive probabilistic neural network model, which corresponds to a different manner of using the generator in the conditional Generative Adversarial Network (cGAN) that has been routinely used for conditional sample genera-tion. By reversing the input and output of ordinary cGAN, the model can be successfully used as a predictive model; moreover, the model is robust against noises since adversarial training is employed. In addition, to measure the uncertainty of predictions, we introduce the entropy and relative entropy for regression problems and classification problems, respectively. The proposed framework is applied to stock market data and an image classification task. As a result, the proposed framework shows superior estimation performance, especially on noisy data; moreover, it is demonstrated that the proposed framework can properly estimate the uncertainty of predictions. © 2021 by the authors. Licensee MDPI, Basel, Switzerland. Estimation with uncertainty via conditional generative adversarial networks Adversarial learning; Deep learning; Generative adversarial network; Portfolio management; Probability estima-tion; Risk estimation Neural Networks, Computer; Uncertainty; Diagnosis; Electronic trading; Entropy; Financial markets; Forecasting; Investments; Medical problems; Predictive analytics; Uncertainty analysis; Adversarial networks; Estimation performance; Input and outputs; Portfolio managements; Predictive modeling; Probabilistic neural network models; Regression problem; Relative entropy; uncertainty; Neural networks",Monitoring and control
154,Deep-learning-based visual data analytics for smart construction management,"Visual data captured at construction sites is a rich source of information for the day-to-day operation of construction projects. The development of deep-learning-based methods has demonstrated their capabilities in analyzing complex visual data and inferring valuable insights. Recent applications of these methods in construction have also shown promising performance in making the construction management process smarter. To understand the current research trends and to highlight future research directions, this study reviews state-of-the-art deep-learning applications on visual data analytics in the context of construction project management. This in-depth review identifies six major fields and fifty-two subfields of construction management where deep-learning-based visual data analytics have been applied. It also proposes a generalized workflow for applying deep-learning-based visual data analytics methods for solving construction management problems. In addition, the study highlights three future research directions where deep-learning-based visual data analytics can be applied on relatively less explored 3D visual data. © 2021 Elsevier B.V. Deep-learning-based visual data analytics for smart construction management 3D visual data; construction management; Deep learning; generalized workflow; visual data analytics Deep learning; Information management; 3D visual data; Construction management; Construction sites; Data analytics; Deep learning; Future research directions; Generalized workflow; Sources of informations; Visual data; Visual data analytic; Project management",Strategic alignment
155,Professional Development of a Production & Facilities Generalist as a Jack-of-All-Trades,"This paper looks at the Professional Development of a Production and Facilities (P&F) Generalist (""a Jack-of-All-Trades""). It discusses the criticality of Continuous Professional Development (CPD) and how to take advantage of the SPE Volunteer Opportunities and Programs (Training Services, Competency Development Matrices, and the Competency Management Tool (CMT)). Most professionals in the SPE Production & Facilities Community graduate with variety of Science, Technology, Engineering and Mathematics (STEM) degrees and many pick up applied Petroleum Engineering and Project Management knowledge on the job. The SPE Competency Matrices provide a framework for developing: 1. A minimum Breadth of Knowledge either as an undergraduate or during the first year of employment in the Exploration & Production (E&P) Sector of the Energy Business. 2. A minimum Depth of Knowledge, acquired over the first 4-5 years or so, in General Engineering and their chosen (or assigned) Technical Function, in this case, either Production & Operations (P&O) or Projects, Facilities & Construction (P, F & C). The mid-career period is often a time of significant turbulence and confusion with many generalists testing-out or being assigned to a variety of roles, such as Subject Matter Experts, Team Leads, Supervisors, and Project Managers, or with other functions and/or new companies before settling on a longer-term career path. Traditional paths for knowledge development are further complicated by an increased focus on Data Science & Engineering Analysis (DSEA), Sustainability, Decarbonization, and the Energy Transition, along with a need to reinforce our Social License to Operate. The largest corporations and major training organizations, including the SPE, provide Competency Management Tools to help practitioners identify proficiency gaps and training requirements for a current or anticipated future assignments. Similarly, the SPE provides ongoing support not only with Business Management Leadership (Soft Skills) Training but also through the Technical Communities and Sections. In the final analysis, most P&F Generalists are ""value-creation professionals"" who just happen to work in the Upstream Oil & Gas Industry and have working knowledge of the acronyms and terminologies typically used in our profession. Copyright © 2022, Society of Petroleum Engineers. Professional Development of a Production & Facilities Generalist as a Jack-of-All-Trades  Employment; Engineering education; Gas industry; Gasoline; Human resource management; Personnel training; Petroleum prospecting; Professional aspects; Project management; Competencies managements; Competency development; Continuous professional development; Management tool; matrix; Production facility; Professional development; Science technologies; Service competencies; Training services; Commerce",Financial management
156,Maximum drawdown distributions: The cross-asset dimension,"Potential severe drawdowns are a central concern of investors and pose a risk often inadequately considered in the risk profiling or portfolio optimization process. In this article, conditional expected drawdowns are extended from a multi-asset perspective by introducing the conditional expected cross-maximum drawdown measure. The dimensions of magnitude and time are combined to describe tail risk dynamics across asset classes. Beyond extending the risk analytics toolbox, approaches are introduced to explicitly and computational efficiently incorporate this perspective in the optimization process. This puts investors in the position to significantly improve the tails of the maximum drawdown distribution of their strategic asset allocation. © 2021 Portfolio Management Research. All Rights Reserved. Maximum drawdown distributions: The cross-asset dimension  ",Strategic alignment
157,A Fuzzy Prescriptive Analytics Approach to Power Generation Capacity Planning,"This study examines the long-term energy capacity investment problem of a power generation company (GenCo), considering the drought threat posed by climate change in hydropower resources in Turkey. The mid-term planning decisions such as maintenance and refurbishment scheduling of power plants are also considered in the studied investment planning problem. In the modeled electricity market, it is assumed that GenCos conduct business in uncertain market conditions with both bilateral contracts (BIC) and day-ahead market (DAM) transactions. The problem is modeled as a fuzzy mixed-integer linear programming model with a fuzzy objective and fuzzy constraints to handle the imprecisions regarding both the electricity market (e.g., prices) and environmental factors (e.g., hydroelectric output due to drought). Bellman and Zadeh’s max-min criteria are used to transform the fuzzy capacity investment model into a model with a crisp objective and constraints. The applicability of methodology is illustrated by a case study on the Turkish electric market in which GenCo tries to find the optimal power generation investment portfolio that contains five various generation technologies alternatives, namely, hydropower, wind, conventional and advanced combined-cycle natural gas, and steam (lignite) turbines. The results show that wind turbines with low marginal costs and steam turbines with high energy conversion efficiency are preferable, compared with hydroelectric power plant investments when the fuzziness in hydroelectric output exists (i.e., the expectation of increasing drought conditions as a result of climate change). Furthermore, the results indicate that the gas turbine investments were found to be the least preferable due to high gas prices in all scenarios. © 2022 by the authors. Licensee MDPI, Basel, Switzerland. A Fuzzy Prescriptive Analytics Approach to Power Generation Capacity Planning climate change; fuzzy mathematical programming; generation investment planning; maintenance and refurbishment scheduling; uncertainty Climate change; Conversion efficiency; Costs; Drought; Electric industry; Gas turbines; Hydroelectric power; Hydroelectric power plants; Integer programming; Power markets; Scheduling; Steam turbines; Analytic approach; Capacity investment; Capacity planning; Fuzzy mathematical programming; Generation investment; Generation investment planning; Investment planning; Maintenance and refurbishment scheduling; Power generation capacities; Uncertainty; Investments",Capacity management
158,"28th International Conference Systems Engineering, ICSEng 2021","The proceedings contain 44 papers. The special focus in this conference is on Systems Engineering. The topics include: Mobile Game Development with Spatially Generated Reverberation Sound; use of Project-Based Learning and Living Personas to Train Future Designers of Interactive Applications; emerging Technologies in Smart Classroom Education; preface; reducing Compound Degree for Optimum Linear Decomposition of Symmetric Index Generation Function; retail Sales Forecasting in the Presence of Promotional Periods; evaluation of Selected Artificial Intelligence Technologies for Innovative Business Intelligence Applications; towards Business Cost Mining: Considering Business Process Reliability; GMDH-Type Neural Networks for Predicting Financial Time Series: A Study of Informational Efficiency of Stock Markets; mean-Reverting Portfolio Optimization via a Surrogate Risk Measure - Conditional Desirability Value at Risk; blockchain-Based Smart Contracts Use for Photovoltaic Energy Trade Transactions; chaotic Chua’s Circuit’s Parameter Estimation Using Composite Identifier and Indirect Adaptive Output Regulation; combined State Merging and Splitting Procedure for Low Power Implementations of Finite State Machines; towards an Extension and Normalization of the Random Vector Anisotropy Magnitude; non-linear Correlation Based Approach to the Identification of Maximally Stationary Systems; LED Grow Light for Optimization of Chlorophyll Excitation Ratio; development of Logistics Models for Oil Cargo Transportation to Reduce Logistics Costs and Improve Wagon Mileage; investigating the Closer Running of Rail Vehicles for Better Network Utilisation; understanding Manufacturing Processes on Basis of Visualized Machine and Sensor Data; optimizing Maintenance Cost of Uniform Rolling Stock by Scheduling Algorithms; utilizing Evidence in Asset Management in the Era of Industry 4.0 and Artificial Intelligence. 28th International Conference Systems Engineering, ICSEng 2021  ",Risk management
159,Predicting Under- and Overperforming SKUs within the Distribution–Market Share Relationship,"This research presents a retail analytics application which uses machine learning (ML) to identify and predict under- and overperforming consumer packaged goods (CPGs) using retail scanner data. Essential to measuring market performance at the SKU level is the relationship between distribution and market share (the velocity curve). We validate that ML can reproduce the velocity curve, and ML is further used to predict underperforming, in-line performing, and overperforming SKUs relative to the velocity curve, based on a range of variables (SKU features) at a point in time. Our ML approach can correctly predict 83% of SKUs as under-, in-line-, or overperforming based on their characteristics. The research analyzes 9,321 SKUs of 2,565 brands across seven product categories of CPGs which were sold in 8,117 stores from 49 different retail chains of five different retail channels located in the US states of California, New York, Texas, and Wisconsin. The retail stores comprise convenience stores, drug stores, food stores, liquor stores, and mass merchandise retail stores. The data is Nielsen retail store scanner data for the calendar year 2014. The relationship between distribution and market share is a market-wide proxy for the ratio of relative sales in a category to, for example, aggregate shelf space, a key retail productivity metric. We further find indications that the distribution of SKUs across different store sizes, the stores’ category specialization, the line length of the brands, the overall performance of the parent brand, and sales consistency are the most important characteristics for the prediction of market share performance beyond the velocity curve. The methods and results presented will help CPG marketers (suppliers and retailers) understand which SKUs are under-, in-line-, or overperforming and the potential factors contributing to that performance. Optimizing assortments and portfolios is essential to decrease failure rates of individual SKUs. ML approaches can evolve to complementary support tools for such management problems. © 2021 New York University Predicting Under- and Overperforming SKUs within the Distribution–Market Share Relationship Category management; Machine learning; Portfolio management; Predictive analytics; SKU performance ",Strategic alignment
160,A Novel Implementation of Siamese Type Neural Networks in Predicting Rare Fluctuations in Financial Time Series,"Stock trading has tremendous importance not just as a profession but also as an income source for individuals. Many investment account holders use the appreciation of their portfolio (as a combination of stocks or indexes) as income for their retirement years, mostly betting on stocks or indexes with low risk/low volatility. However, every stock-based investment portfolio has an inherent risk to lose money through negative progression and crash. This study presents a novel technique to predict such rare negative events in financial time series (e.g., a drop in the S&P 500 by a certain percent in a designated period of time). We use a time series of approximately seven years (2517 values) of the S&P 500 index stocks with publicly available features: the high, low and close price (HLC). We utilize a Siamese type neural network for pattern recognition in images followed by a bootstrapped image similarity distribution to predict rare events as they pertain to financial market analysis. Extending on literature about rare event classification and stochastic modeling in financial analytics, the proposed method uses a sliding window to store the input features as tabular data (HLC price), creates an image of the time series window, and then uses the feature vector of a pre-trained convolutional neural network (CNN) to leverage pre-event images and predict rare events. This research does not just indicate that our proposed method is capable of distinguishing event images from non-event images, but more importantly, the method is effective even when only limited and strongly imbalanced data is available. © 2022 by the authors. Licensee MDPI, Basel, Switzerland. A Novel Implementation of Siamese Type Neural Networks in Predicting Rare Fluctuations in Financial Time Series Convolutional Neural Network (CNN); Image processing; Rare event prediction; Siamese neural networks; Time series ",Strategic alignment
161,AHP–TOPSIS methodology for stock portfolio investments,"This paper presents a methodology for making decisions in the stock market using the AHP-TOPSIS multi-criteria technique. The problem is related to the stock market’s investment process considering the criteria of liquidity, risk, and profitability. The proposed methodology includes integrating economic and financial theories of investment in equity portfolios with the AHP-TOPSIS multi-criteria technique, which allows for evaluating a finite number of alternatives hierarchically under qualitative and quantitative criteria. The methodology has been tested in a real case of selecting a portfolio of high and medium marketability stocks for the Colombian market from April 2012 to April 2017. The computational results show the importance and efficiency of successfully integrating traditional equity portfolio investment criteria and multi-criteria methodologies to find an appropriate balance between profitability and risk in the investment decision-making process in shares in the Colombian stock market. The proposed methodology could be applied to other emerging markets, similar to Colombia. © 2021 by the authors. Licensee MDPI, Basel, Switzerland. AHP–TOPSIS methodology for stock portfolio investments Hierarchical analytics process; Multi-criteria method; Order of preferences; Stock market; Stock portfolio ",Strategic alignment
162,Analyzing a Data Science Online Practitioner Community: Trends and Implications for Data Science Project Management,"The overarching goal of this research was to gain an understanding of what the data science Reddit online community discussed before, during, and after COVID-19. We used a publicly available Reddit API to harvest the r/datascience subreddit first level post data. We then performed manual annotation to explore the taxonomy of trends and themes discussed by the practitioners who belonged to reddit data science community. Then, we augmented the manually annotated data using a BERT model with topic modeling. In short, the key discussion themes, in order of frequency, were: Education, Jobs, Methods (of data science), Hardware and data collection, Data visualization, and Quality. The Quality theme includes discussions on bias, transparency, and fairness. Hence, a key finding was that there were very few discussions on data science project quality, especially trying to minimize the risk of machine learning bias. As discussions on bias are not yet common, data science teams should proactively identify and address potential questions and concerns that might arise in data science projects, especially the need to increase the team's focus on potential bias and fairness. © 2022 IEEE. Analyzing a Data Science Online Practitioner Community: Trends and Implications for Data Science Project Management community of practice; CoP; Data Science; Online Communities; Project Management Data acquisition; Data Science; Data visualization; Online systems; Project management; Communities of Practice; Data collection; Data quality; IS educations; Job's method; Manual annotation; On-line communities; Science community; Science projects; Topic Modeling; Social networking (online)",Value management
163,Forecasting Undergraduate Majors: A Natural Language Approach,"Committing to a major is a fateful step in an undergraduate education, yet the relationship between courses taken early in an academic career and ultimate major issuance remains little studied at scale. Using transcript data capturing the academic careers of 26,892 undergraduates enrolled at a private university between 2000 and 2020, we describe enrollment histories by using natural-language methods and vector embeddings to forecast terminal major on the basis of course sequences beginning at college entry. We find that (a) a student’s very first enrolled course predicts their major 30 times better than random guessing and more than one-third better than majority-class voting, (b) modeling strategies substantially influence forecasting metrics, and (c) course portfolios vary substantially within majors, such that students with the same major exhibit relatively modest overlap. © The Author(s) 2022. Forecasting Undergraduate Majors: A Natural Language Approach colleges; decision-making; degree planning; descriptive analysis; higher education; information retrieval; institutional research; Jaccardian similarity; LASSO; network analysis; NLP; observational research; postsecondary education; predictive analytics; regression analyses; textual analysis; word embedding ",Strategic alignment
164,Selecting advanced analytics in manufacturing: a decision support model,"Advanced analytics offers new means by which to increase efficiency. However, real-world applications of advanced analytics in manufacturing are scarce. One reason is that the management task of selecting advanced analytics technologies (AATs) for application areas in manufacturing is not well understood. In practice, choosing AATs is difficult because a myriad of potential techniques (e.g. diagnostic, predictive, and prescriptive) are suitable for different areas in the value chain (e.g. planning, scheduling, or quality assurance). It is thus challenging for managers to identify AATs that yield economic benefit. We propose a multi-criteria decision model that managers can use to select efficient AATs tailored to company-specific needs. Based on a data envelopment analysis, our model evaluates the efficiency of each AAT with respect to cost drivers and performance across common application areas in manufacturing. The effectiveness of our decision model is demonstrated by applying it to two manufacturing companies. For each company, a customized portfolio of efficient AATs is derived for a sample of use cases. Thereby, we aid management decision-making concerning the efficient allocation of corporate resources. Our decision model not only facilitates optimal financial allocation for operations in the short-term but also guides long-term strategic investments in AATs. © 2022 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group. Selecting advanced analytics in manufacturing: a decision support model Advanced analytics; data envelopment analysis; decision model; manufacturing Decision making; Decision support systems; Efficiency; Managers; Quality assurance; Application area; Decision modeling; Decision support modelling; Economic benefits; Management tasks; Manufacturing IS; Multicriteria decision; Potential techniques; Real-world; Value chains; Data envelopment analysis",Strategic alignment
165,Human resource analytics: a review and bibliometric analysis,"Purpose: This paper aims to identify the current research trends and set the future research agenda in the area of human resource (HR) analytics by an extensive review of the existing literature. The paper aims to capture state of the art and develop an exhaustive understanding of the theoretical foundations, concepts and recent developments in the area. Design/methodology/approach: A portfolio of 125 articles collected from the Scopus database was systematically analyzed using a two-tier method. First, the evolution, current state of the literature and research clusters are identified using bibliometric techniques. Finally, using content analysis, the research clusters are studied to develop the future research agenda. Findings: Based on the bibliometric analysis, network analysis and content analysis techniques, this study provides a comprehensive review of the existing literature. The study also highlights future research themes by identifying knowledge gaps based on content analysis of research clusters. Research limitations/implications: The evolution and the current state of the HR analytics literature are presented. Some specific research questions are also provided to help future research. Originality/value: This study enriches the literature of HR analytics by integrating bibliometric analysis and content analysis to develop a more systematic and exhaustive understanding of the research area. The findings of this study may assist fellow researchers in furthering their research in the identified research clusters. © 2020, Emerald Publishing Limited. Human resource analytics: a review and bibliometric analysis Bibliometrics; HR analytics; HR metrics; Human capital analytics; Literature review; Workforce analytics ",Financial management
166,Dynamic Stop-Loss Approach for Short Term Trades using Deep Learning,"Stop-Loss strategies are often used by investors to combat negative returns by predetermining thresholds at which they should exit trades. Though existing traditional Stop-Loss mechanisms such as Fixed Stop-Loss and Trailing Stop-Loss are empirically proven to have the ability to minimize risks associated with trades, they still face serious challenges when it comes to achieving a balance between risk and return. In this study, we develop a Deep Learning model that combines the concept of Stop-Loss with the capabilities offered by Deep Neural Networks. The architecture is composed of three components where trend detection and price prediction components provide inputs to the stop price prediction component which predicts the variation of stop price. The study focuses on short term trades of minute frequency which involves analysing massive chunks of data that fluctuates rapidly within extremely short time intervals. Despite the advancements that has taken place in the context of Big Data analytics, intraday financial time series analysis has not received much academic attention. The model utilizes convolutional layers to capture spatial features along with Long Short Term Memory networks to capture temporal dependencies in price sequences. The proposed solution gives outstanding results for diverse market conditions and it works specifically well for trades that are downtrending. We evaluate our model for a portfolio that consists of five stock symbols from NASDAQ that are adequately liquid and two cryptocurrencies which are highly circulated. The model delivers a stable outcome across all symbols indicating its ability to be generalized over a range of symbols and its tolerance to diverse market conditions. Results also indicate that Stop-Loss mechanisms possess the potential to work well even in speculative markets such as the cryptocurrency market. The ability to reduce losses without compromising on opportunities to realize profits in dynamic and unstable market conditions is an important property of our Stop-Loss solution. © 2022 IEEE. Dynamic Stop-Loss Approach for Short Term Trades using Deep Learning Algorithmic Trading; CNN; Computational Finance; LSTM; Short Term Trades; Stop-Loss; Time Series Analysis Commerce; Convolutional neural networks; Data Analytics; Deep neural networks; Dynamics; Financial data processing; Financial markets; Harmonic analysis; Investments; Long short-term memory; Algorithmic trading; Computational finance; Loss mechanisms; LSTM; Market condition; Price prediction; Short term; Short term trade; Stop-loss; Time-series analysis; Time series analysis",Strategic alignment
167,Agile supply chain analytic approach: a case study combining agile and CRISP-DM in an end-to-end supply chain,"In the current competitive environment, Big Data Analytics (BDA) has become a prominent metric to reach an integrated, efficient, and effective supply chain (SC). In the literature, the BDA capabilities have been at the forefront of research in operational supply chain management (SCM), however, there has been a paucity of literature regarding its technical and organisational implementation in the industry. Hence, despite its capabilities and importance, many organisations are reluctant to adopt this promising concept in SC operations management, due to the ambiguity of its practical implementation from a technical and organisational point of view. To address this gap, this paper draws on agile project management, data mining model processing, and case study approaches to propose and test a framework for BDA organisational implementation in SCM. The feasibility of the proposed framework is illustrated and tested by a case study in an end-to-end SC within a large corporation. Our contribution lies in handling the organisational, managerial, and socio-technical challenges of BDA projects implementation in SCM. © 2022 Kedge Business School. Agile supply chain analytic approach: a case study combining agile and CRISP-DM in an end-to-end supply chain agile methodology; Big Data Analytics; CRISP-DM methodology; project management; Scrum; Supply chain management ",Value management
168,An Empirical Analysis of Risk Similarity among Major Transportation Projects Using Natural Language Processing,"Risk management is widely recognized as a best practice for public agencies to ensure the successful implementation of major transportation projects. The conventional approach to identify and evaluate project risks is dominated by getting input from subject matter experts at risk workshops. However, the uniqueness of such a risk assessment approach remains unexamined. How different are the risks among various projects? Does the risk register reflect the unique feature of a project? The goal of this study is to measure the similarity of project risks across various groups by evaluating 70 major transportation projects delivered under various methods. The similarity index is calculated at three levels, that is, the entire document of the risk register, individual risk item, and the probability and consequence of each risk using a systematic comparative analysis based on natural language processing (NLP) and a state-of-the-art deep learning algorithm named Word2vec. Our study reports a high similarity of risk registers among different projects at all three levels. The analysis does show a lower similarity of risk registers for public-private partnerships (P3) projects. The primary contributions of this study are (1) develop a new approach to analyze the risk registers at the project level as the main output of risk management practice, and (2) establish the relation of risk uniqueness and project delivery method in transportation projects. Results suggest that a data-driven approach may be possible to help project teams develop a common risk register while allowing the teams to focus on each project's unique risks.  © 2021 American Society of Civil Engineers. An Empirical Analysis of Risk Similarity among Major Transportation Projects Using Natural Language Processing Data analytics; Natural language processing (NLP); Project delivery method; Risk management; Risk register; Transportation project Data Analytics; Deep learning; Natural language processing systems; Project management; Risk analysis; Risk assessment; Data analytics; Empirical analysis; Management IS; Natural language processing; Project delivery method; Project risk; Risk registers; Risks management; Three-level; Transportation projects; Risk management",Risk management
169,"Digital Control Tower - Instantaneous Visibility, Granularity and Decision Support for an LNG Mega Project","Objectives/Scope: Cloud based end to end digital project management platform with digital control tower, as fundamental technologies of the fourth industrial revolution, have enormous potential to improve real time visibility, productivity, collaboration, data sharing, efficiency, predictability, decision power and sustainability in the construction industry. Throughout the lifecycle of capital/mega projects, the engineering, construction, operations, and maintenance stakeholders face numerous issues, including the lack of trust and visibility, inefficiencies, and the fragmentation of the information value chain into fragile data silos. Digital control tower aimed to narrow down the time, budget and quality & safety challenges of a capital project by having overall/end to end control of project from initiation phase to handover and operations phase of the project. Methods, Procedures, Process: Digital control tower provides systematic, 360-degree view of project details along with real-time analytics spanning from design engineering, pre-construction planning, onsite execution, action tracker, construction services, equipment's, quality, budget and safety (leading and lagging indicators). Stakeholders involved throughout the project lifecycle (i.e.: owner, engineering team, pre-construction and planning team, construction management and site team), enables instantaneous connectivity from the top floor to the shop floor for seamless communication by reducing waste time. Digital control tower also enables bird's eye view for real time project performance monitoring and progress, it also enables week wise look ahead task for better monitoring and control, also narrows down the issues or concerns to an activity, area or person. The control tower can create the customizable necessary discipline/function dashboards available to all relevant roles and stakeholders without limitation. Results, Observations, Conclusions: Digital Control Tower leverages Artificial intelligence, Machine learning and Blockchain technologies to seamlessly capture, maintain and process fragmented data set into intelligent predictive analytics which helps the project stakeholders to take right decision on right time, so as to avoid any kind of delays in project. Novel/Additive Information: Digital Control Tower is not a standard dashboard, it is an integrated digital ecosystem where stakeholders can drive projects with real time data for decision making. Copyright © 2022, Society of Petroleum Engineers. Digital Control Tower - Instantaneous Visibility, Granularity and Decision Support for an LNG Mega Project  Budget control; Construction equipment; Construction industry; Data Analytics; Decision making; Decision support systems; Digital storage; Floors; Human resource management; Project management; Safety engineering; Visibility; Waste management; Cloud-based; Decision supports; Digital control; Digital project management; End to end; Industrial revolutions; Management platforms; Mega projects; Pre-construction; Quality safety; Life cycle",Financial management
170,Wine Analytics: Futures or Bottles?,"This chapter features a wine distributor’s budget allocation decision between bottled wine and wine futures under weather and market uncertainty. Every May, a distributor determines its investment amount in wine futures of the recent vintage and in bottled wines of the previous vintage. In September, after observing the weather and market conditions during the growing season of the upcoming vintage, the distributor has the flexibility to adjust its initial investments. In May of the following year, the distributor finally collects revenues from these investments. This chapter summarizes three contributions. First, empirical analysis shows that wine futures exhibit greater price volatility than the bottled wines. Second, a mathematical model, built based on the empirical insights, suggests that a wine distributor should always carry some wine futures in its portfolio. Third, financial demonstration shows that a risk-neutral distributor can improve its profits by 21% and this benefit increases in risk aversion. © 2022, Springer Nature Switzerland AG. Wine Analytics: Futures or Bottles? Market uncertainty; Pricing; Risk aversion; Weather uncertainty; Wine futures ",Risk management
171,Smart Project Management System (SPMS) - An Integrated and Predictive Solution for Proactively Managing Oil & Gas client Projects,"The major challenge Project Management Teams (PMT) currently face is the isolated functionality of diverse support disciplines and their tools, leading to delayed and reactive approaches to the project issues. To become more proactive, enhance efficiency, and improve the productivity of current Project Management practices, ADNOC Gas Processing developed a Smart Project Management System (SPMS) that will: • Collate and integrate, real-time project data from various software platforms currently in use. • Prepare, monitor, and control all project parameters impacting the successful completion of projects. • Predictive project analytics by generating automated proactive alerts with recommendations. • Provide one unified interface for dashboard, reports, and predictive project management. SPMS is a technology that can add real value and drive positive change in project management and business transformations. SPMS will automate all existing processes and make available all information with drill-down dashboards for Project Planning, Scheduling, Progress Measurement, Safety, Quality, Cost and Budget and lessons learned, Change Orders, Risk Management, Document and Transmittal Management. SPMS also will provide features such as lessons learned to generate alert recommendations, Chat-BOT, and an assistant BOT technology to search the Scope of work, Contract, DGS documents…etc. To achieve our objective, the framework adapted is to include integration, automation, chat-bot, and machine learning attributes in the SPMS software solution. As part of data collection brainstorming, sessions and extensive workshops were conducted with all the project stakeholders such as Consultants, Contractors, PMC, TPI, and PMT in developing a state-of-art project management tool. Desktop reviews and interviews were conducted with product teams to evaluate various solutions such as SAP, ORACLE, ACONEX, ASSAI, WRENCH, and PM-Web as part of the feasibility study and selected a framework to develop a single software platform and to carry out a Pilot study. SPMS was implemented and the validation was carried out on two (2 No's) case study projects. The outcome of the research is that SPMS will support the Project Management team (PMT) to better focus on priority issues and maximize their productivity. A time-consuming activity i.e., Report generation of each project too will be automated with real-time, linked data to reduce man-hours. Furthermore, SPMS will provide the senior management with complete access to real-time project data through I-phones and dashboards. In addition, SPMS gives an early insight of project deviations and issues with help of cognitive intelligent solutions provided by machine learning algorithms hence, proactively enabling a project manager to act timely to deliver a project successfully. This paper covers the overview of existing and enhanced project management practices in the Oil and Gas Industry using SPMS, encountered issues, conclusions drawn and appropriate recommendations. Copyright © 2022, Society of Petroleum Engineers. Smart Project Management System (SPMS) - An Integrated and Predictive Solution for Proactively Managing Oil & Gas client Projects artificial intelligence; automation; big data; business model innovation; digitization; project management Artificial intelligence; Automation; Budget control; Data Analytics; Digital storage; Efficiency; Human resource management; Infill drilling; Information management; Productivity; Risk assessment; Risk management; Scheduling; Business model innovation; Digitisation; Integrated solutions; Management team; Project data; Project management practices; Project management system; Real- time; Smart projects; Software platforms; Big data",Strategic alignment
172,Identification of the Barriers to Data-Centric Approach in the Construction Industry,"With the advancement of technology, many industries are improving their workflows and productivity by embracing digital transformation, and specifically data-centric approaches. Construction, however, is often slower to adopt new technology and innovations. A data-centric approach can enrich project delivery value and streamline processes through applications of data science and emerging innovations, such as artificial intelligence. This study aims to identify the barriers to adopting data-centric approaches in the construction industry. Initially, a literature review on Building Information Modeling (BIM) implementation identified 35 barriers. Next, the barriers were organized and refined to be more specific to a data-centric approach with the help of expert focus group meetings. Finally, the barriers were categorized based on experts' opinions into five categories: Process, Information, Financial, People, and Risk. The result of this study can help the construction industry move toward a more data-centric approach through the evaluation of approaches to overcome the identified barriers. © 2022 Construction Research Congress 2022: Project Management and Delivery, Controls, and Design and Materials - Selected Papers from Construction Research Congress 2022. All rights reserved. Identification of the Barriers to Data-Centric Approach in the Construction Industry  Architectural design; Metadata; Project management; Building Information Modelling; Data-centric approaches; Digital transformation; Expert opinion; Focus groups; Literature reviews; Model implementation; Project delivery; Streamline process; Work-flows; Construction industry",Risk management
173,Lessons learned from the impact of COVID-19 on the global construction industry,"Purpose: The construction industry represents most of every country’s finances and vital to continued economic growth and activities, especially in developing countries. The impact of the severe acute respiratory syndrome-2 disease (COVID19) on the government’s income resulted in the expectation of many public projects being cancelled or delayed providing little opportunity for the emergence of new public projects. This study collated a global qualitative perspective (survey interviews) on the lessons learned during the COVID-19 pandemic and the positive and negative impacts for future-proofing the construction sector. Design/methodology/approach: In total, 76 respondents from five continents excluding South America responded to the online open-ended structured questionnaire. Data collected were analysed through artificial inteligence analytics tool – Zoho analytics. Findings: The themes indicating the positive impact obtained from the interview were overhead cost reduction, remote working environment, focus on health and safety, improved productivity and sustainability goals while the themes signifying the negative impact were low business turnover, delays in construction payment and output, difficulties working from home and job losses. Supply chain management, construction project management improvement, concentration on health and safety and effective virtual working environment were collated as themes on lessons learned. Social implications: The major findings of this study emphasise on the need to improve the occupational health and safety and onsite safety measures for future proofing of the construction industry. Originality/value: The findings from the analyses made clear the imperativeness of the built environment research, with a focus on novel framework and strategies for future proofing the construction industry. © 2021, Emerald Publishing Limited. Lessons learned from the impact of COVID-19 on the global construction industry Construction industry; Countries; COVID-19; Epidemic Accident prevention; Cost reduction; Developing countries; Economic analysis; Employment; Industrial hygiene; Project management; Supply chain management; Surveys; Sustainable development; Construction sectors; Country; COVID-19; Design/methodology/approach; Economic activities; Economic growths; Health and safety; Public project; Severe acute respiratory syndrome; Working environment; Construction industry",Strategic alignment
174,Neuro-fuzzy system based model for prediction of project performance in downstream sector of petroleum industry in Iran,"Purpose: Planning phase of a project results to series of crucial decisions which determine the path to objectives achievement. At the same time, in this phase, project encounters the highest level of uncertainty in comparison of all phases of project lifecycle. This paper aims to support early decisions of project based on the progress forecasting. Design/methodology/approach: The scope of study is limited to downstream projects of petroleum industry in Iran, and the proposed model is trained and tested based on 75 Iranian completed petroleum projects. First, types of progress curve functions are investigated, and various types are studied and the most appropriate ones are selected through curve fitting. In the next step, using questionnaire, dependent and independent variables are recognized. Finally, using historical data and s-curve generator functions, a fuzzy inference system (ANFIS) based model have been developed to support early phases decision-making processes. Findings: Based on the analysis of received questionnaires, six functional criteria in two groups as dependent variables and 25 independent variables, in two groups and four clusters are determined and categorized. Eventually, performance prediction model of a project has been developed by using Adaptive Nero Fuzzy Inference System. Originality/value: The main contribution of this study to construction management knowledge is categorizing two groups of variables, which first one defines the project dynamic and the other calculates the key effects on previous one. Also, this investigation improves the current knowledge by analyzing the project system from the dynamic behavior perspective and modeling the defined variables using ANFIS tools. © 2021, Emerald Publishing Limited. Neuro-fuzzy system based model for prediction of project performance in downstream sector of petroleum industry in Iran Early phase decisions; Nero fuzzy system; Petroleum industry of Iran; Project performance prediction; S curve models Curve fitting; Decision making; Forecasting; Fuzzy inference; Fuzzy neural networks; Fuzzy systems; Gasoline; Life cycle; Petroleum industry; Predictive analytics; Surveys; Construction management; Decision making process; Dependent variables; Design/methodology/approach; Fuzzy inference systems; Independent variables; Performance prediction models; Project performance; Project management",Strategic alignment
175,ACCEPTANCE OF CONTEMPORARY TECHNOLOGIES FOR COST MANAGEMENT OF CONSTRUCTION PROJECTS,"The construction industry has become more digital and the traditional methods of construction activities are gradually becoming outdated. In this era of digital construction, various information and communication technologies have been developed and deployed to the site for the management and control of construction activities including cost management. Irrespective of the benefits of adopting these technologies, most of them are still not readily accepted for use for construction management. This study articulated Seven (7) recent technologies driving the industry and evaluated their acceptance for cost management of construction projects. The technologies include mobile technology, Augmented/Virtual Reality (AR/VR), Building Information Modeling (BIM), Internet of Things (IoT), Autonomous Equipment (Drones and Robotics), Artificial Intelligence (AI), and Predictive Analytics (PA). Data was gathered using a restructured questionnaire and technology acceptance model analysis was performed to identify which of the technologies have higher acceptance for cost management based on the criteria of availability, affordability, frequency of use, usefulness for cost management, and acceptance in the industry. Test statistics using Spearman's correlations and Kendall's correlations for each of the technologies and Spearman's Correlations of Technology acceptance with other variables in the TAM Model were performed. The results showed that mobile technology has higher correlation values than other technologies, and therefore has a higher acceptance for cost management. Kendall's coefficient of concordance values and Spearman's correlation values for Mobile technology were all above 0.6 which indicates a high level of agreement among the raters and strong relationships between the compared TAM variables. © 2022 International Council for Research and Innovation in Building and Construction. All rights reserved. ACCEPTANCE OF CONTEMPORARY TECHNOLOGIES FOR COST MANAGEMENT OF CONSTRUCTION PROJECTS Construction Projects; Cost Management; Recent Technologies; Technology Acceptance Architectural design; Construction; Cost benefit analysis; Internet of things; Predictive analytics; Project management; Statistical tests; Construction activities; Construction projects; Correlation value; Cost management; Digital construction; Methods of constructions; Mobile Technology; Recent technology; Spearman correlation; Technology acceptance; Construction industry",Strategic alignment
176,A Study on Operational Risk and Credit Portfolio Risk Estimation Using Data Analytics*,"In this article we consider operational risk and use data analytics to estimate the credit portfolio risk. Specifically, we consider situations in which managers need to make the optimal operational decision on total provision for risk to hedge against the potential risk in the entire supply chain. We build a new structural credit model integrated with data analytics to analyze the joint default risk of credit portfolio. Our model enables the decision maker to better assess the risk of a supply chain, so that they could determine the optimal operational decisions with total provision for risk, and react in a timely manner to economic and environmental changes. We propose an efficient simulation method to estimate the default probability of the credit portfolio with the risk factors having the multivariate t-copula. Moreover, we develop a three-step importance sampling (IS) method for the t-copula credit portfolio risk measurement model to achieve an accurate estimation of the tail probability of the credit portfolio loss distribution. We apply the Levenberg–Marquardt algorithm to estimate the mean-shift vector of the systematic risk factors after the probability measure change. Besides, we empirically examine the changes in the credit portfolio risks of 60 listed Chinese firms in different industries using our proposed method. The results show that our model can help the decision maker make the optimal operational decisions with total provision for risk, which hedges against the potential risk in the entire supply chain. © 2020 Decision Sciences Institute A Study on Operational Risk and Credit Portfolio Risk Estimation Using Data Analytics* Credit Portfolio Risk; Data Analytics; Decision Making; Operational Risk Management; Simulation ",Risk management
177,System and Method on Order Management Using Neural Networks and Risk Modeling,"The transactions of goods and services between enterprise service providers are often driven by contracts and purchase orders. Every month thousands of invoices are billed to customers who settle them based on the usage of services. Considering the vast number of purchase orders that are signed, it requires considerable manual effort by the service provider to process and manage them. Moreover, the invoice's billed data may not be maintained in the same cloud system as the purchase orders. This leads to complexity with data mapping between the two data sets. Sometimes the invoices may get into a dispute due to over exhaustion of allocated funds or may be billed to an expired purchase order. Hence managing the billing service is a huge undertaking along with increased cost.To address these challenges, we developed an order manage- ment system that transforms the monitoring of purchase orders to increase renewals as well as decrease disputes. The system includes an automated purchase order-invoice data mapping model along with a risk analytics model that evaluates the orders against the invoices billed. The output is the set of actionable and non actionable insights based on customer portfolio, risk level as well as market trends in usage of services. We illustrate our method with some promising results on data of one of the world's largest IT service providers. © 2022 IEEE. System and Method on Order Management Using Neural Networks and Risk Modeling Feature engineering and transforming; Finan- cial service applications; Natural language processing; Neural network model; Time series model Mapping; Modeling languages; Natural language processing systems; Purchasing; Sales; Feature engineering and transforming; Feature engineerings; Finan- cial service application; Language processing; Natural language processing; Natural languages; Neural network model; Purchase orders; Services applications; Times series models; Neural network models",Risk management
178,Modeling of Optimal Credit Limits in Microfinance Organizations; [Моделирование оптимальных кредитных лимитов в микрофинансовых организациях],"In an unstable economic situation, the population has a high demand for money, which leads to an increase in credit risks of microfinance organizations (MFIs). This requires the development of system management solutions to minimize them. The need for a systematic approach to risk management in MFIs is caused by the peculiarity of the development of the microcredit market in the Russian Federation: limited time resources when implementing decision-making systems (DSS) due to the short loan term, insufficient qualifications of risk management compared to the banking sector, lack of resources of technical specialists, an increase in the degree of market regulation by the Central Bank (CB). One of the ways to manage credit risk, especially in microfinance organizations, is to set limits on loans issued depending on the degree of risk of the borrower and the expected profitability. This article is devoted to the study of the issue of setting credit limits and their impact on credit risk in the entire portfolio of MFIs. The purpose of this article is to develop a systematic mathematical approach to credit risk management by establishing optimal credit limits in MFIs. The article presents a methodology and a practical example of modeling limits on the example of an MFI, which is in the top 10 of the Russian online microfinance market. Based on real historical data, a mathematical model is built for the segments of primary and repeat borrowers, which allows adjusting the limit policy of the organization to reduce the level of risk, considering the profitability of each customer segment. The weighted least squares method is used as a mathematical tool to estimate the coefficients of polynomial regression, as well as a logistic regression model. The scientific novelty of this article consists in the application of a separate mathematical model for setting limits in the MFI DSS, in addition to scoring. The practical significance of this article is the possibility of using the obtained model as an adviser in the formation of credit and risk policy of a particular MFI. © 2022 Publishing House of the Higher School of Economics. All rights reserved. Modeling of Optimal Credit Limits in Microfinance Organizations; [Моделирование оптимальных кредитных лимитов в микрофинансовых организациях] cluster analysis; credit limit; credit risk; credit scoring; logistic regression; mathematical model; MFI; risk analytics; weighted least squares ",Risk management
179,IBTIKAR Digital Lab - A Collaborative Approach towards Research and Development Challenges in Oil and Gas Upstream,"There are various ideas in everyone is mind. But to convert the ideas into reality, requires initiative and action. Petroleum industry has been using different techniques, science and algorithms over the years to tackle the challenges in order to achieve a business objective. This paper shares the implemented approach to support business challenges by developing a Research and development environment to support the business community. The idea was to find out current business user challenges and support their requirements. IBTIKAR (innovation) lab is the result of this idea which is created to provide required In-House infrastructure (Hardware and Software) support within UAE National Oil Company premises. Research and development are integral part of any company/industry for tackling challenges. Accessing various data types and implementing the concept of Big data has been of a great value. Collaboration between business users, Information technology and Data Management is required to have concrete foundation and to prepare roadmap for the future. The application portfolio in an Oil and Gas upstream varies a lot and now a days, almost all the applications/tool come with a programming interface for integrating Open-Source algorithms. It makes it easier to directly connect the open-source tools to existing applications. The type of data types in upstream varies a lot and since these applications are already developed over the years to handle different kind of data, which further makes data analysis tasks achievable in timely manner. The available algorithms in existing tools and those accessed visa Open-source tools are further used for prediction and classification problems. With the advances in the learning algorithms, the modelling has become easier, and it requires very less coding. The challenges are more towards infrastructure support, as the data types and data size has gradually increased. Machine learning and Deep learning requires High end infrastructure to perform the data science using more CPU's, High End GPU's. In order to adapt to the advancing science and technologies, it makes everyone responsible to learn and implement the data science and Artificial intelligence (AI) concepts in daily tasks and projects to support organization strategy and objectives. In order to get the business community ready for AI, availability of Data science tools and infrastructure is required. This will support and allow the business users of any kind of data to try and test various data types using AI and data science. Following approach was taken from the idea initiation to the execution phase: • Discussion with Business users to find out current business challenges and impediments in overcoming those challenges • Document project scopes for the discussed business challenges. • Discussion with the respective Infrastructure support team to provide the Hardware and software support along with the required data storage space. • Secure budget for developing IBTIKAR lab • Develop environment for the research and development of various data science projects • Integration of Open Source and Free software/s with the existing tools and applications • Agree and support of the respective roles and responsibilities • Agree of establishing a collaborative environment between business users, Information technology and data management engineers. • To have tools covering the life cycle of the machine learning model (from creation to deployment to monitoring) The business users involved throughout the upstream activities, want to use available open-source algorithms and run various Proof of concepts. In order to get value out of data and to extract insight, it is must that business users are empowered with the required infrastructure, trainings, data science tools and Artificial intelligent algorithms in a collaborative Research and development environment. Configuration and implementation of the IBTIKAR environment has aided the business user community in conducting various Proof of concepts and execute process to handle various business objectives and extract value. IBTIKAR environment has delivered value to the business community by providing required High-end resources, a programming environment, available open-source algorithms and it has improved the level of efficiency, collaboration and people engagement. Copyright © 2022, Society of Petroleum Engineers. IBTIKAR Digital Lab - A Collaborative Approach towards Research and Development Challenges in Oil and Gas Upstream  Deep learning; Gasoline; Learning algorithms; Learning systems; Open source software; Petroleum industry; Phase interfaces; Business challenges; Business community; Business objectives; Business-users; Datatypes; Development environment; Oil and gas; Open-source; Research and development; Research environment; Information management",Financial management
180,Severity Prediction of Construction Site Accidents Using Simple and Ensemble Decision Trees,"Workplace safety is always a concern of utmost importance in any organization. Studies have shown that the fatality rate is highest in the construction industry among all other industries. The construction project managers need to understand the risk status of each of their projects and thus implement preventive measures. The introduction of digital tools into construction sites not only reduces the health and safety hazards among workers but also paves the way to the economic growth of the industry. The present study implements techniques of data analytics and Machine Learning (ML) into the construction safety sector. For the analysis, a dataset with 4847 incident reports during 2015–2017 from Occupational Safety and Health Administration (OSHA) database is used. Initially, the major attributes contributing to the incident are identified. Based on these identified factors, they were classified as Before Accident and After Accident attributes, and ML algorithms are used for the prediction of the construction fatality. Performance evaluation of these ML algorithms shows us that Random Forest (RF) has better prediction results for Before Accident attributes, whereas Decision Tree (DT) performed well for After Accident attributes. From a broader perspective, this study will help the safety management team to understand the severity of safety risks faced in each of their construction projects and also facilitate the implementation of proper preventive safety mechanisms. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG. Severity Prediction of Construction Site Accidents Using Simple and Ensemble Decision Trees Accident prevention; Construction safety; Machine learning; Prediction; Safety management; Workplace injuries Accident prevention; Accidents; Construction industry; Data Analytics; Decision trees; Digital devices; Economics; Forecasting; Health hazards; Human resource management; Occupational risks; Petroleum reservoir evaluation; Risk assessment; Structural design; Construction fatalities; Construction project managers; Construction projects; Construction safety; Construction site accidents; Construction sites; Occupational safety and health administrations; Preventive measures; Project management",Strategic alignment
181,Digital Twin and Big Data Technologies Benefit Oilfield Management,"The oil & gas industry has been value added from our digital assets since this new century, which helped our industry dig out more advanced algorithm, more robust logic to address the challenge from HPHT wells and deep-water wells. Nowadays the operators are facing much more challenges in oilfield management especially how to improve their decision efficiency and situation awareness. Thanks to the different sensors we deployed on oilfield from drilling to completion and production, tremendous data contributed to the digital asset we are having now. The digital twin makes oilfield management much easier than ever before, hundreds of wells' performance could be displayed in front of the decision maker or key management level of oil companies, and big data technique helps them get easy understanding of real time behavior on well construction progress, cost management, pain spot of each project. Combining these two methods, it is possible to have an up-to-date awareness of oilfield development status and perceptual intuition to very detail situations. There is a major operator manages over 200 wells per year and some of these wells are challenging exploration well with measured depth over 20000ft which requires experienced team to get the well to total depth, also a lot of shale gas wells with lateral intervals over 8000ft which demands intensive control of cost. All above operations or targets need be done under a safe and efficient way, then the management team taking digital twins to monitor the real time well status which help them get up to date information about whole oilfield status like drilling, completion, production and more. Big data analysis is also used to help enhance the decision- making efficiency and overcome puzzles that traditional method could not solved, like recommending the best practice way on well construction engineering parameters, or ROI (return on investment) assess. The oil company could achieve a better management level with less human resources and much more workload. By the advantages of digital twins and big data analysis, the oil company now managing more than 200 drilling rigs and 300 completion wells in the high efficiency way, and now involving the production wells into next phase digital construction target. Furthermore, considering develop an integrative digital twin of geology and engineering map which get whole formation and well construction more intuitive. Besides, it is proven that digital method like digital twins and big data technique could improve the skill of oilfield management significantly, which optimized the resource and expenditures investigated in modern oil and gas industry. Copyright © 2022, Society of Petroleum Engineers. Digital Twin and Big Data Technologies Benefit Oilfield Management  Costs; Data Analytics; Data handling; Decision making; Efficiency; Gas industry; Human resource management; Infill drilling; Information analysis; Information management; Oil field development; Petroleum prospecting; Project management; Reservoir management; Data technologies; Deep water wells; Digital assets; Management level; Oil companies; Oil/gas industry; Situation awareness; Well constructions; Well performance; Well water; Big data",Financial management
182,Blockchain-Based Methodology for Collaborative Risk Assessment,"Risk assessment is an important part of risk management in construction projects as it involves the identification, evaluation, and prioritization of potential risk events. This enables project teams to properly mitigate them. With the intent of getting risk input from multiple stakeholders, different systems have been developed to support collaborative risk assessment. However, these systems often face challenges such as no data confidentiality, poor data security, lack of transparency, and absence of traceability and auditability for the team members. Blockchain is an emergent decentralized digital technology that can provide solutions to overcome such deficiencies of centralized systems. In this study, the need for a novel blockchain-based methodology for collaborative risk assessment is analyzed. Multiple traits of blockchain technology have been found through the literature review. From these multiple traits, the key functional elements for the methodology have been identified and discussed in further detail. The research points out some of the limitations at the current state so that future work can be conducted to build a methodology that can be even more beneficial for the risk management of construction projects. © 2022 Construction Research Congress 2022: Computer Applications, Automation, and Data Analytics - Selected Papers from Construction Research Congress 2022. All rights reserved. Blockchain-Based Methodology for Collaborative Risk Assessment  Data Analytics; Project management; Risk assessment; Risk management; Block-chain; Construction projects; Data confidentiality; Multiple stakeholders; Potential risks; Prioritization; Project team; Risks assessments; Risks management; Team members; Blockchain",Risk management
183,Empowering the Workforce of the Future Through Strategic Data Science Framework to Demystify Digitalization in ADNOC Onshore to Create Sustainable Business Value,"Oil and Gas industry is seeking new ways to improve efficiencies, reducing operating costs and increasing revenues in the current volatile market conditions. Data Science and all the new emerging technologies enable the discovery of new opportunities and digitalization is a vital element for making business more effective and efficient. While COVID19 has disrupted the world, ADNOC Onshore has recognized the importance of reskilling and empowering the future workforce through strategic enterprise data science and analytics program to achieve 2030 smart growth strategy. This paper talks about successful approach and enablers for development of in-house capability for transformation that lead to generating significant business value The digital transition can pose both challenges and opportunities in this transformation. ADNOC ONSHORE has developed an integrated framework to encourage and accelerate data science capabilities. This framework promotes a vision with collaborative, sustainable mechanisms to develop talent. It is not just the formal learning and additional professional qualification that make it possible to build this in house capability. There are five major areas are enabled this framework such as data science & analytics skills competency model, sustainable on-line collaborative learning program, organizational culture change, democratizing AI through open platform & a digital business model for performing real business problems/use case PoCs. Each area has a detailed program and execution strategy with a collaborative effort from technical and non-technical stakeholders. ADNOC ONSHORE has successfully implemented this framework and able to certify 20 employees as part of this program. The Data Science Competency Model identified and defined the skills required to be successful within the enterprise with a clear learning path and mentorship. The leadership played a pivotal role to encourage data driven decision making and predictive capabilities in addition to executive awareness to lead the change with clear performance indicators. By democratizing AI platform across upstream user community, six real business cases have been successfully developed with clear business value in subsurface and production workflows according to the defined digital business model. The successful business cases have improved efficiency by 75% in performing cement & corrosion log interpretation & well portfolio optimization. Data driven analytics have been evaluated in subsurface workflows such as infill location optimization, gas-lift candidate identification and they have complemented the existing techniques. The framework has been successfully extended to other group companies in ADNOC. The rapid growth of AI in business in the last five years presents an opportunity for oil and gas professionals for enhancing the skills and transformation. This paper talks about an integrated framework, learning path, democratizing AI, engagement of leadership, digital business model for business case evaluation by applying agile way of working and sustainable value creation. Copyright © 2022, Society of Petroleum Engineers. Empowering the Workforce of the Future Through Strategic Data Science Framework to Demystify Digitalization in ADNOC Onshore to Create Sustainable Business Value  Corrosion; Data Analytics; Decision making; Financial data processing; Gas industry; Investments; Learning systems; Machine learning; Operating costs; Personnel; Predictive analytics; Sustainable development; Business case; Business models; Business value; Competency model; Digital business; Integrated frameworks; Learning paths; Oil and Gas Industry; Strategic data; Sustainable business; Efficiency",Capacity management
184,Risk Management in Analytics Solutions Integration and Deploy: Case Study in a Brazilian Financial Institution,"The research seeks to propose a risk management approach for the main risks identified in the integration and implementation of Analytics solutions in a large banking institution in Brazil. Various approaches and techniques were applied during the development of the research. Among them: construction of the SIPOC matrix, survey of the main risks from the literature, validation of risks via semi-structured interviews and applying the FMEA (Failure Mode and Effect Analysis) technique to prioritize risks and manage priority risks. The results indicated that by managing the 3 highest priority risk events, it is fundamentally possible to mitigate the risks with medium, high, and higher risk indexes (RPN) of the process. Such risks are associated with talent retention and training, lack of alignment between clients proposing solutions and recurring changes in the scope of projects. Project managers in Analytics should assess whether their integration/implementation processes foresee preventive, corrective and/or mitigating actions for the associated risk events foreseen in the discovery of this research. Although the theme of Analytics has dominated discussions about innovation and how to obtain the potential value of data in different sectors of the economy, including banking, there are very few studies that have evaluated the data integration process so far, and deploying (deploying) Analytics solutions and their main obstacles and risks to achieve the purpose of reducing the time to put an analytics solution on the air. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG. Risk Management in Analytics Solutions Integration and Deploy: Case Study in a Brazilian Financial Institution Analytics; BPM; FMEA; Project management; Risk management ",Risk management
185,Leveraging the industry 4.0 technologies for improving agility of project procurement management processes,"Recently, due to continually varying demands and shorter time to market, the existing Project Procurement Management (PPM) processes are incapable of coping up with the pace. There is a need of an agile model to manage procurement projects effectively. This article aims to developing strategies for executing the PPM processes with more agility by leveraging the capabilities and merits of industry 4.0 technologies along with selective Critical Success Factors (CSFs). For improving agility in PPM, this study identifies CSFs from the literature and experts’ review, the CSFs were then prioritized based on their significance, followed by establishing relationships and exploring interactions among CSFs using Total Interpretitive Structureal Modelling (TISM) and Fuzzy Cross-Impact Matrix Multiplication Applied to Classification (MICMAC). Furthermore, an agile project implementation plan was developed based on the findings of TISM and FuzzyMICMAC, which provides a systematic approach for strategically achieving the CSFs. Lastly, strategies were developed to improve agility in key processes of PPM by utilizing the new-age technologies Industry 4.0 like Internet of Things (IOT), Mobility, Business Intelligence, Blockchain, Chatbot, Robotic Process Automation (RPA) and other technologies. The strategies and the agile project implementation plan thus developed as an outcome of this research can be leveraged by industries of various domains for improving agility in any of their business processes. © 2021, The Society for Reliability Engineering, Quality and Operations Management (SREQOM), India and The Division of Operation and Maintenance, Lulea University of Technology, Sweden. Leveraging the industry 4.0 technologies for improving agility of project procurement management processes Agility; Business intelligence; Industry 4.0; Project procurement management Enterprise resource planning; Industrial robots; Industry 4.0; Critical success factor; Developing strategy; Internet of Things (IOT); MAtrix multiplication; Process automation; Procurement projects; Project implementation plans; Project procurement; Project management",Value management
186,Predicting the Defects using Stacked Ensemble Learner with Filtered Dataset,"Software defect prediction is a crucial software project management activity to enhance the software quality. It aids the development team to forecast about which modules need extra attention for testing; which part of software is more prone to errors and faults; before the commencement of testing phase. It helps to reduce the testing cost and hence the overall development cost of the software. Though, it ensures in-time delivery of good quality end-product, but there is one major hinderance in making this prediction. This is the class imbalance issue in the training data. Data imbalance in class distribution adversely affects the performance of classifiers. This paper proposes a K-nearest neighbour (KNN) filtering-based data pre-processing technique for stacked ensemble classifier to handle class imbalance issue. First, nearest neighbour-based filtering is applied to filter out the overlapped data-points to reduce Imbalanced Ratio, then, the processed data with static code metrics is supplied to stacked ensemble for prediction. The stacking is achieved with five base classifiers namely Artificial Neural Network, Decision Tree, Naïve Bayes, K-nearest neighbour (KNN) and Support Vector Machine. A comparative analysis among 30 classifiers (5 data pre-processing techniques * 6 prediction techniques) is made. In the experiments, five public datasets from NASA repository namely CM1, JM1, KC1, KC2 and PC1 are used. In total 150 prediction models (5 data pre-processing techniques * 6 classification techniques * 5 datasets) are proposed and their performances are assessed in terms of measures namely Receiver Operator Curve, Area under the Curve and accuracy. The statistical analysis shows that proposed stacked ensemble classifier with KNN filtering performs best among all the predictors independent of datasets. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature. Predicting the Defects using Stacked Ensemble Learner with Filtered Dataset Artificial neural networks (ANN); Class imbalance; Data pre-processing; Decision trees; Defect prediction; Nearest neighbour; ROC and AUC; Software quality; Stacked ensembles; Support vector machine Barium compounds; Computer software selection and evaluation; Decision trees; Defects; Forecasting; NASA; Nearest neighbor search; Neural networks; Predictive analytics; Project management; Software quality; Software testing; Support vector machines; Area under the curves; Classification technique; K nearest neighbours (k-NN); Performance of classifier; Prediction techniques; Receiver operator curves; Software defect prediction; Software project management; Classification (of information)",Risk management
187,Revamping construction supply chain processes with circular economy strategies: A systematic literature review,"The construction sector has suffered from low productivity and considerable wastes due to the fragmentation of its value chain, the large number of diverse stakeholders and the complex nature of the projects. A promising way to reduce construction wastes and encourage value chain integration is to implement circular economy (CE) strategies. Many recent studies in the fields of construction management and sustainability have advanced CE from multiple perspectives. There remains room to refine such knowledge by clearly identifying all the possible strategies and drivers to be carried out in practice that help stakeholders slow, narrow, and close resource loops. A systematic review was conducted in this study to examine the relevant literature on construction circularity to address the knowledge gap. A total of 61 relevant publications in the past ten years were rigorously selected and reviewed in-depth based on an iterative coding procedure. The phase-specific circular economy strategies were classified into five categories: 1) Design phase (including design with LCA, design with reused materials, design with recycled materials, and design for disassembly); 2) Manufacturing phase (including industrial symbiosis); 3) Construction phase (including lean construction methods); 4) Operation and maintenance phase (including service life planning); and 5) End-of-Life phase (including diversion of wastes). Internal drivers were identified to consist of BIM (Building Information Modelling)-based design and evaluation, IoT (Internet of Things)-based material tracking, predictive data analytics, and logistics network optimization. External drivers included material certifications and legislation, financial incentives, market maturity and material flow balance, and social engagement. The review revealed that the BIM-based and LCA-based methods have been widely used; however, logistics network optimization to allow industrial symbiosis was not adequately addressed in the existing literature. The strategies and drivers were also composed into a framework to guide the future implementation of circular construction projects. The framework could help construction researchers and project participants clearly understand circular resource flows across various construction supply chain stages and thus help them to keep up with the global action of “Net Zero Emission” by 2050. © 2021 Elsevier Ltd Revamping construction supply chain processes with circular economy strategies: A systematic literature review BIM; Circular economy; Construction supply chain; Deconstruction; LCA; Recycle; Reuse Architectural design; Construction industry; Data Analytics; Internet of things; Iterative methods; Predictive analytics; Project management; Supply chains; Building Information Modelling; Circular economy; Construction supply chain; Deconstruction; Industrial symbiosis; Logistics network; Network optimization; Reuse; Supply chain process; Value chains; Recycling",Strategic alignment
188,Power BI in ICT for Monitoring of Insurance Activity Based on Indicators of Insurance Portfolios,"The article proves the relevance of operational business intelligence in insurance in terms of VUCA-world. For this purpose, the use of Microsoft Power BI software as a method of information and communication technologies, its purpose and benefits for business monitoring are considered. Among the advantages of using Microsoft Power BI is the analysis of existing insurance contracts and current indicators of insurance portfolios. The research substantiates the feasibility of operational analysis of insurance activities based on the calculation of indicators of insurance portfolios that form insurance statistics, which provides results in real-time. This approach eliminates the shortcomings of common means of retrospective operational analysis based on financial statements. Within the research, the insurance portfolio is defined as a set of insurance contracts for a separate line of business of the insurer on a certain date. It is emphasized that for each date of insurance activity the insurance portfolio is a unique set of contracts. A positive factor in the article is the possibility of timely response of managers to fluctuations in the profitability and technical risk of insurance portfolio. The application of the studied elements made it possible to propose a method of monitoring insurance activities based on the calculation of indicators of insurance portfolios and their visualization on analytical panels (dashboards).  © 2020 IEEE. Power BI in ICT for Monitoring of Insurance Activity Based on Indicators of Insurance Portfolios analytical panel; indicator; information and communication technologies; insurance; insurance activity; insurance contract; insurance portfolio; insurance statistics; line of business; Microsoft Power BI; monitoring; operational business intelligence; real time; visualization; VUCA-world Statistics; Activity-based; Financial statements; Information and Communication Technologies; Insurance contracts; Operational analysis; Operational business; Real time; Technical risks; Insurance",Monitoring and control
189,Software Analytics Tools: An Intentional View,"Software analytic tools consume big amounts of data coming from either (or both) the software development process or the system usage and aggregate them into indicators which are rendered to different types of stakeholders, also offering them a portfolio of techniques and capabilities such as what-if analysis, prediction and alerts. Precisely, the variety of stakeholders and the different goals they pursue justifies the convenience of performing an intentional analysis of the use of software analytics tools. With this aim, we first enumerate the different stakeholders and identify their intentional relationships with software analytics tools in the form of dependencies. Then, we focus on one particular stakeholder, namely the requirements engineer, and identify further intentional elements represented in a strategic rationale model. The resulting model provides an abstract view of the domain which may help stakeholders when deciding on the adoption of software analytic tools in their particular context. © 2022 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0) Software Analytics Tools: An Intentional View Data-driven Software Engineering; iStar; Software Analytic Tools; Software Indicators; Software Metrics Analytic tools; Creative Commons; Data driven; Data-driven software engineering; Istar; Software analytic tool; Software development process; Software indicator; Software metrics; System usages; Software design",Strategic alignment
190,Predicting Project Outcomes with the Association of Project Management,"Project professionals place great value in accurately predicting project outcomes. Itis therefore unsurprising that there has been a rapid acceptance of a new suite of tools promising to forecastproject outcomes better than ever before. Within a few years the use of project data analytics has become widespread throughout project delivery organisations; they have become the norm. Most closely associated with dashboards,project data analytics is transforming projects' ability to see problems sooner and act quicker. Project data analytics however has not yet been 'professionalised'. No single organisation has yet solved how to holistically get the very best out of analytics to deliver more predictable projects, but many are trying across multiple industries. Some organisations have great solutions and ideas, but itis not yet encoded in how the profession delivers. Petrofac as part of the Project Data Analytics Task Force [1&2], a cross industry working group, have collaborated with the Association of Project Management (APM) [3] to publish a ground-breaking guide[4] designed to help project delivery organisations get started in project data analytics. In addition, a five step framework*is offered which is designed for project delivery organisations who are further long their journey with analytics. This framework recommends the steps organisations can take to improve their project predictability from basic systems, through to dashboards and onto machine learning and artificial intelligence. It also references the capabilities organisations need to consider forthe benefits to become embedded. The paper explores how superior project performance is best achieved when project data analytics is blended with the insights and actions our people bring to delivering projects; to improve resultsthis blend of data and people is noted as being essential. The paper highlights this as the most significant factor as to why data analytics programmesmay not bring the value organisations expect and why step 4 (automated performance) of the model is seen as the pivot to success. On realisation of step 5 (intelligent performance), an organisation would be expected to have embraced the market leading approaches to maximising project performance and be well placed to achieve market leading returns on investment and margin. The paper advocates that organisations should: • Adopt a 3-click rule to project information • Send the right action, to the right person, at the right time • Blend human and data insights by quantifying perceptions and makinginsights actionable • Automate Project Data Analytics into the working rhythm of project delivery This paper is deliberately aimed at the project professional and not the data science community. It simplifiesthe typical technical jargon around analytics and provides a wide variety of examples, tips and graphics that the project professional can easily relate to. *For clarity, the five step framework is solely the work of Petrofac and no third party organisation has either been asked to comment on or endorse this. No direct link should be inferred between the Petrofac framework and the Association of Project Management, the only connection is the author's involvement in both. Copyright © 2022, Society of Petroleum Engineers. Predicting Project Outcomes with the Association of Project Management  Artificial intelligence; Commerce; Data Analytics; Forecasting; Investments; Metadata; Breakings; Cross industry; Data analytics; Performance; Project data; Project delivery; Project outcomes; Project performance; Task force; Working groups; Project management",Capacity management
191,A Data-driven project categorization process for portfolio selection,"Purpose: Categorizing projects allows for better alignment of a portfolio with the organizational strategy and goals. An appropriate project categorization helps understand portfolio’s structure and enables proper project portfolio selection (PPS). In practice, project categorization is, however, conducted in intuitive approaches. Furthermore, little attention has been given to project categorization methods in the project management literature. The purpose of this paper is to provide researchers and practitioners with a data-driven project categorization process designed for PPS. Design/methodology/approach: The suggested process was modeled considering the main characteristics of project categorization systems revealed from the literature. The clustering analysis is used as the core-computing technology, allowing for an empirically based categorization. This study also presents a real-world case study in the automotive industry to illustrate the proposed approach. Findings: This study confirmed the potential of clustering analysis for a consistent project categorization. The most important attributes that influenced the project grouping have been identified including strategic and intrinsic features. The proposed approach helps increase the visibility of the portfolio’s structure and the comparability of its components. Originality/value: There is a lack of research regarding project categorization methods, particularly for the purpose of PPS. A novel data-driven process is proposed to help mitigate the issues raised by prior researchers including the inconsistencies, ambiguities and multiple interpretations related to the taken-for-granted categories. The suggested approach is also expected to facilitate projects evaluation and prioritization within appropriate categories and contribute in PPS effectiveness. © 2021, Emerald Publishing Limited. A Data-driven project categorization process for portfolio selection Cluster analysis; Data analytics; Decision-making; Portfolio selection; Project categorization; Project management ",Strategic alignment
192,Data to Value: An ‘Evaluation-First’ Methodology for Natural Language Projects,"While for data mining projects (for example in the context of e-commerce) some methodologies have already been developed (e.g. CRISP-DM, SEMMA, KDD), these do not account for (1) early evaluation in order to de-risk a project (2) dealing with text corpora (“unstructured” data) and associated natural language processing processes, and (3) non-technical considerations (e.g. legal, ethical, project management aspects). To address these three shortcomings, a new methodology, called “Data to Value”, is introduced, which is guided by a detailed catalog of questions in order to avoid a disconnect of large-scale NLP project teams with the topic when facing rather abstract box-and-arrow diagrams commonly associated with methodologies. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG. Data to Value: An ‘Evaluation-First’ Methodology for Natural Language Projects Data science; Methodology; Natural language processing; Process model; Supervised learning; Unstructured data Abstracting; Data mining; Data Science; Machine learning; Natural language processing systems; CRISP-DM; E- commerces; Early evaluation; Language processing; Methodology; Mining projects; Natural language processing; Natural languages; Process-models; Unstructured data; Project management",Risk management
193,Population Health: Identifying Skill Sets and Education Alignment for HIM Professionals,"The COVID-19 pandemic has increased the emphasis on population health, therefore potentially amplifying demand for healthcare workforce professionals in this area. There is an urgent need to explore and define the roles of health information management (HIM) professionals in the population health workforce. This study sought to identify the skill sets and qualifications needed, and HIM education alignment with skills necessary for HIM professionals entering the population health workforce. An intentionally broad internet search of job postings was conducted to determine skills in population health. Population health-related job descriptions and qualification requirements were abstracted and analyzed using ATLAS.ti. Three common job categories were identified: management, analytics, and coding. Skill set requirements included soft skills, problem solving, project management, research, and data analysis. The study results identified HIM educational alignment and found that HIM professionals are generally a good fit to meet the increased need in the population health workforce. Copyright © 2021 by the American Health Information Management Association. Population Health: Identifying Skill Sets and Education Alignment for HIM Professionals COVID-19 pandemic; health information management; population health; qualitative content analysis; skill set; Workforce COVID-19; Curriculum; Health Information Management; Humans; Pandemics; Population Health; Professional Competence; Qualitative Research; SARS-CoV-2; curriculum; education; human; medical information system; pandemic; population health; professional competence; qualitative research",Financial management
194,STATISTICAL TOOLS AND THEIR IMPACT ON PROJECT MANAGEMENT HOW THEY RELATE,"Although a project manager has many tasks, none are more important to success than the ability to lead and manage a project. A project manager typically has a strong business and economic background, but how are decisions made? An economic analysis is telling, but without the knowledge of data analytics and the different statistical analysis tools, decision-making might be difficult. With certain techniques, statistical analysis tools and methods will be explained with their relationship on how they correlate positively or negatively to a project managers' success. Key concepts include data collection, key descriptive statistics (i.e., the mean, standard deviation, range, correlation, and linear regression). They also include building confidence intervals, hypothesis testing, risk analysis, and understanding different statistical software that is available. With these skills, the chance of a project manager's success becomes more likely. © 2021 Editora Mundos Sociais. All rights reserved. STATISTICAL TOOLS AND THEIR IMPACT ON PROJECT MANAGEMENT HOW THEY RELATE Descriptive Statistics; Hypothesis Testing; Project Manager; Project Planning; Risk Analysis; Statistical Methods; Statistical Tools ",Risk management
195,Practical and Open Source Best Practices for Ethical Machine Learning,"The acknowledged operational, ethical, legal and governance risks involved in applying Machine Learning (ML) have generated a need for a clear and thoughtful repository of best practices on how to responsibly govern, manage and implement “responsible ML”. The Foundation for Best Practices in Machine Learning (a non-profit foundation) seeks to promote responsible ML through creating an open-sourced, freely accessible repository of best practices and associated guides. Its model and organisational guides look at both the technical and institutional requirements needed to promote responsible ML. Both blueprints touch on subjects such as “Fairness & Non-Discrimination”, “Representativeness & Specification”, “Product Traceability”, “Explainability” amongst other topics. Where the organisational guide relates to organisation-wide process and responsibilities (i.e. the necessity of setting proper product definitions and risk portfolios); the model guide details issues ranging from cost function specification and optimisation to selection function characterization, from disparate impact metrics to local explanations and counterfactuals. It also addresses issues concerning thorough product management. These guidelines have been developed principally by senior ML engineers, data scientists, data science managers, and legal professionals for ML engineers, data scientists, data science managers, compliance professionals, legal practitioners, and, more broadly, management. The Foundation’s philosophy is that (a) context is key, (b) responsible ML starts with prudent MLOps and product management, and (c) responsible ML needs to be supported by all aspects of an organisation’s structure. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG. Practical and Open Source Best Practices for Ethical Machine Learning  ",Risk management
196,Toward Global Food Security: Transforming OCP Through Analytics,"Humanity relies on cultivated lands to feed itself and thrive. Fertilizers are responsible for 30%–50% of food production, and phosphate, the naturally occurring form of phosphorus, which does not have a substitute, is an essential component of fertilizers. OCP, based in Morocco, is the world’s largest phosphate mining and processing company and therefore plays a critical role in global food security. Over the past decade, OCP, in collaboration with Dynamic Ideas, an analytics consulting company, developed a mixed-integer optimization model to holistically optimize its entire sales and supply chain—from the mines to physical treatments, to chemical facilities, to inventory facilities, and to the port for global distribution. The optimization model brings clarity to a complex supply chain, informs management decisions throughout OCP, and has consistently resulted in an improvement of over 20% in earnings before interest, taxes, depreciation, and amortization (EBITDA) annually. This amounted to over $2.3 billion in the period 2015–2020 (23.6% of the cumulative EBITDA of $9.9 billion over this period). This incremental profitability has fueled OCP’s financing capacity; as a result, OCP is implementing a $20 billion capital expenditures (CAPEX) program. The first phase of the CAPEX program led to the doubling of OCP’s mining capacity and the tripling of its fertilizer production capacity. As a result, OCP increased its fertilizer production capacity by eight million tons in the past decade. The model enabled OCP to produce customized fertilizers that helped improve agricultural yields, which in turn led to increased food production, especially in Africa. The increased production of fertilizers and the availability of customized fertilizers have contributed toward global food security. By demonstrating the interconnectedness of all OCP businesses, the model contributed to creating a culture of collaboration, innovation, and entrepreneurship across the company while breaking the existing silos among departments. This transformation led to the establishment of an analytics-based problem-solving approach throughout OCP and a successful executive education class at the Massachusetts Institute of Technology, thus enriching the university. © 2022 INFORMS Toward Global Food Security: Transforming OCP Through Analytics chemical transformation; Edelman Award; fertilizers; mining; mixed-integer optimization; optimal product portfolio; supply chain ",Strategic alignment
197,Leveraging machine learning to detect data curation activities,"This paper describes a machine learning approach for annotating and analyzing data curation work logs at ICPSR, a large social sciences data archive. The systems we studied track curation work and coordinate team decision-making at ICPSR. Archive staff use these systems to organize, prioritize, and document curation work done on datasets, making them promising resources for studying curation work and its impact on data reuse, especially in combination with data usage analytics. A key challenge, however, is classifying similar activities so that they can be measured and associated with impact metrics. This paper contributes: 1) a set of data curation activities; 2) a computational model for identifying curation actions in work log descriptions; and 3) an analysis of frequent data curation activities at ICPSR over time. We first propose a set of data curation actions to help us analyze the impact of curation work. We then use this set to annotate a set of data curation logs, which contain records of data transformations and project management decisions completed by archive staff. Finally, we train a text classifier to detect the frequency of curation actions in a large set of work logs. Our approach supports the analysis of curation work documented in work log systems as an important step toward studying the relationship between research data curation and data reuse. © 2021 IEEE. Leveraging machine learning to detect data curation activities Data curation; Machine learning; Research infrastructures; Text classification; Workflows Classification (of information); Decision making; Machine learning; Project management; Text processing; Curation; Data archives; Data curation; Data reuse; Machine learning approaches; Research infrastructure; Social science data; Team decision; Text classification; Work-flows; Human resource management",Monitoring and control
198,Integrating artificial intelligence and analytics in smart grids: a systematic literature review,"Purpose: The purpose of this study is to explore the latest approaches in integrating artificial intelligence and analytics (AIA) in energy smart grid projects. Empirical results are synthesized to highlight their relevance from a technology and project management standpoint, identifying several lessons learned that can be used for planning highly integrated and automated smart grid projects. Design/methodology/approach: A systematic literature review leads to selecting 108 research articles dealing with smart grids and AIA applications. Keywords are based on the following research questions: What is the growth trend in Smart Grid projects using intelligent systems and data analytics? What business value is offered when AI-based methods are applied? How do applications of intelligent systems combine with data analytics? What lessons can be learned for Smart Grid and AIA projects? Findings: The 108 selected articles are classified according to the following four research issues in smart grids project management: AIA integrated applications; AI-focused technologies; analytics-focused technologies; architecture and design methods. A broad set of smart grid functionality is reviewed, seeking to find commonality among several applications, including as follows: dynamic energy management; automation of extract, transform and load for Supervisory Control And Data Acquisition (SCADA) systems data; multi-level representations of data; the relationship between the standard three-phase transforms and modern data analytics; real-time or short-time voltage stability assessment; smart city architecture; home energy management system; building energy consumption; automated fault and disturbance analysis; and power quality control. Originality/value: Given the diversity of issues reviewed, a more capability-focused research agenda is needed to further synthesize empirical findings for AI-based smart grids. Research may converge toward more focus on business rules systems, that may best support smart grid design, proof development, governance and effectiveness. These AIA technologies must be further integrated with smart grid project management methodologies and enable a greater diversity of renewable and non-renewable production sources. © 2021, Emerald Publishing Limited. Integrating artificial intelligence and analytics in smart grids: a systematic literature review Artificial intelligence; Data analytics; Energy sector; Project management; Smart grid; Surveys Advanced Analytics; Automation; Data Analytics; Electric power system control; Electric power transmission networks; Energy management; Energy management systems; Energy utilization; Information management; Intelligent systems; Project management; Quality control; Real time systems; SCADA systems; Solar buildings; System stability; Building energy consumption; Design/methodology/approach; Dynamic energy managements; Extract , transform and loads; Home energy management systems; Supervisory control and dataacquisition systems (SCADA); Systematic literature review; Voltage stability assessment; Smart power grids",Financial management
199,How do credit markets react to earnings releases? Empirical analysis and implications for investors,"Previous studies have found that stock prices incorporate information from earnings announcements only gradually over an extended period. The authors investigate whether credit markets react to the earnings releases in a similar way. They find that issuers with more positive earnings surprises had higher subsequent abnormal bond returns than issuers with more negative surprises after the announcement for several months. The results are not driven by risk compensation or illiquidity; are distinct from the momentum phenomenon; and are evident in all time periods, industries, and credit ratings. The authors also find that index-tracking portfolios that match the IG and HY index key analytics but overweight issuers with positive earnings surprises outperformed the respective index across different market regimes, showing that this market anomaly can be exploited by credit investors in practice. Copyright 2020 Pageant Media Ltd. How do credit markets react to earnings releases? Empirical analysis and implications for investors  ",Risk management
200,A Comparative Study of Machine Learning Approaches for Non Performing Loan Prediction,"Credit risk estimation and the risk evaluation of credit portfolios are crucial to financial institutions which provide loans to businesses and individuals. Non-performing loan (NPL) is a loan type in which the customer has a delinquency; because they have not made the scheduled payments for a time period. NPL prediction has been widely studied in both finance and data science. In addition, most banks and financial institutions are empowering their business models with the advancements of machine learning algorithms and analytical big data technologies. In this paper, we studied on several machine learning algorithms to solve this problem and we propose a comparative study of some of the mostly used non performing loan models on a customer portfolio dataset in a private bank in Turkey. We also deal with a class imbalance problem using class weights. A dataset, composed by 181.276 samples, has been used to perform the analysis considering different performance metrics (i.e. Precision, Recall, F1 Score, Imbalance Accuracy (IAM), Specificity). In addition to these, we evaluated the performance of the algorithms and compared the obtained results. According to these performance metrics, LightGBM gave the best results among the logistic regression, SVM, random forest, bagging classifier, XGBoost and LSTM for the dataset. © 2021 IEEE A Comparative Study of Machine Learning Approaches for Non Performing Loan Prediction Big data; Machine learning; Non performing loan prediction; Non performing loans; Supervised learning Big data; Classification (of information); Decision trees; Finance; Learning algorithms; Logistic regression; Long short-term memory; Risk assessment; Risk perception; Support vector machines; Comparatives studies; Credit portfolio; Credit risks; Financial institution; Machine learning algorithms; Machine learning approaches; Non performing loan; Non performing loan prediction; Performance metrices; Risk estimation; Forecasting",Monitoring and control
201,Project performance analysis using hierarchical clustering method,This paper deals with the project management performance analysis in automotive industry and usage of the advanced analytics to optimize the performance of the engineering projects within the vehicle manufacturing facility. The aim is to identify and mitigate the common causes of project delay (slippage) in various engineering projects that are independent of the technical discipline where a particular project belongs to. We have used the hierarchical clustering technique here to identify the sub-groups of similar projects and further deeply analysed those ones with the highest average slippage. © 2021 International Journal of Emerging Technology and Advanced Engineering. All rights reserved. Project performance analysis using hierarchical clustering method Cluster analysis; Data; Delay; Distance; Hierarchical clustering; Metric; Project; Slippage; Strategy ,Monitoring and control
202,A hybrid energy systemworkflow for energy portfolio optimization,"This manuscript develops a workflow, driven by data analytics algorithms, to support the optimization of the economic performance of an Integrated Energy System. The goal is to determine the optimum mix of capacities from a set of different energy producers (e.g., nuclear, gas, wind and solar). A stochastic-based optimizer is employed, based on Gaussian Process Modeling, which requires numerous samples for its training. Each sample represents a time series describing the demand, load, or other operational and economic profiles for various types of energy producers. These samples are synthetically generated using a reduced order modeling algorithm that reads a limited set of historical data, such as demand and load data from past years. Numerous data analysis methods are employed to construct the reduced order models, including, for example, the Auto Regressive Moving Average, Fourier series decomposition, and the peak detection algorithm. All these algorithms are designed to detrend the data and extract features that can be employed to generate synthetic time histories that preserve the statistical properties of the original limited historical data. The optimization cost function is based on an economic model that assesses the effective cost of energy based on two figures of merit: The specific cash flow stream for each energy producer and the total Net Present Value. An initial guess for the optimal capacities is obtained using the screening curve method. The results of the Gaussian Process model-based optimization are assessed using an exhaustive Monte Carlo search, with the results indicating reasonable optimization results. The workflow has been implemented inside the Idaho National Laboratory’s Risk Analysis and Virtual Environment (RAVEN) framework. The main contribution of this study addresses several challenges in the current optimization methods of the energy portfolios in IES: First, the feasibility of generating the synthetic time series of the periodic peak data; Second, the computational burden of the conventional stochastic optimization of the energy portfolio, associated with the need for repeated executions of system models; Third, the inadequacies of previous studies in terms of the comparisons of the impact of the economic parameters. The proposed workflow can provide a scientifically defendable strategy to support decision-making in the electricity market and to help energy distributors develop a better understanding of the performance of integrated energy systems. © 2021 by the authors. Licensee MDPI, Basel, Switzerland. A hybrid energy systemworkflow for energy portfolio optimization Economic optimization; Gaussian process; HERON; Integrated energy systems; RAVEN; Synthetic time history generation Cost functions; Data Analytics; Decision making; Financial data processing; Fourier series; Gaussian distribution; Gaussian noise (electronic); Optimization; Risk analysis; Risk assessment; Stochastic models; Time series; Autoregressive moving average; Computational burden; Data analysis methods; Gaussian process models; Idaho national laboratories; Integrated energy systems; Statistical properties; Stochastic optimizations; Reduced order models; Data reduction",Financial management
203,Accelerating Hydrocarbon Maturation and Project Delivery by 30% with Digitalization - Standardizing on the Fly Analysis to Enable Informed Decision Making Using Petabytes of Petro Technical Data,"In recent years, with the steep drop and increased volatility in oil price, there is an urgency for making our field (re-development) plans more dynamic and efficient with faster payback and with particular emphasis on robustness against uncertainties. This paper describes a root because analysis and a methodology to achieve up to ~30% improvement in field development planning project cycle and developing a better-integrated reservoir understanding. A comprehensive integrated analysis of available data is a key success criterion for robust decision-making. A detailed value stream mapping and a timeline analysis for data analysis in the hydrocarbon maturation process revealed that our process cycle efficiency is only 16% with a significant room for improvement. Any improvement can be directly translated to man-hour cost saving and acceleration of oil delivery. Effective use of technology and digitalization for knowledge management, standardized ways of working and easy access to historical data, analysis and diagnostics were identified as key focus areas to improve delivery. An innovative process and web based digital platform, iResDAT, is developed for accelerating data analysis. It mines from volumes of petro-technical databases and translates data into standardized diagnostics using latest data analytics and visualization technologies. It has already reduced dramatically the time to mine critical subsurface data and prepare required integrated diagnostics that are auditable and can be re-created in a few seconds. Based on the early pilot studies the cycle time reduction in the data analysis phase is close to 30% with improved quality and standardization of the integrated analysis. It has already transformed the ways of working where the subsurface discussion can happen across disciplines using a single platform that enforces early integration for reservoir understanding and associated uncertainty characterization. It is a web-based platform where the diagnostic dashboards are crowd sourced; sustained and enhanced by the business to ensure the relevance and sustainability with the Corporate Data Management and IT functions. It is a building block towards quality controlled and auditable data analysis and interpreted dataset, which may form the backbone for any advanced analytics in future to enable digitally enabled hydrocarbon maturation. © Copyright 2021, Society of Petroleum Engineers Accelerating Hydrocarbon Maturation and Project Delivery by 30% with Digitalization - Standardizing on the Fly Analysis to Enable Informed Decision Making Using Petabytes of Petro Technical Data  Data Analytics; Data visualization; Decision making; Hydrocarbons; Information analysis; Knowledge management; Petroleum reservoir evaluation; Project management; Quality control; Websites; Decisions makings; Informed decision; Integrated analysis; Oil Prices; On-the-fly; Petabytes; Project delivery; Re-development; Steep drop; Technical data; Data handling",Value management
204,Assessment of Systems Requirements Specification Skills Based on an Industry Body of Knowledge,"Contribution - The acquisition of systems requirements knowledge and skills by systems developers is a desirable outcome of a STEM graduate program in technology management. In this paper, we assess whether the learning outcomes of such a STEM program are being met. We present a process to align the program curriculum with an industry-standard to ensure the acquisition of those competencies. Background - We selected the International Institute of Business Analysts (IIBA) as a vendor-neutral body of knowledge for the Business Analyst profession. We used a well-documented process to align a university curriculum to the industry needs, following earlier for project management and data analytics. Research questions - To align the curriculum to the IIBA BABOK (Business Analysis Body of Knowledge), we asked: (1) whether students graduating from the program had acquired good business analysis and systems requirements competencies; and (2) If they had not, how the curriculum may be modified to help students acquire those competencies. Methods - The curriculum was reviewed, and we assured proper topical coverage by pertinent course to the IIBA BABOK knowledge. Results - Using the existing curriculum, we found that most students were able to pass the assessment based on the IIAB BABOK. We identified some deficiencies by knowledge domain. The assessment results were sufficiently granular to make modifications to the curriculum and course contents to improve student's acquisition of systems requirements specifications skills. © 2021 IEEE. Assessment of Systems Requirements Specification Skills Based on an Industry Body of Knowledge Body of Knowledge; business analysis; business analyst; certification; curriculum alignment; IIBA; requirements specifications Data Analytics; Engineering education; Project management; Requirements engineering; Specifications; Students; Body of knowledge; Business analysis; Business analysts; Certification; Curriculum alignments; International institute of business analyst; Requirements specifications; System requirements; System requirements specifications; Curricula",Stakeholder management
205,DEVISING SCIENTIFIC AND METHODOLOGICAL TOOLS TO STRENGTHEN THE ECONOMIC SECURITY OF A REGION THROUGH THE IMPROVEMENT OF TECHNOLOGIES FOR MARKETING SUPPORT OF TOURISM,"The concept of marketing support for strengthening the economic security of a region by intensifying tourist business was developed. It was established that the economic security of a region is the result of the influence of a number of processes of different nature and degrees of influence. The safety assessment depends on its purpose and groups of partial indicators that provide a comprehensive assessment. The result of the evaluation procedure should be a grounded conclusion on one of the possible states: economic security, economic danger, economic risk, and economic threat. The tourism sector, influencing the level of security of a region through the generation of additional financial and related flows, requires clarification of the content of scientific and methodological approaches to management. It is proposed to separate administrative methods and market mechanisms as complementing each other. At the level of separate tourist enterprises and their associations, it is advisable to implement marketing strategies for traditional expansion of the market, product, and innovative updating of the portfolio of tourist products. The main tools and technologies for achieving set goals are digital information and communication technologies. Their application is associated with the use of the method for economic and mathematical modeling, as well as SWOT analysis technology and the use of a special BITOUR platform. The use of the proposed theoretical and methodical proposals will make it possible to assess the economic security of a region in terms of its possible condition. It will also be useful in the development of strategic and tactical plans for regulating tourism business in the system of ensuring the economic development of a region © 2021. All Rights Reserved. DEVISING SCIENTIFIC AND METHODOLOGICAL TOOLS TO STRENGTHEN THE ECONOMIC SECURITY OF A REGION THROUGH THE IMPROVEMENT OF TECHNOLOGIES FOR MARKETING SUPPORT OF TOURISM big data; business analytics; economic security; information and communication technologies; marketing strategy; tourism business ",Risk management
206,Risk optimisation analytics: A case study on Brown Research Associates India (BRAI),"Risk optimization using business analytics is gaining momentum in India over the last decade. In order to tap this huge opportunity, most of the startups are getting into the analytics field engaging in financial market survey to provide their customers with valid data. The objective of this study is to help “Sai Builders” in solving their portfolio investment problem as well as sinking funds problem using linear programming and to obtain the total optimum returns by satisfying all constraints. The authors solve the problem of minimizing portfolio risk measures. In addition, the expected return of the portfolio is maximized subject to the aforementioned risk measures. By using numerical experiments, they illustrate the impact of these risk measures on portfolio optimization. The analysis is done by using Excel Solver, and the optimum solution is achieved. Copyright © 2021, IGI Global. Risk optimisation analytics: A case study on Brown Research Associates India (BRAI) Excel solver; Linear programming; Portfolio; Risk optimization; Risk score; Sinking funds ",Strategic alignment
207,Constructing Insights on Learning Analytic Student Activity Data from an Online Undergraduate Construction Management Course,"The growth of online learning has contributed to researchers exploring innovative ways to develop successful learning environments for students and instructors. Learning analytics is one of the emerging fields that can improve academic decision-making and increase the quality and value of the learning experience when implemented into online courses. This paper discusses how learning analytics describe student activity using a multiple-case study of an online construction management course. The study collected learning analytics through the Canvas Learning Management System (LMS) from forty-five students across two course sections. Results from the learning analytics indicated that student activity is at its peak in the afternoon and evening. The analyses also found discussion participation to be an indicator of course performance. By knowing when and how students are active, instructors can increase instructor-student interactions and improve student outcomes in performance. Establishing learning analytics in online courses can better inform instructors of student interactions and ultimately she would light on the unknown world within online courses. © American Society for Engineering Education, 2021 Constructing Insights on Learning Analytic Student Activity Data from an Online Undergraduate Construction Management Course  Computer aided instruction; Decision making; E-learning; Project management; Construction Management Courses; Course performance; Decisions makings; Learning environments; Learning experiences; Learning management system; Multiple-case study; Online course; Online learning; Student interactions; Students",Risk management
208,Process Mining for Six Sigma: Utilising Digital Traces,"Six Sigma is one of the most successful quality management philosophies of the past 20 years. However, the current challenges facing companies, such as rising process and supply chain complexity, as well as high volumes of unstructured data, cannot easily be answered by relying on traditional Six Sigma tools. Instead, the Process Mining (PM) technology using big data analytics promises valuable support for 6S and its data analysis capabilities. The article presents a design science research project in which a method for the integration of PM in Six Sigma's DMAIC project structure was developed. This method could be extended, refined and tested during three evaluation cycles: an expert evaluation with Six Sigma professionals, a technical experiment and finally a multi case study in a company. The method therefore was eventually endorsed by 6S experts and successfully applied in a first pilot setting. This article presents the first developed method for the integration of PM and Six Sigma. It follows the recommendations of many researchers to test Six Sigma as an application field of PM as well as using the potential of big data analytics. The method can be used by researchers and practitioners alike to implement, test and verify its design in organisations. © 2020 Elsevier Ltd Process Mining for Six Sigma: Utilising Digital Traces big data analytics; data science; DMAIC; process mining; project management; six sigma Advanced Analytics; Big data; Data Analytics; Data mining; Philosophical aspects; Process monitoring; Silicon; Six sigma; Supply chains; Application fields; Design-science researches; Expert evaluation; Management philosophy; Project structure; Six Sigma tools; Supply chain complexity; Unstructured data; Process engineering",Strategic alignment
209,Application of enhanced RPB tool in Malaysian oilfields for gas injection RF benchmarking and analogue identification,"Identification of an optimal field development plan is one of the most critical decisions for oil asset management. In the new norm of low oil prices, this assumes even more relevance for mature oilfields to maximize overall recovery. In greenfield developments and oilfields in early production life, the absence of a clear roadmap detailing their future development strategy can often lead to missed opportunities and sub-optimal recovery. To bridge this gap, Petroliam Nasional Berhad (PETRONAS) began identifying new improved oil recovery (IOR) opportunities in mature fields and started formulating an optimal development plan in greenfields. In 2019, PETRONAS embarked on implementing the Reservoir Performance Benchmarking (RPB) tool to evaluate reservoir recovery potential with waterflood. This paper will detail the Phase-II development of this data-analytics based tool which focuses on delivering a comprehensive roadmap, which includes other recovery mechanisms such as gas injection and reservoir under primary recovery. Phase-I of RPB tool (SPE-196443) had considered water injection as a key secondary recovery process to evaluate the benchmark recovery factor for an oil reservoir. As part of Phase-II development, this has been further enhanced to evaluate field recovery potential and provide the benchmark recovery factor for primary recovery and gas injection processes. For fields under primary recovery, a comparative assessment between volumetric depletion and varying aquifer/gas-cap drive is conducted to ascertain the recovery potential. Assessment for incremental secondary recovery gains considers both gas injection and water injection scenarios in the enhanced benchmarking methodology. In addition, the benchmarking calibration methodology has also focused on incorporating additional reservoir parameters specific to each of the recovery processes for overall estimation of the benchmark value. The benchmarking tool also identifies analogue reservoirs to enable replication of best development practices and optimization of the development strategy. With the deployment of this enhanced RPB tool, a comprehensive 5-year roadmap has been developed to improve Malaysian oilfields recovery. This has enabled PETRONAS to augment its resource funnel inventory, streamline its opportunity ranking and integrate project maturation tracking with existing digital platforms of its entire portfolio. This is a novel benchmarking tool to assess reservoir potential recovery factor for primary and secondary recovery processes (both water and gas injection) along with analogue identification for replication of best development practices. © 2021, Society of Petroleum Engineers. Application of enhanced RPB tool in Malaysian oilfields for gas injection RF benchmarking and analogue identification  Aquifers; Benchmarking; Data Analytics; Digital storage; Enhanced recovery; Gases; Gasoline; Oil well flooding; Petroleum reservoir evaluation; Petroleum reservoirs; Secondary recovery; Strategic planning; Benchmarking tools; Development strategies; Greenfield; Malaysians; Performance benchmarking; Phase II; Recovery factors; Recovery process; Reservoir performance; Roadmap; Recovery",Risk management
210,Research on the Construction Method of Intelligent Prediction and Analysis Model for the Whole Process of Power Grid Project Cost,"The government's supervision of power grid enterprises will gradually focus on the transmission and distribution price, and the investment and income will be more strictly supervised. Under the new management requirements, the company must pay more attention to the compliance of the investment process, further strengthen the investment risk control, put an end to inefficient or invalid investment, strengthen the all-round and whole process supervision, and scientifically and accurately determine and carry out effective project cost control and management. It is the key to achieve project management objectives, and also an important measure of investment fine management and control. This paper takes historical cost data as the research object, constructs the whole process intelligent prediction and analysis model of power grid project cost, assists investment decision-making, reduces the balance rate, and improves the efficiency and efficiency of the company's investment and lean management level. © The Authors, published by EDP Sciences, 2020. Research on the Construction Method of Intelligent Prediction and Analysis Model for the Whole Process of Power Grid Project Cost  Catalysis; Compliance control; Cost benefit analysis; Decision making; Efficiency; Electric power transmission networks; Investments; Predictive analytics; Project management; Smart power grids; Construction method; Historical cost datum; Intelligent prediction; Investment decision making; Investment risks; Management and controls; Management objectives; Transmission and distribution; Costs",Strategic alignment
211,A risk prediction model for software project management based on similarity analysis of context histories,"Context: Risk event management has become strategic in Project Management, where uncertainties are inevitable. In this sense, the use of concepts of ubiquitous computing, such as contexts, context histories, and mobile computing can assist in proactive project management. Objective: This paper proposes a computational model for the reduction of the probability of project failure through the prediction of risks. The purpose of the study is to show a model to assist teams to identify and monitor risks at different points in the life cycle of projects. The work presents as scientific contribution to the use of context histories to infer the recommendation of risks to new projects. Method: The research conducted a case study in a software development company. The study was applied in two scenarios. The first involved two teams that assessed the use of the prototype during the implementation of 5 projects. The second scenario considered 17 completed projects to assess the recommendations made by the Átropos model comparing the recommendations with the risks in the original projects. In this scenario, Átropos used 70% of each project's execution as learning for the recommendations of risks generated to the same projects. Thus, the scenario aimed to assess whether the recommended risks are contained in the remaining 30% of the executed projects. We used as context histories, a database with 153 software projects from a financial company. Results: A project team with 18 professionals assessed the recommendations, obtaining a result of 73% acceptance and 83% accuracy when compared to projects already being executed. The results demonstrated a high percentage of acceptance of the recommendation of risks compared to the other models that do not use the characteristics and similarities of projects. Conclusion: The results show the applicability of the risk recommendation to new projects, based on the similarity analysis of context histories. This study applies inferences on context histories in the development and planning of projects, focusing on risk recommendation. Thus, with recommendations considering the characteristics of each new project, the manager starts with a larger set of information to make more assertive project planning. © 2020 A risk prediction model for software project management based on similarity analysis of context histories Agile management; Project management; Risk identification; Risk management; Risk prediction; Risk response; Software management Life cycle; Predictive analytics; Project management; Risk management; Risk perception; Software design; Ubiquitous computing; Computational model; Event management; Financial companies; Risk prediction models; Scientific contributions; Similarity analysis; Software project; Software project management; Risk assessment",Risk management
212,Associative classification model for forecasting stock market trends,"This paper proposes an associative classification model based on three technical indicators to forecast future trends of stock market. Our methodology assessed the performance of nine technical indicators, using a portfolio of ten stocks and a 12-year time series. The experimental results showed that the use of a set of technical indicators leads to higher classification rates compared to the use of sole technical indicators, reaching an accuracy of 88.77%. The proposed approach also uses a multidimensional data cube that allows automatic updating of stock market asset values, which are essential to keep the forecast updated. The results indicate that our approach can support investors and analysts to operate in the stock market. Copyright © 2021 Inderscience Enterprises Ltd. Associative classification model for forecasting stock market trends Associative classification; Business intelligence; Data mining; Stock market trends; Technical indicators Classification (of information); Commerce; Competitive intelligence; Financial markets; Forecasting; Investments; Associative classification; Classification models; Classification rates; Future trends; Market trends; Model-based OPC; Performance; Stock market trend; Technical indicator; Times series; Data mining",Financial management
213,Information and algorithmic support of a multi-level integrated system for the investment strategies formation,"The article summarizes the accumulated practical experience of the authors in the development of algorithms for the formation of investment strategies. For this purpose, the optimization of the studied parameters, information support of investment activities, verification, monitoring and adjustment in the testing mode and the subsequent practical application of the described tools are considered. The system is based on the main provisions of the Markowitz portfolio theory. The analytical block of the Information System Portfolio Investor includes Profitability-Risk model; empirical models of optimal complexity; hybrid predictive model systems; the principle of combining (integrating) both models and forecasts, as well as decision rules; optimization of the training sample length (modified Markowitz model); optimization of the frequency of monitoring and adjusting the composition of the investment portfolio. The principles of design and development of the information block of the system, its replenishment and functioning are described in detail. All the above listed components of the algorithmic content of the investment decision making system are described sequentially. The system modules have been successfully tested on a wide class of financial instruments: ordinary shares, preferred shares, government and corporate bonds, exchange commodities, stock, commodity, industry and bond indices, exchange-traded investment funds and real estate funds. The implemented Markowitz model with a dynamic database of historical data can significantly increase the efficiency of investment decisions, which is facilitated by taking into account the characteristics of both the markets under study and the corresponding financial instruments.  Copyright © 2021 for this paper by its authors. Information and algorithmic support of a multi-level integrated system for the investment strategies formation  Commerce; Control systems; Decision making; Financial markets; Predictive analytics; Risk assessment; Strategic planning; Information support; Investment decision making; Investment decisions; Investment portfolio; Investment strategy; Markowitz portfolio theories; Practical experience; Principles of design; Investments",Strategic alignment
214,Resilience of business strategy to emergent and future conditions,"Assuring future performance of systems-of-systems through advanced-technology investments is a perpetual challenge of industry and agencies. Among the complicating factors are technology innovation, escalating scales, and diversity of software and hardware applications, increasing availability and scrutiny of big data, and evolving business, environmental, and legal contexts. These factors are engaging system owner/operators to continually reprioritize these investments, even as transparent principles for investment are needed for appropriate oversight and auditing. In this paper, a branch of resilience analysis offers to address multiple layers of uncertainty that arise from technology plans around future disruptions to large-scale systems-of-systems. The paper presents a methodology to quantify and manage the disruptive influence of individual systems perspectives to the prioritization of technology investments across the system-of-systems. The methodology is demonstrated through a case study on an information technology investment portfolio of the US Department of Commerce, USA. The experience suggests how fiscal limitations, combining with several other factors, has the largest disruption to the prioritization of investments. The results furthermore describe how investments perform relative to one another and characterize where the system-of-systems might be resilient to the perspectives of constituent systems. Abbreviations: BEA: Bureau of Economic Analysis; BIS: Bureau of Industry and Security; DOC: United States Department of Commerce; FY: Fiscal Year; IT: Information Technology; MCDA: Multicriteria Decision Analysis; NOAA: National Oceaic and Atmospheric Administration; NTIA: National Telecommunications and Information Administration; USPTO: United States Patent and Trademark Office. © 2019 Informa UK Limited, trading as Taylor & Francis Group. Resilience of business strategy to emergent and future conditions business analytics; engineering management; government; information technology; multi-criteria analysis; resilience; Risk management; schedule; systems-of-systems Application programs; Commerce; Decision theory; Economic analysis; Environmental regulations; Environmental technology; Information technology; Investments; Large scale systems; Risk assessment; Risk management; Scheduling; System of systems; Uncertainty analysis; Business analytics; Engineering management; government; Multi Criteria Analysis; resilience; Systems of systems; Information management",Strategic alignment
215,Closed-Loop Data Analytics for Wells Construction Management in Real Time Centre,"The execution phase of the wells technical assurance process is a critical procedure where the drilling operation commences and the well planning program is implemented. During drilling operations, the real-time drilling data are streamed to a real-time centre where it is constantly monitored by a dedicated team of monitoring specialists. If any potential issues or possible opportunities arise, the team will communicate with the operation team on rig for an intervention. This workflow is further enhanced by digital initiatives via big data analytics implementation in PETRONAS. The Digital Standing Instruction to Driller (Digital SID) is a drilling operational procedures documentation tool meant to improve the current process by digitalizing information exchange between office and rig site. Boasting multi-operation usage, it is made fit to context and despite its automated generation, this tool allows flexibility for the operation team to customize the content and more importantly, monitor the execution in real-time. Another tool used in the real-time monitoring platform is the dynamic monitoring drilling system where it allows real-time drilling data to be more intuitive and gives the benefit of foresight. The dynamic nature of the system means that it will update existing roadmaps with extensive real-time data as they come in, hence improving its accuracy as we drill further. Furthermore, an automated drilling key performance indicator (KPI) and performance benchmarking system measures drilling performance to uncover areas of improvement. This will serve as the benchmark for further optimization. On top of that, an artificial intelligence (AI) driven Wells Augmented Stuck Pipe Indicator (WASP) is deployed in the real-time monitoring platform to improve the capability of monitoring specialists to identify stuck pipe symptoms way earlier before the occurrence of the incident. This proactive approach is an improvement to the current process workflow which is less timely and possibly missing the intervention opportunity. These four tools are integrated seamlessly with the real-time monitoring platform hence improving the project management efficiency during the execution phase. The tools are envisioned to offer an agile and efficient process workflow by integrating and tapering down multiple applications in different environments into a single web-based platform which enables better collaboration and faster decision making. Copyright © 2021, International Petroleum Technology Conference. Closed-Loop Data Analytics for Wells Construction Management in Real Time Centre  Benchmarking; Decision making; Drills; Gasoline; Infill drilling; Project management; 'current; Data analytics; Drilling data; Drilling operation; Execution phasis; Monitoring platform; Real time drilling; Real time monitoring; Real- time; Work-flows; Data Analytics",Capacity management
216,Big Data IAOM Project Management and Workflow Automation in a Giant Gas Field Digitization Drive,"Integrated asset modeling, application of big data, and automation are among the top emerging trends in the oil and gas industry. The value associated with such implementation projects is very closely linked with the efficient use of the project management approach and a robust strategy to handle the technological challenges. This paper puts light on such initiatives implemented in a giant gas field. In this giant gas condensate field, a vast amount of data is generated and monitored on a daily basis. The frequent need to deliver the dynamic production target was driving this project implementation so that a value-driven system can be established while achieving the business KPIs. A phased approach was used to target multiple requirements into business deliverables where the early offline phases provided a robust base for full online integration. This project followed the agile approach focusing on getting insights from multiple stakeholders and domain experts and developing a lesson-learnt repository in all the project phases. The online integration solution is a critical differentiator in the workforce and process efficiency improvement. The multiple technical solution workflows helped in reducing manual efforts and streamlining the methodology in a standardized fashion. In addition, the standard project management practices, such as initializing the phases in a planned manner, followed by an interactive execution, monitoring, and controlling stages, ensured delivering project outcomes in an efficient way. This implementation also established a robust collaborative team effort to identify various different roles and responsibilities for stakeholders. This helped in the end phase when the project sustainability was essential. A strong team base maintained and updated the integrated system while delivering daily well and facility surveillance objectives and KPIs from users ranging from planning, engineering, operation, and management team. A special focus on IT team involvement throughout the project phase led to a successful data integration and diagnostic, as the core of the solution was a data-driven analytical framework integrated with multiple corporate and real-time data sources. In addition, this solution was equipped with various one-of-its-kind solution features such as business intelligence, advanced surveillance, dynamic-reservoir integration, manage-by-exception workflows, intelligence alerts, along with a strong digital framework and data architecture. The unique hybrid and agile project management approach focusing on delivering emerging trends and technologies to end-users in the most efficient way paved the way for achieving asset digitalization and standardization goals. © Copyright 2021, Society of Petroleum Engineers Big Data IAOM Project Management and Workflow Automation in a Giant Gas Field Digitization Drive  Big data; Digital storage; Gas condensates; Gas industry; Human resource management; Information management; Project management; Reservoir management; Digitisation; Emerging trends; Giant gas fields; Implementation projects; Integrated Asset Modeling; Model application; Oil and Gas Industry; Online integration; Project phasis; Work-flows; Data integration",Value management
217,"Organization and IT Strategic Alignment, Determination of IT Process Priorities using COBIT 5","The role of information technology (IT) for many organizations today is not just a supporting role, but already has strategic role as an organizational enabler. Organizations that do not utilize information systems and technology will inevitably be unable to compete in fierce business competition and satisfy customers. Therefore companies need to align organizational strategies with IT strategies in order to achieve organizational goals. However, it is often difficult for organizations to translate their business strategy into an appropriate IT strategy. There a needs to be an appropriate, comprehensive and easy approach for organizations to be able to align organizational and IT strategies. COBIT 5 provides a fairly generic, comprehensive and easy approach to doing this. The approach is expected to provide guidance on aligning organizational and IT strategies while providing direction at the operational level for organizations to identify the IT business processes that suit their needs and determine their priority levels. COBIT 5 is best practices framework, provides a reference guide for IT governance and management processes, in total 37 processes. This research takes the object of a company in the field of sales distribution of mechanical and electrical products and services, which are quite competitive that seeks to implement appropriate IT strategies to support the achievement of their organizational goals. In the implementation of IT governance and management, the company found difficulty to determine priority of which IT processes need to be implemented first in order to be aligned with the organization and IT strategy, due to their limited resources. This study can help companies align their business and IT strategies and can provide priority proposals for IT business processes that companies should implement. By using COBIT 5 approach, companies can map IT process priorities based on mapping of organizational goals, IT objectives and IT processes. The results of this study are IT process priorities recommended for the company: Manage IT Risk, Manage IT Programs and Projects, Manage Change Acceptance and Transitioning, Manage IT Security, Manage IT Changes, Manage IT Services Operations, Manage IT Portfolios, and Manage Availability and Capacity. © 2020 IEEE. Organization and IT Strategic Alignment, Determination of IT Process Priorities using COBIT 5 COBIT 5); IT Process; IT Strategy Air navigation; Customer satisfaction; Data Science; E-learning; Information systems; Information use; Learning systems; Security of data; Business competition; Information systems and technologies; Management process; Mechanical and electrical products; Organizational goals; Organizational strategy; Sales distributions; Strategic alignment; Competition",Governance
218,Knowledge learning of insurance risks using dependence models,"Learning the customers’ experience and behavior creates competitive advantages for any company over its rivals. The insurance industry is an essential sector in any developed economy and a better understanding of customers’ risk profile is critical to decision making in all aspects of insurance operations. In this paper, we explore the idea of using copula-based dependence models to learn the hidden risk of policyholders in property insurance. Specifically, we build a novel copula model to accommodate the dependence over time and over space among spatially clustered property risks. To tackle the computational challenge caused by the discreteness feature of large-scale insurance data, we propose an efficient multilevel composite likelihood approach for parameter estimation. Provided that latent risk induces correlation, the proposed customer learning method offers improved predictive analytics by allowing insurers to borrow strength from related risks in predicting new risks and also helps reveal the relative importance of the multiple sources of unobserved heterogeneity in updating policyholders’ risk profile. In the empirical study, we examine the loss cost of a portfolio of entities insured by a government property insurance program in Wisconsin. We find both significant temporal and spatial association among property risks. However, their effects on the predictive distribution of loss cost are different for the new and renewal policyholders. The two sources of dependence are complements for the former and substitutes for the latter. These findings are shown to have substantial managerial implications in key insurance operations such as experience rating, capital allocation, and reinsurance arrangement. Copyright: © 2020 INFORMS Knowledge learning of insurance risks using dependence models Gaussian copula; Insurance operation; Machine learning; Multilevel model; Predictive analytics; Spatially clustered data Competition; Decision making; Insurance; Predictive analytics; Sales; Competitive advantage; Composite likelihood; Computational challenges; Developed economies; Managerial implications; Predictive distributions; Temporal and spatial; Unobserved heterogeneity; Learning systems",Strategic alignment
219,Development of a Digital ESP Performance Monitoring System Based on Artificial Intelligence,"Wintershall Dea is developing together with partners a digital system to monitor and optimize electrical submersible pump (ESP) performance based on the data from Mittelplate oil field. This tool is using machine learning (ML) models which are fed by historic data and will notify engineers and operators when operating conditions are trending beyond the operating envelope, which enables an operator to mitigate upcoming performance problems. In addition to traditional engineering methods, such a system will capture knowledge by continuous improvement based on ML. With this approach the engineer has a system at hand to support the day-to-day work. Manual monitoring and on demand investigations are now backed up by an intelligent system which permanently monitors the equipment. In order to create such a system, a proof of concept (PoC) study has been initiated with industry partners and data scientists to evaluate historic events, which are used to train the ML-systems. This phase aims to better understand the capabilities of machine learning and data science in the subsurface domain as well as to build up trust for the engineers with such systems. The concept evaluation has shown that the intensive collaboration between engineers and data scientist is essential. A continuous and structured exchange between engineering and data science resulted in a mutual developed product, which fits the engineer's needs based on the technical capabilities and limits set by ML-models. To organize such a development, new project management elements like agile working methods, sprints and scrum methods were utilized. During the development Wintershall Dea has partnered with two organizations. One has a pure data science background and the other one was the data science team of the ESP manufacturer. After the PoC period the following conclusions can be derived: (1) data quality and format is key to success; (2) detailed knowledge of the equipment speeds up the development and the quality of the results; (3) high model accuracy requires a high number of events in the training dataset. The overall conclusion of this PoC is that the collaboration between engineers and data scientists, fostered by the agile project management toolkit and suitable datasets, leads to a successful development. Even when the limits of the ML-algorithms are hit, the model forecast, in combination with traditional engineering methods, adds significant value to the ESP performance. The novelty of such a system is that the production engineer will be supported by trusted ML-models and digital systems. This system in combination with the traditional engineering tools improves monitoring of the equipment and taking decisions leading to increased equipment performance. © Copyright 2021, Society of Petroleum Engineers Development of a Digital ESP Performance Monitoring System Based on Artificial Intelligence  Engineers; Information management; Intelligent systems; Machine learning; Oil fields; Project management; Submersible pumps; Digital system; Electrical submersible pumps; Engineering methods; Machine learning models; Performance based; Performance monitoring systems; Proof of concept; Pump performance; Traditional engineerings; Wintershall; Monitoring",Monitoring and control
220,"Data Science for Water Cut Analysis System by Utilizing ESP Sensor Data, Conceptual Model, and Its Proposed Solution","Objective/scope: It has been a challenge to analyze and estimate reliable water cut. The current well test data is not sufficient to satisfy the required information for prediction of the rate and water cut behaviors. Only on wells having stable and good behaviors, water cut levels can be estimated appropriately. The wells have Electrical Submersible Pump (ESP) sensor reading and data acquisition recorded in real-time help to fill this gap. The data are stored and available in KOC data repositories, such as Corporate Database, Well Surveillance Management System (WSMS), and Artificial Lift Management System (ALMS) Engineers spend this effort in spreadsheets and working with multiple data repositories. It is fit for data analysis by combining the data into a simple data set and presentation. Nevertheless, spreadsheets do not address a number of important tasks in a typical analyst's pipeline, and their design frequently complicates the analyses. It may take hours for single well analysis and days for multi-wells analysis and could be too late to plan and take preventive actions. Concerning the above situation, collaboration has been performed between NFD-North Kuwait and Information Management Team. In this first phase, this initiative is to design a conceptual integrated preventive system, which provide easy and quick tool to compute water cut estimation from well tests and downhole sensors data by using data science approach. Method, procedure, process: There are 5 steps were applied in this initial work. It was included but not limited to user interview, exercise and performed data dissemination. It included gather full knowledge and defining the goal. Mapping pain points to solution also conducted to identify the technical challenge and find ways to overcome them. In the end of this stage, data and process review was conducted and applied for a given simple example to understand the requirements, demonstrate technical functionality and verify technical feasibility. Then conceptual design was built based on the requirements, features, and solutions gathered. Integrated system solution was recommended to include intermediate layer for integration, data retrieval, running calculation-heavy process in background, model optimization, visual analytics, decision-making, and automation. A roadmap with complete planning of different phases is then provided to achieve the objective. Results, observations, conclusions: Process, functionalities, requirements, and finding have been examined and elaborated. The conceptual design has proved and assured the utilization of ESP sensor data in helping to estimate continuous well water cut's behavior. Further, the next implementation phase of data science expects an increase of confidence level of the results into higher degree. The design is promising to achieve the requirement to provide seamless, scalable, and easy to deploy automation capability tools for data analytic workflow with several major business benefits arising. Proposed solution includes combination of technologies, implementation services, and project management. The proposed technology components are distributed into 3 layers, source data, data science layer, and visual analytics layer. Furthermore, a roadmap of the project along with the recommendation for each phase has also been included. Novel/additive information: Data Science for Exploration and Production is new area in which research and development will be required. Data science driven approach and application of digital transformation enables an integrated preventive system providing solution to compute water cut estimation from well tests and downhole sensors data. In the next larger scale of implementation, this system is expected to provide automated workflow supporting engineers in their daily tasks leveraging Data to Decision (D2D) approach. Machine learning is a data analytics technique that teaches computers to do what comes naturally to human, which is learn from experience. Machine learning algorithm use computational methods to learn information from the data without relying on predetermined equation as a model. Adding artificial intelligence and machine learning capability into the process requires knowledge on input data, the impact of data on the output, understanding of machine learning algorithm and building the model required to meet the expected output. © Copyright 2021, Society of Petroleum Engineers Data Science for Water Cut Analysis System by Utilizing ESP Sensor Data, Conceptual Model, and Its Proposed Solution  Conceptual design; Data acquisition; Data visualization; Human resource management; Information management; Oil wells; Search engines; Spreadsheets; Data repositories; Downholes; Electrical submersible pumps; Management systems; Roadmap; Sensors data; Simple++; Visual analytics; Water cuts; Well analysis; Decision making",Strategic alignment
221,Accurate response in agricultural supply chains,"In this paper, we introduce an accurate response framework in the context of commercial seed production by deploying the multiordering newsvendor model with dynamic forecast evolution to determine the timing and the quantity of production. We also demonstrate the challenges associated with applying the Martingale Model of Forecast Evolution (MMFE) to real data and propose practical remedies. More specifically, we fit the MMFE to the data for a variety of seeds (SKUs) produced by a major seed manufacturer and rank these SKUs based on their demand volume and volatility. We then assess the value of the MMFE-based accurate response by benchmarking it against the classic newsvendor model. We find that the MMFE-based accurate response can considerably increase the seed manufacturer's profits by neatly dividing the product portfolio into four quadrants, according to demand volume and volatility, to determine the production timing and quantity. Such portfolio categorization would also enable the salesforce to better allocate their efforts to increase forecasting accuracy for the most critical products in their portfolio. © 2020 Elsevier Ltd Accurate response in agricultural supply chains Accurate response; Business analytics; Commercial seeds; Evolution of forecasts; MMFE; Newsvendor problem article; benchmarking; forecasting; profit; seed production",Capacity management
222,Agile Auditing: Fundamentals and Applications,"Master new, disruptive technologies in the field of auditing Agile Auditing: Fundamentals and Applications introduces readers to the applications and techniques unlocked by tested and proven agile project management principles. This book educates readers on an approach to auditing that emphasizes risk-based auditing, collaboration, and speedy delivery of meaningful assurance assessments while ensuring quality results and a focus on the areas that pose the greatest material risks to the business under audit. The discipline of auditing has been forever changed via the introduction of new technologies, including: Machine learning Virtual Conferencing Process automation Data analytics Hugely popular in software development, the agile approach is just making its way into the field of audit. This book provides concrete examples and practical solutions for auditors who seek to implement agile techniques and methods. Agile Auditing is perfect for educators, practitioners, and students in the auditing field who are looking for ways to introduce greater levels of efficiency and effectiveness to their discipline. © 2021 by John Wiley & Sons, Inc. All rights reserved. Agile Auditing: Fundamentals and Applications  ",Risk management
223,How to measure and manage the UK Government’s major project portfolio,"Purpose: The purpose of this paper is to stimulate changes to the way performance data is used to improve performance taking the government’s use of project data as an example. Design/methodology/approach: This paper uses systems theory to review the way the government’s major projects portfolio should be analysed. Findings: This paper concludes that broader engagement in the analytics process should be considered as a way of improving insights and learning from reviews. The paper suggests that report alone has limited value. Research limitations/implications: By taking a systems approach, this study raises questions about the methods used to manage data analysis and system improvements. Systems thinking is a useful tool to consider applications such as the performance of the government’s project portfolio, but there are many other approaches that can be applied. Practical implications: This study makes very specific recommendations around the roles and responsibilities of people and teams at different levels in the system. Roles and activities are described together with recommendations about interfering in and overreaching these roles and activities. Originality/value: This paper synthesises a number of systems approaches together with a view of why “we measure” to create a framework for analysing approaches to performance improvement. The practical application provided here gives insights into how these approaches can be used in real-life contexts. © 2020, Emerald Publishing Limited. How to measure and manage the UK Government’s major project portfolio Learning; Performance management systems; Public sector; Systems ",Strategic alignment
224,Agile Project Management Based on Data Analysis for Information Management Systems,"Nowadays, many projects and product managers, industry, and portfolio leads understand that data from the project or portfolio can be valuable for increasing their activities. There are many different types of project and portfolio lifecycle processes of managers daily duties: pre-sales and sales, mobilization, delivery, and closure phases. Definitely, in research, we focus on the processes, staffing, governance, and reporting activities. The day-by-day tasks are quite regulated and clearly described using templates and techniques as a company standard. Our literature review shows that Data Science methods can increase the level of project management and project success in several business problems. This study gives new opportunities to improve project management evaluation and results for managers, industry, and delivery leads. The proposed approach allows doing a project, portfolio management, and agile development more accurately, considering best practices and project performance data. Moreover, our results can provide more efficient benefits for different internal and external stakeholders. © 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG. Agile Project Management Based on Data Analysis for Information Management Systems Agile development; Data analysis; Data science; Lean; Machine learning; Project management; Safe; Scrum Agile manufacturing systems; Data reduction; Data Science; Financial data processing; Information management; Investments; Life cycle; Machine learning; Managers; Agile development; Agile project management; Information management systems; Lean; Machine-learning; Product manager; Product portfolios; Project managers; Safe; Scra; Project management",Strategic alignment
225,The effect of Bellwether analysis on software vulnerability severity prediction models,"Vulnerability severity prediction (VSP) models provide useful insight for vulnerability prioritization and software maintenance. Previous studies have proposed a variety of machine learning algorithms as an important paradigm for VSP. However, to the best of our knowledge, there are no other existing research studies focusing on investigating how a subset of features can be used to improve VSP. To address this deficiency, this paper presents a general framework for VSP using the Bellwether analysis (i.e., exemplary data). First, we apply the natural language processing techniques to the textual descriptions of software vulnerability. Next, we developed an algorithm termed Bellvul to identify and select an exemplary subset of data (referred to as Bellwether) to be considered as the training set to yield improved prediction accuracy against the growing portfolio, within-project cases, and the k-fold cross-validation subset. Finally, we assessed the performance of four machine learning algorithms, namely, deep neural network, logistic regression, k-nearest neighbor, and random forest using the sampled instances. The prediction results of the suggested models and the benchmark techniques were assessed based on the standard classification evaluation metrics such as precision, recall, and F-measure. The experimental result shows that the Bellwether approach achieves F-measure ranging from 14.3% to 97.8%, which is an improvement over the benchmark techniques. In conclusion, the proposed approach is a promising research direction for assisting software engineers when seeking to predict instances of vulnerability records that demand much attention prior to software release. © 2020, Springer Science+Business Media, LLC, part of Springer Nature. The effect of Bellwether analysis on software vulnerability severity prediction models Bellwether; Feature selection; Machine learning algorithms; Severity; Software vulnerability Decision trees; Deep learning; Deep neural networks; Feature extraction; Forecasting; Genetic algorithms; Learning systems; Logistic regression; Natural language processing systems; Nearest neighbor search; Predictive analytics; Bellwether; Classification evaluation; K fold cross validations; NAtural language processing; Prediction accuracy; Severity; Software vulnerabilities; Vulnerability prioritization; Learning algorithms",Strategic alignment
226,"CONNECTING SYSTEMS SCIENCE, THINKING, DYNAMICS AND ENGINEERING TO SYSTEMS PRACTICE FOR MANAGING COMPLEXITY","Few organizations are fully integrated, program and engineering disciplines often work in isolation, and most organizations have misgivings in their ability to manage complexity. On the other hand, there are a few bright spots by way of mature forward-looking organizations that have achieved limited integration through Systems Thinking, advanced models and tools. This requires overcoming the hurdles of resistance to change and adopting new best practices. Complexity is inherent in the organization, its stakeholders and in its cross-functional processes. In large technical projects, complexity exists within the product design and its interrelated components. Adding to this complexity, how the techno-socio-economic and cultural factors affect the organization, its processes and product are not well understood. This disconnectedness and inability to understand underlying relationships have led to project cost overruns, rework and delays. Despite the use of traditional discipline specific-tools and models, product and project failures continue. When problems occur, shifting the burden from an optimal solution to a quick symptomatic solution can often occur without easy-to-use integrated decision support tools, as well as the processes for using them. It has been recognized that integrated decision support tools and models are needed to overcome the challenges in managing complexity. However, for these tools to be useful, they need to address the complexities of the organization, processes, product and the practical hurdles that affect them. As well, these tools need to monitor the emergent behaviour and performance of these connected entities throughout the product lifecycle. There are a number of foundational pillars proposed for the integration of organizations and the development of integrated tools. These pillars include Systems Thinking, dynamics, engineering and a digital thread between discipline specific models and tools. Systems Thinking provides for a new perspective and appreciation of the interrelationships within an organization, its processes and its product design. System Dynamics (SD), as a rigorous tool for Systems Thinking, provides for an understanding of the factors and complexity in these interrelationships. Systems Engineering (SE) looks at the design, integration and management of complex systems. With advancement of data analytics, visualization and intelligence augmentation comes easier construction of digital threads for connecting disparate models and tools. Integrated management models can facilitate and add rigor to Systems Thinking and reduce complexity through a better understanding of interrelationships. Moreover, a well constructed integrated model can provide for the gaming of change scenarios, trade studies, knowledge growth, and a decision support tool for optimal solutions. Standards are in the works for developing digital threads and frameworks to enable integration. However, the challenge is much broader than just solving the digital thread interconnection issue. With use of existing technology, Systems Thinking and systems dynamics, an integrated decision-support model was developed and presented at the International Council on Systems Engineering Project Management (PM) and Systems Engineering (SE) Integration Working Group (Jonkers 2020, INCOSE PM-SE WG). However, the authors of the current paper experienced resistance from SE and PM practitioners and senior leadership at this forum. Similar resistance was experienced during presentation of the model to mature engineering companies that designed and manufactured safety critical products. These hurdles and potential ways to overcome them are discussed in this paper. Partially integrated models and tools exist in systems engineering, project management and the social sciences. Model-based systems engineering and integrated model-based tools have made inroads predominately in the aerospace industry, but for the most part, there has been reluctance to adopt such tools. While a structured approach by way of foundational pillars has been proposed, it can be difficult for organizations to decide what tools to adopt without a roadmap or process to guide them. As a first step, increasing knowledge in Systems Thinking is viewed as a catalyst in moving toward integration of people, processes and the product. Systems Thinking has been viewed as the cornerstone to enabling positive outcomes including shared perspectives and a shared vision, knowledge growth and a learning culture. The next step involves shaping a governance framework based on proposed foundational pillars for integration. This framework includes best practices by associations that offer practical tools for integration. These associations include those who follow a systematic planned approach such as six-sigma methodology and systems engineering. © ISSS 2021. All right reserved. CONNECTING SYSTEMS SCIENCE, THINKING, DYNAMICS AND ENGINEERING TO SYSTEMS PRACTICE FOR MANAGING COMPLEXITY  Commerce; Data Analytics; Data visualization; Decision support systems; Knowledge management; Life cycle; Product design; Best practices; Decision support modelling; Decision supports; Integrated decision; Integrated modeling; Integrated tools; Optimal solutions; Support tool; System Dynamics; System thinkings; Systems thinking",Strategic alignment
227,Systematic Market and Asset Liquidity Risk Processes for Machine Learning: Robust Modeling Algorithms for Multiple-Assets Portfolios,"This chapter presents contemporary machine learning techniques for the computation of market and asset liquidity risk for multiple-assets portfolios. Furthermore, this research focuses on the theoretical aspects of asset liquidity risk and presents two critically robust machine learning processes to measuring the market liquidity risk for trading securities as well as for asset management objectives. To that end, this chapter extends research literature related to the computation of market and asset liquidity risks by providing generalized theoretical modeling algorithms that can assess both market and liquidity risks and integrate both risks into multiple-assets portfolio settings. The robust modeling algorithms can have practical applications for multiple-securities portfolios and can have many uses and application in financial markets, particularly in light of the 2007–2009 global financial meltdown in issues related to machine learning for the policymaking process and machine learning techniques for the Internet of Things (IoT) data analytics. In addition, risk assessment algorithms can aid in advancing risk management practices and have important applications for financial technology (FinTech), artificial intelligence, and machine learning in big data environments. © 2021, Springer Nature Switzerland AG. Systematic Market and Asset Liquidity Risk Processes for Machine Learning: Robust Modeling Algorithms for Multiple-Assets Portfolios Al Janabi model; Artificial intelligence; Economic capital; Emerging markets; Financial engineering; Financial markets; Financial risk management; Internet of Things (IoT); Liquidity risk; Liquidity-Adjusted Value at Risk; Machine learning; Portfolio management ",Risk management
228,A cross-sectional machine learning approach for hedge fund return prediction and selection,"We apply four machine learning methods to cross-sectional return prediction for hedge fund selection. We equip the forecast model with a set of idiosyncratic features, which are derived from historical returns of a hedge fund and capture a variety of fundspecific information. Evaluating the out-of-sample performance, we find that our forecast method significantly outperforms the four styled Hedge Fund Research indices in almost all situations. Among the four machine learning methods, we find that deep neural network appears to be overall most effective. Investigating the source of methodological advantage of our method using a case study, we find that cross-sectional forecast outperforms forecast based on time series regression in most cases. Advanced modeling capabilities ofmachine learning further enhance these advantages.We find that the return-based features lead to higher returns than the benchmark of a set of macroderivative features, and our forecast method yields best performance when the two sets of features are combined.  © 2020 INFORMS. A cross-sectional machine learning approach for hedge fund return prediction and selection Cross-sectional; Deep neural network; Forecast; Gradient boosting; Hedge fund; Lasso; Machine learning; Portfolio; Random forest; Return prediction Benchmarking; Deep learning; Deep neural networks; Financial markets; Fintech; Forecasting; Predictive analytics; Wooden fences; Advanced modeling; Forecast method; FORECAST model; Machine learning approaches; Machine learning methods; Research index; Sets of features; Time-series regression; Learning systems",Strategic alignment
229,Semantic enrichment of building and city information models: A ten-year review,"Building Information Models (BIMs) and City Information Models (CIMs) have flourished in building and urban studies independently over the past decade. Semantic enrichment is an indispensable process that adds new semantics such as geometric, non-geometric, and topological information into existing BIMs or CIMs to enable multidisciplinary applications in fields such as construction management, geoinformatics, and urban planning. These two paths are now coming to a juncture for integration and juxtaposition. However, a critical review of the semantic enrichment of BIM and CIM is missing in the literature. This research aims to probe into semantic enrichment by comparing its similarities and differences between BIM and CIM over a ten-year time span. The research methods include establishing a uniform conceptual model, and sourcing and analyzing 44 pertinent cases in the literature. The findings plot the terminologies, methods, scopes, and trends for the semantic enrichment approaches in the two domains. With the increasing availability of data sources, algorithms, and computing power, they cross the border to enter each other's domain. Future research will likely gain new momentums from the demands of value-added applications, development of remote sensing devices, intelligent data processing algorithms, interoperability between BIM and CIM software platforms, and emerging technologies such as big data analytics. © 2021 Elsevier Ltd Semantic enrichment of building and city information models: A ten-year review Building Information Model; City Information Model; Geographic Information System; Intelligent building; Semantic enrichment; Smart city Advanced Analytics; Application programs; Data Analytics; Data handling; Information theory; Project management; Remote sensing; Semantics; Building Information Model - BIM; Construction management; Emerging technologies; Intelligent data processing; Semantic enrichment; Software platforms; Topological information; Value added applications; Architectural design",Financial management
230,Front end work process digital transformation: Challenges and opportunities,"Digital transformation is shifting the structure of work in nearly every industry and fundamentally changing the value proposition for customers. As part of PETRONAS’ overall digital transformation, Front End Engineering (FEE) has embarked on an ambitious program to digitalize and integrate the company's Front End project realization processes and applications into a single digital tool, referred to as Concept Factory. This paper reviews the journey to initiate, frame and deliver the Front End work process digitalization. The Concept Factory digital transformation program first focused on a strategy to identify the pain points within the traditional project realization work process and how this is impacting both quality and speed of delivery. Once the pain points were identified, assessment of how digitalization may eliminate the pain points and enhance the project realization process value was completed. This assessment also included an end-to-end review of where the current Front End work processes to identify barriers that challenged the ease of digitalization; these included highly manual and siloed work processes, data management and tools; insufficient leveraging off the extensive Company knowledge databases and analogue projects; and inefficient technical and cost benchmarking to assure robustness of Front End work. This resulted in a more significant Front End process transformation being needed to increase the potential value creation through the digital transformation. A stepwise, iterative approach using Agile project management techniques has been used to harness the full capabilities of digital integration and analytics to FEL-2 rather than merely digitalizing the existing manual workflow. This will be done by first automating and upgrading databases and discrete data hand-offs to be ""digital ready"", independently developing and digitalizing the full suite of Front End technical and cost analysis tools, then integrating these tools within a common Concept Factory analytics platform for both stand-alone Front End analysis and as a domain tool within the broader Field Development Planning digital framework. Several technical and organizational challenges were identified that need to be overcome from business case syndication to adoption. As the daily work routines of employees are being radically changed to adapt to the rapid change of digital technology, ongoing alignment was done to engage the Front End team and broader stakeholder groups in the process through demonstrations and feedback sessions. In addition, cascading technical needs through the digital team execution required ongoing alignment through daily Scrums, Sprint Planning and demonstration sessions. Fully integrated Front End process digitalization has rarely been attempted within E&P companies. However, this has the potential to disrupt the Front End work process from a manual, siloed generation of deliverables to an automated and integrated techno-commercial process focused on replication, speed and accuracy, a re-focus the Front End team on Value Creation, Assurance and Risk Management initiatives. © 2021, Society of Petroleum Engineers. Front end work process digital transformation: Challenges and opportunities  Application programs; Cost benefit analysis; Digital devices; Gasoline; Health; Knowledge management; Project management; Digital tools; Digital transformation; Front end; Front-end process; PETRONAS; Realization process; Transformation front; Value creation; Value proposition; Work process; Iterative methods",Strategic alignment
231,Pricing service maintenance contracts using predictive analytics,"As more manufacturers shift their focus from selling products to end solutions, full-service maintenance contracts gain traction in the business world. These contracts cover all maintenance related costs during a predetermined horizon in exchange for a fixed service fee and relieve customers from uncertain maintenance costs. To guarantee profitability, the service fees should at least cover the expected costs during the contract horizon. As these expected costs may depend on several machine-dependent characteristics, e.g. operational environment, the service fees should also be differentiated based on these characteristics. If not, customers that are less prone to high maintenance costs will not buy into or renege on the contract. The latter can lead to adverse selection and leave the service provider with a maintenance-heavy portfolio, which may be detrimental to the profitability of the service contracts. We contribute to the literature with a data-driven tariff plan based on the calibration of predictive models that take into account the different machine profiles. This conveys to the service provider which machine profiles should be attracted at which price. We demonstrate the advantage of a differentiated tariff plan and show how it better protects against adverse selection. © 2020 Elsevier B.V. Pricing service maintenance contracts using predictive analytics Calibration; Contract pricing; Maintenance; Predictive analytics; Servitization Maintenance; Predictive analytics; Profitability; Risk management; Sales; Adverse selection; Dependent characteristics; Maintenance contracts; Maintenance cost; Operational environments; Predictive models; Pricing services; Service contract; Costs",Strategic alignment
232,Mean–variance portfolio optimization using machine learning-based stock price prediction,"The success of portfolio construction depends primarily on the future performance of stock markets. Recent developments in machine learning have brought significant opportunities to incorporate prediction theory into portfolio selection. However, many studies show that a single prediction model is insufficient to achieve very accurate predictions and affluent returns. In this paper, a novel portfolio construction approach is developed using a hybrid model based on machine learning for stock prediction and mean–variance (MV) model for portfolio selection. Specifically, two stages are involved in this model: stock prediction and portfolio selection. In the first stage, a hybrid model combining eXtreme Gradient Boosting (XGBoost) with an improved firefly algorithm (IFA) is proposed to predict stock prices for the next period. The IFA is developed to optimize the hyperparameters of the XGBoost. In the second stage, stocks with higher potential returns are selected, and the MV model is employed for portfolio selection. Using the Shanghai Stock Exchange as the study sample, the obtained results demonstrate that the proposed method is superior to traditional ways (without stock prediction) and benchmarks in terms of returns and risks. © 2020 Elsevier B.V. Mean–variance portfolio optimization using machine learning-based stock price prediction eXtreme Gradient Boosting; Firefly algorithm; Mean–variance model; Portfolio selection; Stock prediction Financial data processing; Financial markets; Forecasting; Machine learning; Optimization; Predictive analytics; Accurate prediction; Construction approaches; Firefly algorithms; Future performance; Portfolio optimization; Portfolio selection; Shanghai stock exchanges; Stock price prediction; Fintech",Strategic alignment
233,Situational analysis of complex offshore network for strategizing sequence for green field development,"This paper establishes the approach to strategize the appropriate sequencing and monetization of the green field development through performing situational analysis for the complex offshore facilities to recognize new hydrocarbon molecules. As prudent operator for the complex network its crucial to pursue strategic ideas and innovative concepts to optimize supply demand balance, fulfill contractual obligations to optimize resources to maximize value creation, whilst protecting investment decisions for hydrocarbon monetization for the green field development. It is therefore necessary to implement successful business plans with appropriate sequencing of new fields by robust assessment to decipher the pain points to achieve optimal solution by gaining better understanding of network characteristic, supply distribution and operating envelope for line ups of new green field development. Situational Analysis for the complex offshore system is defined as robust investigation of the surrounding facts/realities to scrutinize the unique features in terms of capabilities, risks, uncertainty, opportunity, and exposures. The approach followed in the paper is the creation of mathematical model for the network/infrastructure embedded with business rules and deployment for evaluation and optimization. This approach is to timely deliver the management decisions for developing sequencing strategy, establishing priority of supply guidelines and allocation principles. This paper describes that a state of art approach which was followed by developing end to end network model by simulation engineers with close collaboration with strategic planning, portfolio optimization and including operations in single platform. The simulation model was further validated and deployed to analyze current network impediments in terms of technical and commercial allocation principles. The modelling approach was kept straightforward and scalable to allow for the future development if any. Analytics of the modelling could assist in gauging the potentials for enhancing system capacity by implementing appropriate reforms to optimize evacuation strategies. Obstacles across system architecture could be estimated and its reconfiguration was planned by means of variations in operating philosophy, alterations in the network assembly with appropriate debottlenecking recommendation. The allocation principles applied during business plans consider the commercial element on initial basis, before instead the physical and technical constraints were evaluated. The results of the allocation were then simulated and reallocated back to relevant demand center with relevant technical constraints of the network. This enabled team to identify the gap for supply/demand and propose solution to address the gap at an enterprise level to be substantial, to build a case whereby monetization of green fields will be necessary. Above methodology describes how by developing an end to end mathematical model that summarizes the microscopic details of a complex offshore system to facilitate on the way to analyze and strategize new field development line-ups. The novelty is with the simulation model built in a single platform, allows a seamless data transfer from various elements such as fields, facilities, pipelines, and terminal and is one stop solution for accessing impediments across architecture The above approach elaborates on result matter approach that steer and advocate on the situational analysis for new field sequencing by ascertaining CAPEX/OPEX optimization that could steer decision of lining up of four new fields within span of two years at appropriate intensity of the network with optimal monetization. Copyright 2021, Society of Petroleum Engineers. Situational analysis of complex offshore network for strategizing sequence for green field development  Arts computing; Data transfer; Energy resources; Financial data processing; Hydrocarbons; Investments; Molecules; Network architecture; Offshore oil well production; Risk assessment; Simulation platform; Uncertainty analysis; Contractual obligations; Hydrocarbon molecules; Investment decisions; Network characteristics; Portfolio optimization; Supply distributions; Supply-demand balances; Technical constraints; Complex networks",Strategic alignment
234,Stock selection strategy based on double objective programming,"This paper studies the securities market in which the stock market fluctuates. Because the market fluctuation plays an important role in the analysis of people's risk return, the maximization of shareholders' equity and the effective supervision of the regulators, how to study the law of the securities market fluctuation and what are the causes of the market fluctuation have become the necessary problems in the current study of the securities market.Lagrange interpolation is used to complete the data, and the time series prediction model is established to predict the closing of the next 100 days. The multi-objective programming is carried out by using the prediction data. Under the consideration of multiple constraints, lingo programming is used to solve the equations. The final portfolio is as follows: stock 2 accounts for 0.18, stock 3 accounts for 0.3, stock 5 accounts for 0.22, and stock 10 accounts for 0.29.The model was evaluated and five groups of data were selected.Using MAPE index to judge the error between the predicted result and the real value, the accuracy of the predicted result can be judged if the calculated result is about 0.1.By analyzing the new portfolio and processing the attachments, the investment benefits of the newly selected portfolio are all positive, and the expected income is also positive, which verifies the effectiveness of the model. © Published under licence by IOP Publishing Ltd. Stock selection strategy based on double objective programming  Big data; Commerce; Forecasting; Investments; Multiobjective optimization; Predictive analytics; Risk assessment; Time series analysis; Investment benefit; Market fluctuations; Multiobjective programming; Multiple constraint; Objective programming; Securities market; Stock selections; Time series prediction; Financial markets",Strategic alignment
235,Application of Features and Neural Network to Enhance the Performance of Deep Reinforcement Learning in Portfolio Management,"Portfolio management is the decision-making process of allocating a certain amount of funds to multiple financial assets and continuously changing the distribution weights to increase returns and reduce risks. With the advance in artificial intelligence technology, it has become possible to use computers for self-learning and large-scale calculations, and to achieve optimized portfolio management. This paper mainly studies and analyzes the problem of portfolio optimization in the digital currency market, uses Poloniex's historical transaction data of digital currency to conduct experiments, and proposes a strategy based on the framework of deep reinforcement learning algorithms. The investment strategy framework uses Convolutional Neural Network and Visual Geometry Group Network. In addition to the closing price, highest price and lowest price, we also consider other internal or external features such as Network Value to Transaction Volume Ratio, Market Value to Realized Value Ratio, Return on Investment and Volatility. The results show that the return rate of our algorithm based on VGG with NVT as feature is 11.05% better than the work of Jiang et al. and at least 110% better than investment strategies such as Moving Average Reversion and Robust Median Reversion.  © 2021 IEEE. Application of Features and Neural Network to Enhance the Performance of Deep Reinforcement Learning in Portfolio Management convolutional neural network; portfolio management; reinforcement learning; visual geometry group network Advanced Analytics; Big data; Convolutional neural networks; Decision making; Electronic money; Electronic trading; Investments; Learning algorithms; Reinforcement learning; Strategic planning; Artificial intelligence technologies; Decision making process; External features; Financial assets; Investment strategy; Large-scale calculation; Portfolio managements; Portfolio optimization; Deep learning",Strategic alignment
236,Priorities for science to overcome hurdles thwarting the full promise of the ‘digital agriculture’ revolution,"Abstract. The world needs to produce more food, more sustainably, on a planet with scarce resources and under changing climate. The advancement of technologies, computing power and analytics offers the possibility that ‘digitalisation of agriculture’ can provide new solutions to these complex challenges. The role of science is to evidence and support the design and use of digital technologies to realise these beneficial outcomes and avoid unintended consequences. This requires consideration of data governance design to enable the benefits of digital agriculture to be shared equitably and how digital agriculture could change agricultural business models; that is, farm structures, the value chain and stakeholder roles, networks and power relations, and governance. We argue that this requires transdisciplinary research (at pace), including explicit consideration of the aforementioned socio-ethical issues, data governance and business models, alongside addressing technical issues, as we now have to simultaneously deal with multiple interacting outcomes in complex technical, social, economic and governance systems. The exciting prospect is that digitalisation of science can enable this new, and more effective, way of working. The question then becomes: how can we effectively accelerate this shift to a new way of working in agricultural science? As well as identifying key research areas, we suggest organisational changes will be required: new research business models, agile project management; new skills and capabilities; and collaborations with new partners to develop ‘technology ecosystems’. © 2018 The Authors. © 2018 The Authors. Journal of The Science of Food and Agriculture published by John Wiley & Sons Ltd on behalf of Society of Chemical Industry. © 2018 The Authors. Journal of The Science of Food and Agriculture published by John Wiley & Sons Ltd on behalf of Society of Chemical Industry. Priorities for science to overcome hurdles thwarting the full promise of the ‘digital agriculture’ revolution digital agriculture; digital science; digitalisation; precision agriculture; technology; value chain Agriculture; Computer Systems; Decision Making; Digital Technology; Food Supply; Humans; agriculture; catering service; computer system; decision making; devices; economics; human; procedures",Value management
237,Mapping socially responsible investing: A bibliometric and citation network analysis,"An increasing number of financial investors base their investment decisions on sustainability considerations. At the same time, there is growing evidence that socially responsible investment acts as an essential lever to improve both the economic system's long term viability and the performance and risk profiles of investment portfolios. We map the academic research in socially responsible investment and illustrate key research avenues and trends by conducting bibliometric research based on systematic key search-string-based analysis of the Clarivate Analytics Web of Science and Elsevier Scopus databases. We contribute to the body of knowledge in the associated fields of socially responsible investment, sustainable investing and industrial sustainability in four ways: (1) identifying the most influential articles in the field, (2) mapping the interdisciplinary character of socially responsible investing and its bibliometric similarity to adjacent fields, (3) visualising nature and trends of the research field via network analysis, and by (4) synthesising areas for further research. While socially responsible investing has gained considerable traction already, there is to date no holistic bibliometric analysis on the topic. This is the research gap this paper addresses and helps to close. The study's findings help academics and practitioners to navigate the literature on socially responsible investing, provide a systematic basis for developing the field, and suggest promising avenues for further research. © 2021 Elsevier Ltd Mapping socially responsible investing: A bibliometric and citation network analysis Bibliometric analysis; Impact investing; Responsible investing; Socially responsible investing; Sustainable investing Mapping; Sustainable development; Bibliometric; Bibliometrics analysis; Citation networks; Financial investors; Impact investing; Investment decisions; Responsible investing; Socially responsible investing; Socially responsible investments; Sustainable investing; Investments",Strategic alignment
238,Design of a process mining alignment method for building big data analytics capabilities,"Process mining is a big data analytics technique that supports business process management in an evidence-based way. Nowadays, companies struggle to build the required capabilities that lift process mining beyond technical proof-of-concept implementations. As research on process mining is largely limited to algorithm design and project management recommendations, current research does not understand well how process mining and complementary resources and capabilities can be aligned. By understanding those interrelations, companies learn to leverage their organizational potential during the execution of process mining more effectively and efficiently. In this paper, we address this research gap by using the design science research approach to develop a process mining alignment method. Our method supports companies mapping their individual technical requirements of process mining to their underlying organizational resources. We evaluate our method through a series of interviews with IT consultants. © 2021 IEEE Computer Society. All rights reserved. Design of a process mining alignment method for building big data analytics capabilities  Advanced Analytics; Alignment; Big data; Data Analytics; Enterprise resource management; Project management; Algorithm design; Alignment methods; Business process management; Design-science researches; Evidence-based; Proof of concept; Resources and capabilities; Technical requirement; Data mining",Strategic alignment
239,Manhole cover detection on rasterized mobile mapping point cloud data using transfer learned fully convolutional neural networks,"Large-scale spatial databases contain information of different objects in the public domain and are of great importance for many stakeholders. These data are not only used to inventory the different assets of the public domain but also for project planning, construction design, and to create prediction models for disaster management or transportation. The use of mobile mapping systems instead of traditional surveying techniques for the data acquisition of these datasets is growing. However, while some objects can be (semi)automatically extracted, the mapping of manhole covers is still primarily done manually. In this work, we present a fully automatic manhole cover detection method to extract and accurately determine the position of manhole covers from mobile mapping point cloud data. Our method rasterizes the point cloud data into ground images with three channels: intensity value, minimum height and height variance. These images are processed by a transfer learned fully convolutional neural network to generate the spatial classification map. This map is then fed to a simplified class activation mapping (CAM) location algorithm to predict the center position of each manhole cover. The work assesses the influence of different backbone architectures (AlexNet, VGG-16, Inception-v3 and ResNet-101) and that of the geometric information channels in the ground image when commonly only the intensity channel is used. Our experiments show that the most consistent architecture is VGG-16, achieving a recall, precision and F2-score of 0.973, 0.973 and 0.973, respectively, in terms of detection performance. In terms of location performance, our approach achieves a horizontal 95% confidence interval of 16.5 cm using the VGG-16 architecture. © 2020 by the authors. Licensee MDPI, Basel, Switzerland. Manhole cover detection on rasterized mobile mapping point cloud data using transfer learned fully convolutional neural networks CAM localization; F-CNN; Manhole cover; Mobile mapping; Point cloud; Transfer learning Convolution; Data acquisition; Data communication systems; Disaster prevention; Disasters; Mapping; Network architecture; Predictive analytics; Project management; Rasterization; Confidence interval; Construction design; Detection performance; Disaster management; Geometric information; Location algorithms; Mobile mapping systems; Spatial classification; Convolutional neural networks",Strategic alignment
240,A model to manage cooperative projects risks to create knowledge and drive sustainable business,"Efficient cooperation between organizations across all the phases of a project lifecycle is a critical factor to increase the chances of project success and drive sustainable business. However, and according to research, despite the large benefits that efficient organizational cooperation pro-vides to organizations, they are still often reluctant to engage in cooperative partnerships. The re-viewed literature argues that the major reason for such a trend is due to the lack of efficient and actionable supportive models to manage organizational cooperative risks. In this work we propose a model to efficiently support the management of organizational cooperative risks in project envi-ronments. The model, MCPx (management of cooperative projects), was developed based on four critical scientific pillars, (1) project risk management, (2) cooperative networks, (3) social network analysis, and (4) business intelligence architecture, and will analyze in a quantitative way how project cooperative behaviors evolve across a bounded time period, and to which extent they can turn into a cooperative project risk (essentially potential threats). For this matter, the MCPx model will quantitatively analyze five key project cooperative behavioral dimensions, (1) communication, (2) information sharing, (3) trust, (4) problem solving and (5) decision making, which show how dynamic interactions between project stakeholders evolve across time. The implementation and func-tioning principles of the MCPx model are illustrated with a case study. © 2021 by the authors. Licensee MDPI, Basel, Switzerland. A model to manage cooperative projects risks to create knowledge and drive sustainable business Business intelligence; Cooperative networks; Knowledge creation; Project cooperative risks; Project management; Risk management; Social network analysis; Sustainable business business; cooperative sector; knowledge; lifestyle; network analysis; project management; social network; sustainability",Risk management
241,"Big data, technology capability and construction project quality: a cross-level investigation","Purpose: Embracing big data has been at the forefront of research for project management. Although there is a consensus that the adoption of big data has significantly positive impact on project performance, far less is known about how this innovative information technology becomes an effective driver of construction project quality improvement. This study aims to better understand the mechanism and conditions under which big data can effectively improve project quality performance. Design/methodology/approach: Adopting Chinese construction enterprises as samples, the theoretical framework proposed in this paper is verified by the empirical results of the two-level hierarchical linear model. The moderated mediation analysis is also conducted to test the hypotheses. Finally, the empirical findings are validated by a comparative case study. Findings: The results show that big data facilitates the development of technology capability, which further produces remarkable quality performance. That is, a project team's technology capability acts as a mediator in the relationship between organizational adaptability of big data and predictive analytics and project quality performance. It is also observed that two types of project team interdependence (goal and task interdependence) positively moderate the mediation effect. Research limitations/implications: The questionnaire study from China only represents the relationship within a short time interval in the current context. Future studies should apply longitudinal designs to properly test the causality and use multiple data sources to ensure the validity and robustness of the conclusions. Practical implications: The value of big data in terms of quality improvement could not be determined in a vacuum; it also depends on the internal capability development and elaborate design of project governance. Originality/value: This study provides an extension of the existing big data studies and fuels the ongoing debate on its actual outcomes in project management. It not only clarifies the direct effect of big data on project quality improvement but also identifies the mechanism and conditions under which the adoption of big data can play an effective role. © 2020, Emerald Publishing Limited. Big data, technology capability and construction project quality: a cross-level investigation Big data; Hierarchical linear model; Project quality; Team governance; Technology capability Predictive analytics; Project management; Capability development; Construction enterprise; Construction projects; Design/methodology/approach; Hierarchical linear modeling; Organizational adaptabilities; Questionnaire studies; Theoretical framework; Big data",Value management
242,Disc cutter wear prediction based on the friction work principle,"As the main rock-breaking tool of the tunnel boring machine, wear of the disc cutter is affected by geological conditions, equipment factors, and tunneling parameters when it interacts with rock. Because of the complex factors affecting disc cutter wear, it is difficult to accurately predict the wear of the disc cutter. In this study, the rock-breaking mechanism and the force of the disc cutter were analyzed, and a theoretical prediction model of disc cutter wear was established based on the friction work principle. The parameters in the disc cutter wear prediction model were determined by simulation, and a prediction method of disc cutter wear is proposed. Finally, the wear prediction model of the disc cutter was verified by field wear data. The results show that the average error between the predicted value of the disc cutter and the actual wear data from the field is 16.1%. The wear prediction model of the disc cutter has high accuracy and adaptability. The research results provide an effective method for wear prediction of the disc cutter, which is of great significance and engineering value for cutter replacement and construction management. © 2021, Canadian Science Publishing. All rights reserved. Disc cutter wear prediction based on the friction work principle Disc cutter; Friction work; Tunnel boring machine; Wear prediction Boring machines (machine tools); Construction equipment; Forecasting; Friction; Predictive analytics; Project management; Tunneling machines; Construction management; Geological conditions; Prediction methods; Research results; Rock breaking mechanism; Tunnel boring machines; Tunneling parameter; Wear-prediction models; Wear of materials",Strategic alignment
243,Curriculum development to acquire analytical competencies using an industry BOK,"Contribution - The acquisition of analytical competencies is a desirable outcome of a technical graduate program. We present a process to align the program curriculum with an industry-standard to ensure those competencies. The result is not necessarily to create certified students (although that is certainly desirable) but to confirm that the program helps students acquire the analytical competencies as outlined in an industry standard. We describe a path for alignment of programs to develop these desirable competencies and our experience with the process. Background - We selected the INFORMS Certified Analytics Professional certification as a vendor-neutral book of knowledge and knowledge structure for analytics professionals and a well-developed credential in the profession. We used well-documented processes to align a university curriculum to industry needs, employed in aligning similar programs to such competencies as a project manager using the PMI PMP certification. Research questions - To align the curriculum to the INFORMS CAP, we asked: (1) whether students graduating from the program had acquired adequate analytical competencies; and (2) If they had not, how can the curriculum be modified to help gain those competencies. Methods - The curriculum was reviewed, and we assured initially that topical coverage of the pertinent course of study aligned adequately with the INFORMS CAP knowledge structure. Results - Using the existing curriculum, we found that a majority of the students could pass the INFORMS CAP certification practice exam at the end of their program of study. The exam results were sufficiently granular to modify the curriculum and course contents to improve future assessments' passing rate. © 2020 IEEE. Curriculum development to acquire analytical competencies using an industry BOK Analytical competencies; BOK; Book of knowledge; Certification; Curriculum alignment; INFORMS CAP Project management; Students; Teaching; Curriculum development; Future assessment; Graduate program; Industry standards; Knowledge structures; Professional certifications; Research questions; University curricula; Curricula",Risk management
244,Data Query Method of Science and Technology Management Based on Relational Engine,"As the country promotes the reform of science and technology project management, the number of science and technology projects has been increasing. At the same time, science and technology management data is growing rapidly with the characteristics of complex correlation and multi-dimensionality. How to effectively store the enormous data and mine the contained value of the data have become a crucial problem for current researches. First, we propose a science and technology management data model to illustrate the internal relation of project, topic, unit, and person, and then define the model by mathematical representation. According to science and technology management data query requirements, we present a data query method of science and technology management based on relational engine, which comprises query parser, query optimizer, and query executor. Furthermore, we design a prototype system to achieve the query requirements of science and technology management data. This research contributes to data query by providing a query method with relational engine, and improves the efficiency of data query in science and technology management. © 2021 IEEE. Data Query Method of Science and Technology Management Based on Relational Engine data model; data query; query optimization; relational engine; science and technology project Advanced Analytics; Big data; Cloud computing; Engines; Industrial management; Information management; Project management; Complex correlation; Mathematical representations; Multidimensionality; Prototype system; Query methods; Query optimizer; Science and Technology; Science and technology project managements; Search engines",Value management
245,Mapping bioenergy stakeholders: A systematic and scientometric review of capabilities and expertise in bioenergy research in the United Kingdom,"This work, led by the SUPERGEN Bioenergy Hub of United Kingdom (UK), examines the current status of the UK bioenergy research, identifies important research gaps and makes recommendations for exploitation of current capabilities and future research development. It was based on a survey-based research covering 71 bioenergy research stakeholders’ responses and a taxonomy map with key bioenergy topics and subtopics carefully defined. This novel study adapts the concept of “business intelligence” to innovation, in order to transform data into actionable intelligence that informs about strategic decisions. The map shows that the UK bioenergy research explores the whole bioenergy chain, and the areas with high probability of exploitation and improvement identified are: biomass pre-treatment; application of bioenergy products and standardisation, portfolio of commercialisation opportunities, and research into market opportunities. Working on them will help technology and bio-products to be market-ready. To complement the outcomes of this map, a scientometric review was done through analysing the trend of the number of publication, publication impacts, and stakeholders’ co-authorships. The study reveals that pyrolysis had the highest number of publications during 2017, in agreement with the major number of participants; and the highest publication growth was found in both pyrolysis and gasification. Conversely, combustion, which had the lowest number of stakeholders (by 30%), had the highest number of publications until 2015, indicating combustion research is more concentrated in specific stakeholders. Hydrolysis and fermentation showed high number of research stakeholders, but the lowest number of publications suggest that more effort in publication should be done. © 2020 Elsevier Ltd Mapping bioenergy stakeholders: A systematic and scientometric review of capabilities and expertise in bioenergy research in the United Kingdom Bioenergy; Biomass; Knowledge management; Research gaps; Research stakeholders; Scientometric review; State of the art; Taxonomy map Combustion; Commerce; Knowledge management; Pyrolysis; Reviews; Taxonomies; Bio-energy; Current capability; Current status; Research gaps; Research stakeholder; Scientometric review; Scientometrics; State of the art; Taxonomy map; United kingdom; Biomass",Value management
246,Digital Transformation Journey of Field Operations at Abu Dhabi Offshore Field in UAE,"Field operations generate large volumes of data from various equipment and associated Meta data such as inspection due dates, maintenance schedule, people on board, etc. The data is often stored in silos with a data guardian for each entity. The objective of this project was to volarize the data by developing engineered KPI's to drive decision making and make data accessible for everyone in the organization to foster cross collaboration. Data analytics and visualization solutions were developed to automate low value-added tasks either using robotic process automation scripts or business intelligence reporting tool. Data was residing either in spreadsheet or native applications. With support of IT, centralized database was established. Scrum agile project management techniques were used to develop digital solutions. A high-level digital road map was created consulting all teams including stake holders. Use cases were identified and captured in lean A3 problem solving format. Each use case clearly identified the benefits to organization, and this was used to prioritize the use cases. A sprint was set-up with agile team and products were developed as per end user's expectation. The constant feedback loop via daily stand-up meetings helped the team deliver value added products. Digital solutions were developed to automate low value-added tasks so employees can focus on improving systems instead of producing reports. By developing engineering KPI's and predictive analytics, technical authority could shift from reactive maintenance to pro-active maintenance. Using linear regression machine learning, early warning digital solution was developed to monitor and notify technical authority to clean strainers. The production team achieved 0.75 full time equivalent (FTE) in time savings by automating reports. By visualizing operations data such as flaring, production profiles; the team minimized flaring leading to 1% OPEX cost saving. Around 10% of chemical budget was saved by monitoring chemical injections at all platforms. Similar cost savings were achieved by visualizing data for other disciplines such as maintenance and HSE teams. By being better informed about wells annuli pressure build-up via email notifications, wells integrity team reduced the associated risk. By forming a multi-disciplinary agile team with business and delivery team, digital team deployed 20+ digital products over a short time frame of 2 years. © Copyright 2021, Society of Petroleum Engineers Digital Transformation Journey of Field Operations at Abu Dhabi Offshore Field in UAE  Behavioral research; Budget control; Data Analytics; Data visualization; Decision making; Digital storage; Maintenance; Offshore oil well production; Predictive analytics; Project management; Abu Dhabi; Agile teams; Cost saving; Digital solutions; Digital transformation; Due dates; Field operation; Large volumes; Meta-data; Offshore fields; Benchmarking",Capacity management
247,Construction Cost Control Method Based on BIM information Integration Platform,"In order to meet the needs of the construction unit economic development and create more profit points, this paper constructs a BIM based cost control model for construction projects. Based on the existing research results, we adopt the method of combining qualitative and quantitative to summarize the definition and content of cost plan, cost control and building information model. Then we focus on the application of earned value method in BIM in construction stage, and constructs the cost prediction model. On this basis, through the cost planning and cost control analysis of the actual project cases, the effective combination of BIM Technology and cost management and its implementation application are studied and analyzed, which proves the application value of building information model technology in the construction cost planning and control. © 2021 IEEE. Construction Cost Control Method Based on BIM information Integration Platform BIM; collision; cost control; project management Architectural design; Budget control; Cost effectiveness; Electric power transmission networks; Information theory; Predictive analytics; Project management; Smart power grids; BIM technologies; Building Information Model - BIM; Construction costs; Construction projects; Construction stages; Cost control models; Cost prediction models; Information integration; Cost benefit analysis",Strategic alignment
248,VIAProMa: An Agile Project Management Framework for Mixed Reality,"With the COVID-19 pandemic, distributed and remote working became a necessity but in agile project management, social interactions like daily standup meetings in Scrum are vital for the project success. Mixed reality can provide a new way of combining remote collaboration with innovative 3D visualizations to analyze the project status. In this paper, we present a visual immersive analytics framework for project management (VIAProMa). It imports data from project management tools like the GitHub issue tracker for open-source projects. With these task data as the basis, it can generate three-dimensional visualizations, e.g. about the overall progress or the competences of individual developers. Developers, stakeholders and end users can meet in the collaborative virtual environment as avatars and establish a spatial structure with the task cards and visualizations. Therefore, VIAProMa with its adapted and customized mixed reality project management features supports both the shared meetings and the information flow in the project. The shared environment makes it a suitable tool for DevOpsUseXR, an extension to the DevOps workflow, where end users are able to participate in the development process in mixed reality. The resulting implementation is available as an open-source project with cross-platform capabilities targeting the Microsoft HoloLens, HTC VIVE and Android smartphones, as well as tablets. The framework is applied in university teaching classes to convey agile methodology in mixed reality programming practices. © 2021, Springer Nature Switzerland AG. VIAProMa: An Agile Project Management Framework for Mixed Reality Agile methodology; Mixed reality; Project management Mixed reality; Open source software; Three dimensional computer graphics; Visualization; Agile Methodologies; Agile project management; End-users; Immersive; Mixed reality; Open source projects; Project management frameworks; Project success; Remote working; Social interactions; Project management",Risk management
249,SLD Instrumentation in Icing Wind Tunnels – Investigation Overview,"A collaborative effort to better understand cloud characterization probes in Supercooled Large Drop (SLD) conditions, as well the ability to simulate these conditions in several icing wind tunnels, was undertaken by NASA, NRCC, CIRA, ECCC, FAA and Met Analytics, Inc. Both drop sizing and liquid water content, LWC, were measured with various probes using current to emerging technologies. To ensure the best possible data quality from the newest probes, the probe manufacturers, SEA, Inc. and Artium, Inc. were invited to support testing and data analysis efforts. A common set of probes was identified to test in each of the three participating facilities: NRCC’s Altitude Icing Wind Tunnel, NASA’s Icing Research Tunnel and CIRA’s Icing Wind Tunnel. From the common set of probes, a subset were identified to use for comparison across the three facilities. These were the CDP-2 and 2D-S for drop sizing, and the Multi-wire for LWC. The LWC value was also checked by measuring the ice accretion thickness under hard rime conditions on a NACA-0012 airfoil. A common test matrix with sweeps in both LWC and median volume diameter, MVD, was developed. Each facility achieved these conditions as determined by their own calibration. The MVD ranged from 20 to at least 200 um, and LWC ranged from 0.5 to 3 g/m3. The comparison probes tested at common conditions in each facility were intended to allow for a direct comparison, and check of potential facility bias.; The authors would like to thank the sponsoring agencies: NASA Aerosciences Evaluation and Test Capabilities Portfolio Office – Capability Advancement Project (AETC-CA), FAA Technical Center, NRCC Aerodynamics Laboratory, ECCC Atmospheric Science and Technology Branch, and CIRA SLD-FZDZ program. We also thank the dedicated facility engineers and technicians at the AIWT and IRT who so well supported these tests. © 2021, American Institute of Aeronautics and Astronautics Inc, AIAA. All rights reserved. SLD Instrumentation in Icing Wind Tunnels – Investigation Overview  Aerodynamics; Aviation; Drops; NASA; Quality control; Software testing; Wind tunnels; 'current; Condition; Data quality; Emerging technologies; Ice accretion; Liquid water content; Supercooled large drops; Support testing; Test capability; Test matrix; Probes",Risk management
250,Develop JMP 16_Based STEAMS and Six Sigma DMAIC Training Curriculum for Data Scientist,"Traditional Six Sigma curriculums including DMAIC, DFSS, DFR, Lean are not developed specifically and effectively for today’s AI Data Science fields. This project will demonstrate an innovative Six Sigma training curriculum for Data Scientists. There are several objectives in this modern Six Sigma training curriculum: (1) adopt modern Text Mining and Data Mining techniques on Root Because and Problem Solving Analyses such as DFMECA, C&E, QFD, SIPOC, VSM, (2) integrate various JMP statistical platforms holistically to analyze pattern recognition and discover the insights, (3) enhance predictive modeling capability through Neural, Partition, Principal Component algorithms, (4) utilize modern Quality and Process platforms such as Goal Plot, Model-Driven Multivariate SPC, Process History Process and Screening to enhance production quality control, (5) map these modern JMP Platforms into the Six Sigma DMAIC/DFSS/Lean framework for facilitating the Six Sigma Project execution, and (6) integrate database through Query Builder to monitor the production status in real-time. This curriculum is not just designed for a Data Scientist but is also powerful for those working in process, quality, reliability, supply chin, business, statistics, and marketing fields who want to become a reliable decision maker and true project leader. This Six Sigma DMAIC for Data Scientist Approach can be commonly applied to other Quality Engineering, Project Management and Business Excellence methodology such as DFSS, DFR, Lean, 8D, PMP. © IEOM Society International. Develop JMP 16_Based STEAMS and Six Sigma DMAIC Training Curriculum for Data Scientist Data Science; DMAIC; JMP; Six Sigma; STEAMS ",Value management
251,Effect of Corner Strength Enhancement on Shear Behaviour of Stainless Steel Lipped Channel Sections,"During the cold-forming process of manufacturing, stainless steel sheets undergo plastic deformations, in particularly around corner regions of press braked sections. These plastic deformations lead to significant changes in material properties of stainless steel compared to its flat sheet properties. Consequently, yield strength and ultimate strength increments can be envisaged and this process is termed as cold working. Stainless steel exhibits significant level of strain hardening under plastic deformations. This is the main reason for these strength enhancements. In the structural design process of stainless steel sections, these strength increments are required to be considered to harness the benefits arising from it. Therefore, previous research proposed predictive models for these strength enhancements. In this context, the effect of corner strength enhancement on press-braked stainless steel lipped channel sections under shear was examined in this paper. 120 finite element models were developed. Different corner radii and section thicknesses were taken into account. Results highlighted that the effect of cold working on the shear capacity of stainless steel lipped channel sections is more significant in compact sections compared to slender sections where up to 9% increment was observed. Further analysis was conducted using 40 finite element models to highlight the inelastic reserve capacity available in compact stainless steel lipped channel sections in shear. From the results, it was concluded that when web slenderness is less than 0.25 more than 40% shear capacity increment can be achieved due to strain-hardening of stainless steel. © 2021, Springer Nature Singapore Pte Ltd. Effect of Corner Strength Enhancement on Shear Behaviour of Stainless Steel Lipped Channel Sections Cold working; Corner strength enhancement; Lipped channel sections; Shear capacity; Stainless steel Cold working; Finite element method; Plastic deformation; Plastics industry; Predictive analytics; Presses (machine tools); Project management; Shear flow; Strain hardening; Structural design; Cold forming process; Inelastic reserve capacity; Lipped-channel section; Predictive models; Stainless steel sections; Strength enhancement; Strength increments; Ultimate strength; Stainless steel",Capacity management
252,Multimedia Portfolio Behavior Analysis Based on Bayesian Game Algorithm,"A portfolio prediction model based on Bayesian game algorithm is proposed to improve the accuracy of correlation analysis model in portfolio application. Firstly, the portfolio problem is analyzed. The market value constraint and the upper bound constraint are combined considering the general portfolio model according to Markowitz theory, it improves the portfolio model and obtains the portfolio model with mixed constraints. Secondly, a probabilistic portfolio monitoring strategy based on incomplete information non-cooperative Bayesian game is proposed. The framework combines the enterprise investment behavior detection module with the specified rules to identify the portfolio pattern, the interaction between portfolio selection and monitored investment behavior is modeled as a two-person non-cooperative Bayesian game, which allows portfolio selection to adopt a probability monitoring strategy based on game Bayesian Nash equilibrium, thereby reducing the computational complexity of the portfolio approach introduced into the game algorithm. Finally, the effectiveness of the proposed algorithm is verified by simulation experiments. © 2019, Springer Science+Business Media, LLC, part of Springer Nature. Multimedia Portfolio Behavior Analysis Based on Bayesian Game Algorithm Bayesian; Enterprise investment; Game algorithms; Investment portfolio; Non-cooperation Game theory; Genetic algorithms; Investments; Bayesian; Bayesian Nash equilibria; Correlation analysis model; Incomplete information; Investment portfolio; Monitoring strategy; Non cooperations; Upper bound constraints; Predictive analytics",Strategic alignment
253,What Are the Critical Success Factors for Agile Analytics Projects?,"To get value from BI (Business Intelligence) and Big Data initiatives, organizations need to develop the capability to successfully execute their analytics projects. Via updating Chow and Cao’s list of 12 success factors for agile projects, 43 attributes of these potential critical success factors (CSFs) were identified. Data from four case studies of analytics projects suggest that the critical success factors for analytics projects may be Strong Customer Involvement and a Methodical Project Definition Process. © 2020 Taylor & Francis. What Are the Critical Success Factors for Agile Analytics Projects? agile project management; agile projects; Analytics projects; project success factors Information systems; Case-studies; Critical success factor; Success factors",Capacity management
254,A Deep Residual Shrinkage Neural Network-based Deep Reinforcement Learning Strategy in Financial Portfolio Management,"Reinforcement Learning algorithms are widely applied in many fields, such as price index prediction, image recognition, and natural language processing. This paper introduces a novel algorithm based on the classical Deep Reinforcement Learning algorithm and Deep Residual Shrinkage Neural Network for portfolio management. In this algorithm, the Ensemble of Identical Independent Evaluators framework put forward by Jiang et al. is adopted in the policy function. Following this, we adopt the Deep Residual Shrinkage Neural Network to function as the identical independent evaluator to optimize the algorithm. We use the cryptocurrency market in this research to assess the efficacy of our strategy with eight traditional portfolio management strategies as well as Jiang et al.'s reinforcement learning strategy. In our experiments, the Accumulated Yield is used to reflect the profit of the algorithm. Despite having a high commission rate of 0.25% in back-Tests, results show that our algorithm can achieve 44.5%, 105.4%, and 148.8% returns in three different 50-days back-Tests, which is five times more than the profit of other non-reinforcement learning strategies and Jiang et al.'s strategy. Furthermore, the Sharpe ratio demonstrates that the extra reward per unit risk of the our strategy is still better than other traditional portfolio management strategies and Jiang et al.'s strategy by at least 50% in different time horizons.  © 2021 IEEE. A Deep Residual Shrinkage Neural Network-based Deep Reinforcement Learning Strategy in Financial Portfolio Management algorithmic trading; cryptocurrency; deep reinforcement learning; portfolio management; residual network; residual shrinkage network Advanced Analytics; Big data; Deep learning; Deep neural networks; Financial data processing; Image recognition; Investments; Learning systems; Natural language processing systems; Neural networks; Profitability; Reinforcement learning; Shrinkage; Back-test; Financial portfolio; NAtural language processing; Novel algorithm; Portfolio managements; Price index; Sharpe ratios; Time horizons; Learning algorithms",Risk management
255,Intel realizes $25 billion by applying advanced analytics from product architecture design through supply chain planning,"Due to its scale, the complexity of its products and manufacturing processes, and the capital-intensive nature of the semiconductor business, efficient product architecture design integrated with supply chain planning is critical to Intel's success. In response to an exponential increase in complexities, Intel has used advanced analytics to develop an innovative capability that spans product architecture design through supply chain planning with the dual goals of maximizing revenue and minimizing costs. Our approach integrates the generation and optimization of product design alternatives using genetic algorithms and device physics simulation with large-scale supply chain planning using problem decomposition and mixed-integer programming. This corporate-wide capability is fast and effective, enabling analysis of many more business scenarios in much less time than previous solutions, while providing superior results, including faster response time to customers. Implementation of this capability over the majority of Intel's product portfolio has increased annual revenue by an average of $1.9 billion and reduced annual costs by $1.5 billion, for a total benefit of $25.4 billion since 2009, while also contributing to Intel's sustainability efforts. © 2021 INFORMS Intel realizes $25 billion by applying advanced analytics from product architecture design through supply chain planning Design and planning integration; Device physics simulation; Edelman award; Genetic algorithms; Mixed-integer programming; Problem decomposition; Product design optimization; Semiconductor manufacturing; Supply chain planning optimization ",Monitoring and control
256,Optimizing healthcare,"Collective Social Intelligence (CSI) is an approach to new product development research that centers on creating an effective capability for big data analysis, the internet of things (IoT), online social networks and project portfolio management to improve the ways organizations and communities relate to function collaboratively as a viable system. This has implications for all facets of society, most pertinently the institutions underpinning education, healthcare and government. In this paper we examine people, process and technology in relation to the need for optimizing healthcare, taking full advantage of design thinking and systems thinking to produce change and continuous improvement through collaboration. We show healthcare policy and implementation as one example of why this approach is vitally important and how this approach creates change and improvement. We also describe CSI and our software RD as solutions to mitigate the negative effects of social media on our institutions. This approach is predicated on a convergence of existing and highly regarded strategies for management and information flow with mechanisms and cultures driving modes of communication and visualization. © 2021 IEEE. Optimizing healthcare Big data analytics; Collaboration viable systems; Collective social intelligence; Continuous improvement; Healthcare; Internet of things; Knowledge flow to value; Social media Financial data processing; Health care; Internet of things; Investments; Product development; Research and development management; Social networking (online); Continuous improvements; Information flows; Internet of thing (IOT); New product development; On-line social networks; Project portfolio management; Social intelligence; Strategies for managements; Information management",Strategic alignment
257,Benchmarking and visualization of building portfolios by applying text analytics to maintenance work order logs,"Computerized maintenance management systems (CMMSs) of large commercial and institutional buildings archive work order logs. These logs contain invaluable operational performance indices pertaining to occupants and building systems. However, there are barriers to extract and visualize meaningful metrics from CMMSs. This study proposes a three-part framework to facilitate this process. The first part includes text mining techniques to preprocess and normalize the textual and other types of data. The second part develops benchmarking methods and metrics to evaluate the maintenance performance of buildings as well as to identify the buildings with most work orders on a complex. The last part adopts visualization methods to transform valuable information such as total work orders, the intensity of work orders of buildings and their change rates into more intuitive forms. To do so, a visualization method is implemented via building information model (BIM). Finally, the framework is applied to a case study to demonstrate how effectively the proposed methods perform in practice. The application of the framework can be used to quantify the distribution of WOs among buildings. For example, the case study revealed that 80 percent of all WOs originated from fewer than half of the buildings of a university campus. © Copyright © 2021 ASHRAE. Benchmarking and visualization of building portfolios by applying text analytics to maintenance work order logs  Benchmarking; Maintenance; Text mining; Visualization; Benchmarking methods; Building Information Model - BIM; Computerized maintenance management system; Institutional building; Maintenance performance; Operational performance; Text mining techniques; Visualization method; Architectural design",Strategic alignment
258,Financial Time Series Prediction Model Based Recurrent Neural Network,"Financial time series prediction is usually considered as one of the most difficult challenges because of huge external factors, which are usually stochastic and sensitive so that we can hardly recognize the patterns from historical information. Besides, traditional time series prediction models cannot adapt to the changes in financial circumstances. To overcome these problems, we design a prediction model based on recurrent neural network with gating units, which can learn historical information and adapt the market changes through a specific inner structure. Experiments carried on the Shanghai Securities Composite Index show that the prediction results of our model have more competitive performance compared to those of other traditional models. Our model has good interpretability, and the effects of model hyperparameters on prediction accuracy are also analyzed. On the basis, we proceed with the long-term trend analysis and estimate precisely the tipping points of the stock market. These results give application prospects in risk assessment and portfolio management for the finance industry.  © 2020 IEEE. Financial Time Series Prediction Model Based Recurrent Neural Network Financial time series; Prediction model; Recurrent neural network Commerce; Electronic trading; Financial markets; Forecasting; Investments; Predictive analytics; Risk assessment; Stochastic systems; Time series; Application prospect; Competitive performance; Financial circumstances; Financial time series predictions; Historical information; Portfolio managements; Prediction accuracy; Time series prediction; Recurrent neural networks",Risk management
259,Making sense of business analytics in project selection and prioritisation: insights from the start-up trenches,"Purpose: The study aims to provide insights in the sensemaking process and the use of business analytics (BA) for project selection and prioritisation in start-up settings. A major focus is on the various ways start-ups can understand their data through the analytical process of sensemaking. Design/methodology/approach: This is a comparative case study of two start-ups that use BA in their projects. The authors follow an interpretive approach and draw from the constructivist grounded theory method (GTM) for the purpose of data analysis, whereby the theory of sensemaking functions as the sensitising device that supports the interpretation of the data. Findings: The key findings lie within the scope of project selection and prioritisation, where the sensemaking process is implicitly influenced by each start-up's strategy and business model. BA helps start-ups notice changes within their internal and external environment and focus their attention on the more critical questions along the lines of their processes, operations and business model. However, BA alone cannot support decision-making around less structured problems such as project selection and prioritisation, where intuitive judgement and personal opinion are still heavily used. Originality/value: This study extends the research on BA applied in organisations as tools for business development. Specifically, the authors draw on the literature of BA tools in support of project management from multiple perspectives. The perspectives include but are not limited to project assessment and prioritisation. The authors view the decision-making process and the path from insight to value, as a sensemaking process, where data become part of the sensemaking roadmap and BA helps start-ups navigate the decision-making process. © 2021, Emerald Publishing Limited. Making sense of business analytics in project selection and prioritisation: insights from the start-up trenches Business analytics; Project prioritisation; Project selection; Sensemaking; Software development; Start-up ",Strategic alignment
260,The Five Elements of AI to Leverage Data and Dominate Your Industry,"Using data to make better decisions has been a formula for success for almost two decades now. We went from business analytics to predictive analytics, Big Data and now artificial intelligence (AI). Industry experts predict that within this decade we will see the second wave of AI companies generating $13 trillion GDP growth (Ng, AI transformation playbook. How to lead your company into the AI era, 2018). This growth will be dominated by industry incumbents who understand how to innovate their business model and thus outgrow their industry rivals. Winners will build AI assets to defend against challengers while others will vanish from the market. In this article, you will learn the recipe to generate value from data and AI by combining five main ingredients. After reading you understand what AI really is and how to build an effective AI product portfolio, an engaging AI culture and organizational structure, professionally train and hire AI experts, built a hands-on data governance and a solid data and AI technology platform. © 2021, The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG. The Five Elements of AI to Leverage Data and Dominate Your Industry  ",Financial management
261,A survey study of success factors in data science projects,"In recent years, the data science community has pursued excellence and made significant research efforts to develop advanced analytics, focusing on solving technical problems at the expense of organizational and socio-technical challenges. According to previous surveys on the state of data science project management, there is a significant gap between technical and organizational processes. In this article we present new empirical data from a survey to 237 data science professionals on the use of project management methodologies for data science. We provide additional profiling of the survey respondents' roles and their priorities when executing data science projects. Based on this survey study, the main findings are: (1) Agile data science lifecycle is the most widely used framework, but only 25% of the survey participants state to follow a data science project methodology. (2) The most important success factors are precisely describing stakeholders' needs, communicating the results to end-users, and team collaboration and coordination. (3) Professionals who adhere to a project methodology place greater emphasis on the project's potential risks and pitfalls, version control, the deployment pipeline to production, and data security and privacy. © 2021 IEEE. A survey study of success factors in data science projects data science; factor analysis; project management; success factors; survey Data Analytics; Data Science; Human resource management; Information management; Life cycle; Project management; Empirical data; Organisational; Organizational process; Research efforts; Science community; Science projects; Sociotechnical; Success factors; Technical challenges; Technical process; Surveys",Value management
262,Digital field development planning: A collaboration between technology & process to enable fast and efficient field development planning,"O&G industry is facing difficult business climate with many uncertainties and challenges. Companies including National Oil Companies, NOCs have to be more efficient particularly in developing fields. The challenge is to create an environment to allow E&P companies to efficiently optimize their Field Development Plan, FDP processes and align with technology that enables integration & collaboration between different E&P domains. The environment should be agile to allow changing of circumstances while providing in-depth understanding of the risks and uncertainties involved. PETRONAS has a large portfolio of domestic and international oil & gas assets and is one of the leading NOCs in the world. With the ongoing potential of uncertainty of oil price, it is even more important to fast track field development planning while understanding the risk across domains and recognizing value from investments. PETRONAS has embarked on a digital field development pilot project called Live FDP that enriches internal existing FDP processes & tools to provide integration and generate efficiencies across multidiscipline in E&P workflows and systems that leverage on capabilities enabled by a Digital Cloud based solution. The Digital Planning Application methodology starts with Project Orchestration: Building FDPs using multi-disciplinary inputs and sensitivities followed by managing and framing via capturing an opportunity framework and concept decision. The process will then lead to generating multiple scenarios and evaluations for development options via seamless connectivity and integration with other systems in an Open Platform. At this point, process automation via connectivity of technical domain inputs to Value Based Decision Making will take place alongside Data Discovery & Benchmarks, underpinned by insights, Optimization & Advisory. The Data Analytics will then enable powerful business intelligence & analytics reporting capabilities translated into a Digital Dashboard: alignment with the UPMS process and management systems. Such systems allow project maturation to be completed fast and thus future scalability with expansion apart from Development phase to other phases such as Exploration, Drilling, Facility & Business Planning Workflows can be implemented. Based on recent internal evaluation on a pilot project in Peninsular Malaysia, by conducting Live FDP, the process efficiency in FDP evaluation scenarios was improved by up to 50% while simulation runs were shortened from 2 hrs to 20mins. On top of that, Integration & Collaboration involving benchmarking capability and via Data Ecosystem that allow cross domain collaboration between departments. This provides business continuity through data log for auditing purpose, single source of truth that leads to the increase of confidence and less uncertainties with breadth of multiple scenarios that allow techno-commercial evaluations and benchmarking with internal and external data. This paper will open up the mindset on the ways of how FDP can be developed with a new digital application that improves the project efficiency involving online cloud-based technology that allows multiple iterative processes in both technical and commercial aspects of the project. This is the new way of working that suits the difficult business climate that the O&G industry is currently facing. © 2021, Society of Petroleum Engineers. Digital field development planning: A collaboration between technology & process to enable fast and efficient field development planning  Data Analytics; Data integration; Efficiency; Gasoline; Information management; Integration; Investments; Business climate; Cloud-based; Development planning; Field development; Field development plans; National Oil Company; PETRONAS; Pilot programs; Uncertainty; Work-flows; Decision making",Risk management
263,Features extraction of wind ramp events from a virtual wind park,"In the European renewable energy portfolio, wind has a sizeable share in the total energy production. The Nordic and Baltic energy systems in particular are benefiting from wind energy to reach the greenhouse gas emissions reduction objectives set by the EU. The wind energy production varies with time, and this intermittent characteristic imposes a challenge for full utilization of renewable energy potential. The power system operator needs to ensure timely power supply of demand. An accurate estimation of power output from a non-dispatchable generation resource such as a wind farm is essential for the operator to ensure the supply–demand balance and adequate sizing of reserve power capacity. Existing methods of feature extraction and prediction such as linear regression often overlook the significant variations or do not utilize in the model building. However, this method misinterprets the trend in data. Understanding the properties of the variations in more details would reduce the uncertainty and significantly improve the feature extraction to aid in decision making. Furthermore, as the volume, shape and type of dataset start to increase and new methods are required to extract meaningful information from the patterns in the big data. The objective of the paper is to present a novel Ramping BehaviourAnalysis (RBAθ) model that identifies and quantifies the variations in a time-varying dataset. The variations are classified into significant and stationary events. The former refers to the significant swings beyond a set threshold range and the latter refers to the swings that are relatively within the threshold limits. The features associated to each event include start time, end time, change in magnitude, persistence of an event, angle at which the event took place and frequency of occurrences of the features. In addition, the rain-flow cycles count is extracted from the original data for each event as a sum of half cycles and full cycles. The model is validated using simulated wind power production data from a virtual wind park spread across Estonia and the results are elaborated. The spatial dynamics of the virtual windfarm are captured through localized spatial autocorrelation of the events with the geospatial locations of the turbines. The results demonstrate that RBAθ precisely and accurately identify and quantify the time varying power generation into events with subsequent features. The volume of the data is significantly reduced in the process of summarizing time series data into a series of events. Thereby RBAθ can be also used for data compression and reconstruction with minor losses. The system operators can use the proposed algorithm in operational scheduling, maintenance and investment-capacity building decisions. © 2020 The Authors Features extraction of wind ramp events from a virtual wind park Data science; Feature extraction; Rainflow counting; Ramp events; Renewable energy; Time series variations; Wind power production Data mining; Decision making; Emission control; Extraction; Feature extraction; Gas emissions; Greenhouse gases; Investments; Large dataset; Wind power; Generation resources; Greenhouse gas emissions reductions; Power system operators; Renewable energy potentials; Spatial autocorrelations; Time-varying dataset; Wind energy production; Wind power production; Data reduction",Financial management
264,"Using Data Analytics and Visualization Dashboard for Engineering, Procurement, and Construction Project's Performance Assessment","This study demonstrates the application of design principles for tools in Engineering, Procurement, and Construction (EPC) projects for project management purposes. It advocates the use of proper data analytics and visualization that can be implemented to support effective project progress reporting as well as performance monitoring. At first, an Entity Relationship Diagram (ERD) of the collected data was developed, and then the database was retrieved into the Microsoft Power BI for analysis and visualization. The project's relevant details were visualized and analyzed in terms of the major key performance indications that help evaluate the current situation of projects and aids in future decision making for project performance and portfolio management. A real case of a construction company has been selected and examined. Analytical results support finding the story behind the data. On the other hand, the effects point out that the use of the suitable facts analytics method coupled with the right analytics method and appropriate data visualization software would result in optimum use of information for future aspiration of project success and proper project progress reporting and performance evaluation. It will help companies to transfer from traditional data storage style to big data analytics and powerful use of enterprise business data for companies' growth and success in the field of EPC projects and the construction industries as a whole. Furthermore, it can be used for quicker and extra decisive choices aiming to keep projects on music about their security performance, scheduled time, cost, and great level. Using the proposed dashboard is creating a summary of the accessible records that prints a photo of how initiatives and portfolios are performing, permitting decision-makers to take their future strategic steps aiming for the improvement of their initiatives and accordingly success in their cutting-edge and future endeavors to reap the objective of all stakeholders of this organization. In this paper, it was demonstrated that the visualization of the contractor's performance and KPI are bringing assurance on contractor performance in addition to daily operational monitoring. Furthermore, it helps managers in organizing the workload to ensure the project's completion timely and meeting the customer demand as expected. © 2021 IEEE. Using Data Analytics and Visualization Dashboard for Engineering, Procurement, and Construction Project's Performance Assessment dashboard; data analytics; entity relationship diagram; portfolio management; power BI Advanced Analytics; Construction industry; Contractors; Data Analytics; Data visualization; Decision making; Digital storage; Financial data processing; Image enhancement; Investments; Visualization; Construction companies; Contractor performance; Entity relationship diagrams; Operational monitoring; Performance assessment; Performance monitoring; Portfolio managements; Visualization software; Project management",Capacity management
265,Data-driven methodology to support long-lasting logistics and decision making for urban last-mile operations,"Last-mile operations in forward and reverse logistics are responsible for a large part of the costs, emissions, and times in supply chains. These operations have increased due to the growth of electronic commerce and direct-to-consumer strategies. We propose a novel data-and model-driven framework to support decision making for urban distribution. The methodology is composed of diverse, hybrid, and complementary techniques integrated by a decision support system. This approach focuses on key elements of megacities such as socio-demographic diversity, portfolio mix, logistics fragmentation, high congestion factors, and dense commercial areas. The methodological framework will allow decision makers to create early warning systems and, with the implementation of optimization, machine learning, and simulation models together, make the best utilization of resources. The advantages of the system include flexibility in decision making, social welfare, increased productivity, and reductions in cost and environmental impacts. A real-world illustrative example is presented under conditions in one of the most congested cities: the megacity of Bogota, Colombia. Data come from a retail organization operating in the city. A network of stakeholders is analyzed to understand the complex urban distribution. The execution of the methodology was capable of solving a complex problem reducing the number of vehicles utilized, increasing the resource capacity utilization, and reducing the cost of operations of the fleet, meeting all constraints. These constraints included the window of operations and accomplishing the total number of deliveries. Furthermore, the methodology could accomplish the learning function using deep reinforcement learning in reasonable computational times. This preliminary analysis shows the potential benefits, especially in understudied metropolitan areas from emerging markets, supporting a more effective delivery process, and encouraging proactive, dynamic decision making during the execution stage. © 2021 by the authors. Licensee MDPI, Basel, Switzerland. Data-driven methodology to support long-lasting logistics and decision making for urban last-mile operations Customer-centric supply chains; Digital twin; Emerging markets; Framework; Hybrid methods; Nanostores; Prescriptive analytics; Urban logistics ",Financial management
266,Review of recent advances in petroleum fluid properties and their representation,"It is well-known that petroleum fluid properties (also known as Pressure-Temperature-Volume -PVT) is needed for many petroleum engineering and other interdisciplinary engineering applications. Especially in recent years, multi-disciplinary acquisition of such data and its use became more and more important as seamless and efficient integration became a necessity. Fluid properties for every step of the value chain are extremely important for project evaluation, safety and decision making along with geological and geophysical information. One of the key points about the reservoir fluids is that once the fluids enter the wellbore, all the geologic complexities and rocks will be left behind. Therefore, the focus will be more on fluid compositions and properties and how they are linked and behave within the source (reservoir) to create the maximum value. During this process, fluids are the only desired subsurface elements (with an attached commercial unit pricing) that change hands from one discipline to another discipline. Parallel to this fluids/fluid properties are evaluated for relevant applications and also used in different software applications while they move from reservoir to the wellbore and facilities and all the way to the refineries then to the sales. Therefore, there is always special focus in terms of capturing their needed properties for various applications. Along the way, we face various needs and challenges. Such needs span a wide spectrum of applications, from property valuations, health safety and environment, and all the way from reservoir to well and facilities performance de-risking and management. In this study, we focus on current state-of-the-art in this area and what the current and forward-looking advancements are while highlighting various elements of the progress. In addition, lifecycle management of the fluids and the associated information extracted with modern techniques will be included in this study. Particular attention was given to new developments in terms of hardware, sensors and miniaturization, along with numerical techniques in terms of new/coupled physics and as well as data analytics, molecular simulation, and advanced computational methods. © 2020 Elsevier B.V. Review of recent advances in petroleum fluid properties and their representation Equation of State (EOS); Machine Learning - PVT; Microfluidics; PVT - pressure volume and temperature; PVT New Developments; PVT State of the art Application programs; Boreholes; Data Analytics; Decision making; Gasoline; Life cycle; Numerical methods; Oil field equipment; Oil wells; Project management; Safety engineering; Engineering applications; Geophysical information; Life-cycle management; Molecular simulations; Numerical techniques; Project evaluation; Property valuation; Software applications; Reservoir management",Value management
267,Optimal Portfolio Construction of Islamic Financing Instrument in Malaysia,"Conventional financing and Islamic financing have the same mechanism of operation, although their nature is different. This difference creates different risk, which requires a proper investigation and observation. Thus, the conventional portfolio manager cannot use the same set of proportion or strategy when they want to invest in Islamic financial product. This study determines the optimal portfolio combination and its proportion for Islamic bank financing which include several contracts (Murabahah, Mudharabah, Musharakah, Bai Bithaman Ajil, Ijarah, Ijarah Thumma Al Bai, Istisna’). We apply single index model (SIM) since SIM enables precise calculation of the composition of each asset (financing) by identifying the value of Excess Return to Beta (ERB) as well as the cut-off point based on the acquisition of equivalent rate of profit sharing for each financing. The results show that the optimal composition of portfolio consist of Ijarah or leasing (59.93%), Musharakah or joint venture (29.18%) and Murabahah or sales (10.89%). The portfolio expected return is 1.20% with portfolio risk of 1.67%. © 2021, Springer Nature Singapore Pte Ltd. Optimal Portfolio Construction of Islamic Financing Instrument in Malaysia Financial analytics; Islamic finance; Portfolio optimisation Finance; Excess returns; Financial analytic; Financial products; Islamic finances; Malaysia; Optimal portfolios; Point-based; Portfolio managers; Portfolio optimization; Single index models; Wages",Capacity management
268,"15th International Conference on Knowledge Management in Organizations, KMO 2021","The proceedings contain 37 papers. The special focus in this conference is on Knowledge Management in Organizations. The topics include: Understanding How Patient, Caregiver and Healthcare Professional Come Together During Treatment; knowledge Gain in Production Planning and Execution Systems; fuzzy Evaluation System for Innovation Ability of Science and Technology Enterprises; a Study on Profit Generation Model by Service Innovation of Electronic Manufacturers in the Age of IoT Digitization; The Deficit’s Threat of Contextual Intelligence and KM in the Coaching Process of an Academic and Scientific Incubator for the Survival of Start-Ups; relationship Between a Company’s Knowledge Management Strategy and Its Business Sustainability; project Management in Small and Medium Enterprises to Improve Management Knowledge; benchmarking in Colombian Sterilization Departments; global Export Strategy of the “Born Global” Companies, Business Modeling, and Dynamic Capabilities: Eight Knowledge Management Cases of Country and City Data Analytics; a Practical Taxonomy of Knowledge; the Moderator Effect of Emotional Labor Among Organizational Innovation and Perceived Organizational Support on Department Store Floor Managers’ Job Performance; the Use of Scenarios in Company’s Planning; prototype Framework for the Implementation of Telemedicine Platforms; implementation of the Management System of Knowledge for the Process Management of a Sterilization Central; the Usability Evaluation Method of E-learning Platform Based on Fuzzy Comprehensive Evaluation; Critical Success Factors of Hybrid-ERP Implementations; investigating Trust in Expert System Advice for Business Ethics Audits; Analysis of WEB Browsers of HSTS Security Under the MITM Management Environment; the Importance of the Digital Preservation of Data and Its Application in Universities. 15th International Conference on Knowledge Management in Organizations, KMO 2021  ",Strategic alignment
269,ADVANCED ENTERPRISE ASSET MANAGEMENT SYSTEMS: IMPROVE PREDICTIVE MAINTENANCE and ASSET PERFORMANCE by LEVERAGING INDUSTRY 4.0 and the INTERNET of THINGS (IOT),"Advanced Enterprise Asset Management (EAM) is an approach through which an organization's assets are systematically and proactively managed throughout their lifecycle - from installation through disposition. The objective of EAM is to prolong the service life and maximize utilization of the assets via adoption of leading-edge standards, practices, and technology. Organizations that implement advanced EAM benefit from reduced operating expenses (OPEX), reduced capital replacement expenses (CAPEX), increased uptime, and overall higher quality asset capability within their portfolio. Successful EAM leverages ISO55000 and IAM 2.0 standards to implement predictive, proactive and reliability centered maintenance best practices. Implementing an EAM provides leading edge technology to the rail industry to track and audit maintenance work using mobility tools, heads-up virtual reality displays, augmented reality expertise and the Internet of Things (IoT); combined with artificial intelligence and machine learning to bolster predictive maintenance and simulate asset performance based on different scenarios. EAM will evolve rapidly following the world's rapid transformation into the IoT over the next decade. As rail and transit assets become outfitted with interconnected intelligent sensors whose outputs are collected via active and passive devices, real-time data is available for EAM to track, plan and upgrade assets. As systems are modernized, EAM will leverage the IoT revolution to provide critical information to operations in planning railway management scenarios, including predictive maintenance functionality and edge analytics. © 2021 by ASME. ADVANCED ENTERPRISE ASSET MANAGEMENT SYSTEMS: IMPROVE PREDICTIVE MAINTENANCE and ASSET PERFORMANCE by LEVERAGING INDUSTRY 4.0 and the INTERNET of THINGS (IOT)  Artificial intelligence; Asset management; Augmented reality; Industry 4.0; Information management; Life cycle; Predictive maintenance; Railroad transportation; Transit time devices; Active and passive device; Enterprise asset management system; Enterprise asset managements; Internet of thing (IOT); Leading edge technology; Rapid transformations; Reliability centered maintenance; Virtual-reality display; Internet of things",Governance
270,Risk and uncertainty in team building: Evidence from a professional basketball market,"While recent surveys suggest that team oriented production has become commonplace within the production process, its impact on worker productivity is less clear. Both the theoretical and empirical literature highlight the disparate effects working in teams may have worker productivities. The present paper investigates how well management is able to internalize these uncertain effects. Using National Basketball Association game-by-game data over 36 seasons, we estimate each team's ex-post labor portfolio and find that, beginning in the mid-1990s, NBA teams are able to reduce their portfolio's performance risk. Further the timing of the reduction coincides with the rise in basketball analytics. © 2020 Elsevier B.V. Risk and uncertainty in team building: Evidence from a professional basketball market Expected return; National basketball association; Peer effects; Portfolio; Team production; Uncertainty ",Risk management
271,Assessment of sustainable integration of new products into value chain through a generic decision support model: An application to the forest value chain,"Integrating new products into an existing value chain network is a crucial step in a company's strategy to remain competitive. Businesses are evolving in an economy deeply affected by the global development through unstable markets, changes in trade agreements, increased preoccupations with land use and pollution, and technological advancements. Intrinsically, a company creates a product portfolio to target specific market needs, balance resources and capacities, lower market risks, and ensure stable revenue input. Nevertheless, changing markets increasingly complicate the set of parameters that a company must consider when securing an efficient product portfolio. This research aims to help organizations assess prospective products and optimize product integration by detecting synergies of an existing production/distribution network. To this end, we developed an analytics tool in the form of a mixed-integer linear programming model to evaluate a regional production/distribution network's strategical-level decisions. The model allows mathematical representation of a given network composed of divergent manufacturing processes, bill of materials, distribution nodes, and business-to-business circular economy. The model is applied to a realistic case study in Quebec's Mauricie region, where the introduction of eight bioproducts is evaluated for the forest value chain. The processes considered are: pressurized hot water extraction, fast pyrolysis, organosolv fractionation, and kraft lignin recovery. Our results show that biorefineries have potential economic, social and environmental impacts on the existing forest industry but are tightly linked to the governmental subsidies underlying the forest industry's incapacity to self-sufficiently sustain economic long-term viability in its development towards market maturity. © 2019 Assessment of sustainable integration of new products into value chain through a generic decision support model: An application to the forest value chain Biorefinery; Decision support tool; Forest value chain; Network design; New product assessment; Sustainability ",Strategic alignment
272,Analysis of the effectiveness of the successive concessions method to solve the problem of diversification,"The subject of the paper is multicriteria problems that arise when modeling the complex diversification of a centralized pharmacy network. The purpose of the work is to analyze the peculiarities of solving the three-criteria problem of pharmacy network diversification by the method of successive concessions in the MATLAB package. The paper solves the following problems: research of the advantages of the proposed three-criteria model of pharmacy network diversification in relation to the classical two-criteria model of portfolio theory; construction of the relation of dominance on a set of criteria; determination of the area of stability in the space of the parameters of the concessions method; evaluating the effectiveness of the method for problems of different sizes. The following methods are used: classical portfolio theory, multicriteria optimization, the successive concessions method, computer modeling of the Pareto set. The results obtained: a study of the processes of complex diversification of the pharmacy network by building portfolio models and solving the relevant multicriteria problems by the successive concessions method. Acceptable sets and sets of pareto-optimal portfolios for the risk management are graphically found, taking into account the activity of the network itself and the client portfolio. Conclusions: The results of computer modeling and numerical analysis of solutions by sequential concessions will be useful for automating the business processes of pharmacy networks, risk management, analysis of market data to improve their efficiency. © 2021 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0). CEUR Workshop Proceedings (CEUR-WS.org) Analysis of the effectiveness of the successive concessions method to solve the problem of diversification Entropy; Multicriteria problem; Optimal portfolio problem; Pareto set; Pharmacy network; Successive concessions method Data Science; Financial markets; Machine learning; Multiobjective optimization; Pareto principle; Risk assessment; Risk management; Stability criteria; Business Process; Computer modeling; Different sizes; Following problem; Multicriteria optimization; Multicriteria problems; Portfolio theories; Results of computer modeling; Complex networks",Strategic alignment
273,Design Comorbidity Portfolios to Improve Treatment Cost Prediction of Asthma Using Machine Learning,"Comorbidity is an important factor to consider when trying to predict the cost of treating asthma patients. When an asthmatic patient suffered from comorbidity, the cost of treating such a patient becomes dependent on the nature of the comorbidity. Therefore, lack of recognition of comorbidity on asthmatic patient poses a challenge in predicting the cost of treatment. In this study, we proposed a comorbidity portfolio design that improves the prediction cost of treating asthmatic patients by regrouping frequently occurred comorbidities in different cost groups. In the experiment, predictive models, including logistic regression, random forest, support vector machine, classification regression tree, and backpropagation neural network were trained with real-world data of asthmatic patients from 2012 to 2014 in a large city of China. The 10-fold cross validation and random search algorithm were employed to optimize the hyper-parameters. We recorded significant improvements using our model, which are attributed to comorbidity portfolios in area under curve (AUC) and sensitivity increase of 46.89% (standard deviation: 4.45%) and 101.07% (standard deviation: 44.94%), respectively. In risk analysis of comorbidity on cost, respiratory diseases with a cumulative proportion in the adjusted odds ratio of 36.38% (95%CI: 27.61%, 47.86%) and circulatory diseases with a cumulative proportion in the adjusted odds ratio of 23.83% (95%CI: 15.95%, 35.22%) are the dominant risks of asthmatic patients that affects the treatment cost. It is found that the comorbidity portfolio is robust, and provides a better prediction of the high-cost of treating asthmatic patients. The preliminary characterization of the joint risk of multiple comorbidities posed on cost are also reported. This study will be of great help in improving cost prediction and comorbidity management.  © 2013 IEEE. Design Comorbidity Portfolios to Improve Treatment Cost Prediction of Asthma Using Machine Learning Comorbidity portfolio; identification of treatment cost; machine learning; performance improvement Asthma; Comorbidity; Health Care Costs; Humans; Machine Learning; Neural Networks, Computer; Backpropagation; Decision trees; Diseases; Epidemiology; Forecasting; Logistic regression; Patient treatment; Predictive analytics; Risk analysis; Risk assessment; Statistics; Support vector machines; Support vector regression; 10-fold cross-validation; Back propagation neural networks; Classification regression; Cost prediction; Predictive models; Random search algorithm; Sensitivity increase; Standard deviation; adult; algorithm; area under the curve; Article; artificial neural network; aspergillosis; asthma; back propagation neural network; body mass; bronchitis; classification algorithm; comorbidity; controlled study; diagnostic test accuracy study; disease severity; female; health care cost; health care system; health insurance; hospital readmission; hospitalization; human; ICD-10; learning algorithm; length of stay; lung emphysema; machine learning; major clinical study; male; predictive value; prevalence; quality of life; receiver operating characteristic; risk assessment; sensitivity analysis; sensitivity and specificity; support vector machine; time series analysis; asthma; comorbidity; health care cost; Cost benefit analysis",Risk management
274,Big Data in Smart Infrastructure,"Infrastructure is becoming smarter due to technical advances such as the Internet of Things (IoT) that enables a greater interconnectivity between assets and Artificial Intelligence (AI) to enhance the decision making task; this paper proposes a Big Data Framework in Smart Infrastructure such as Airports, Stations Intelligent Transport Systems and Buildings. The interconnection of infrastructure systems using Local Area Networks, Wi-Fi, Radio or Mobile Networks is not enough to optimize and expand Infrastructure services and functionality; Big Data integration, management and analytics will play a key role in the next Smart Infrastructure stages when Infrastructure will learn and adapt to users. However, the inconvenient truth behind the Big Data hosted in the cloud is the intrinsically associated Cybersecurity threat and risk. This paper provides the practical application of Digital as a Service (DaaS) with considerations and recommendations that cover the implementation of Big Data in Smart Infrastructure with some practical examples of real Infrastructure projects: Buildings, Airport, Stations and Intelligent Transport Systems. In addition, flagship projects have also to be delivered on time, budget and quality within a Health and Safety environment in order to be successful where the understanding of requirements and solutions is key and an Agile approach has some benefits to the traditional project management delivery. © 2021, Springer Nature Switzerland AG. Big Data in Smart Infrastructure Big Data; Cloud; Digital as a Service; Smart Cities; Smart Infrastructure Big data; Budget control; Data integration; Decision making; Information management; Infrastructure as a service (IaaS); Intelligent systems; Intelligent vehicle highway systems; Internet of things; Mobile radio systems; Project management; Security of data; Wi-Fi; Infrastructure project; Infrastructure services; Infrastructure systems; Intelligent transport systems; Interconnectivity; Internet of thing (IOT); Smart infrastructures; Technical advances; Data as a service (DaaS)",Value management
275,"Realistic interplays between data science and chemical engineering in the first quarter of the 21st century, part 2: Dos and don'ts","Under various names, such as, data science, Industry 4.0, or smart manufacturing, digital technologies are transforming our world. Although value statements and promises are published in a steady stream, uptake in the chemical and process industries has been moderate. Successful transformations are not confined to tasks, the “what is”. They also require great care in how they are carried out. This overview, aimed at all participants in the digital transformation of the chemical industry, presents “dos and don'ts” method recommendations for three successive steps: strategy development to define goals, (organisational) mobilisation for implementation, and project delivery. Successful strategy development requires assembling an empowered and skilled team; truly understanding the data science and digital transformation topics; accepting emergence and iteration; and focusing on real needs. Mobilising an organisation is essential so that it can translate strategy to tactics and value. Within organisations, one must therefore: enable project identification; set up a supportive organisational structure and skilful people within it. Looking outside, participation in partnerships is essential to access external resources. Delivery of valuable projects is the end goal. A diverse portfolio is needed, as well as effective collaborations between subject matter experts and data scientists. Technically, the use of software best practice is beneficial, and care must be taken of the data themselves. In the longer term, data science opportunities will extend beyond merely improving traditional analytics to make them faster, better, and more user-friendly. The early identification of beneficial future trends requires encouraging those individuals who have an interest in disruptive currents, and the perceptiveness to sense their areas of application. © 2021 Institution of Chemical Engineers Realistic interplays between data science and chemical engineering in the first quarter of the 21st century, part 2: Dos and don'ts Chemical engineering; Data science; Digital transformation; Industry 4.0; Smart manufacturing Chemical industry; Iterative methods; Metadata; Planning; Project management; Digital technologies; Digital transformation; Organisational structure; Project identification; Smart manufacturing; Software best practices; Strategy development; Subject matter experts; Data Science",Risk management
276,Value Extracting in Relative Performance Appraisal with Network DEA: An Application to U.S. Equity Mutual Funds,"In the mutual fund industry, beating a comparable benchmark index is an important criterion of mutual fund (MF) performance evaluation. Benchmarking MFs against peers also do receive considerable attention in MF performance appraisal literature. Evidence of data envelopment analysis (DEA) playing a big part here is increasing. DEA appraises performance in a multidimensional framework and can accommodate data from different sources and in different formats. A question that arise is how to extract information of value from data in DEA-based performance appraisal. This chapter discusses contribution of network DEA in MF performance appraisal in general and highlight that when MF management process is conceptualised as a network structure, it is possible to extract valuable information from MF specific data analogous to data mining in the case of big data. Information of value in this context aligns with the concept of value dimension in big data. MF performance appraisal studies that use DEA demonstrate how different types of network structures can reveal performance from different perspectives such as operational management, marketing and selling management, disbursements (cost, expenses and fees) management, and portfolio management. Network DEA models enable decomposition of overall management performance at individual sub-process levels. This is valuable information to MF managers to make effective decisions, as they are able to gauge how their MFs operate at sub-process levels from different overall management perspectives. This chapter highlights that information extracted through MF performance appraisal using network DEA is practical and such knowledge inspires solutions to MF management problems in the real world. Moreover, information extracted via such application is valuable to all stakeholders including MF investors to face up to many challenges in managed fund industry landscape. © 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG. Value Extracting in Relative Performance Appraisal with Network DEA: An Application to YOU.S. Equity Mutual Funds Big data analytics; Mutual fund performance; Network DEA application ",Strategic alignment
277,Realizing value from shadow analytics: A case study,"Despite suggestions that analytics projects have value, the literature has, at times, glossed over the obstacles organizations must overcome to realize value from analytics. This research reports on a longitudinal case study of one organization's efforts to improve its profit margins by incorporating analytics into how it generates revenue. The case describes a business unit launching the organization's analytics initiatives, which they deliberately hid from their information technology department to reduce interference. Our study finds that realizing shadow analytics value involves a 3-phase sensemaking process that redefines organizational structures and sets organizations on the path towards digital transformation. These findings offer implications to the mechanisms and structures necessary for realizing value from analytics. Practically, analytics projects may require managers to rethink project management practices and business unit's and IT department's roles in analytics projects. © 2021 Elsevier B.V. Realizing value from shadow analytics: A case study Digital innovation; Digital technologies; Digital transformation; Emergent strategies; Return on investment; Shadow system Information systems; Management information systems; Business units; Digital transformation; Information technology departments; Longitudinal case study; Organizational structures; Profit margin; Project management practices; Research reports; Project management",Strategic alignment
278,Data-driven approaches in FinTech: a survey,"Purpose: This paper aims to explore the latest study of the emerging data-driven approach in the area of FinTech. This paper attempts to provide comprehensive comparisons, including the advantages and disadvantages of different data-driven algorithms applied to FinTech. This paper also attempts to point out the future directions of data-driven approaches in the FinTech domain. Design/methodology/approach: This paper explores and summarizes the latest data-driven approaches and algorithms applied in FinTech to the following categories: risk management, data privacy protection, portfolio management, and sentiment analysis. Findings: This paper details out comparison between different existed works in FinTech with traditional data analytics techniques and the latest development. The framework for the analysis process is developed, and insights regarding the implementation, regulation and workforce development are provided in this area. Originality/value: To the best of the authors’ knowledge, this paper is first to consider broad aspects of data-driven approaches in the application of FinTech industry to explore the potential, challenges and limitations of this area. This study provides a valuable reference for both the current and future participants. © 2021, Emerald Publishing Limited. Data-driven approaches in FinTech: a survey Data mining; Data-driven approach; Deep learning; FinTech; Machine learning; Survey ",Strategic alignment
279,Optimization of energy consumption in chemical production based on descriptive analytics and neural network modeling,"Improving the energy efficiency of chemical industries and increasing their environmental friendliness requires an assessment of the parameters of consumption and losses of energy resources. The aim of the study is to develop and test a method for solving the problem of optimizing the use of energy resources in chemical production based on the methodology of descriptive statistics and training of neural networks. Research methods: graphic and tabular tools for descriptive data analysis to study the dynamics of the structure of energy carriers and determine possible reserves for reducing their consumption; correlation analysis with the construction of scatter diagrams to identify the dependences of the range of limit values of electricity consumption on the average rate of energy consumption; a method for training neural networks to predict the optimal values of energy consumption; methods of mathematical optimization and standardization. The authors analyzed the trends in the energy intensity of chemical industries with an assessment of the degree of transformation of the structure of the energy portfolio and possible reserves for reducing the specific weight of electrical and thermal energy; determined the dynamics of energy losses at Russian industrial enterprises; established the correlation dependence of the range of limiting values of power consumption on the average rate of power consumption; determined the optimal limiting limits of the norms for the loss of electrical energy by the example of rubbers of solution polymerization. The results of the study can be used in the development of software complexes for intelligent energy systems that allow tracking the dynamics of consumption and losses of energy resources. Using the results allows you to determine the optimal parameters of energy consumption and identify reserves for improving energy efficiency. © 2021 by the authors. Licensee MDPI, Basel, Switzerland. Optimization of energy consumption in chemical production based on descriptive analytics and neural network modeling Chemical production; Descriptive analytics; Energy resources; Network modeling; Numerical optimization method BFGS; Training neural networks ",Financial management
280,Application of a New Hybrid MCDM Technique Combining Grey Relational Analysis with AHP-TOPSIS in Ranking of Stocks in the Indian IT Sector,"Ranking of stocks based on an ever-increasing number of financial and technical indicators with varying success rates in predicting the future performance of the stocks both in the short and in the long run, is a major decision problem for financial analysts and it involves applying certain weightage or degree of importance to some of the criteria i.e. stock performance indicators relative to others. The multi-criteria decision problem of ranking of stocks for portfolio construction have been approached by several trading analysts and researchers in the past, using different Multi-Criteria Decision Making (MCDM) techniques. The present paper proposes a novel and hybrid MCDM techniques that combines three of the most popularly used techniques: the Grey Relational Analysis (GRA), Analytic Hierarchy Process (AHP) and Technique of Order of Performance by Similarity to Ideal Solution (TOPSIS) and this new hybrid MCDM method has been applied to the problem of ranking 8 stocks from the Indian IT sector in order of preference of inclusion in a portfolio based on the stock performance indicated by Price-to-Book Value Ratio, Price-to-Earnings Ratio, Return on Equity (ROE), Momentum (%), MACD(12,26,9) and 125-day Rate of Change (ROC) metric. The novelty of the proposed hybrid model lies in the unique step-by-step approach to combining the three existing MCDM methods. Application of the GRA compares the alternatives based on actual quantitative parameters. Thus, the relative weights of the decision matrices are not formulated based on any individual’s assumption or opinion. © 2021, Springer Nature Switzerland AG. Application of a New Hybrid MCDM Technique Combining Grey Relational Analysis with AHP-TOPSIS in Ranking of Stocks in the Indian IT Sector Analytic Hierarchy Process (AHP); Grey Relational Analysis; MCDM methods; Portfolio construction; Ranking of stocks; TOPSIS Advanced Analytics; Analytic hierarchy process; Decision making; Hierarchical systems; Intelligent computing; Analytic hierarchy process (ahp); Grey relational analyses (GRA); Grey relational analysis; Multi-criteria decision making; Multi-criteria decision problems; Quantitative parameters; Step-by-step approach; Technical indicator; Financial markets",Strategic alignment
281,Implementing machine learning for finance: A systematic approach to predictive risk and performance analysis for investment portfolios,"Bring together machine learning (ML) and deep learning (DL) in financial trading, with an emphasis on investment management. This book explains systematic approaches to investment portfolio management, risk analysis, and performance analysis, including predictive analytics using data science procedures. The book introduces pattern recognition and future price forecasting that exerts effects on time series analysis models, such as the Autoregressive Integrated Moving Average (ARIMA) model, Seasonal ARIMA (SARIMA) model, and Additive model, and it covers the Least Squares model and the Long Short-Term Memory (LSTM) model. It presents hidden pattern recognition and market regime prediction applying the Gaussian Hidden Markov Model. The book covers the practical application of the K-Means model in stock clustering. It establishes the practical application of the Variance-Covariance method and Simulation method (using Monte Carlo Simulation) for value at risk estimation. It also includes market direction classification using both the Logistic classifier and the Multilayer Perceptron classifier. Finally, the book presents performance and risk analysis for investment portfolios. By the end of this book, you should be able to explain how algorithmic trading works and its practical application in the real world, and know how to apply supervised and unsupervised ML and DL models to bolster investment decision making and implement and optimize investment strategies and systems. © 2021 by Tshepo Chris Nokeri. All rights reserved. Implementing machine learning for finance: A systematic approach to predictive risk and performance analysis for investment portfolios Algorithmic Trading; Deep Learning; Finance; Investment Portfolio; Investment Risk Analysis; Machine Learning; Python; Stock Market; Supervised Machine Learning ",Strategic alignment
282,"15th International Conference on Dynamical Systems: Theory and Applications, 2019","The proceedings contain 27 papers. The special focus in this conference is on Dynamical Systems: Theory and Applications. The topics include: Mobile Game Development with Spatially Generated Reverberation Sound; use of Project-Based Learning and Living Personas to Train Future Designers of Interactive Applications; emerging Technologies in Smart Classroom Education; preface; reducing Compound Degree for Optimum Linear Decomposition of Symmetric Index Generation Function; retail Sales Forecasting in the Presence of Promotional Periods; evaluation of Selected Artificial Intelligence Technologies for Innovative Business Intelligence Applications; towards Business Cost Mining: Considering Business Process Reliability; GMDH-Type Neural Networks for Predicting Financial Time Series: A Study of Informational Efficiency of Stock Markets; mean-Reverting Portfolio Optimization via a Surrogate Risk Measure - Conditional Desirability Value at Risk; blockchain-Based Smart Contracts Use for Photovoltaic Energy Trade Transactions; chaotic Chua’s Circuit’s Parameter Estimation Using Composite Identifier and Indirect Adaptive Output Regulation; combined State Merging and Splitting Procedure for Low Power Implementations of Finite State Machines; towards an Extension and Normalization of the Random Vector Anisotropy Magnitude; non-linear Correlation Based Approach to the Identification of Maximally Stationary Systems; LED Grow Light for Optimization of Chlorophyll Excitation Ratio; development of Logistics Models for Oil Cargo Transportation to Reduce Logistics Costs and Improve Wagon Mileage; investigating the Closer Running of Rail Vehicles for Better Network Utilisation; understanding Manufacturing Processes on Basis of Visualized Machine and Sensor Data; optimizing Maintenance Cost of Uniform Rolling Stock by Scheduling Algorithms; utilizing Evidence in Asset Management in the Era of Industry 4.0 and Artificial Intelligence. 15th International Conference on Dynamical Systems: Theory and Applications, 2019  ",Risk management
283,Patent-trademark linking framework for business competition analysis,"A major concern of technology-based firms (TBFs) is gaining a competitive edge, for which TBFs develop technologies to embed in their products and services to differentiate themselves from their competitors. TBFs that share similar products or services must compete against each other, which means that business competition among TBFs should be conducted at the product and service levels. However, faced with challenges in obtaining information of the products and services for which TBFs apply their own technologies, previous studies have not thoroughly analyzed the business competition from the perspectives of products and services. Therefore, this study proposes a framework to better understand the business competition among TBFs by linking their business areas and technologies, which refer to products and services in their trademarks, and the technologies described in their patents, respectively. This framework provides the identification of technology-based business portfolios and an understanding of the business competition from both an overall business perspective and within specific areas, considering the technological capabilities and business willingness of the TBFs. This study academically contributes as the first study to conduct a business competition analysis at the product and service levels and industrially contributes by suggesting a systematic process that will help in understanding the rapidly evolving business competition environment. © 2020 Elsevier B.V. Patent-trademark linking framework for business competition analysis Business intelligence; Competitive information; Goods and services; Patent; Technology-based firms; Trademark Commerce; Patents and inventions; Service industry; Business competition; Business perspective; Business portfolios; Product and services; Products and services; Systematic process; Technological capability; Technology-based firms; Competition",Financial management
284,Technologies for Advancing Offshore Enhanced Oil Recovery Capabilities,"Last year the Department of Energy (DOE) presented a description of the expansion of its research portfolio from one focused on research primarily for onshore applications to one that includes projects specifically for offshore application. That paper (OTC-30469-MS) also included key research results for the portfolio beginning with projects initiated in 2007. This paper follows on that theme and presents an overview of the Department's current research portfolio focusing on recent-past learnings, current learnings, and research gaps identified from the projects in the current research portfolio 2017-2023. Discussion includes projects that are sponsored by the Department as part of its public-private partnerships with principal investigators from industry and academia, and those projects sponsored by the Department at its National Laboratories. The discussion also includes an overview of activities and projects jointly pursued by DOE and the Department of the Interior's Bureau of Safety and Environmental Enforcement (BSEE) pursuant to the July 2020 Memorandum of Collaboration signed by both agencies. Major insights presented in this paper focus on innovative mid-Technology Readiness Level (mid-TRL) technologies that will enable cost-effective enhanced oil recovery in deepwater and ultra-deepwater including insights for cement and wellbore integrity, flow assurance, life extension of offshore platforms and risers, sensors and telecommunications, early kick detection, chemical delivery, data analytics involving big data sets and modeling, and advanced sensors for EOR operations. Many of the projects reviewed in this paper are part of the portfolio of projects that are sponsored by the Department at the National Laboratories while at the same time includes projects that are cost-shared with private sector and research partners in academia. The breadth of the portfolio illustrates the overall approach of the offshore research portfolio especially for enhanced oil recovery. Recently the National Petroleum Council completed a study for the Secretary of Energy titled Meeting the Dual Challenge: A roadmap to at-scale deployment of carbon capture, use, and storage in which the potential for the use and potential long-term storage of CO2 used in enhanced oil recovery is considered for both onshore and offshore settings (NPC 2019). © 2021, Offshore Technology Conference. All rights reserved. Technologies for Advancing Offshore Enhanced Oil Recovery Capabilities  Chemical detection; Cost effectiveness; Data Analytics; Digital storage; Offshore oil fields; Offshore oil well production; Offshore oil wells; Offshore technology; 'current; Department of Energy; Enhanced-oil recoveries; National laboratory; Offshore applications; Offshores; Public/private partnerships; Recovery capabilities; Research gaps; Research results; Enhanced recovery",Strategic alignment
285,"Digital Water: Enabling a More Resilient, Secure and Equitable Water Future","This book shows how digital technologies are transforming how we locate, manage, treat, distribute, and use water. Water resources are under stress from over-allocation, increased demand, pollution, climate change, and outdated public policies. Historical approaches to delivering water for human consumption, industrial production, agriculture, power generation, and ecosystems are no longer adequate to meet demands. As a result, we need to vastly improve the efficiency and effectiveness of our public and private sector processes in water management. The author describes recent advances in data acquisition (e.g., satellite imagery, drones, and on-the-ground sensors and smart meters), big data analytics, artificial intelligence, and blockchain, which provide new tools to meet needs in both developing and developed economies. For example, a digital water technology portfolio brings the value of real-time system-wide monitoring – and response – within the capability of water providers of all sizes and sophistication. As such, digital water promises to increase the long-term value of water resource assets while assisting in compliance with regulations and helping respond to the demands of population growth and evolving natural and business ecosystems. Including many practical examples, the author concludes that digital and smart water technologies will not only better manage water assets but also enable the public sector to provide universal access to safe drinking water, the private sector to continue to grow, and ecosystems to thrive. © 2022 William Sarni. All rights reserved. Digital Water: Enabling a More Resilient, Secure and Equitable Water Future  ",Financial management
286,Project duration-cost-quality prediction model based on Monte Carlo simulation,"Based on the earned value management theory, the project duration and cost forecast data are obtained through Monte Carlo simulation, combined with the knowledge of mathematical statistics to in-depth analysis of the data, and then the project quality calculation is completed by establishing the quality correlation function. On the basis of existing scholars' research, at the process level, the management focus is identified by calculating process influence and quality, and at the project level, the operation of the entire project is simulated and predicted by the calculation of the total construction period, cost, and quality. © Published under licence by IOP Publishing Ltd. Project duration-cost-quality prediction model based on Monte Carlo simulation  Budget control; Cost benefit analysis; Functions; Inductively coupled plasma mass spectrometry; Predictive analytics; Project management; Statistics; Value engineering; Construction period; Earned value management; In-depth analysis; Process levels; Project duration; Project quality; Quality correlations; Quality prediction models; Monte Carlo methods",Value management
287,Key resources for industry 4.0 adoption and its effect on sustainable production and circular economy: An empirical study,"Developing countries like South Africa is aiming to be a nation that has fully harnessed the potential of Industry 4.0 technological innovation to grow the economy and uplift people of the country. The country has motivated manufactures to focus on smart manufacturing considering the sustainable development aspects to develop circular economy capabilities. However, manufacturers are facing various resource related challenges, which has slowed down the progress of Industry 4.0 adoption. This study aspires to develop a theoretical model linking key resources for Industry 4.0 adoption that are essential to drive technological progress; and its effect on sustainable production and circular economy capabilities. The review of literature led to the identification of thirty-five resources that are essential for the adoption of Industry 4.0. Further, exploratory factor analysis was used to group the variables under relevant factors and thereafter research team developed a theoretical model that was further tested using PLS-SEM technique. Research findings indicate that production systems, human resources, project management, management leadership, green logistics, green design, information technology, big data analytics and collaborative relationships are key resources for Industry 4.0 adoption; second, Industry 4.0 adoption have a positive relationship with sustainable production and finally, sustainable production has a positive relationship with circular economy capabilities. The study concludes with theoretical and practical implications. © 2020 Elsevier Ltd Key resources for industry 4.0 adoption and its effect on sustainable production and circular economy: An empirical study Circular economy; Ethical business development; Industry 4.0; Sustainable production; Sustainable resources Advanced Analytics; Data Analytics; Developing countries; Digital storage; Factor analysis; Green manufacturing; Industry 4.0; Information management; Project management; Sustainable development; Collaborative relationships; Empirical studies; Exploratory factor analysis; Smart manufacturing; Sustainable production; Technological innovation; Technological progress; Theoretical modeling; Industrial economics",Capacity management
288,Corporate Governance Efficiency: Automation of Corporate Governance Procedures,"Since the time of the first corporations, shareholders have expected boards to manage corporate governance processes in the best way. At the same time, the era of digital technology can significantly increase the effectiveness of corporate governance procedures through automation of corporate governance procedures as business processes. The board of directors and the corporate secretary can rely on performance indicators and manage the effectiveness of corporate governance processes. This chapter discusses opportunities for improving the business processes, including automation and analytics. The author considers approaches to project management of corporate governance procedures automation and its limitations. © 2022 by IGI Global. All rights reserved. Corporate Governance Efficiency: Automation of Corporate Governance Procedures  ",Risk management
289,The influence of digital technologies on supply chain coordination strategies,"Purpose: This paper aims to investigate the impact of the strategic transformation of engineering to order company (ETO) at the level of the internal value-adding chain of operations on its position as a sub-supplier. The transformation is motivated and enabled by end-to-end business intelligence related to processes revolving around the product’s design, configuration and engineering. The investigation builds on case-based research following the company’s decision of converting its product portfolio to only one family of products, thus increasing process efficiency whilst at the same time enlarging its market reach by offering individualized and innovative products. By digitally integrating operations related to sales, product development and production preparation, the traditional trade-off between cost-effective solutions with high product variety and low lead-time is significantly reduced. Design/methodology/approach: A design science research project has been conducted to create knowledge on the effects of integration across the value-adding chain of operations. Several design cycles illustrate how development based on business intelligence and available technological enablers for inter-operation integration influence the traditional approach towards supply chain pipeline selection strategies. Findings: Relating to digital transformation, the consequences and means of adopting digital business intelligence for integrating several administrative and engineering operations in small-medium enterprises (SME) are studied. The product delivery performance of the SME is improved, thus, having ETO lead-time comparable to manufacturing to order company. The findings show how the adoption of state-of-the-art technological solutions for cross-operation digital integration challenges traditional supply chain, coordination models. Research limitations/implications: The conclusions are drawn based on a single case. The limitations associated with case-based research call for further work to support generalization. Furthermore, the long-term influence of the effects of increased interoperability on supply chain coordination strategies requires further investigation. Practical implications: As technological solutions evolve, new opportunities for supply chain management arise, which put into question the traditional understanding that complex supply chain pipeline characteristics should be handled by complexity reducing initiatives, which opens up new competitive opportunities for companies in high-cost countries. Social implications: Enabling the use of human resources towards expanding the business (rather than running it only) are aligned with the current economic and political situation in high-cost countries like Denmark and potentially releases skilled employees from repetitive and low value-adding work and reengages them in business development. Originality/value: By embracing flexibility and volatility as an opportunity, this publication exemplifies how to move beyond hedging the supply chain volatility, but systematically enable the supply chain to deal with complexity efficiently. © 2021, Emerald Publishing Limited. The influence of digital technologies on supply chain coordination strategies Both; Configuration; Interoperability; Manufacturing configuration; Product configuration; Supply chain; Supply chain coordinated strategy; Supply chain strategy selection; Technology; Transition; Value-adding chain ",Strategic alignment
290,Planning for Unknown in The New Age of Digital: A Paradigm for Offshore Oil and Gas Risk Assessment and Management,"Major Oil and Gas operators and service companies look to undertake large scale digital transformations aimed at producing integrated, connected, and intelligent enterprises. These transformations require accelerating the journey to the cloud to modernize the entire application portfolio. By transitioning to the cloud, firms enjoy improved data analytics which allow for evolution to next generation digital work environment. This shift, however, comes with workforce challenges. Employees in all categories and at most levels will require significant cross-and up-skilling to take full advantage of the digital transformation. As vendors, suppliers, service companies, and operators move products and equipment around an expanding ecosystem of assets, security threats are likely to increase due to further geopolitical instability. Data based decision making, which enables the optimization of assets and automation of operations to minimize workforce risk exposure must be implemented with consideration of enterprise risk reduction (across the asset and workforce operational risk life cycle). As Oil and Gas operations become more geographically dispersed and diverse, they are exposed to new and evolving risk factors which can directly impact value. These risk factors make asset acquisition, development, management, and maintenance all more challenging. Analyses of risk in a digital foundation risk-based platform is most valuable at the earliest stages of asset development in determining whether to proceed with the planned development through to end-of-life decommissioning. Successful firms must create an end-to-end digital roadmap which delineates between technical and transactional activities and outlines effective stakeholder engagement at each project stage. The fundamental thesis of this paper is that although risk can be mitigated and reduced through the introduction of digital tools into oil and gas operations, it can never be completely removed. Furthermore, while industry research on the impact of digitalization usually rely heavily on cost savings, optimization, and health, safety, and environment (HSE) related cases, they typically fail to consider the contribution of digitalization on risk assessment and management. This paper argues that we need to move away from the focus on cost savings, process optimization, and HSE metrics improvement metrics. This paper sets up a mechanism for developing risk-based strategies for implementation of digital solutions. Digital technologies transform the way upstream oil and gas companies solve key business problems and can significantly reduce risk. Pockets of digital solutions already exist in the industry within disciplines such as reservoir, drilling site operational efficiency, pipeline, and flow assurance modeling. However, there are still significant opportunities to use digital technologies to reduce portfolio risk by enabling better integration across disciplines, asset organizations, and players in the value chain. When considering the use of digital technologies in the following areas, © 2021, Offshore Technology Conference. All rights reserved. Planning for Unknown in The New Age of Digital: A Paradigm for Offshore Oil and Gas Risk Assessment and Management  Accident prevention; Decision making; Digital devices; Gas industry; Gases; Health risks; Life cycle; Offshore oil well production; Offshore technology; Optimization; Personnel; Public utilities; Risk assessment; Risk management; Cost saving; Digital solutions; Digital technologies; Digital transformation; Health safety; Oil and gas operations; Optimisations; Risk assessment and managements; Risk factors; Service companies; Data Analytics",Financial management
291,Analysis of Success Factors Ranking: Machine Learning Projects of E-Commerce in Indonesia,"A research about software development project revealed that only 31% projects were successfully conducted and the entire was challenged. On the other report, 62% IT projects failed in terms of delivery time, 49% over the cost, and 47% projects obtained higher maintenance cost. In the era of the machine learning also digital intelligence as now, e-commerce located in Indonesia competes to win a market acquisition by giving recommended products through the digital intelligence approach. But this key objective has a challenge in terms of the deliverable process. Study case from an e-commerce in Indonesia stated that given seven projects, only one projects or system still being used until the end, two projects were decommissioned, and also a project was halted due to facing a lot of issues. Hence, this applied research was initiated to identify the rank of success factors in the field of machine learning projects. Qualitative combined by quantitative methods were used sequentially to get the critical factors and measure the rank based on hierarchical analytics using AHP. Six projects' success criteria and fifteen success factors criteria evaluated by the four experts on high level position both developers and project managers. The highest rank of the projects' success criteria is user's satisfaction by 27.7%, meet the quality standard by 22.8%, and meet the time by 18.8%. In terms of critical success factors, the top three are clarity on the project's goal and objective by 15.25%, project manager's capability by 9.37%, and followed by clear communication by 8.82%. © 2021 IEEE. Analysis of Success Factors Ranking: Machine Learning Projects of E-Commerce in Indonesia AHP; e-commerce; machine learning; project management; success factor ranking Hierarchical systems; Machine learning; Managers; Project management; Software design; E- commerces; Indonesia; IT project; Learning projects; Machine-learning; Project managers; Project success criteria; Software development projects; Success factor ranking; Success factors; Electronic commerce",Value management
292,Par munis: Sub-par performance,"It is well recognized that institutional municipal portfolio managers prefer premium bonds to those selling near par. This article shows that such aversion to par bonds is justified because they are expected to underperform comparable premium or discount bonds in the near term. The extent of the underperformance depends on the shape of the yield curve and is positively correlated with the level of expected interest rate volatility. The underperformance is attributable to tax considerations. When an investor purchases a municipal bond (muni) below par, the resulting gain is taxed at maturity, and the price is depressed by the present value of the tax. This tax effect amplifies the interest rate sensitivity of discount munis. Munis selling near par are also negatively convex; the potential decline attributable to higher interest rates exceeds the increase stemming from commensurately lower rates. The underperformance of near-par munis relative to those selling at a high premium (or at a deep discount) arises from the resulting combination of extended duration and negative convexity. The changing value of tax liabilities creates a unique challenge in determining interest rate sensitivity and expected return—which conventional analytics fail to recognize. The tax-neutral analytics used in this article not only incorporate the value of future tax costs but also provide an accurate method for predicting muni price changes and investment returns. © 2021 Pageant Media Ltd. Par munis: Sub-par performance  ",Value management
293,"Strategic Management, Decision Theory, and Decision Science: Contributions to Policy Issues","This book contains international perspectives that unifies the themes of strategic management, decision theory, and data science. It contains thought-provoking presentations of case studies backed by adequate analysis adding significance to the discussions. Most of the decision-making models in use do take due advantage of collection and processing of relevant data using appropriate analytics oriented to provide inputs into effective decision-making. The book showcases applications in diverse fields including banking and insurance, portfolio management, inventory analysis, performance assessment of comparable economic agents, managing utilities in a health-care facility, reducing traffic snarls on highways, monitoring achievement of some of the sustainable development goals in a country or state, and similar other areas that showcase policy implications. It holds immense value for researchers as well as professionals responsible for organizational decisions. © The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2021. Strategic Management, Decision Theory, and Decision Science: Contributions to Policy Issues Algorithmic Decision Theory; Data Science; Strategic Management; Theory of Choice; Theory of Constraints ",Strategic alignment
294,Effect of data privacy and security investment on the value of big data firms,"The use of big data analytics (BDA) and related technologies by firms can raise concerns regarding data privacy and security (DPS). In this study, with stock return and risk as indicators of firm performance, we investigate how DPS investment affects the financial performance of BDA firms (firms with BDA investment) and non-BDA firms (firms without BDA investment). We use a difference-in-differences approach and apply the propensity score matching to 1400 DPS announcements made by 228 US firms whose stocks are publicly traded on US stock markets from 2004 to 2018. The main findings are as follows: (1) On average, DPS investment significantly reduces firms' systematic risk. (2) The risk reduction effects are greater for non-BDA firms than for BDA firms. Our results indicate that DPS investment reduces firm risk and that the business value of specific information technology (IT) investment (e.g., DPS) is influenced by firms' other IT assets (e.g., BDA). The present findings advance the literature on the business value of IT and firms' IT portfolio management in digitized business environments. © 2021 Elsevier B.V. Effect of data privacy and security investment on the value of big data firms Big data; Data Privacy; Data Security; Event study; IT investment; IT risk Advanced Analytics; Big data; Commerce; Data Analytics; Data privacy; Financial data processing; Financial markets; Business environments; Business value of it; Data privacy and securities; Difference-in-differences; Financial performance; Firm Performance; Propensity score matching; Specific information; Investments",Strategic alignment
295,The Use of Features to Enhance the Capability of Deep Reinforcement Learning for Investment Portfolio Management,"Reinforcement learning algorithms and neural networks have been widely used in stock market forecasting, image recognition processing and many other fields. In this research, we adopt features based on asset prices and transaction volume to have better description of the current state. This research aims to input new features combination into the neural network to assist agent in analyzing the market environment for 11 cryptocurrencies. The experimental data contains historical data of the price and transaction volume of the sample from 30 days before backtest sets, as well as features (the volume, the rate of change, the moving average, the stochastic oscillator, the Eliot oscillator and the On Balance Volume) calculated using the historical data. The efficacy of our strategy is comparing to that of nine traditional strategies, i.e., Aniticor, Online Moving Average Reversion, Passive Aggressive Mean Reversion, Confidence Weighted Mean Reversion, Robust Median Reversion, Online Newton Step, Universal Portfolios and Exponential Gradient. The experiments result shows the use of features can increase the final profit, which is about 10% more profitable compare with strategy established by Jiang et al. in July 2017. Furthermore, the best combination of our features outperforms all other traditional strategies by at least 7.6%.  © 2021 IEEE. The Use of Features to Enhance the Capability of Deep Reinforcement Learning for Investment Portfolio Management cryptocurrencies; deep reinforcement learning; features Advanced Analytics; Big data; Commerce; Electronic trading; Image recognition; Investments; Learning algorithms; Neural networks; Profitability; Reinforcement learning; Stochastic systems; Historical data; Investment portfolio; Market environment; Moving averages; Rate of change; Stochastic oscillators; Stock market forecasting; Universal portfolios; Deep learning",Monitoring and control
296,Clean manufacturing powered by biology: how Amyris has deployed technology and aims to do it better,"Amyris is a fermentation product company that leverages synthetic biology and has been bringing novel fermentation products to the market since 2009. Driven by breakthroughs in genome editing, strain construction and testing, analytics, automation, data science, and process development, Amyris has commercialized nine separate fermentation products over the last decade. This has been accomplished by partnering with the teams at 17 different manufacturing sites around the world. This paper begins with the technology that drives Amyris, describes some key lessons learned from early scale-up experiences, and summarizes the technology transfer procedures and systems that have been built to enable moving more products to market faster. Finally, the breadth of the Amyris product portfolio continues to expand; thus the steps being taken to overcome current challenges (e.g. automated strain engineering can now outpace the rest of the product commercialization timeline) are described. © 2020, The Author(s). Clean manufacturing powered by biology: how Amyris has deployed technology and aims to do it better Amyris; Farnesene; Fermentation; Scale-up; Technology transfer Automation; Fermentation; Synthetic Biology; Article; biotechnology; degradation; fermentation; nonhuman; risk assessment; scale up; synthetic biology; automation; synthetic biology",Risk management
297,Leveraging purchase regularity for predicting customer behavior the easy way,"The valuation of future customer activity is a mainstay of any organization seeking to efficiently manage its customer portfolio. In the area of customer-base analytics, the ongoing race for predictive power has yielded a large corpus of research to assist managers in this respect. Approaches in the tradition of stochastic models have been particularly successful because they rely only on easy-to-compute key metrics and integrate them within a parsimonious probability-modeling framework. Recent advances in this field have demonstrated that incorporating the timing regularity of past purchases can improve predictive accuracy relative to purely recency/frequency-based approaches. This paper expands that idea and introduces generalizations of a well-established probability model, the BG/NBD (Fader et al., 2005a), by replacing the exponential with a more flexible Erlang-k interarrival timing process. The resulting model variants are capable of leveraging regularity while retaining almost the same level of data requirements and algorithmic efficiency. Using extensive simulation studies and six data sets covering a wide range of empirical settings the authors demonstrate substantial improvements in predictive accuracy against the baseline models and performance gains close to or on par with a more complex model alternative. The availability of efficient and easily accessible implementations of the new model variants in the R-package BTYDplus allows marketing analysts to apply them in large-scale scenarios of data-rich environments on a continuous basis. © 2020 Elsevier B.V. Leveraging purchase regularity for predicting customer behavior the easy way Customer-base analysis; Pareto/GGG; Pareto/NBD; Probability models; Regularity; Timing models ",Governance
298,An uncertainty-based framework for technology portfolio selection for future aircraft program,"One critical step of conceptual design of future aircraft is the selection of technologies to be integrated in the system. This paper presents a framework under development to support the selection and prioritization of technologies in the presence of uncertainty for future Aerospace & Defense programs. The work is a product of a collaboration between Saab Aeronautics, Embraer, and Instituto Tecnológico de Aeronáutica. This article expands the capabilities from previous publications by introducing Uncertainty Quantification to a problem involving a larger number of technologies and multiple Measures of Performance. Strategies to handle the upsized problem are investigated and visual analytics is explored to communicate results in an intuitive and understandable fashion. A de-coupled strategy is proposed to enable pursuing Effectiveness-Based Design by translating Measures of Effectiveness into Measures of Performance to guide the selection and prioritization of technology clusters. © 2021, American Institute of Aeronautics and Astronautics Inc, AIAA. All rights reserved. An uncertainty-based framework for technology portfolio selection for future aircraft program  Aircraft; Aviation; Aerospace defense; Aerospace projects; Aircraft programs; Critical steps; Defence programs; Measure of performance; Portfolio selection; Selection and prioritisation; Technology portfolios; Uncertainty; Conceptual design",Strategic alignment
299,Optimal drawdown for Woodford and mayes in the Anadarko Basin using data analytics,"One of the enduring pieces of the jigsaw puzzle for all unconventional plays is drawdown (DD), a technique for attaining optimal return on investment. Assessment of the DD from producing wells in unconventional resources poses unique challenges to operators; among them the fact that many operators are reluctant to reveal the production, pressure, and completion data required. In addition to multiple factors, various completion and spacing parameters add to the complexity of the problem. This work aims to determine the optimum DD strategy. Several DD trials were implemented within the Anadarko Basin in combination with various completion strategies. Privately obtained production and completion data were analyzed and combined with well log analysis in conjunction with data analytics tools. A case study is presented that explores a new strategy for DD producing wells within the Anadarko Basin to optimize a return on investment. We use scatter-plot smoothing to develop a predictive relationship between DD and two dependent variables-estimated ultimate recovery (EUR) and initial production (IP) for 180 days of oil-and introduce a model that evaluates horizontal well production variables based on DD. Key data were estimated using reservoir and production variables. The data analytics suggested the optimal DD value of 53 psi/D for different reservoirs within the Anadarko Basin. This result may give professionals additional insight into more fully understanding the Anadarko Basin. Through these optimal ranges, we hope to gain a more complete understanding of the best way to DD wells when they are drilled simultaneously. Our discoveries and workflow within the Woodford and Mayes Formations may be applied to various plays and formations across the unconventional play spectrum. Optimal DD techniques in unconventional reservoirs could add billions of dollars in revenue to a company's portfolio and dramatically increase the rate of return, as well as offer a new understanding of the respective producing reservoirs. © 2021 Society of Petroleum Engineers Optimal drawdown for Woodford and mayes in the Anadarko Basin using data analytics  Anadarko Basin; United States; Data Analytics; Earnings; Oil field development; Oil well logging; Profitability; Resource valuation; Well completion; Well logging; Dependent variables; Estimated ultimate recoveries; Horizontal well production; Predictive relationships; Production variables; Scatter plot smoothing; Unconventional reservoirs; Unconventional resources; drawdown; numerical model; reservoir; well technology; Horizontal wells",Monitoring and control
300,Price series cross-correlation analysis to enhance the diversification of itemset-based stock portfolios,"Planning buy-and-hold strategies for stock trading is a challenging financial task. It entails building a portfolio of stocks maximizing the expected return in the medium- or long-term while minimizing investments' risk. Diversification is the most common strategy to manage risk in financial investments. It entails spreading bets across multiple assets, typically by picking stocks from different financial sectors. This paper presents a time series clustering-based strategy to improve the effectiveness of stock diversification across sectors. It analyzes the cross-correlation among price series in order to identify groups of stocks belonging to different sectors that unexpectedly show similar trends as well as dissimilarities among stocks of the same sector. The diversification strategy has been integrated into a state-of-the-art itemset-based approach to stock portfolio generation. The performance achieved on the YOU.S. stock market show relevant improvements in portfolio returns and drawdown control.  © 2020 ACM. Price series cross-correlation analysis to enhance the diversification of itemset-based stock portfolios diversification; machine learning; stock portfolio management; time series clustering Commerce; Data Science; Investments; Buy-and-hold strategy; Cross correlations; Cross-correlation analysis; Diversification strategies; Financial investments; Financial sectors; Stock market show; Time series clustering; Financial markets",Strategic alignment
301,"27th European Conference on Systems, Software and Services Process Improvement, EuroSPI 2020","The proceedings contain 63 papers. The special focus in this conference is on Systems, Software and Services Process Improvement. The topics include: A Concept for Virtual Reality Based Industrial Maintenance Training Preparation; analysis of Improvement Potentials in Current Virtual Reality Applications by Using Different Ways of Locomotion; integrating Stakeholder and Social Network Analysis into Innovation Project Management; adaptive Predictive Energy Management Strategy Example for Electric Vehicle Long Distance Trip; toolbox of Methods for the Digital Business Transformation: A Blueprint for the Education and Training of Service Engineers; spin-Off Strategies of Established Companies Due to Digitalization and Disruption; democratizing Innovation in the Digital Era: Empowering Innovation Agents for Driving the Change; educating Future Engineers – Meeting Challenges at Secondary School and University; social and Human Factor Classification of Influence in Productivity in Software Development Teams; an Automated Pipeline for the Generation of Quality Reports; a Barbell Strategy-oriented Regulatory Framework and Compliance Management; An Interpretation and Implementation of Automotive Hardware SPICE; Application of Mutually Integrated International Standards (A-SPICE PAM 3.1 & IATF 16949/2016); metrics and Dashboard for Level 2 – Experience; forecasting Completion Deadlines in Software Testing; experience with the Performance of Online Distributed Assessments – Using Advanced Infrastructure; towards a Model Based Process Assessment for Data Analytics: An Exploratory Case Study; employability Assessment of Agile Methods for Software Quality: An Empirical Case Study; building an Environment for Agility: A Releasable Inventory Tool; A Systematical Approach for “System Item Integration and Testing” in Context of ISO 26262; a Framework for Automated Testing. 27th European Conference on Systems, Software and Services Process Improvement, EuroSPI 2020  ",Strategic alignment
302,Sizing a renewable microgrid for flow shop manufacturing using climate analytics,"A variety of methods have been proposed to assist the integration of microgrid in flow shop systems with the goal of attaining eco-friendly operations. There is still a lack of integrated planning models in which renewable portfolio, microgrid capacity and production plan are jointly optimized under power demand and generation uncertainty. This paper aims to develop a two-stage, mixed-integer programming model to minimize the levelized cost of energy of a flow shop powered by onsite renewables. The first stage minimizes the annual energy use subject to a job throughput requirement. The second stage aims at sizing wind turbine, solar panels and battery units to meet the hourly electricity needs during a year. Climate analytics are employed to characterize the stochastic wind and solar capacity factor on an hourly basis. The model is tested in four locations with a wide range of climate conditions. Three managerial insights are derived from the numerical experiments. First, time-of-use tariff significantly stimulates the wind penetration in locations with medium or low wind speed. Second, regardless of the climate conditions, large-scale battery storage units are preferred under time-of-use rate but it is not the case under a net metering policy. Third, wind- and solar-based microgrid is scalable and capable of meeting short-term demand variation and long-term load growth with a stable energy cost rate. © 2019 Elsevier Ltd Sizing a renewable microgrid for flow shop manufacturing using climate analytics Climate analytics; Flow shop scheduling; Levelized cost of energy; Microgrid sizing; Renewable portfolio; Time-of-use Electric batteries; Integer programming; Machine shop practice; Production control; Stochastic systems; Wind; Climate analytics; Flow-shop scheduling; Levelized costs; Micro grid; Renewable portfolio; Time of use; Climate models",Monitoring and control
303,Medical device portfolio cleanup,"This book mainly focuses on product portfolio management of medical devices. Portfolio management decisions have a significant impact and influence the performance at each stage in the product lifecycle. Consistent criteria should be used to select and prioritize balanced portfolios, and also enable better resource distribution. Medical device manufacturers are responsible for ensuring that their medical products are safe. However, safe does not mean zero risk. A safe product is one that has reasonable risks, given the magnitude of the benefit expected. ISO14971 standard specifies a risk management process by which a manufacturer can identify the hazards associated with their medical device, estimate and evaluate the risks, control these risks, and monitor effectiveness of the controls, throughout the product lifecycle. The main elements of the risk management process, that is, risk analysis, risk evaluation, and risk control are generally documented in a risk management file. The new trends to optimize costs in medical device supply chain operations include virtual centralization of supply chains such as multitenant warehousing, proper centralized inventory practices, use of analytics, streamlining workflow, etc. The application of these techniques can provide affordable healthcare solutions in developing countries. © 2020 Elsevier Inc. All rights reserved. Medical device portfolio cleanup Decommissioning and disposal of devices; Inventory management; ISO 14971; Medical device management; Portfolio management and clean up; Postmarket surveillance ",Risk management
304,Ontology-based knowledge representation for industrial megaprojects analytics using linked data and the semantic web,"The fourth industrial revolution has affected most industries, including construction and those within the delivery chain of megaprojects. These major paradigm shifts, however, did not considerably improve the track record in predicting project outcomes and estimating required resources. One reason is the lack of unified data definitions and expandable knowledge representation across project lifecycle to represent megaprojects for analytics. This paper proposes and evaluates a unified ontology for project knowledge representation that facilitates data collection, processing, and utilization for industrial megaprojects through their lifecycle. The proposed Uniform Project Ontology, or UPonto, provides a data infrastructure for project analytics by enabling logical deductions and inferences, and flexible expansion and partitioning of the data utilizing linked data and the semantic web. The ontology facilitates cost normalization processes, temporal queries, and graph queries using SPARQL, while defining universal semantics for a wide range of project risk factors and characteristics based on comprehensive research of the empirical project risk and success literature augmented by practical considerations gained through expert consultations. UPonto forms the basis for a project knowledge graph to utilize unstructured data; it as well provides semantic definitions for smart IoT agents to consume project risk data and knowledge. © 2020 Elsevier Ltd Ontology-based knowledge representation for industrial megaprojects analytics using linked data and the semantic web Industrial megaprojects; Knowledge representation; Ontology; Project analytics; Risk analysis; Semantic web Knowledge representation; Life cycle; Linked data; Ontology; Project management; Semantic Web; Comprehensive research; Data infrastructure; Expert consultation; Industrial revolutions; Normalization process; Project lifecycle; Temporal queries; Unstructured data; Data handling",Risk management
305,Boosting Insights in Insurance Tariff Plans with Tree-Based Machine Learning Methods,"Pricing actuaries typically operate within the framework of generalized linear models (GLMs). With the upswing of data analytics, our study puts focus on machine learning methods to develop full tariff plans built from both the frequency and severity of claims. We adapt the loss functions used in the algorithms such that the specific characteristics of insurance data are carefully incorporated: highly unbalanced count data with excess zeros and varying exposure on the frequency side combined with scarce but potentially long-tailed data on the severity side. A key requirement is the need for transparent and interpretable pricing models that are easily explainable to all stakeholders. We therefore focus on machine learning with decision trees: Starting from simple regression trees, we work toward more advanced ensembles such as random forests and boosted trees. We show how to choose the optimal tuning parameters for these models in an elaborate cross-validation scheme. In addition, we present visualization tools to obtain insights from the resulting models, and the economic value of these new modeling approaches is evaluated. Boosted trees outperform the classical GLMs, allowing the insurer to form profitable portfolios and to guard against potential adverse risk selection. © 2020, © 2020 Society of Actuaries. Boosting Insights in Insurance Tariff Plans with Tree-Based Machine Learning Methods  ",Strategic alignment
306,Stock Market Prediction and Portfolio Optimization Using Data Analytics,"Analyzing the past records and precisely predicting the future trends is an important facet in a financially volatile market like the stock market! This paper employs Auto-ARIMA and Holt-Winters models for predicting the future value of stocks and Linear programming for portfolio optimization. Continuously training the model using the latest market data, helps the model get trained on the latest market behavior, so that it can be deployed further for more no. of portfolios in future; The trend obtained can be analyzed for the same to predict better investment plans in the stock market. Thus, helping people optimally invest in the stocks, depending upon the present market analysis, would help them fetch maximum return and suffer comparatively lesser loss. © 2020, Springer Nature Singapore Pte Ltd. Stock Market Prediction and Portfolio Optimization Using Data Analytics Auto-ARIMA; Holt-Winters; Linear programming; Market analysis; Portfolio optimization; Stock market Artificial intelligence; Commerce; Data Analytics; Data mining; Financial data processing; Financial markets; Forecasting; Linear programming; Auto-ARIMA; Holt-Winters; Investment plan; Market analysis; Market behavior; Portfolio optimization; Stock market prediction; Volatile markets; Investments",Strategic alignment
307,Data Analysis of Readiness Programs of Machine-Building Enterprises,"One of the important aspects of providing the high level of the enterprises IT-readiness at machine-building enterprises is using the data science approach. By using data analysis, we mean the ability of the enterprises’ experts to increase using data by the most effective appliance of modern data science algorithms. In our research, the analysis has been carried out and the proposed approach can be used in real practice to evaluate the implementation of program projects to boost IT readiness. For example, at the initial stage, when the project is just starting and we do not know the real values of features, we can assign to modifications the average value for each, and then when the project arrives the real value of modification, we can calculate the target and track the dynamics of the assessment, the quality of the program projects to boost IT-readiness of machine-building enterprises. Thus, an important question is the compliance of the enterprise to the necessary level of IT-readiness which is directly connected with data analysis. © The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG 2020. Data Analysis of Readiness Programs of Machine-Building Enterprises Data science; IT-readiness; Machine learning; Program management; Project management Data handling; Data Science; Information systems; Information use; Project management; Average values; Enterprise IS; Enterprise IT; IT-readiness; Machine building enterprise; Machine-learning; Program management; Real values; Machine learning",Risk management
308,A predictive modeling approach to estimating seismic retrofit costs,"This article presents a methodology for estimating seismic retrofit costs from historical data. In particular, historical retrofit-cost data from Federal Emergency Management Agency (FEMA) 156 is used to build a generalized linear model (GLM) to predict retrofit costs as a function of building characteristics. While not as accurate as an engineering professional’s estimate, this methodology is easy to apply to generate quick estimates and is especially useful for decision makers with large building portfolios. Moreover, the predictive modeling approach provides a measure of uncertainty in terms of prediction error. The article uses prediction error to compare different modeling choices, including the choice of distribution for costs. Finally, the proposed retrofit-cost model is implemented to estimate the cost to retrofit a portfolio of federal buildings. The application illustrates how the choice of distribution affects cost estimates. © The Author(s) 2020. A predictive modeling approach to estimating seismic retrofit costs cost estimation; earthquake risk reduction; prediction; resilience; seismic retrofit Cost estimating; Decision making; Forecasting; Predictive analytics; Retrofitting; Risk management; Seismology; Uncertainty analysis; Building characteristics; Engineering professionals; Federal Emergency Management Agency; Generalized linear model; Measure of uncertainty; Prediction errors; Predictive modeling; Seismic retrofits; building; earthquake prediction; modeling; seismic retrofit; Cost benefit analysis",Monitoring and control
309,Artificial intelligence for ETF market prediction and portfolio optimization,"In asset allocation and time-series forecasting studies, few have she would light on using the different machine learning and deep learning models to verify the difference in the result of investment returns and optimal asset allocation. To fill this research gap, we develop a robo-advisor with different machine learning and deep learning forecasting methodologies and utilize the forecasting result of the portfolio optimization model to support our investors in making decisions. This research integrated several dimensions of technologies, which contain machine learning, data analytics, and portfolio optimization. We focused on developing robo-advisor framework and utilized algorithms by integrating machine learning and deep learning approaches with the portfolio optimization algorithm by using our predicted trends and results to replace the historical data and investor views. We eliminate the extreme fluctuation to maintain our trading within the acceptable risk coefficient. Accordingly, we can minimize the investment risk and reach a relatively stable return. We compared different algorithms and found that the F1 score of the model prediction significantly affects the result of the optimized portfolio. We used our deep learning model with the highest winning rate and leveraged the prediction result with the portfolio optimization algorithm to reach 12% of annual return, which outperform our benchmark index 0050.TW and the optimized portfolio with the integration of historical data. © 2019 Association for Computing Machinery. Artificial intelligence for ETF market prediction and portfolio optimization Artificial intelligent (AI); Deep learning; Exchange traded funds (ETF); Financial market prediction; Machine learning; Portfolio optimization Commerce; Data Analytics; Data integration; Deep learning; E-learning; Electronic trading; Forecasting; Investments; Learning algorithms; Learning systems; Optimization; Artificial intelligent; Exchange-traded funds; Integrating machines; Investment returns; Market prediction; Portfolio optimization; Portfolio optimization models; Time series forecasting; Machine learning",Strategic alignment
310,A Categorization of Cloud-Based Services and their Security Analysis in the Healthcare Sector,"The contribution of cloud-based services in the healthcare environments is a vital issue in the 21st century. In this paper, presenting its benefits and tools in hospitals, clinics as well as diagnostic centers, the already existing applications and services are separated in categories, which basically concern: data storage, computing power, network, PaaS, SaaS, data analytics, business intelligence and project management. Then, some security and risk assessment issues in cloud-based services are analyzed thoroughly together with some case studies. Furthermore, a risk analysis with a comparative diagram between secured and non-secured cloud systems in health is listed. Finally, conclusions with the suggested future work are presented.  © 2020 IEEE. A Categorization of Cloud-Based Services and their Security Analysis in the Healthcare Sector cloud-based services; diagnostic centers; Hospitals; information systems; risk assessment; security Computer aided design; Computer networks; Data Analytics; Digital storage; Health care; Health risks; Information management; Project management; Risk analysis; Risk assessment; Service industry; Social networking (online); Case-studies; Cloud systems; Cloud-based; Computing power; Data storage; Healthcare environments; Healthcare sectors; Security analysis; Storage as a service (STaaS)",Risk management
311,The digital twin in a brownfield environment: How to manage dark data,"This paper addresses the challenges of creating a reliable and accurate as-operated digital twin for abrownfield process plant. Besides massive amounts of information spread over silos, there is so-called ""darkdata"", which is a type of unstructured/untagged data in different formats: 1D (datasheets, lists, records), 2D(drawings, logical connectivity), 3D (physical layout, sizes). This paper describes an approach/solution thatmakes it possible to validate, connect and make use of this data. The solution described in this paper was developed collaboratively between Siemens and BentleySystems. Using a combination of tools and experience from both companies, it is now possible to leveragetraditionally inaccessible data to create a single digital twin. The paper will not focus on the solution itself,but rather the process and steps that operators can employ to aggregate, contextualize, validate, and visualizetheir data so that it can be used to make quicker/more informed decisions. The solution is an open, cloud-based platform that federates data sources, provides functionalities in theform of microservices, and facilitates the management of the digital twin of the asset, throughout its entirelifecycle. The solution was built as a set of microservices including collaborative process engineering andfunctional asset information management (i.e., 1D information such as specifications and data sheets and2D schematics), and 3D design and physical asset modeling. It also contains information management fromasset and project management perspectives, including maintenance history, reliability data, and failure modeanalyses as well as analytics services and the capability to trace back versions. Many functionalities drawon software that has been used extensively across process industry for many years. The user is able to use pre-existing 3D plant models or projects authored in most available 3D designtools and mix and match those with models based on photogrammetry or 3D point clouds to create a ""realitymesh."" The portal accommodates the need for an up-to-date version of accurate asset data, which is criticalto making the correct maintenance-related and operational decisions quickly as well as the requirement forhistorical snapshots (essential to engineering purposes and project progress reviews). This approach is key in that it enables O&G operators to solve the challenge of creating a digital twinwithout disruption to the existing physical or virtual environment. Combining 1D, 2D, and 3D in a singleenvironment provides functional context to physical representations and vice versa. The more ""dark data""that can be made visible, tagged, validated, and linked to other information, the more valuable and context-rich information will become. © 2020, Offshore Technology Conference. The digital twin in a brownfield environment: How to manage dark data  Digital twin; Information management; Offshore oil well production; Project management; Cloud based platforms; Collaborative process; Informed decision; Operational decisions; Physical assets; Process industries; Project progress; Reliability data; Offshore technology",Governance
312,SKI: An Agile Framework for Data Science,"This paper explores data science project management by first noting the need for a new process management framework and then defines a process framework that effectively supports the needs of a data science team. The paper also reports on a pilot study of teams using the framework. The framework adheres to the lean Kanban philosophy but augments Kanban by providing a structured iteration process for teams to incrementally explore and learn via lean hypothesis testing. Specifically, the Structured Kanban Iteration (SKI) framework focuses on having teams define capability-based iterations (as opposed to Kanban-like no iterations or Scrumlike time-based sprints). Furthermore, unlike Kanban, the framework leverages Scrum best practices to define roles, meetings and artifacts. Thus, SKI implements the Kanban process, but with a more repeatable and structured approach. © 2019 IEEE. SKI: An Agile Framework for Data Science Agile; Big Data; Data Science; Process Methodology Big data; Data Science; Iterative methods; Project management; Agile; Best practices; Hypothesis testing; Iteration process; Pilot studies; Process framework; Process management; Structured approach; Human resource management",Value management
313,Software project failures prediction using logistic regression modeling,"The prediction of software project failure early can help in taking an enhancement steps that can steer the project outcome from failure to success. A range of risks may affect the software project during the development process and may lead to project failure. This paper presents a software project failure evaluation model developed based on real data collected from different software project reports, surveys and case studies. The constructed dataset describes the relationship between software project failure and independent failure factors. In this paper, the researchers have developed a failure prediction model using logistic regression method. This model can be used by project managers to assess the expected failures. The developed model helps in estimating the project outcome (Failed/Success). Furthermore, the model provides a probability of software project failure. The model is developed to enable the project decision makers to perform evaluation for the project status during any phase of the software development life cycle. © 2020 IEEE. Software project failures prediction using logistic regression modeling Failure factors; Failure prediction; Failure probability; Logistic regression Computer software; Decision making; Forecasting; Life cycle; Logistic regression; Project management; Software design; Development process; Evaluation modeling; Failure prediction models; Logistic regression method; Logistic Regression modeling; Project decision; Project managers; Software development life cycle; Predictive analytics",Risk management
314,Systems Thinking in Software Projects-an Artificial Neural Network Approach,"The authors discuss why the current conceptual base of project management research and practice continues to attract criticism since it does not adequately address the complexity that leads to software-project failure. To do so, the study explores systems thinking and artificial neural networks to she would light on complexity in software-project behavior using nonlinear functional relationships between critical success factors and project success to utilize their connectedness as an approach in order to create project-outcome prediction models. The artificial neural networks were used to create two project-outcome prediction models: one for a binary classification task to discriminate failed from successful projects using a multi-input-single-output configuration and one for a multi-task binary classification to discriminate success from failure in multiple project-success dimensions using a multi-input multi-output configuration. The results yielded high-performance values for a binary classification task, performed to predict overall project success, and slightly lower performance values for the multi-task binary classification, which was also performed to predict success in project-success dimensions. It was found that the nonlinear behavior of critical success factors may be used to create prediction models, by embedding equifinality and connectedness constructs that prove to be useful to understand projects as complex, multi-loop, and nonlinear systems. Further research is needed to investigate the causality between critical success factors in order to explore the possible propagation of critical success factors within a project system network and its implications on project success.  © 2013 IEEE. Systems Thinking in Software Projects-an Artificial Neural Network Approach Artificial neural networks; critical success factors; prediction models; project success; systems thinking Backpropagation; Complex networks; Forecasting; MIMO systems; Predictive analytics; Project management; System theory; Artificial neural network approach; Binary classification; Critical success factor; Functional relationship; Multi input single outputs; Multi-input multi-output configurations; Nonlinear behavior; Project management research; Neural networks",Capacity management
315,Revisiting the Roles of Structural Engineering Design Professionals under Their Increasing Risks and Associated Liabilities,"Structural engineering is an age-long profession which historically has played a central role in shaping our landscapes and societies with countless iconic structures. However, throughout the years, the role of structural engineers has significantly changed. From the ""master builder"", responsible for all project aspects, their role nowadays is often confined to ensuring structural integrity. They are considered as supporting members led by others and are seldom involved in project decision-making. The paper aims at addressing the marginalized role of structural engineers which currently is not aligned with the vital importance of their profession. The methodology adopted consisted of a literature review to explore two main aspects. First, the role and competencies of structural engineers were investigated with respect to established and emerging fields such as structural and construction safety, integrated design, new project delivery methods, sustainable design, BIM, and other technological developments, including artificial intelligence and data analytics. Second, the new increased responsibilities, challenges, and liabilities that structural engineers are subjected to were identified. The findings revealed that in most fields, structural engineers play secondary roles especially as compared to architects. Unrealistic requirements and deadlines are frequently imposed on them and they operate in a legal context that does not properly shield them from professional liabilities. This is often hindering their performance as they become more conservative and averse to risk to the detriment of innovation and value engineering. Structural engineers should be proactive in redefining a more prominent and wider role for themselves. They should also regain control of decisions that directly affect their work. This will help their profession reach new heights while mitigating their risks and liabilities. © 2020 American Society of Civil Engineers. Revisiting the Roles of Structural Engineering Design Professionals under Their Increasing Risks and Associated Liabilities  Advanced Analytics; Architectural design; Artificial intelligence; Data Analytics; Decision making; Engineers; Professional aspects; Structural design; Sustainable development; Value engineering; Construction safety; Engineering design; Integrated designs; Literature reviews; Professional liability; Project decision; Structural engineer; Technological development; Project management",Value management
316,Application of Advanced Data Analytics in the Audit Process,"The audit profession has not changed much through history. Impact of technology and advanced analytics applied in audits today can be best shown through banking audits and loan review process. Several authors designed tools to leverage the power of advanced analytics, to identify and analyze risks in loan portfolios, to inspect data integrity, prepare a tailored audit plan, and to communicate the results of audits to clients with more impact and insight. After deployment of such tools, audit of the loan review process is going through change, especially in time resources that are saved. Instead of testing a limited number of randomly selected samples, auditors can toady analyze in detail the entire population of transactions. While it is clear that lower level accounting and auditing skills can easily be replaced by technology, human ability to understand, interpret and react in a business situation cannot be replaced by technology. © 2020, Springer Nature Switzerland AG. Application of Advanced Data Analytics in the Audit Process Advanced visualization; Artificial intelligence; Audit; Data analytics; Process automatization; Smart audit Artificial intelligence; Data Analytics; Data visualization; Intelligent computing; Advanced visualizations; Audit; Business situations; Data integrity; Human abilities; Loan portfolio; Review process; Smart audit; Advanced Analytics",Financial management
317,The secure act and your retirement objectives,This article summarizes how prescriptive analytics techniques are used in practice by retirees to maximize retirement portfolio longevity. The authors then contribute to this applied research by assessing how the SECURE Act affects the value of a retiree’s bequest. © 2020 Pepperdine University. The secure act and your retirement objectives  ,Strategic alignment
318,Request for information frequency and their turnaround time in construction projects: A data-analytic study,"Purpose: The purpose of this paper is to study the nature of request for information (RFIs) on construction projects by using data analytics to understand the frequency of RFIs, when they occur on projects, and the relationship between project characteristics and frequency of RFIs and between project characteristics and RFI turnaround time. Design/methodology/approach: A data-analytic approach using RStudio and Minitab software on 168 construction project cases in Australia and New Zealand involving 1,032,949 correspondences and 53,042 RFI event records made available by Aconex, one of the world largest cloud-based project management platform. Findings: Large and complex projects tend to have significantly larger number of RFI events per day and longer RFI turnaround when compared with smaller and less complex projects. Projects with fewer users per organisation recorded a higher RFI turnaround time when compared with projects with more users per organisation – users mean persons involved in managing the project using the online platform (an index of project complexity). RFIs occur early on less complex projects and occur later on more complex projects. Research limitations/implications: Benchmarks of RFI incidences and turnaround time have been developed for various project characteristics and, practitioners can use them to monitor the RFI performance of projects. Organisations need to pay greater attention to staffing levels needed to handle RFIs to reduce RFI turnaround time. Originality/value: A data-analytic study of RFI yielded insights for managing RFIs. The findings of previous studies on RFIs are difficult to generalise because they are based on single project case study. The influence of project characteristics on RFI frequency and RFI turnaround time is not yet known. © 2019, Emerald Publishing Limited. Request for information frequency and their turnaround time in construction projects: A data-analytic study Communications; Data analysis; Data analytics; Documentation; Project control; Project management; Quantitative techniques; Request for information ",Strategic alignment
319,Ten caveats of learning analytics in health professions education: A consumer’s perspective,"A group of 22 medical educators from different European countries, gathered in a meeting in Utrecht in July 2019, discussed the topic of learning analytics (LA) in an open conversation and addressed its definition, its purposes and potential risks for learners and teachers. LA was seen as a significant advance with important potential to improve education, but the group felt that potential drawbacks of using LA may yet be under-exposed in the literature. After transcription and interpretation of the discussion’s conclusions, a document was drafted and fed back to the group in two rounds to arrive at a series of 10 caveats educators should be aware of when developing and using LA, including too much standardized learning, with undue consequences of over-efficiency and pressure on learners and teachers, and a decrease of the variety of ‘valid’ learning resources. Learning analytics may misalign with eventual clinical performance and can run the risk of privacy breaches and inescapability of documented failures. These consequences may not happen, but the authors, on behalf of the full group of educators, felt it worth to signal these caveats from a consumers’ perspective. © 2020, © 2020 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group. Ten caveats of learning analytics in health professions education: A consumer’s perspective computer-based; information handling; Portfolio; trends Clinical Competence; Communication; Europe; Health Occupations; Humans; Learning; article; consumer; conversation; education; genetic transcription; human; human experiment; learning; occupation; privacy; teacher; clinical competence; Europe; interpersonal communication; medical profession",Strategic alignment
320,IT-Application behaviour analysis: Predicting critical system states on openstack using monitoring performance data and log files,"Recent studies have proposed several ways to optimize the stability of IT-services with an extensive portfolio of processual, reactive or proactive approaches. The goal of this paper is to combine monitored performance data, such as CPU utilization, with discrete data from log files in a joint model to predict critical system states. We propose a systematic method to derive mathematical prediction models, which we experimentally test using a downsized clone of a real life contract management system as a testbed. First, this testbed is used for data acquisition under variable and fully controllable system loads. Next, based on the monitored performance metrics and log file data, we train models (logistic regression and decision trees) that unify both, numeric and textual, data types in a single incident forecasting model. We focus on 1) investigating different cases to identify an appropriate prediction time window, allowing to prepare countermeasures by considering prediction accuracy and 2) identifying variables that appear more likely than others in the predictive models. Copyright © 2020 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved. IT-Application behaviour analysis: Predicting critical system states on openstack using monitoring performance data and log files Data science; Failure prediction; IT-operations; Log file analysis Data acquisition; Decision trees; Forecasting; Logistic regression; Testbeds; Trees (mathematics); Contract management; Controllable systems; Forecasting modeling; Mathematical prediction models; Monitoring performance; Optimize the stabilities; Performance metrics; Prediction accuracy; Predictive analytics",Monitoring and control
321,Prediction-Based Portfolio Optimization Models Using Deep Neural Networks,"Portfolio optimization is a hot research topic, which has attracted many researchers in recent decades. Better portfolio optimization model can help investors earn more stable profits. This paper uses three deep neural networks (DNNs), i.e., deep multilayer perceptron (DMLP), long short memory (LSTM) neural network and convolutional neural network (CNN) to build prediction-based portfolio optimization models which own the advantages of both deep learning technology and modern portfolio theory. These models first use DNNs to predict each stock's future return. Then, predictive errors of DNNs are applied to measure the risk of each stock. Next, the portfolio optimization models are built by integrating the predictive returns and semi-absolute deviation of predictive errors. These models are compared with three equal weighted portfolios, where their stocks are selected by DMLP, LSTM neural network and CNN respectively. Also, two prediction-based portfolio models built with support vector regression are used as benchmarks. This paper applies component stocks of China securities 100 index in Chinese stock market as experimental data. Experimental results present that the prediction-based portfolio model based on DMLP performs the best among these models under different desired portfolio returns, and high desired portfolio return can further improve the performance of this model. This paper presents the promising performance of DNNs in building prediction-based portfolio models. © 2013 IEEE. Prediction-Based Portfolio Optimization Models Using Deep Neural Networks Deep neural network; prediction-based portfolio; semi-absolute deviation Convolutional neural networks; Deep learning; Deep neural networks; Electronic trading; Financial markets; Fintech; Forecasting; Investments; Multilayer neural networks; Optimization; Predictive analytics; Risk assessment; Support vector regression; Chinese stock market; Hot research topics; Learning technology; Modern portfolio theories; Portfolio optimization; Portfolio optimization models; Prediction-based; Semi-absolute deviation; Long short-term memory",Strategic alignment
322,Smart City Digital Twin-Enabled Energy Management: Toward Real-Time Urban Building Energy Benchmarking,"To meet energy-reduction goals, cities are challenged with assessing building energy performance and prioritizing efficiency upgrades across existing buildings. Although current top-down building energy benchmarking approaches are useful for identifying overall efficient and poor performers across a portfolio of buildings at a city scale, they are limited in their ability to provide actionable insights regarding efficiency opportunities. Concurrently, advances in smart metering data analytics combined with new data streams available via smart metering infrastructure present the opportunity to incorporate previously undetectable temporal fluctuations into top-down building benchmarking analyses. This paper leveraged smart meter electricity data to develop daily building energy benchmarks segmented by strategic periods to quantify their variation from conventional, annual energy benchmarking strategies and investigate how such metrics can lead to near real-time energy management. The periods considered include occupied periods during the school year, unoccupied periods during the school year, occupied periods during the summer, unoccupied periods during the summer, and peak summer demand periods. Results showed that temporally segmented building energy benchmarks are distinct from a building's overall benchmark. This demonstrates that a building's overall benchmark masks periods in which a building is over- or underperforming during the day, week, or month; thus, temporally segmented energy benchmarks can provide a more specific and accurate measure for building efficiency. We discussed how these findings establish the foundation for digital twin-enabled urban energy management platforms by enabling identification of building retrofit strategies and near-real-time efficiency in the context of the performance of an entire building portfolio. Temporally segmented energy benchmarking measures generated from smart meter data streams are a critical step for integrating smart meter analytics with building energy benchmarking techniques, and for conducting smarter energy management across a large geographic scale of buildings. © 2019 American Society of Civil Engineers. Smart City Digital Twin-Enabled Energy Management: Toward Real-Time Urban Building Energy Benchmarking Building energy benchmarking; Commercial buildings; Community energy-efficiency; Digital twin; Energy management; Smart city Benchmarking; Data Analytics; Electric measuring instruments; Electric power measurement; Energy management; Office buildings; Smart city; Smart meters; Building efficiency; Building energy; Building energy performance; Commercial building; Digital twin; Energy benchmarking; Management platforms; Temporal fluctuation; Energy efficiency",Capacity management
323,Current Status and Future Opportunities for Big Data Research in the Construction Industry,"Big data technology is expected that these technologies will bring new opportunities and values to various industries. However, the construction industry has not put in much effort in using big data. This paper investigates past and current studies on big data analytics to identify the future technology for the construction industry. The research trend within the industry is examined for prediction and proposal of future opportunities. The study is conducted in stages as described below. First, this study investigates the implementation cases. After that, we determine the past and current research trends. Finally, this study analysis current trends and future directions of big data implementation.There is room for research in construction economics and market, business development, and portfolio management. Research activities in non-construction areas such as biology, management, and social science are the same level as the construction industry. As the proposed model for construction projects was generated based valid research activities, a big data analytics model can be used for implementing big data projects. Thismodel is to divide construction data into project-specific (common) data and industry-specific (spatial) data. The model categorizes a variety of data contents based on characteristics of the data, either project specific data or industry-specific data. The data are then grouped by data analysis methods such as statistics, data mining, or machine learning methods. Finally, the data are grouped by the nature of data, either common information or regional information. The benefit of this approach is to assure the success of big data implementation projects by reconciling project information and industry information together since the success of a big data implementation relies on the integrationof both data.Using this study, realistic resource planning and allocation for future can be projected in advance. Moreover, dynamic property of this model can provide to construction companies with real-time industry and project data. © BEIESP. Current Status and Future Opportunities for Big Data Research in the Construction Industry Big data; Big data analytics model; Big data trend; construction industry; Implementation cases ",Strategic alignment
324,Deriving wisdom from virtual investing communities: an alternative strategy to stock recommendations,"Purpose: Colossal information is available in cyberspace from a variety of sources such as blogs, reviews, posts and feedback. The mentioned sources have helped in improving various business processes from product development to stock market development. This paper aims to transform this wealth of information in the online medium to economic wealth. Earlier approaches to investment decision-making are dominated by the analyst's recommendations. However, their credibility has been questioned for herding behavior, conflict of interest and favoring underwriter's firms. This study assumes that members of the online crowd who have been reliable, profitable and knowledgeable in the recent past will continue to be so soon. Design/methodology/approach: The authors identify credible members as experts using multi-criteria decision-making tools. In this work, an alternative actionable investment strategy is proposed and demonstrated through a mock-up. The experimental prototype is divided into two phases: expert selection and investment. Findings: The created portfolio is comparable and even profitable than several major global stock indices. Practical implications: This work aims to benefit individual investors, investment managers and market onlookers. Originality/value: This paper takes into account factors: the accuracy and trustworthiness of the sources of stock market recommendations. Earlier work in the area has focused solely intelligence of the analyst for the stock recommendation. To the best of the authors’ knowledge, this is the first time that the combined intelligence of the virtual investment communities has been considered to make stock market recommendations. © 2020, Emerald Publishing Limited. Deriving wisdom from virtual investing communities: an alternative strategy to stock recommendations Business analytics; Investment management; Multi-criteria decision-making; Multicriteria programming; Operations research; Stock recommendation; User-generated content; Virtual investing communities ",Strategic alignment
325,Mobilizing intuitive judgement during organizational decision making: When business intelligence is not the only thing that matters,"Academics have argued that data-driven decision processes will replace intuitive judgements, but the empirical aspects of this claim are understudied. We provide empirical findings of how managers communicate and share intuitive judgements when BI system's output is prescribed to be the main information source for decision making. We investigate organizational decision making regarding IT project portfolio investments. We used a rich empirical dataset from a longitudinal, qualitative study investigating the prioritization of IT projects in a large financial institution. Our findings show that decision makers employ four techniques to communicate and share intuitive judgements during organizational decision making, which built on the BI output. Furthermore, we found that the use of these techniques depends on the decision maker's familiarity with the group and the convergence of perceptions about either a project, or specific issues in the group. © 2019 Elsevier B.V. Mobilizing intuitive judgement during organizational decision making: When business intelligence is not the only thing that matters BI system; Intuitive judgement; Organizational decision making; Project prioritization process; Strategic decisions Investments; Large dataset; Data driven decision; Empirical findings; Financial institution; Information sources; Intuitive judgement; Organizational decision making; Project prioritization; Strategic decisions; Decision making",Strategic alignment
326,"The role of big data, data science and data analytics in financial engineering","Financial engineering is the process of creating innovative solutions for the existing financial problems of a company by using applications of mathematical methods. Financial engineering uses tools and knowledge from the fields of computer science, big data, data science, data analytics, statistics, economics and applied mathematics to address current financial issues as well as to devise new and innovative financial products. Financial Engineering is helpful in derivative pricing, financial regulation, execution, corporate finance, portfolio management, risk management, trading of structured products. Therefore, financial engineering is used by Commercial Banks, Investment Banks, Insurance companies and other fund hedging agencies. The present study focus on the role of big data, data science and data analytics in financial engineering as a successful tool at all stages of insurance business management practices. How these insurance companies are using said three data tools effectively as fasteners of financial engineering for the successful design, development and implementation of innovative business processes and products in this competitive and ever-changing insurance market with innovative product features and strategies. © 2019 Association for Computing Machinery. The role of big data, data science and data analytics in financial engineering Big Data; Data Analytics; Data Science; Financial Engineering; Insurance Big data; Commerce; Data Analytics; Data Science; Financial data processing; Financial markets; Insurance; Investments; Product design; Risk management; Applied mathematics; Business management; Financial engineering; Financial regulations; Innovative solutions; Insurance companies; Mathematical method; Portfolio managements; Advanced Analytics",Value management
327,"19th Mexican International Conference on Artificial Intelligence, MICAI 2020","The proceedings contain 77 papers. The special focus in this conference is on Artificial Intelligence. The topics include: Impact of Memory Control on Batch Learning in Human Activity Recognition Scenario in Comparison to Data Stream Learning; automated Characterization and Prediction of Wind Conditions Using Gaussian Mixtures; a Survey on Freezing of Gait Detection and Prediction in Parkinson’s Disease; zeChipC: Time Series Interpolation Method Based on Lebesgue Sampling; machine Leaning Based Urdu Language Tutor for Primary School Students; selection Schemes Analysis in Genetic Algorithms for the Maximum Influence Problem; a Comparative Analysis of Evolutionary Learning in Artificial Hydrocarbon Networks; fatty Chain Acids Risk Factors in Sudden Infant Death Syndrome: A Genetic Algorithm Approach; An NSGA-III-Based Multi-objective Intelligent Autoscaler for Executing Engineering Applications in Cloud Infrastructures; speaker Identification Using Entropygrams and Convolutional Neural Networks; the Improvement Direction Mapping Method; a Genetic Programming Framework for Heuristic Generation for the Job-Shop Scheduling Problem; a Genetic Algorithm Approach for a Truck and Trailer Routing Problem in a Loading/Unloading Bays Application; a Tensor-Based Markov Decision Process Representation; object-Based Goal Recognition Using Real-World Data; comparing Multi-issue Multi-lateral Negotiation Approaches for Group Recommendation; guidance in the Visual Analytics of Cartographic Images in the Decision-Making Process; risk Sensitive Markov Decision Process for Portfolio Management; risk-Sensitive Piecewise-Linear Policy Iteration for Stochastic Shortest Path Markov Decision Processes; why Majority Rule Does Not Work in Quantum Computing: A Pedagogical Explanation; convolutional Neural Networks with Hebbian-Based Rules in Online Transfer Learning; how Powersets of Individual Fuzzy Sets Can Be Defined?. 19th Mexican International Conference on Artificial Intelligence, MICAI 2020  ",Risk management
328,Fundamentals of institutional asset management,"This book provides the fundamentals of asset management. It takes a practical perspective in describing asset management. Besides the theoretical aspects of investment management, it provides in-depth insights into the actual implementation issues associated with investment strategies. The 19 chapters combine theory and practice based on the experience of the authors in the asset management industry. The book starts off with describing the key activities involved in asset management and the various forms of risk in managing a portfolio. There is then coverage of the different asset classes (common stock, bonds, and alternative assets), collective investment vehicles, financial derivatives, common stock analysis and valuation, bond analytics, equity beta strategies (including smart beta), equity alpha strategies (including quantitative/systematic strategies), bond indexing and active bond portfolio strategies, and multi-asset strategies. The methods of using financial derivatives (equity derivatives, interest rate derivatives, and credit derivatives) in managing the risks of a portfolio are clearly explained and illustrated. © 2021 by World Scientific Publishing Co. Pte. Ltd. Fundamentals of institutional asset management  ",Risk management
329,Digital innovation to drive intelligent utility enterprise,"SAP is well known for delivering an integrated Enterprise Suite for many years. An Intelligent Utility Enterprise requires more than just an Intelligent Integrated Business Suite but also seamless integration into all types of real time operations of a Utility. SAP has developed an Intelligent Enterprise Framework, which supports all type of innovation scenarios. In this document we demonstrate the capability of SAP's Intelligent Enterprise Framework and show through real live examples and innovation projects how SAP enables innovation for the Utilities industry. © 2019 IEEE. Digital innovation to drive intelligent utility enterprise AI; Big Data; Blockchain; Digital Platform; Flexiciency; FutureFlow; InteGrid; Intelligent Enterprise; IT/OT Integration; ML; Portfolio Management; Predictive Analytics Artificial intelligence; Big data; Blockchain; Digital storage; Financial data processing; Investments; Predictive analytics; Digital platforms; Flexiciency; FutureFlow; InteGrid; Intelligent enterprise; Portfolio managements; Information management",Capacity management
330,Valuing and Investing in Equities: CROCI: Cash Return on Capital Investment,"Valuing and Investing in Equities: CROCI: Cash Return on Capital Investment develops a common-sense framework for value investors. By distinguishing investors from speculators, it acknowledges the variety of styles and goals in the financial markets. After explaining the intuition behind due diligence, portfolio construction, and stock picking, it shows the reader how to perform these steps and how to evaluate their results. Francesco Curto illuminates the costs and opportunities afforded by valuation strategies, inflation, and bubbles, emphasizing their effects on each other within the CROCI framework. Balancing analytics with an engaging clarity, the book neatly describes a comprehensive, time-tested approach to investing. Annual returns from this investment approach demand everyone is attention. © 2020 Elsevier Inc. Valuing and Investing in Equities: CROCI: Cash Return on Capital Investment  ",Strategic alignment
331,"21st International Conference on Product-Focused Software Process Improvement, PROFES 2020","The proceedings contain 28 papers. The special focus in this conference is on Product-Focused Software Process Improvement. The topics include: A Practice-Informed Conceptual Model for a Combined Approach of Agile, User-Centered Design, and Lean Startup; lean R&D: An Agile Research and Development Approach for Digital Transformation; a Portfolio-Driven Development Model and Its Management Method of Agile Product Line Engineering Applied to Automotive Software Development; how to Integrate Security Compliance Requirements with Agile Software Engineering at Scale?; impediment Management of Agile Software Development Teams; a Study of the Agile Coach’s Role; exploring the Microservice Development Process in Small and Medium-Sized Organizations; integration of Security Standards in DevOps Pipelines: An Industry Case Study; an Empirical Investigation into Industrial Use of Software Metrics Programs; software Startup Practices – Software Development in Startups Through the Lens of the Essence Theory of Software Engineering; improving a Software Modernisation Process by Differencing Migration Logs; redefining Legacy: A Technical Debt Perspective; a Systematic-Oriented Process for Tool Selection: The Case of Green and Technical Debt Tools in Architecture Reconstruction; data Labeling: An Empirical Investigation into Industrial Challenges and Mitigation Strategies; from a Data Science Driven Process to a Continuous Delivery Process for Machine Learning Systems; data Pipeline Management in Practice: Challenges and Opportunities; demystifying Data Science Projects: A Look on the People and Process of Data Science Today; on Clones and Comments in Production and Test Classes: An Empirical Study; preface; kuksa ∗: Self-adaptive Microservices in Automotive Systems; Dimensions of Consistency in GSD: Social Factors, Structures and Interactions. 21st International Conference on Product-Focused Software Process Improvement, PROFES 2020  ",Strategic alignment
332,"Erratum regarding missing Declaration of Competing Interest statements in previously published articles (Operations Research Perspectives (2018) 5 (256–264), (S2214716018301234), (10.1016/j.orp.2018.08.005))","Declaration of Competing Interest statements were not included in the published version of the following articles that appeared in previous issues of Operations Research Perspectives. The appropriate Declaration/Competing Interest statements, provided by the Authors, are included below: 1 ""An integrated network design and scheduling problem for network recovery and emergency response"" Operations Research Perspectives, 2018; Volume 5 (218–231) Declaration of Competing Interest: The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. https://doi.org/10.1016/j.orp.2018.08.001.2 ""Integrated approach for computing aggregation weights in cross-efficiency evaluation"" Operations Research Perspectives, 2018; Volume 5 (256–264) Declaration of Competing Interest: The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. https://doi.org/10.1016/j.orp.2018.08.005.3 ""Robust optimization: Sensitivity to uncertainty in scalar and vector cases, with applications"" Operations Research Perspectives, 2018; Volume 5 (113–119).4 ""Modeling the rational behavior of individuals on an e-commerce system"" Operations Research Perspectives, 2018; Volume 5 (22–31).5 ""On slowdown variance as a measure of fairness"" Operations Research Perspectives, 2018; Volume 5 (33–144) Declaration of Competing Interest: The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. https://doi.org/10.1016/j.orp.2018.05.001.6 ""Integrating pricing and capacity decisions in car rental: A matheuristic approach"" Operations Research Perspectives, 2018; Volume 5 (334–356).7 ""Business analytics in manufacturing: Current trends, challenges and pathway to market leadership Operations Research Perspectives, 2019; Volume 6 (100,127).8 ""Construction of currency portfolios by means of an optimized investment strategy"" Operations Research Perspectives, 2018; Volume 5 (32–44).9 ""Hybrid time-quality-cost trade-off problems"" Operations Research Perspectives, 2018; Volume 5 (306–318).10 ""Hybrid multimode resource-constrained maintenance project scheduling problem"" Operations Research Perspectives, 2019; Volume 6 (100,129).11 ""Decision rules for robotic mobile fulfillment systems"" Operations Research Perspectives, 2019; Volume 6 (100,128).12 ""Impact of TQM on organisational performance: The case of Indian manufacturing and service industry"" Operations Research Perspectives, 2018; Volume 5 (199–217).13 ""Analysis of a multi-component system with failure dependency, N-policy and vacations Operations Research Perspectives, 2018; Volume 5 (191–198). © 2020 Erratum regarding missing Declaration of Competing Interest statements in previously published articles (Operations Research Perspectives (2018) 5 (256–264), (S2214716018301234), (10.1016/j.orp.2018.08.005))  ",Value management
333,A Multi-Agent Reinforcement Learning Approach for Stock Portfolio Allocation,"Stock portfolio allocation is one of the most challenging and interesting problems of modern finance. Recently, deep reinforcement learning applications have shown promising results in automating portfolio allocation. However, most current approaches use a single agent learning model which could inadequately capture the complex dynamics arising from the interactions of many traders in today's stock market. In this paper, we explore the applicability of multi-agent deep reinforcement learning to this problem by implementing single-agent, 2-agent, 3-agent, and 4-agent deep deterministic policy gradients (DDPG) algorithms in a competitive setting. Upon analyzing the results obtained using standardized metrics, we observe that there is a significant improvement in the performance of our learning models with the introduction of multiple agents. © 2021 Owner/Author. A Multi-Agent Reinforcement Learning Approach for Stock Portfolio Allocation deep learning; multi-agent systems; portfolio allocation; reinforcement learning; stock trading Commerce; Data Science; Deep learning; Electronic trading; Learning systems; Multi agent systems; Multivariable control systems; Complex dynamics; Learning models; Multi-agent reinforcement learning; Multiple agents; Policy gradient; Single agent learning; Single-agent; Stock portfolio; Reinforcement learning",Strategic alignment
334,Application of Deep Q-Network in Portfolio Management,"Machine Learning algorithms and Neural Networks are widely applied to many different areas such as stock market prediction, facial recognition and automatic machine translation. This paper introduces a novel strategy based on the classic Deep Reinforcement Learning algorithm, Deep QNetwork, for stock market portfolio management. It is a type of deep neural network which is optimized by Q Learning. To adapt the Deep Q-Network for stock market production, we first discretize the action space so that portfolio management becomes a problem that Deep Q-Network can solve. Following this, we combine the Convolutional Neural Network and dueling Q-Net to enhance the recognition ability of the algorithm. We choose five low-relevant American stocks to test our model. It is found that the Deep Q-Network based strategy outperforms the ten other traditional strategies. The profit of Deep Q-Network algorithm is 30% more than the profit of other strategies. Moreover, the Sharpe ratio and Max Drawdown demonstrates that the risk of policy associated with Deep Q-Network is the lowest. © 2020 IEEE. Application of Deep Q-Network in Portfolio Management convolutional neural network; portfolio management; Q learning Advanced Analytics; Big data; Commerce; Convolutional neural networks; Deep neural networks; Electronic trading; Face recognition; Financial markets; Investments; Learning algorithms; Profitability; Reinforcement learning; Automatic machines; Facial recognition; Network algorithms; Network-based strategy; Novel strategies; Portfolio managements; Recognition abilities; Stock market prediction; Deep learning",Strategic alignment
335,Asset Management: Tools And Issues,"Long gone are the times when investors could make decisions based on intuition. Modern asset management draws on a wide-range of fields beyond financial theory: economics, financial accounting, econometrics/statistics, management science, operations research (optimization and Monte Carlo simulation), and more recently, data science (Big Data, machine learning, and artificial intelligence). The challenge in writing an institutional asset management book is that when tools from these different fields are applied in an investment strategy or an analytical framework for valuing securities, it is assumed that the reader is familiar with the fundamentals of these fields. Attempting to explain strategies and analytical concepts while also providing a primer on the tools from other fields is not the most effective way of describing the asset management process. Moreover, while an increasing number of investment models have been proposed in the asset management literature, there are challenges and issues in implementing these models. This book provides a description of the tools used in asset management as well as a more in-depth explanation of specialized topics and issues covered in the companion book, Fundamentals of Institutional Asset Management. The topics covered include the asset management business and its challenges, the basics of financial accounting, securitization technology, analytical tools (financial econometrics, Monte Carlo simulation, optimization models, and machine learning), alternative risk measures for asset allocation, securities finance, implementing quantitative research, quantitative equity strategies, transaction costs, multifactor models applied to equity and bond portfolio management, and backtesting methodologies. This pedagogic approach exposes the reader to the set of interdisciplinary tools that modern asset managers require in order to extract profits from data and processes. © 2021 by World Scientific Publishing Co. Pte. Ltd. Asset Management: Tools And Issues  ",Strategic alignment
336,EnergyStar++: Towards more accurate and explanatory building energy benchmarking,"Building energy performance benchmarking has been adopted widely in the USA and Canada through the Energy Star Portfolio Manager platform. Building operations and energy management professionals have long used this simple 1–100 score to understand how their building compares to its peers. This single number is easy to use but is created by potentially inaccurate multiple linear regression (MLR) models and lacks much further information about why a building achieves that score. This paper proposes a methodology that enhances the existing Energy Star calculation method by increasing accuracy and providing additional model output processing to help explain why a building is achieving a particular score. Two new prediction models were proposed and tested: multiple linear regression with feature interactions (MLRi) and gradient boosted trees (GBT). Both models performed better than a baseline Energy Star MLR model as well as four baseline models from previous benchmarking studies. This paper shows that for six building types, on average, the third-order MLRi models achieved a 4.9% increase in adjusted R2 and a 7.0% decrease in normalized root mean squared error (NRMSE) over the baseline MLR model. More substantially, the most accurate GBT models, on average, achieved a 24.9% increase in adjusted R2 and a 13.7% decrease in NMRSE against the baseline MLR model. In addition, a set of techniques was developed to help determine which factors most influence a building's energy use versus its peers using SHapley Additive exPlanation (SHAP) values. The SHAP force visualization, in particular, offered an accessible overview of the aspects of the building that influenced the score that even non-technical users can interpret. This methodology was tested on the 2012 Commercial Building Energy Consumption Survey (CBECS)(1,812 buildings) and public data sets from the energy disclosure programs of New York City (11,131 buildings) and Seattle (2,073 buildings). © 2020 Elsevier Ltd EnergyStar++: Towards more accurate and explanatory building energy benchmarking Building energy benchmarking; Building performance rating; Feature interaction; Gradient boosting trees; Interpretable machine learning; Multiple linear regression Benchmarking; Energy utilization; Linear regression; Mean square error; Office buildings; Stars; Building energy performance; Building operations; Commercial building; Feature interactions; Management professionals; Multiple linear regression models; Multiple linear regressions; Root mean squared errors; Predictive analytics",Monitoring and control
337,"Evaluation index system construction for geological environmental bearing capacity and its application in Henan Province, China","Geological environment is a resource base, environmental base, and engineering base for human socioeconomic activities. The main function of the geological environment is to provide stable and safe living space and essential resources for the existence and development of human society. However, geological environmental problems become increasingly prominent in some provinces in China due to fragility of the geological environment, uneven population distribution, backward economic development, and massive construction projects. The function of geological environments in some provinces to support sustainable economic and social development is evidently insufficient, and the threshold values of their geological environmental bearing capacity nearly reach the upper limit. For example, Henan Province, China, an evaluation index system for geological environmental bearing capacity, was established from three aspects, as follows: natural hazard, geological disaster, and disaster control. The weights of evaluation indexes were calculated using the variation coefficient method. Finally, the geological environmental bearing capacity of Henan Province during 2010-2018 was measured through the comprehensive evaluation method. Results showed that the constructed evaluation index system for geological environmental bearing capacity, which consisted of 34 indexes, was scientific and reasonable. During 2010-2018, the geological environmental bearing capacity of Henan Province was superior or excellent. Geological environmental bearing capacity can be improved by reinforcing the construction of legal rules and laws for geological environmental protection, carrying out research work regarding geological environmental bearing capacity, launching geological environmental monitoring and early warning system construction, and implementing feasibility demonstration for the geological environment of major construction projects. The study results can provide practical guidance and reference to realize the predictive analysis of geological environmental bearing capacity and coordinate human socioeconomic activities and geological environment. © 2020 Technoscience Publications. All rights reserved. Evaluation index system construction for geological environmental bearing capacity and its application in Henan Province, China Evaluation index system; Geological environmental bearing capacity; Henan Province; Socioeconomic activities China; Henan; Bearing capacity; Bearings (machine parts); Disasters; Economic and social effects; Economics; Engineering research; Environmental protection; Function evaluation; Geology; Predictive analytics; Comprehensive evaluation; Construction projects; Environmental Monitoring; Environmental problems; Evaluation index system; Geological environment; Geological environmental protections; Major construction project; bearing capacity; disaster management; early warning system; engineering geology; environmental monitoring; feasibility study; project management; research work; socioeconomic status; vulnerability; Environmental regulations",Capacity management
338,Evolution Paths for Knowledge Discovery and Data Mining Process Models,"Despite the hype around data analytics, the success rate of analytics initiatives remains very low and the value of data in organisations is left hidden. Various research studies show that the main barriers to analytics adoption are organisational and the lack of structured approaches on how to conduct analytics initiatives is a possible because of analytics project failures. Data mining process models then become fundamental means to support analytics project management and minimise the risk of data dredging. In this paper, Knowledge Discovery and Data Mining process models are reviewed starting from the most popular models currently in use. Four distinctive research paths for data mining process models have emerged. These evolution paths seem to address limitations of the CRISP–DM model which remains the de facto standard in industry. The research streams identified include the evolution of the human role; the relevance of iteration and interactions; the role of data and knowledge repositories; and the integration of software engineering/agile methodologies. In the future, these four research streams should be combined to support the development of more encompassing process models. © 2020, Springer Nature Singapore Pte Ltd. Evolution Paths for Knowledge Discovery and Data Mining Process Models CRISP–DM; Data analytics; Data mining; Knowledge discovery in databases; Process models ",Value management
339,"Energy transition, strategies and competencies: Outcomes from an ai decision support system","The project ZEST (Zero Emission - Skills for Transition) uses an AI to investigate the changing needs of the energy sector in terms of competences to face the challenges of the energy transition. The results indicate that the Oil and Gas sector is generally adapting to this challenge in the continuity of its core capabilities, in particular by realigning its portfolio. Data Science and Machine Learning are relevant to foster agility, confirming what is widely communicated in the media. Soft-skills which favour innovation, such as searching alternatives, changing paradigms, adaptability, anticipating situations, versatility, future orientation, are perceived as central to addressing the energy transition. We are preparing a second run of the AI to explore which additional players in the energy transition, beside the O&G and the mining industry, will most likely require the expertise of geoscientists. © Geoscience and Engineering in Energy Transition Conference, GET 2020.All right reserved. Energy transition, strategies and competencies: Outcomes from an ai decision support system  Data Science; Gas industry; Geology; Petroleum industry; Changing needs; Core capabilities; Energy sector; Energy transitions; Future orientations; Geoscientists; Oil and Gas Sector; Zero emission; Decision support systems",Strategic alignment
340,"Data analytics on graphs Part III: Machine learning on graphs, from graph topology to applications","Modern data analytics applications on graphs often operate on domains where graph topology is not known a priori, and hence its determination becomes part of the problem definition, rather than serving as prior knowledge which aids the problem solution. Part III of this monograph starts by a comprehensive account of ways to learn the pertinent graph topology, ranging from the simplest case where the physics of the problem already suggest a possible graph structure, through to general cases where the graph structure is to be learned from the data observed on a graph. A particular emphasis is placed on the use of standard “relationship measures” in this context, including the correlation and precision matrices, together with the ways to combine these with the available prior knowledge and structural conditions, such as the smoothness of the graph signals or sparsity of graph connections. Next, for learning sparse graphs (that is, graphs with a small number of edges), the utility of the least absolute shrinkage and selection operator, known as (LASSO) is addressed, along with its graph specific variant, the graphical LASSO. For completeness, both variants of LASSO are derived in an intuitive way, starting from basic principles. An in-depth elaboration of the graph topology learning paradigm is provided through examples on physically well defined graphs, such as electric circuits, linear heat transfer, social and computer networks, and spring-mass systems. We also review main trends in graph neural networks (GNN) and graph convolutional networks (GCN) from the perspective of graph signal filtering. Particular insight is given to the role of diffusion processes over graphs, to show that GCNs can be understood from the graph diffusion perspective. Given the largely heuristic nature of the existing GCNs, their treatment through graph diffusion processes may also serve as a basis for new designs of GCNs. Tensor representation of lattice-structured graphs is next considered, and it is shown that tensors (multidimensional data arrays) can be treated as a special class of graph signals, whereby the graph vertices reside on a high-dimensional regular lattice structure. Finally, the concept of graph tensor networks is shown to provide a unifying framework for learning of big data on irregular domains. This part of monograph concludes with an in-dept account of emerging applications in financial data processing and underground transportation network modeling. More specifically, by means of portfolio cuts of an asset graph, we show how domain knowledge can be meaningfully incorporated into investment analysis, while the underground transportation example addresses vulnerability of stations in the London underground network to traffic disruption. © 2017 Institute of Electrical and Electronics Engineers Inc.. All rights reserved. Data analytics on graphs Part III: Machine learning on graphs, from graph topology to applications Big data on graphs; Graph neural networks; Graph theory; Graph topology learning; Graphs; Machine learning on graphs; Random data on graphs; Signal processing on graphs; Systems on graphs; Tensors; Vertex-frequency estimation Convolutional neural networks; Data Analytics; Diffusion; Financial data processing; Graph theory; Graphic methods; Heat transfer; Investments; Machine learning; Network security; Regression analysis; Risk assessment; Signal processing; Tensors; Convolutional networks; Emerging applications; Graph neural networks; Least absolute shrinkage and selection operators; Multidimensional data; Structural condition; Tensor representation; Transportation network model; Graph structures",Strategic alignment
341,A journey towards providing intelligence and actionable insights to development teams in software delivery,"For delivering high-quality artifacts within the budget and on schedule, software delivery teams ideally should have a holistic and in-process view of the current health and future trajectory of the project. However, such insights need to be at the right level of granularity and need to be derived typically from a heterogeneous project environment, in a way that helps development team members with their tasks at hand. Due to client mandates, software delivery project environments employ many disparate tools and teams tend to be distributed, thus making the relevant information retrieval, insight generation, and developer intelligence augmentation process fairly complex. In this paper, we discuss our journey in this area spanning across facets like software project modelling and new development metrics, studying developer priorities, adoption of new metrics, and different approaches of developer intelligence augmentation. Finally, we present our exploration of new immersive technologies for human-centered software engineering. © 2019 IEEE. A journey towards providing intelligence and actionable insights to development teams in software delivery Actionable Insights; Project Management; Software Analytics; Software Delivery Budget control; Project management; Actionable Insights; Delivery projects; Development teams; High quality; Human-centered software engineerings; Immersive technologies; Project environment; Software project; Software engineering",Risk management
342,Risk-based decision support system for U.S. Air force water and wastewater: Infrastructure asset management,"Despite the large quantities of utility infrastructure data and various DOD processes, there was no management of that data in a central repository where it could be analyzed effectively for asset management. YOU.S. Pacific Air Forces (HQ PACAF/A7OP) engaged AECOM to develop and implement risk-based decision support for their wastewater infrastructure asset management. A consistent, repeatable approach for infrastructure requirements prioritization and investment across PACAF's portfolio of installations was developed and implemented for over nine installations worldwide. The risk-based decision support system utilizes a GIS platform with ArcGIS software, along with CartoPac (a data collection/management tool). Relational databases were developed for PACAF for each type of infrastructure, to include attributes, that will store the probability of failure and criticality data for each asset. These tables relate and join to the DOD's spatial data standards for facilities, infrastructure, and environment (SDSFIE) GIS structure. SDSFIE attributes are recorded to that geodatabase, while additional conditions and operational data are recorded in relational databases. Sub-meter GPS-enabled devices were used to map the water assets. Visual condition assessment surveys were conducted and supplemented with engineering analysis such as hydraulic modeling. Visual condition assessment surveys were conducted and supplemented with tools such as polecams, CCTV cameras, and flow monitors. A reporting service called business intelligence and reporting tools (BIRT) was developed to be used in a server enterprise environment. Report templates are informed by the underlying GIS and risk scoring data and are updated in real-Time. The system provides the DOD with information required to rationally address high-risk requirements, gradually upgrade systems to required levels of service, and improve O and M management.  © 2020 American Society of Civil Engineers. Risk-based decision support system for YOU.S. Air force water and wastewater: Infrastructure asset management  Asset management; Decision making; Decision support systems; Investments; Pipelines; Relational database systems; Risks; Surveys; Engineering analysis; Enterprise environment; Infrastructure asset management; Probability of failure; Requirements prioritization; Risk-based decision-support system; Utility infrastructure; Wastewater infrastructure; Information management",Risk management
343,SPortfolio: Stratified Visual Analysis of Stock Portfolios,"Quantitative Investment, built on the solid foundation of robust financial theories, is at the center stage in investment industry today. The essence of quantitative investment is the multi-factor model, which explains the relationship between the risk and return of equities. However, the multi-factor model generates enormous quantities of factor data, through which even experienced portfolio managers find it difficult to navigate. This has led to portfolio analysis and factor research being limited by a lack of intuitive visual analytics tools. Previous portfolio visualization systems have mainly focused on the relationship between the portfolio return and stock holdings, which is insufficient for making actionable insights or understanding market trends. In this paper, we present s Portfolio, which, to the best of our knowledge, is the first visualization that attempts to explore the factor investment area. In particular, sPortfolio provides a holistic overview of the factor data and aims to facilitate the analysis at three different levels: a Risk-Factor level, for a general market situation analysis; a Multiple-Portfolio level, for understanding the portfolio strategies; and a Single-Portfolio level, for investigating detailed operations. The system's effectiveness and usability are demonstrated through three case studies. The system has passed its pilot study and is soon to be deployed in industry. © 2020 IEEE. SPortfolio: Stratified Visual Analysis of Stock Portfolios factor investment; financial data analysis; Stock portfolio; visual analytics Commerce; Factor analysis; Investments; Risk assessment; Visualization; Financial Data Analysis; Market situation; Portfolio analysis; Portfolio managers; Portfolio strategies; Stock portfolio; Visual analytics; Visualization system; article; human; investment; pilot study; risk factor; Financial markets",Risk management
344,Investigating profitability performance of construction projects using big data: A project analytics approach,"The construction industry generates different types of data from the project inception stage to project delivery. This data comes in various forms and formats which surpass the data management, integration and analysis capabilities of existing project intelligence tools used within the industry. Several tasks in the project lifecycle bear implications for the efficient planning and delivery of construction projects. Setting up right profit margins and its continuous tracking as projects progress are vital management tasks that require data-driven decision support. Existing profit estimation measures use a company or industry wide benchmarks to guide these decisions. These benchmarks are oftentimes unreliable as they do not factor in project-specific variations. As a result, projects are wrongly estimated using uniform rates that eventually end up with entirely unusual margins either due to underspends or overruns. This study proposed a project analytics approach where Big Data is harnessed to understand the profitability distribution of different types of construction projects. To this end, Big Data architecture is recommended, and a prototype implementation is shown to store and analyse large amounts of projects data. Our data analysis revealed that profit margins evolve, and the profitability performance varies across several project attributes. These insights shall be incorporated as knowledge to machine learning algorithms to predict project margins accurately. The proposed approach enabled the fast exploration of data to understand the underlying pattern in the profitability performance for different types of construction projects. © 2019 The Authors Investigating profitability performance of construction projects using big data: A project analytics approach Big data; Machine learning; Profitability performance; Project analytics; System architecture Big data; Construction industry; Decision support systems; Information management; Learning algorithms; Learning systems; Life cycle; Machine learning; Profitability; Analysis capabilities; Construction projects; Continuous tracking; Data architectures; Data driven decision; Project analytics; Prototype implementations; System architectures; Project management",Capacity management
345,Big Data-Driven Cognitive Computing System for Optimization of Social Media Analytics,"The integration of big data analytics and cognitive computing results in a new model that can provide the utilization of the most complicated advances in industry and its relevant decision-making processes as well as resolving failures faced during big data analytics. In E-projects portfolio selection (EPPS) problem, big data-driven decision-making has a great importance in web development environments. EPPS problem deals with choosing a set of the best investment projects on social media such that maximum return with minimum risk is achieved. To optimize the EPPS problem on social media, this study aims to develop a hybrid fuzzy multi-objective optimization algorithm, named as NSGA-III-MOIWO encompassing the non-dominated sorting genetic algorithm III (NSGA-III) and multi-objective invasive weed optimization (MOIWO) algorithms. The objectives are to simultaneously minimize variance, skewness and kurtosis as the risk measures and maximize the total expected return. To evaluate the performance of the proposed hybrid algorithm, the data derived from 125 active E-projects in an Iranian web development company are analyzed and employed over the period 2014-2018. Finally, the obtained experimental results provide the optimal policy based on the main limitations of the system and it is demonstrated that the NSGA-III-MOIWO outperforms the NSGA-III and MOIWO in finding efficient investment boundaries in EPPS problems. Finally, an efficient statistical-comparative analysis is performed to test the performance of NSGA-III-MOIWO against some well-known multi-objective algorithms. © 2013 IEEE. Big Data-Driven Cognitive Computing System for Optimization of Social Media Analytics Big data-driven cognitive computing system; E-projects portfolio selection problem; fuzzy system; social media Advanced Analytics; Data Analytics; Decision making; Genetic algorithms; Higher order statistics; Multiobjective optimization; Risk assessment; Social networking (online); Cognitive computing systems; Comparative analysis; Data driven decision; Decision making process; Invasive weed optimization; Multi objective algorithm; Non- dominated sorting genetic algorithms; Social media analytics; Big data",Strategic alignment
346,Comparative analysis of ARIMA model and neural network in predicting stock price,"ARIMA model is a time series model, which is used to analyze and predict the mean value of the sequence. Neural network can be used to predict in various fields. In this essay, the neural network algorithm is used for the financial time series to predict the trend of stock price change, and the results are compared with the traditional ARIMA. It is found that the neural network algorithm can better predict the change of stock price. In the empirical analysis, Python is used to establish three models for analysis and prediction, which can provide a more appropriate model reference for investors to judge the short-term trend of stock prices and portfolio decision-making. © Springer Nature Singapore Pte Ltd. 2020. Comparative analysis of ARIMA model and neural network in predicting stock price ARIMA model; Prediction; The neural network Advanced Analytics; Big data; Decision making; Electronic trading; Embedded systems; Forecasting; Investments; Smart city; Time series; Appropriate models; ARIMA modeling; Comparative analysis; Empirical analysis; Financial time series; Neural network algorithm; Portfolio decisions; Time series modeling; Financial markets",Value management
347,4D and 5D BIM: A system for automation of planning and integrated cost management,"The construction sector has difficulty in catching the rapidly developing technology era. While the sectors are rapidly adapting themselves to the new era, firms that do not invest in innovation lose their value at a fast pace. When we look at the Fortune 500 list, it is seen that the biggest leap is made by companies in the IT sector. However, the situation is observed to be the opposite for construction companies. Despite the enormous investment costs and the huge risks involved, construction companies are unable to compete with the leaders of the digital world and falling down in the list. The losses of time and cost are increasing in the construction sector which lags behind in the digitizing world. The systematic analysis of these losses with analytical methods remains limited and there are problems in project management approaches. The industry today invests into digital transformation with Building Information Modeling (BIM) to deal with these problems in the construction sector and to produce a solution according to their dynamics in the digital world. BIM is the digital twin model of actual construction consisting of data-rich 3D objects. Accurate management of a data-rich model is possible with data management techniques. In order to fully benefit from this digital reflection of construction projects, the sector needs to establish processes based on basic theories of digital data management. This study is designed to provide a methodology in which the BIM 4D and 5D processes are handled as integrated and business intelligence approaches. While approaching Information Modeling from Building perspective, some basic approaches to data science have been integrated with 4D and 5D processes. The main objective of the study is to contribute to the digitalization process and to present a proposal for the application of the business intelligence in BIM processes. © Springer Nature Switzerland AG 2020. 4D and 5D BIM: A system for automation of planning and integrated cost management 4D and 5D BIM; Automation of construction; Business intelligence Competitive intelligence; Construction industry; Digital twin; Information analysis; Information management; Information theory; Project management; 4D and 5D BIM; Automation of planning; Building Information Model - BIM; Construction companies; Construction projects; Construction sectors; Data management techniques; Digital transformation; Architectural design",Strategic alignment
348,A data science and open source software approach to analytics for strategic sourcing,"Data science has emerged as a significant capability upon which firms compete. Although many data scientists and the high-performing companies that employ them seem to have developed robust methods to employ data sciences practices to achieve competitive advantages, there have been few attempts at defining and explaining how and why data science helps firms to achieve desired outcomes. In this paper, we describe how data science, which combines computer programming, domain knowledge, and analytic skillsets to scientifically extract insights from data, can be used to help meet the growing demand of analytic needs across an organization's value chain. This is done through the illustration of an applied data science initiative to a strategic sourcing problem via the use of open-source technology. In doing so, we contribute to the growing data science literature by demonstrating the application of unique data science capabilities. Moreover, the paper provides a tutorial on how to use a specific R package along with an actual case in which that package use used. © 2020 Elsevier Ltd A data science and open source software approach to analytics for strategic sourcing Data science; Decision support; Open source; Purchasing portfolio; R programming; Strategic sourcing; Supply chain management Competition; Computer programming; Data Science; Open Data; Open systems; Competitive advantage; Domain knowledge; Growing demand; Open-source technology; Robust methods; Skill-sets; Strategic sourcing; Value chains; Open source software",Strategic alignment
349,Fuzzy model estimation of the risk factors impact on the target of promotion of the software product,"This article studies the application of fuzzy logic to the risk analysis of a new software product development and marketing in specific case of a small size IT company. Identification and analysis of external and internal risk factors show that this type of business activity could be evaluated as high-risk enterprise. So, the purpose of the paper is to develop robust method to evaluate probability of occurrence of major risk events and their impact on the company financial health. The fuzzy logic is used to estimate degrees of threat of each relevant risk factor due to lack of reliable statistical data. The novelty of proposed approach is the inclusion into the model the risk event time. © 2020, © 2020 Informa UK Limited, trading as Taylor & Francis Group. Fuzzy model estimation of the risk factors impact on the target of promotion of the software product decision making; Fuzzy models; marketing analytics and product management; project management; risk factors; software promotion Application programs; Commerce; Computer circuits; Decision making; Health risks; Marketing; Project management; Risk analysis; Risk assessment; Risk perception; Business activities; Fuzzy models; Probability of occurrence; Product management; Risk factors; Software product development; Software products; Statistical datas; Fuzzy logic",Strategic alignment
350,Organisational project evaluation via machine learning techniques: an exploration,"This study explores ways an organisation can save time; review all proposed innovative, internal ideas; and, identify relevant start-up companies able to bring these ideas to fruition within a knowledge management framework. It uses text-mining techniques, including Python for data extraction and manipulation and topic modelling with Latent Dirichlet Allocation and Jaccard similarity indexes as a basis for evaluation of potentially valuable project ideas. Results show that internal organisational project ideas can be automatically matched with external data regarding potential implementation partners using big data knowledge management approaches. This ensures internal ideas are not overlooked or lost, but rather considered further so potentially profitable and viable opportunities are not missed. Increased use of big data to predict innovation and add value opens new channels to utilise text analysis in organisations and ensure internal innovation through a sustainable knowledge management approach. © Operational Research Society 2019. Organisational project evaluation via machine learning techniques: an exploration big data; concept assessment; data analytics; data extraction; innovation; Jaccard similarity; LDA; Text mining; topic modelling Big data; Machine learning; Project management; Statistics; Text mining; Data extraction; Knowledge management framework; Latent Dirichlet allocation; Machine learning techniques; Organisational; Project evaluation; Start-up companies; Text mining techniques; Knowledge management",Strategic alignment
351,Conceptualizing and operationalizing team task interdependences: BIM implementation assessment using effort distribution analytics,"Building Information Modelling (BIM) is a technological innovation currently at the forefront of digital transformation in the built environment. To achieve satisfactory outcomes with BIM, adopters need to find the most appropriate implementation strategy that is economically efficient. The research discussed here explores why and how distribution of effort spent on various tasks over project life cycle can be used as a metric for assessing and improving the performance of BIM implementation. Using quantitative data collected from a single in-depth case study of a BIM-enabled design and build project and subsequent interviews with three project actors and 11 BIM experts, to validate and triangulate the findings, we explore the use of effort data to analyze and visualize effort distribution patterns. The visualizations revealed the dynamics of team collaboration and task interdependences in BIM-enabled work and their impact on effort distribution. Lack of timely input by the contractor, subcontractors and suppliers influenced unusual patterns in the distribution of efforts indicating potential sources of inefficiency and unnecessary costs in the BIM process; thereby revealing the pivotal role of procurement structure and suggesting the need for timely involvement of key project participants. The contribution of this work is twofold. Methodologically, effort distribution analytics can provide insights that managers can use to improve BIM implementation process. Theoretically, the findings can be used to support informed decision-making, control cost, optimize resources, manage cash flow and to structure fees. © 2019, © 2019 Informa UK Limited, trading as Taylor & Francis Group. Conceptualizing and operationalizing team task interdependences: BIM implementation assessment using effort distribution analytics Building information modelling; construction; effort distribution; performance assessment; team collaboration Construction; Contractors; Costs; Decision making; Information theory; Life cycle; Building Information Modelling; Distribution of efforts; effort distribution; Implementation process; Implementation strategies; Performance assessment; Team collaboration; Technological innovation; building; conceptual framework; construction industry; construction method; decision making; information system; modeling; performance assessment; project management; temporal analysis; Architectural design",Strategic alignment
352,Energy affordability in practice: Oracle Utilities Opower's business Intelligence to meet low and moderate income need at Eversource,"Over 70 million low and moderate income (LMI) Americans struggle to pay their electricity bills each year. Low-income households suffer a disproportionate energy burden, defined as the percentage of gross household income spent on energy costs. Eversource recognizes the needs of its LMI customers and has a long and successful history of helping low income customers weatherize their homes working with local Community Action Program (CAP) agencies, and wanted to do more. Eversource and Oracle Utilities-Opower partnered to develop a first-of-its-kind analytical approach to characterize communities in need and target the customers who need help. While most experts have relied solely on energy burden as a singular metric to characterize need, the Eversource-Opower team intends to employ a data-driven methodology that will leverage individual energy consumption, available customer demographic data, third party data and innovative, open source data sets. The resultant curated portfolio of metrics move beyond burden to illuminate specific characteristics of target communities and create nuanced programs that are specific to the socio-economic needs of a particular area. Preliminary results indicate that by combining the portfolio of metrics, we can find customers who slip through the cracks when we market based on income or neighborhood alone. © 2019 Elsevier Inc. Energy affordability in practice: Oracle Utilities Opower's business Intelligence to meet low and moderate income need at Eversource Affordability; Energy burden; Energy efficiency; Metrics; Residential ",Stakeholder management
353,"Life sciences M&As in the fourth quarter of 2018: Notes, trends and a closer look at APAC","During the fourth quarter of 2018, Cortellis Deals Intelligence, from Clarivate Analytics, tracked 140 new biopharma mergers and acquisitions (M&A) with a total disclosed deal value of $33.1 billion as part of its ongoing coverage of M&A activity in the life sciences sector. This compared to 153 and $20.3 billion in the third quarter and 151 and $100.7 billion in the fourth quarter of 2017. The transactions included 30 high-value new M&As accounted in Q4 2018 worth in excess of $100 million with 12 worth more than $500 million. © 2019 TeknoScienze. All rights reserved. Life sciences M&As in the fourth quarter of 2018: Notes, trends and a closer look at APAC Biopharma; Business development; Cortellis; Dealmaking; Licensing; Life sciences; Portfolio & licensing ",Risk management
354,Management of analytics-as-a-service - results from an action design research project,"The ability to generate business-relevant information from data and to exploit it to improve business processes, decision-making, products, and services (business analytics) is a key success factor for businesses today. Answering the call for further research on success-relevant practices and instruments for managing business analytics, we report on the results of a three-year action design research (ADR) project at a global car manufacturer. Drawing on the socio-technical systems theory, we identify seven meta-requirements and specify four principles for the design of an instrument to manage Analytics-as-a-Service (AaaS) portfolios. Our results reinforce the importance of coordinating different socio-technical components in business analytics initiatives and demonstrate how concrete management instruments, such as a proposed portfolio management tool, contribute to socio-technical alignment. For practitioners, the documented design components provide guidance on how to design and implement similar instruments that support the systematic management of AaaS portfolios. © Operational Research Society 2020. Management of analytics-as-a-service - results from an action design research project analytics-as-a-service; artificial intelligence; descriptive analytics; machine learning; Portfolio management; predictive analytics; prescriptive analytics Advanced Analytics; Automobile manufacture; Decision making; Financial data processing; Investments; Business analytics; Car manufacturers; Design and implements; Key success factors; Portfolio managements; Provide guidances; Sociotechnical systems theory; Systematic management; Concrete industry",Strategic alignment
355,"Algorithmic Trading Methods: Applications Using Advanced Statistics, Optimization, and Machine Learning Techniques","Algorithmic Trading Methods: Applications using Advanced Statistics, Optimization, and Machine Learning Techniques, Second Edition, is a sequel to The Science of Algorithmic Trading and Portfolio Management. This edition includes new chapters on algorithmic trading, advanced trading analytics, regression analysis, optimization, and advanced statistical methods. Increasing its focus on trading strategies and models, this edition includes new insights into the ever-changing financial environment, pre-trade and post-trade analysis, liquidation cost & risk analysis, and compliance and regulatory reporting requirements. Highlighting new investment techniques, this book includes material to assist in the best execution process, model validation, quality and assurance testing, limit order modeling, and smart order routing analysis. Includes advanced modeling techniques using machine learning, predictive analytics, and neural networks. The text provides readers with a suite of transaction cost analysis functions packaged as a TCA library. These programming tools are accessible via numerous software applications and programming languages. © 2021 Elsevier Inc. All rights reserved. Algorithmic Trading Methods: Applications Using Advanced Statistics, Optimization, and Machine Learning Techniques  ",Strategic alignment
356,SNAPS: Sensor analytics point solutions for detection and decision support systems,"In this review, we discuss the role of sensor analytics point solutions (SNAPS), a reduced complexity machine-assisted decision support tool. We summarize the approaches used for mobile phone-based chemical/biological sensors, including general hardware and software requirements for signal transduction and acquisition. We introduce SNAPS, part of a platform approach to converge sensor data and analytics. The platform is designed to consist of a portfolio of modular tools which may lend itself to dynamic composability by enabling context-specific selection of relevant units, resulting in case-based working modules. SNAPS is an element of this platform where data analytics, statistical characterization and algorithms may be delivered to the data either via embedded systems in devices, or sourced, in near real-time, from mist, fog or cloud computing resources. Convergence of the physical systems with the cyber components paves the path for SNAPS to progress to higher levels of artificial reasoning tools (ART) and emerge as data-informed decision support, as a service for general societal needs. Proof of concept examples of SNAPS are demonstrated both for quantitative data and qualitative data, each operated using a mobile device (smartphone or tablet) for data acquisition and analytics. We discuss the challenges and opportunities for SNAPS, centered around the value to users/stakeholders and the key performance indicators users may find helpful, for these types of machine-assisted tools. © 2019 by the authors. Licensee MDPI, Basel, Switzerland. SNAPS: Sensor analytics point solutions for detection and decision support systems ART; Artificial reasoning tools; Cyber-physical systems; DADA; Data analytics; Drag and drop analytics; Knowledge graphs; Machine-assisted platform; Machine-assisted tools; MAP; MAT; SARA; Sense-analyze-respond-actuate; Sensor; Sensor-analytics point solutions; Smart systems; SNAPS; Trans-disciplinary convergence Artificial intelligence; Benchmarking; Cyber Physical System; Data acquisition; Data Analytics; Distributed computer systems; Embedded systems; Maps; Real time systems; Sensors; Signal processing; Signal transduction; Artificial reasoning; DADA; Drag and drop; Knowledge graphs; SARA; Sense-analyze-respond-actuate; Sensor analytics; Smart System; SNAPS; Trans-disciplinary convergence; Decision support systems",Strategic alignment
357,Reliability-based assessment method of buried pipeline at fault crossings,"The tectonic fault, which is one of the most common geohazards in field, poses great threat to buried pipe segments. Pipes will process to buckling or fracture due to large strain induced by continuously increasing ground displacements during earthquakes. Therefore, it is imperative to conduct safety analysis on pipes which are buried in seismic areas for the sake of ensuring normal operation. However, the highly nonlinearity of pipe response restricts the proceeding of reliability assessment. In this study, a hybrid procedure combining finite element method and artificial neural network is proposed for reliability-based assessment. First of all, the finite element model is developed on ABAQUS platform to simulate pipe response to strike-slip fault displacements. Thus, the strain demand value (the peak strain value obtained by finite element model in each design case) can be collected for database establishment, which is the preparation for neural network training. Thoroughness of the strain demand database can be achieved by a fully comprehensive calculation with consideration of influencing factors involving pipe diameter and wall thickness, operating pressure, magnitude of fault displacement, intersection angle between pipeline and fault plane, and characteristic value of backfill mechanics. Sequentially, Back Propagation Neural Network (BPNN) with double hidden layers is trained based on the developed database, and the surrogate strain demand prediction model can be obtained after accuracy verification. Hence, the strain-based limit state function can be respectively determined for tensile and compressive conditions. The strain capacity term is simply assumed based on published papers, the strain demand term is naturally superseded by the surrogate BPNN model, and Monte Carlo Simulation is employed to compute the probability of failure (POF). At last, the workability of the proposed approach is tested by a case study in which basic variables are referred to the Second West-to-East natural gas transmission pipeline project. It indicates that ANN is a good solver for reliability problems with implicit limit state functions especially for highly nonlinear problems. The proposed method is capable of computing POFs, which is an exploratory application for reliability research on pipes withstanding fault displacement loads. Copyright © 2020 ASME Reliability-based assessment method of buried pipeline at fault crossings BP neural network (BPNN); Finite element method; Monte Carlo simulation; Reliability analysis; Strike-slip faults; X80 pipes ABAQUS; Backpropagation; Crack propagation; Database systems; Finite element method; Monte Carlo methods; Multilayer neural networks; Natural gas pipelines; Offshore oil well production; Predictive analytics; Project management; Reliability; Safety engineering; Strike-slip faults; Back-propagation neural networks; Implicit limit state; Limit state functions; Natural gas transmission pipeline; Neural network training; Probability of failure; Reliability assessments; Reliability problems; Offshore pipelines",Capacity management
358,Investigating Cohort Similarity as an Ex Ante Alternative to Patent Forward Citations,"Forward citations are arguably the most widely used empirical metric for patents, including as indicators of patent information content, cumulative innovation, value, and knowledge flows. However, forward citations have major shortcomings. Citations require long time horizons to accrue, and therefore they cannot be observed until several years after a patent issues. Citation data are often noisy, discontinuous, and highly skewed, complicating empirical analysis. Moreover, recent studies have questioned the reliability of citation data. As such, the most widely used empirical metric of patents is also the most suspect. This study constructs a measure of patents that correlates with forward citations, but is observable ex ante, immediately upon patent issuance or even earlier upon publication of a patent application. In addition, this measure is continuous and evenly distributed, such that it is suitable for large-scale patent analytics applications. Finally, unlike citations, the measure is portable across patent systems, facilitating cross-border comparisons of portfolios and datasets. Specifically, I construct a measure of the similarity of a patent to its technological-temporal cohort, based on linguistic analysis of claim text. I employ advanced computational linguistic techniques to analyze the claims of all YOU.S. patents issued in the period 1976–2017, over 6 million patents in total, and I calculate the average degree of conceptual similarity of each patented invention to all others in the same technology field and time period cohort. I then extend the methodology to all issued EP patents, over 1.6 million in total. I validate the resulting measures against multiple established patent metrics for YOU.S. and EP patents. I test the robustness of this measure as a forecast for future patent citations in empirical research and big-data applications. I find that cohort similarity correlates significantly with forward citations received by both YOU.S. and EP patents. Cohort similarity also substitutes for citations in leading prior studies of R&D output and innovation. Finally, I demonstrate that, unlike citations, cohort similarity is comparable across the YOU.S. and EP patent systems. Accordingly, cohort similarity may be useful for empirical patent research, comparative studies of patent policy, and analytics of large-scale patent portfolios. © 2019 The Author. Journal of Empirical Legal Studies published by Cornell Law School and Wiley Periodicals, Inc. Investigating Cohort Similarity as an Ex Ante Alternative to Patent Forward Citations  ",Strategic alignment
359,Stochastic Package Queries in Probabilistic Databases,"We provide methods for in-database support of decision making under uncertainty. Many important decision problems correspond to selecting a ""package"" (bag of tuples in a relational database) that jointly satisfy a set of constraints while minimizing some overall ""cost"" function; in most real-world problems, the data is uncertain. We provide methods for specifying - -via a SQL extension - -and processing stochastic package queries (SPQS), in order to solve optimization problems over uncertain data, right where the data resides. Prior work in stochastic programming uses Monte Carlo methods where the original stochastic optimization problem is approximated by a large deterministic optimization problem that incorporates many ""scenarios"", i.e., sample realizations of the uncertain data values. For large database tables, however, a huge number of scenarios is required, leading to poor performance and, often, failure of the solver software. We therefore provide a novel ßs algorithm that, instead of trying to solve a large deterministic problem, seamlessly approximates it via a sequence of smaller problems defined over carefully crafted ""summaries"" of the scenarios that accelerate convergence to a feasible and near-optimal solution. Experimental results on our prototype system show that ßs can be orders of magnitude faster than prior methods at finding feasible and high-quality packages. © 2020 Association for Computing Machinery. Stochastic Package Queries in Probabilistic Databases data integration; decision making; optimization; package queries; portfolio optimization; prescriptive analytics; probabilistic databases; simulation; stochastic programming Decision making; Stochastic programming; Stochastic systems; Decision making under uncertainty; Deterministic optimization problems; Deterministic problems; Near-optimal solutions; Optimization problems; Probabilistic database; Relational Database; Stochastic optimization problems; Monte Carlo methods",Value management
360,A comparative study on machine learning techniques in assessment of financial portfolios,"Finance and especially computational finance is one of the areas in which Artificial Intelligence and machine learning has found a deep impact in the way financial problems are handled. As compared to traditional approaches the machine learning techniques have greater ease of use, higher degree of accuracy and the evaluation time to get the end result has been considerably reduced. In this paper, a general way of assessing risks and prediction of returns of different types of stocks using various machine learning techniques is reviewed and discussed and compared with the traditional methods. Earlier, more statistical and numerical methods were used for the same purpose. However, the nature of data in financial time series is nonlinear and chaotic. They do not follow linear characteristics, and are often a combination of white noise. To interpret this data meaningfully requires removal of noise and learning only the data that is actually suitable for analysis. Hence machine learning methods are found to be more suitable for applying in the financial domain. In the paper, the machine learning methods are reviewed which are used in the area of stock selection for portfolio construction and portfolio management. © 2020 IEEE. A comparative study on machine learning techniques in assessment of financial portfolios Deep Learning; Machine Learning; Mean Variance; Portfolio optimization; Predictive Analytics; Sharpe Ratio Financial data processing; Financial markets; Investments; Numerical methods; Risk assessment; White noise; Computational finance; Financial portfolio; Financial time series; Linear characteristics; Machine learning methods; Machine learning techniques; Portfolio managements; Traditional approaches; Machine learning",Strategic alignment
361,Online risk monitoring using offline simulation,"Estimating portfolio risk measures and classifying portfolio risk levels in real time are important yet challenging tasks. In this paper, we propose to build a logistic regression model using data generated in past simulation experiments and to use the model to predict portfolio risk measures and classify risk levels at any time. We further explore regularization techniques, simulation model structure, and additional simulation budget to enhance the estimators of the logistic regression model to make its predictions more precise. Our numerical results show that the proposed methods work well. Our work may be viewed as an example of the recently proposed idea of simulation analytics, which treats a simulation model as a data generator and proposes to apply data analytics tools to the simulation outputs to uncover conditional statements. Our work shows that the simulation analytics idea is viable and promising in the field of financial risk management. Copyright © 2019 Informs. Online risk monitoring using offline simulation Classification; Lasso; Logistic regression; Monte Carlo simulation; Simulation analytics; Variance reduction Budget control; Data Analytics; Logistic regression; Numerical methods; Risk management; Risk perception; Financial risk management; Logistic Regression modeling; Numerical results; Off-line simulations; Regularization technique; Risk monitoring; Simulation model; Simulation outputs; Risk assessment",Monitoring and control
362,A neural network enhanced volatility component model,"Volatility prediction, a central issue in financial econometrics, attracts increasing attention in the data science literature as advances in computational methods enable us to develop models with great forecasting precision. In this paper, we draw upon both strands of the literature and develop a novel two-component volatility model. The realized volatility is decomposed by a nonparametric filter into long- and short-run components, which are modeled by an artificial neural network and an ARMA process, respectively. We use intraday data on four major exchange rates and a Chinese stock index to construct daily realized volatility and perform out-of-sample evaluation of volatility forecasts generated by our model and well-established alternatives. Empirical results show that our model outperforms alternative models across all statistical metrics and over different forecasting horizons. Furthermore, volatility forecasts from our model offer economic gain to a mean-variance utility investor with higher portfolio returns and Sharpe ratio. © 2020, © 2020 Informa UK Limited, trading as Taylor & Francis Group. A neural network enhanced volatility component model ARMA process; Exchange rates; Volatility prediction; Wavelet analysis ",Risk management
363,An integrated recommender system for improved accuracy and aggregate diversity,"Information explosion creates dilemma in finding preferred products from the digital marketplaces. Thus, it is challenging for online companies to develop an efficient recommender system for large portfolio of products. The aim of this research is to develop an integrated recommender system model for online companies, with the ability of providing personalized services to their customers. The K-nearest neighbors (KNN) algorithm uses similarity matrices for performing the recommendation system; however, multiple drawbacks associated with the conventional KNN algorithm have been identified. Thus, an algorithm considering weight metric is used to select only significant nearest neighbors (SNN). Using secondary dataset on MovieLens and combining four types of prediction models, the study develops an integrated recommender system model to identify SNN and predict accurate personalized recommendations at lower computation cost. A timestamp used in the integrated model improves the performance of the personalized recommender system. The research contributes to behavioral analytics and recommender system literature by providing an integrated decision-making model for improved accuracy and aggregate diversity. The proposed prediction model helps to improve the profitability of online companies by selling diverse and preferred portfolio of products to their customers. © 2019 An integrated recommender system for improved accuracy and aggregate diversity Aggregate diversity; Behavioral analytics; Decision support system; E-business; Extreme learning; Recommender system Aggregates; Artificial intelligence; Behavioral research; Decision making; Decision support systems; Electronic trading; Forecasting; Nearest neighbor search; Sales; Behavioral analytics; eBusiness; Extreme learning; Information explosion; Integrated decision makings; K nearest neighbor (KNN); Personalized recommendation; Personalized recommender systems; Recommender systems",Strategic alignment
364,"Fixed Income Analytics: Bonds in High and Low Interest Rate Environments, Second Edition","This book analyses and discusses bonds and bond portfolios. Different yields and duration measures are investigated for negative and positive interest rates. The transition from a single bond to a bond portfolio leads to the equation for the internal rate of return. Its solution is analysed and compared to different approaches proposed in the financial industry. The impact of different yield scenarios on a model bond portfolio is illustrated. Market and credit risk are introduced as independent sources of risk. Different concepts for assessing credit markets are described. Lastly, an overview of the benchmark industry is offered and an introduction to convertible bonds is given. This second edition also includes a chapter on multi-currency portfolios as well as a discussion on currency hedging. This book is a valuable resource not only for students and researchers but also for professionals in the financial industry. © Springer Nature Switzerland AG 2020. All rights are reserved. Fixed Income Analytics: Bonds in High and Low Interest Rate Environments, Second Edition banking; Bond Analytics; Convertible; Credit Market; Fixed-income benchmarks; Flat yield curve concept; Internal Return Rate; quantitative finance; Risk Market; Spread analysis; Straight Bonds; Yield Measures ",Value management
365,Analysis of digital twins and application value of power engineering based on BIM,"With the continuous advancement of computer and Internet technologies, the application of BIM technology in power engineering construction has also been widely developed. The paper proposes the integration of BIM technology and digital twinning technology to realize the informationization and intellectual construction of power engineering. Based on the BIM power engineering project management platform, the paper introduces digital twinning technology, and applies advanced methods such as Internet of Things and big data to give real-time dynamic information to the BIM model, and establishes power engineering digital twins approach from the perspective of the whole process and all participants. In addition, the application value of BIM technology and digital twin technology in power engineering is analyzed. Through the integration of BIM technology and digital twin technology, digital and intelligent management of power engineering construction can be realized, and real-time perception, resource sharing and application capabilities of smart building can be promoted. © Springer Nature Singapore Pte Ltd. 2020. Analysis of digital twins and application value of power engineering based on BIM Comprehensive informationization; Digital twins; Project management; Smart building Advanced Analytics; Architectural design; Big data; Embedded systems; Intelligent buildings; Project management; Smart city; Application capability; Informationization; Intelligent management; Internet technology; Power engineering; Real time perception; Real-time dynamics; Resource sharing; Digital twin",Financial management
366,Chapter 22: Information technology issues in Mexico,"This chapter investigates the top organizational, technological, and individual issues of information technology (IT) workers in Mexico, a country just south of the US border. It utilized the standard World IT Project survey instrument, which was carefully translated into Spanish by the local research team. Our findings indicate that the top five organizational issues are: IT reliability and efficiency, security and privacy, alignment between IT and business, project management, and IT strategic planning. The top five technology issues are: customer relationship management (CRM) systems, networks and telecommunications, business intelligence/analytics, enterprise application integration, and software as a service. As per individual issues, IT employees seem to be extremely satisfied with their jobs with a high sense of accomplishment, feel secure in their jobs and plan to be in the IT industry for a long time. One concern relates to gender equality and opportunities presented to women in the IT industry. Women and men are not treated equally in the workplace; while men are hired directly and hold full-time positions, women are hired as external consultants or on an outsourcing basis from a third party. © 2020 The Author(s). Chapter 22: Information technology issues in Mexico  ",Risk management
367,Take back control of your capital project with an EPC 4.0 strategy,"Objectives/Scope:: CapEx Project delivery in the Oil and Gas industry is markedly underperforming other industries. McKinsey and IHS have reported that for projects over $1BN: 98% of Projects incur cost overruns or delays • The average cost increase is 80% of the original value • The average slippage is 20 months behind original schedule • The advent of ""Industry 4.0"" is highlighted by the adoption of integrated technology and software tools that focus on management of data across entire projects teams. This has enabled a leap in project execution performance in Aerospace, Manufacturing, and Agriculture. This paper investigates how the current Oil and Gas project execution methodologies can also be enhanced to leverage an Industry 4.0 strategy. This approach has been dubbed EPC 4.0. Methods, Procedures, Process: Through a process of dissecting existing workflows, business processes, contracting strategies, organizational structures, and information management practices used in capital project execution, the authors have proposed revised practices leveraging digital technology capabilities. These practices take advantage of systems that bring true data integration between all project tools, leveraging a single source of truth that can be visualized in 1D (data), 2D (schematics) or 3D (model), 4D (time) and 5D (cost) enabling project analytics and collaboration between all project disciplines, project teams and project stakeholders. Results, Observations, Conclusions: The result is an EPC 4.0 blueprint. A series of recommended best practices that are enabled by moving to a unified data centric strategy that will enhance project outcomes. The paper will cover the impact that EPC 4.0 has on Contracting Methodologies and Project Team Structures • Data Management and data centric workflows/business processes • Conceptual/Front End Design Maturity • Enhanced project oversight, reporting and management • Project collaboration within teams and between organizations • Enhanced Change Management • Preparation for handover to operations • Early adopters of an EPC 4.0 strategy are seeing immediate results. Typical savings are a 50% reduction in engineering in costs or 5% reduction in CapEx. In addition, higher quality output from engineering flows through to procurement, construction, and commissioning yielding a further 10% CapEx savings. More importantly, predictability in project outcomes have significantly increased with teams more effectively able to mitigate and manage cost overruns and schedule delays. Novel/Additive Information: The key innovation at the heart of the of EPC 4.0 strategy is the stripping away of the project practices that have built up over the years to mitigate the complexity and volume of information generated during capital project execution. By centralizing and simplifying data management, project contributors can focus on their principal role. For example, engineers can focus on engineering tasks, not on information management tasks. © 2020, Society of Petroleum Engineers Take back control of your capital project with an EPC 4.0 strategy  3D modeling; Aerospace industry; Agricultural robots; Cost engineering; Costs; Data integration; Gas industry; Gasoline; Human resource management; Industry 4.0; Information management; Petroleum industry; Contracting strategy; Data centric workflows; Integrated technologies; Oil and Gas Industry; Oil and gas projects; Organizational structures; Project collaboration; Project stakeholders; Project management",Monitoring and control
368,Towards effective AI-powered agile project management,"The rise of Artificial intelligence (AI) has the potential to significantly transform the practice of project management. Project management has a large socio-technical element with many uncertainties arising from variability in human aspects, e.g. customers' needs, developers' performance and team dynamics. AI can assist project managers and team members by automating repetitive, high-volume tasks to enable project analytics for estimation and risk prediction, providing actionable recommendations, and even making decisions. AI is potentially a game changer for project management in helping to accelerate productivity and increase project success rates. In this paper, we propose a framework where AI technologies can be leveraged to offer support for managing agile projects, which have become increasingly popular in the industry. © 2019 IEEE. Towards effective AI-powered agile project management Artificial Intelligence; Software Engineering Artificial intelligence; Human resource management; Risk perception; Software engineering; Agile project management; AI Technologies; Human aspects; Making decision; Project managers; Project success; Risk predictions; Sociotechnical; Project management",Risk management
369,"Estimating the value, ownership structure and turnover rate for investible commercial real estate from transaction datasets","Purpose: Commercial real estate (CRE) is a major investment asset. Yet detailed information on the value of investible CRE in different cities is lacking. The authors propose an innovative method to measure the value of investible CRE using transaction datasets. Design/methodology/approach: The authors take transaction prices and index them to produce a time series of values for each asset. The sum of the values at each point represents the value of investible CRE at that date. The authors’ method is applied to transaction data for New York, London and Toronto. Findings: London had the highest proportions of institutional and foreign ownership, and its turnover was more resilient to the downturn in global CRE following the GFC. The results illustrate the potential of the authors’ method to she would light on the characteristics of investible CRE markets. Research limitations/implications: The authors use data from Real Capital Analytics (RCA). This provides good coverage of transactions for investible CRE in the cities that the authors examine, but data from other sources might lead to different estimates. Practical implications: Measuring the value and turnover of investible CRE is important for portfolio strategies that account for the size and liquidity of investment markets. Knowledge of these features, and of ownership patterns, provides a better understanding of market operation. Originality/value: The authors’ modification of the perpetual inventory technique is simple, novel and practical. The authors propose this approach given the absence of a building-by-building inventory of investible CRE in many markets. © 2020, Emerald Publishing Limited. Estimating the value, ownership structure and turnover rate for investible commercial real estate from transaction datasets Market size; Ownership; Stock; Transactions; Turnover ",Strategic alignment
370,A conceptual framework for interdisciplinary decision support project success,"A quantitative and qualitative analysis review was used to create a conceptual framework that identifies the differences and similarities between management information systems, decision support systems, and the emerging interdisciplinary areas of big data, computational social science, and data science. The framework groups this information by project and technology critical success factors and clarifies definitions with explanations and examples. It identifies considerations, risks, and issues relevant to project management. These results close a gap in the literature by providing a framework for the terminology and context needed to plan interdisciplinary decision support projects. © 2019 IEEE. A conceptual framework for interdisciplinary decision support project success analytics; big data; business intelligence; data science; project management; success factors Artificial intelligence; Big data; Competitive intelligence; Data Science; Decision support systems; Project management; analytics; Computational social science; Conceptual frameworks; Critical success factor; Decision supports; Project success; Quantitative and qualitative analysis; Success factors; Information management",Value management
371,"19th Mexican International Conference on Artificial Intelligence, MICAI 2020","The proceedings contain 77 papers. The special focus in this conference is on Artificial Intelligence. The topics include: Impact of Memory Control on Batch Learning in Human Activity Recognition Scenario in Comparison to Data Stream Learning; automated Characterization and Prediction of Wind Conditions Using Gaussian Mixtures; a Survey on Freezing of Gait Detection and Prediction in Parkinson’s Disease; zeChipC: Time Series Interpolation Method Based on Lebesgue Sampling; machine Leaning Based Urdu Language Tutor for Primary School Students; selection Schemes Analysis in Genetic Algorithms for the Maximum Influence Problem; a Comparative Analysis of Evolutionary Learning in Artificial Hydrocarbon Networks; fatty Chain Acids Risk Factors in Sudden Infant Death Syndrome: A Genetic Algorithm Approach; An NSGA-III-Based Multi-objective Intelligent Autoscaler for Executing Engineering Applications in Cloud Infrastructures; speaker Identification Using Entropygrams and Convolutional Neural Networks; the Improvement Direction Mapping Method; a Genetic Programming Framework for Heuristic Generation for the Job-Shop Scheduling Problem; a Genetic Algorithm Approach for a Truck and Trailer Routing Problem in a Loading/Unloading Bays Application; a Tensor-Based Markov Decision Process Representation; object-Based Goal Recognition Using Real-World Data; comparing Multi-issue Multi-lateral Negotiation Approaches for Group Recommendation; guidance in the Visual Analytics of Cartographic Images in the Decision-Making Process; risk Sensitive Markov Decision Process for Portfolio Management; risk-Sensitive Piecewise-Linear Policy Iteration for Stochastic Shortest Path Markov Decision Processes; why Majority Rule Does Not Work in Quantum Computing: A Pedagogical Explanation; convolutional Neural Networks with Hebbian-Based Rules in Online Transfer Learning; how Powersets of Individual Fuzzy Sets Can Be Defined?. 19th Mexican International Conference on Artificial Intelligence, MICAI 2020  ",Risk management
372,Early determination of Optimal Test Flows with Requirements Analytics,"Optimal test flows in software testing involves a nuanced approach of combining knowledge of multiple test scenarios that traverse common paths or test scenarios that are segments of one or several paths based on the optimizing cost function. Identification of these flows and obtaining related information-optimum number of test cases, dependencies, complexity of requirements early in the development lifecycle is a key to realistic planning, dependency management and optimum test strategy in project delivery. An early stage view-from a working set of requirements-into the optimized flows and their complexity enables determination of resource needs, and dependencies. The paper proposes a method to obtain optimal test flows from requirements. Natural language processing of requirements enables determination of attributes, rules and relationship amongst the requirements that provides information on the number of flows, minimum necessary yet adequate test flows to ensure coverage and expose severe defects. It also provides information of the likely dependencies. Project results from application of the method support the proposed computation model. Applying this method enables early determination of optimal test flows, dependencies to identify an apt test window and key flows to uncover high severity defects that minimizes risks to project's planned schedule and product quality. © 2019 IEEE. Early determination of Optimal Test Flows with Requirements Analytics defect discovery cost; natural language processing; Optimal test flow; optimizing testing cost; requirement analytics Cost functions; Defects; Life cycle; Natural language processing systems; Project management; Testing; Computation model; Multiple test; NAtural language processing; Optimized flow; Optimum number; Project delivery; Test scenario; Test strategies; Software testing",Risk management
373,Grouping and labeling in digital learning portfolios,"This paper describes an application of clustering and labeling methods in actual learning records of e-portfolios implemented through personal blogs. In the clustering process, we have used k-means algorithm with Euclidian distance and random initialization of the centroids, using the elbow method to determine the best value of k; we used centroid labels for labeling. Analysis of 4,241 instances of the model (posts) have shown that absence of formal descriptors was overcome through representation of the titles given to posts, suggesting its use for automatic structuring of contents of this nature. © 2019 IEEE. Grouping and labeling in digital learning portfolios Educational data mining; Learning analytics; Learning portfolios Data mining; E-learning; Clustering process; Digital-learning; Educational data mining; Euclidian distance; Labeling methods; Learning analytics; Learning portfolio; Learning record; K-means clustering",Strategic alignment
374,Cost Management Model Based on Multidimensional System,"China’s economy has maintained medium and high speed development, and market competition is very fierce. Enterprises must change their management thinking. From managing products to operating customers, the customer relationship management system is a tool that helps SMEs optimize customer resources, improve customer management capabilities, and improve SME management Horizontal information system. As the current multi-dimensional system management effect evaluation fails to adopt a scientific evaluation method, this article explains the concept of multi-dimensional management of distribution network construction projects based on traditional project management theory. Based on the existing multi-dimensional management of distribution network construction projects in China, Analysis of the problem, put forward the idea of systematically establishing a multidimensional management system. The system chose the mature Microsoft VS.NET integrated development environment to develop the system. The MVC theory was integrated into the system in the design, which realized the layering of the program, which made the program portable and reduced the difficulty and cost of maintenance. By using the UML unified modeling language to design the system’s business flowchart, use case diagram, etc., the business and technical specifications of the system design are clearly presented. In addition to the basic daily management functions of small and medium-sized enterprises, the system also uses the open source KETTLE tool to mine the data, complete the data transfer, conversion, and loading, build a new multidimensional analysis model, and implement business intelligence data analysis functions for business management. Provide a highly reliable source of data. Provides support for system data storage by using SQLSERVER 2005 data. This article mainly uses the analytic hierarchy process to analyze the cost of multi-dimensional systems to obtain weight values to determine each importance. Through experiments, it is found that in multi-dimensional system cost management, management costs are prominently important, and the management cost situation in all indicators of multi-dimensional systems Management has the greatest impact, followed by engineering management fees, physical costs, and ARE & D expenses. Therefore, in multidimensional system cost management, while focusing on engineering management fees, physical costs, and ARE & D expenses, each organizational unit should focus on strict management costs, check. Project management fees, physical costs, and ARE & D expense management fees. © Springer Nature Switzerland AG 2020. Cost Management Model Based on Multidimensional System Analytic hierarchy process; Multidimensional analysis model; Multidimensional system; System data storage Analytic hierarchy process; Competition; Computer simulation languages; Cost engineering; Costs; Data transfer; Digital storage; Function evaluation; Information analysis; Open systems; Project management; Public relations; Sales; Security of data; System theory; Telecommunication services; Unified Modeling Language; Customer relationship management systems; Data storage; Integrated development environment; Multi-dimensional analysis; Multidimensional management; Multidimensional systems; Project management theory; Small and medium sized enterprise; Information management",Monitoring and control
375,Financial analytics: Time series analysis impact of crude oil prices on automotive stock,"The financial times series methods for analyzing data and predict future values based on fast available data and it consists of four components such as seasonal, trend, cyclical and random variations. This study attempts to understand the impact of crude oil prices in automotive stock. Crude oil being an important economic factor, studying it with respect to automotive stock is imperative its effect, in order to build portfolio. The research problem is framed to reduce the uncertainty factor regarding crude oil. The study checks whether crude is a considerable macro-economic factor to look upon during the investments. Moreover, it also contributes in constructing the investor’s portfolio in an efficient way. This research problem of the study is to identify the impact of crude oil prices on automotive stock of a company using various statistical tools and also to provide insights with regard to investing in automotive sector and understanding the impact of crude factors on company’s performance. This day in financial analytics innovation gives solutions for econometrics analysis, forecasting and simulation. For achieving the research objectives, the research has used the statistical tool are Excel and Eview for data analysis. The secondary data were collected from different online resources. The time series method employed to find the tests result implied that there was a significant relationship between the variables. This paper discusses the basic concepts of time series analytics, related literature review, business analytical process, data insights and conclusion. © IJSTR 2020. Financial analytics: Time series analysis impact of crude oil prices on automotive stock Business analytics; Financial analytics; Time series ",Monitoring and control
376,Crafting and Shaping Knowledge Worker Services in the Information Economy,"This book offers a hands-on approach to prepare businesses for managing the impact of technology transformation by the pragmatic, consistent, and persistent application of proven business principles and practices. Technology is rapidly transforming our businesses and our society. Knowledge worker roles are being impacted, and as operations are being automated, business models are changing as the use of cloud-based services lowers costs and provides flexibility. This book provides a guide towards managing the environment of uncertainly caused by the rapid changes in technology by combining strategy and leadership to influence the environment, instil the right behaviours, and strengthen the skills that will enable businesses to be adaptive, responsive, and resilient. © The Editor(s) (if applicable) and The Author(s), under exclusive licence to Springer Nature Singapore Pte Ltd. 2020. Crafting and Shaping Knowledge Worker Services in the Information Economy Adaptiveness and Responsiveness within Knowledge Worker Services; Analytics; Artificial Intelligence; Automation; Big Data; Business Architecture for Knowledge Worker Services; Business Transformation; Cloud-Based Services; ICT Services and Operations within Knowledge Worker Services; Knowledge Worker Services Transformation; Knowledge Workers; Operational Transformation within Knowledge Worker Services; OPEX Sourced Knowledge Worker Services; People within Knowledge Worker Services; Project Management within Knowledge Worker Services; Real Time Decision Making; Resiliency within Knowledge Worker Services; Risk Management within Knowledge Worker Services; Service Management; Training and Skilling within Knowledge Worker Services ",Value management
377,Patent data analytics for technology benchmarking: R-based implementation,"We have utilized data analytics techniques to produce highly detailed, accurate, and actionable insights on patent data to enable the decision makers to take informed decisions. We have developed a unique method to help business professionals easily understand the patent landscape around a particular technology domain. The data inputs for the analyses are the patent statistics and the organization's technology priorities. We have used and implemented clustering algorithms on the patent data while considering the organization's technology priorities to identify solutions that can help organizations gain a competitive advantage, identify potential collaboration targets, technology-product alignment, business decision making, strategy planning and other strategic decisions. © 2020 Elsevier Ltd Patent data analytics for technology benchmarking: R-based implementation Business decision making; Clustering; Competitive benchmarking; Patent portfolio analytics; Technology management ",Strategic alignment
378,Advancing injury and violence prevention through data science,"Introduction: The volume of new data that is created each year relevant to injury and violence prevention continues to grow. Furthermore, the variety and complexity of the types of useful data has also progressed beyond traditional, structured data. In order to more effectively advance injury research and prevention efforts, the adoption of data science tools, methods, and techniques, such as natural language processing and machine learning, by the field of injury and violence prevention is imperative. Method: The Centers for Disease Control and Prevention's (CDC) National Center for Injury Prevention and Control has conducted numerous data science pilot projects and recently developed a Data Science Strategy. This strategy includes goals on expanding the availability of more timely data systems, improving rapid identification of health threats and responses, increasing access to accurate health information and preventing misinformation, improving data linkages, expanding data visualization efforts, and increasing efficiency of analytic and scientific processes for injury and violence, among others. Results: To achieve these goals, CDC is expanding its data science capacity in the areas of internal workforce, partnerships, and information technology infrastructure. Practical Application: These efforts will expand the use of data science approaches to improve how CDC and the field address ongoing injury and violence priorities and challenges. © 2020 National Safety Council and Elsevier Ltd Advancing injury and violence prevention through data science CDC; Data science; Injury; Violence Centers for Disease Control and Prevention, YOU.S.; Data Science; Humans; United States; Violence; Wounds and Injuries; Data Science; Disease control; Health risks; Learning algorithms; Natural language processing systems; Project management; Centers for disease control and preventions; Health informations; Information technology infrastructure; Injury prevention; NAtural language processing; Rapid identification; Science strategies; Structured data; human; injury; prevention and control; United States; violence; Data visualization",Risk management
379,"PRICE AND PERFORMANCE TRENDS FOR CELLULAR TRAIL CAMERAS EXPLAINED WITH A TIME TREND, GOOGLE KEYWORD TRENDS, AND A USE CASE OF SUBURBAN DEER MANAGEMENT","Price and performance improvements for trail cameras, remote cameras designed for wildlife observation, have given wildlife researchers a widely accepted new tool. After their introduction in 2010, cellular trail cameras have become popular, saving travel time and reducing disturbance to wildlife. A use case of trail cameras for suburban deer management illustrates desirable product features and risks of using citizen science for research. A time trend of camera prices identifies a common product price pattern for technology products, a decline following a logistic or inverted s-curve. Data from Google Keyword Trends captures the changing level of market interest correlated with high statistical significance compared to price and corresponding to performance improvements thus providing evidence of the usefulness of the Google Trends data for project management and systems analysis. © 2020 Issues in Information Systems. All rights reserved. PRICE AND PERFORMANCE TRENDS FOR CELLULAR TRAIL CAMERAS EXPLAINED WITH A TIME TREND, GOOGLE KEYWORD TRENDS, AND A USE CASE OF SUBURBAN DEER MANAGEMENT Business Intelligence; Internet of Things; Mobile Technology; Technology Forecast ",Risk management
380,Data science education through education data: An end-to-end perspective,"The increasing importance of data science has created a pressing need for more trained professionals. Hence, it is necessary to design efficient education techniques to rapidly train the next generation of students. We describe a holistic approach that integrates multiple objectives within a single semester course taught at the first year level. The objectives include basic data processing techniques, statistical analysis, teamwork, and project management. We conducted a project that combined different public datasets related to the cost and quality of education in the US. Students demonstrated a high degree of involvement in this exercise as they were curious to explore data that related to their own college selection process. It is well known that it is difficult to perform a cost-quality analysis in the field of college education.We used college ranking data created by the Wall Street Journal, and combined it with data from the Integrated Postsecondary Education Data System database (https://nces.ed.gov/ipeds/) that provided college costs and graduation data. Students worked in teams to download sections of the data from multiple sources, performed cleaning and verification, and merged the data for analysis. This taught them the values of teamwork, goal-setting, and process measurement for timely project completion.We determined that the Pearson correlation between the rank of a college and in-state tuition was-0.65 (p<0.005) for private colleges, and-0.34 (p<0.005) for public colleges. This indicates that higher ranked colleges tend to charge higher tuition. The Pearson correlation between the rank of all colleges and their graduation rates is-0.75 (p<0.005), and between rank and retention rates is-0.71 (p<0.005). Private college tuition is typically $40,000 annually, whereas public colleges cost $20,000. These figures suggest that public colleges provide good value. This has policy implications for government funding of education. © 2019 IEEE. Data science education through education data: An end-to-end perspective curricular development; data science; project management; statistical analysis Correlation methods; Cost benefit analysis; Data handling; Data Science; Human resource management; Project management; Public policy; Quality control; Statistical methods; Curricular development; Data processing techniques; Data system database; Education technique; Postsecondary education; Process measurements; Quality of education; Wall Street Journal; Students",Value management
381,Applying process mining to support management of predictive analytics/data mining projects in a decision making center,"A Decision Making Centers (DMC) Environment facilitates the understanding of complex problems and simplifies the decision making process of policy makers and stakeholders by playing out several what if scenarios. One of the key elements in this environment is the management of Predictive Analytics and Data Mining (PADM) techniques required for real-time and accurate representation of data in scenarios. However, there has been a gap in understanding the best practices that facilitate the execution of PADM in environments such as a DMC. In this paper we aim to address this gap. Specifically, we apply process mining techniques to reveal: 1) process flow and executed activities in PADM projects and 2) The relationship and time distribution of activities within a PADM project execution. To this end two successful projects were analyzed in order to find relationships among activities and time metrics during execution. The present study aims to support managers in the execution of large scale PADM projects. © 2019 IEEE. Applying process mining to support management of predictive analytics/data mining projects in a decision making center Best practices; Business process management; Data mining; Decision making; Process design; Process modeling; Project management Data mining; Enterprise resource management; Predictive analytics; Process design; Project management; Best practices; Business process management; Complex problems; Decision making process; Process Modeling; Project execution; Time distribution; What-if scenarios; Decision making",Governance
382,Classification and Predictive Analysis of the Stocks Listed with NIFTY50,"Indian stock market has its prominent position in the globe. In 2018, the healthy economic growth in India has supported its stock market and become one of the largest stock market in the world. India's ascent reflects the growing blow of emerging markets. It also indicates its economy is positioned for sustained growth, even if the manufacturing sector is not firing on all cylinders. SENSEX and NIFTY are considered as the barometers of Indian stock market. Approximately 1600 companies are listed on National Stock Exchange of India Ltd. (NSE), from which fifty companies are listed with the prestigious index NIFTY50.The NIFTY50, is the leading index on the NSE, which is commonly known as NIFTY. It is derived from economic research and is created for the interest of investors, who wants to invest and trade in Indian equities. The NIFTY 50 stocks comprises of leading Indian companies from various sectors. The stocks of listed companies are relatively less volatile and offer a rather steady return. The NIFTY 50 covers major sectors of the Indian economy and offers great exposure to the investment managers to Indian stock market in one's competent portfolio. The companies listed with NIFTY50, show significantly diversified behavior with respect to their price movements. Thus, the risk and returns associated with the stocks found to be wide-ranging in nature. Also, the range of the beta factors of these stocks is significantly varied. The present study is an attempt to analysis the fifty stocks of NIFTY50 based on the returns offered by the stocks, risk associated with these stocks and their respective beta factors. The weekly data of past years have been collected and used to calculate the returns, risk and beta factors associated with the fifty stocks listed in NIFTY50. Using cluster analysis, the fifty stocks of NIFTY50 are classified into segments based on their respective returns, risk and beta values. Further for each segment, a predictive model for returns is Proposed. © 2019 ACM. Classification and Predictive Analysis of the Stocks Listed with NIFTY50 Beta Factor; Classification NIFTY50; Cluster Analysis; Predictive Model; Return; Risk; Stocks Cluster analysis; Commerce; Image processing; Investments; Predictive analytics; Risk assessment; Economic growths; Economic research; Emerging markets; Investment managers; Manufacturing sector; Predictive modeling; Price movement; Stock exchange; Financial markets",Strategic alignment
383,Adoption of Big Data analytics in construction: development of a conceptual model,"Purpose: Big Data (BD) is being increasingly used in a variety of industries including construction. Yet, little research exists that has examined the factors which drive BD adoption in construction. The purpose of this paper is to address this gap in knowledge. Design/methodology/approach: Data collected from literature (55 articles) were analyzed using content analysis techniques. Taking a two-pronged approach, first study presents a systematic perspective of literature on BD in construction. Then underpinned by technology–organization–environment theory and supplemented by literature, a conceptual model of five antecedent factors of BD adoption for use in construction is proposed. Findings: The results show that BD adoption in construction is driven by a number of factors: first, technological: augmented BD–BIM integration and BD relative advantage; second, organizational: improved design and execution efficiencies, and improved project management capabilities; and third, environmental: augmented availability of BD-related technology for construction. Hypothetical relationships involving these factors are then developed and presented through a new model of BD adoption in construction. Research limitations/implications: The study proposes a number of adoption factors and then builds a new conceptual model advancing theories on technologies adoption in construction. Practical implications: Findings will help managers (e.g. chief information officers, IT/IS managers, business and senior managers) to understand the factors that drive adoption of BD in construction and plan their own BD adoption. Results will help policy makers in developing policy guidelines to create sustainable environment for the adoption of BD for enhanced economic, social and environmental benefits. Originality/value: This paper develops a new model of BD adoption in construction and proposes some new factors of adoption process. © 2019, Emerald Publishing Limited. Adoption of Big Data analytics in construction: development of a conceptual model Asset management; BIM; Data analysis; Project management; Sustainability; Technological innovation ",Strategic alignment
384,Automatically learning compact quality-aware surrogates for optimization problems,"Solving optimization problems with unknown parameters often requires learning a predictive model to predict the values of the unknown parameters and then solving the problem using these values. Recent work has shown that including the optimization problem as a layer in the model training pipeline results in predictions of the unobserved parameters that lead to higher decision quality. Unfortunately, this process comes at a large computational cost because the optimization problem must be solved and differentiated through in each training iteration; furthermore, it may also sometimes fail to improve solution quality due to non-smoothness issues that arise when training through a complex optimization layer. To address these shortcomings, we learn a low-dimensional surrogate model of a large optimization problem by representing the feasible space in terms of meta-variables, each of which is a linear combination of the original variables. By training a low-dimensional surrogate model end-to-end, and jointly with the predictive model, we achieve: i) a large reduction in training and inference time; and ii) improved performance by focusing attention on the more important variables in the optimization and learning in a smoother space. Empirically, we demonstrate these improvements on a non-convex adversary modeling task, a submodular recommendation task and a convex portfolio optimization task. © 2020 Neural information processing systems foundation. All rights reserved. Automatically learning compact quality-aware surrogates for optimization problems  Financial data processing; Optimization; Predictive analytics; Adversary modeling; Complex optimization; Computational costs; Decision quality; Linear combinations; Optimization problems; Portfolio optimization; Predictive modeling; Learning systems",Strategic alignment
385,Computational approaches and data analytics in financial services: A literature review,"The level of modeling sophistication in financial services has increased considerably over the years. Nowadays, the complexity of financial problems and the vast amount of data require an engineering approach based on analytical modeling tools for planning, decision making, reporting, and supervisory control. This article provides an overview of the main financial applications of computational and data analytics approaches, focusing on the coverage of the recent developments and trends. The overview covers different methodological tools and their uses in areas, such as portfolio management, credit analysis, banking, and insurance. © 2019, © 2019 Operational Research Society. Computational approaches and data analytics in financial services: A literature review data analytics; financial modeling; Financial services; risk management Decision making; Financial data processing; Investments; Risk management; Computational approach; Financial applications; Financial modeling; Financial service; Literature reviews; Methodological tools; Portfolio managements; Supervisory control; Data Analytics",Strategic alignment
386,Pathways from data to value: Identifying strategic archetypes of analytics-based services,"The digital transformation offers organizations new opportunities to expand their existing service portfolio in order to achieve competitive advantages. A popular way to create new customer value is the offer of analytics-based services (ABS) - services that apply analytical methods to data to empower customers to make better decisions and to solve complex problems. However, research still lacks to provide a profound conceptualization of this novel service type. Similarly, actionable insights on how to purposefully establish ABS in the market to enrich the service portfolio remain scarce. Our cluster analysis of 105 ABS offered by start-ups identifies four generic ABS archetypes and unveils their specific service objectives and pronounced characteristics. The findings contribute to a more profound theorizing process on ABS by providing a detailed characterization of different ABS types and a systematization regarding strategic opportunities to enrich service portfolios in practice. © Proceedings of the 15th International Conference on Business Information Systems 2020 ""Developments, Opportunities and Challenges of Digitization"", WIRTSCHAFTSINFORMATIK 2020. Pathways from data to value: Identifying strategic archetypes of analytics-based services Analytics-based services; Archetypes; Cluster analysis; Service portfolio Cluster analysis; Competition; Information use; Analytical method; Competitive advantage; Complex problems; Customer values; Digital transformation; Service portfolio; Strategic opportunity; Information systems",Strategic alignment
387,"8th World Conference on Information Systems and Technologies, WorldCIST 2020","The proceedings contain 135 papers. The special focus in this conference is on Information Systems and Technologies. The topics include: Improving Project Management Practices in a Software Development Team; integrated Model of Knowledge Management and Innovation; trust and Reputation Smart Contracts for Explainable Recommendations; predicting an Election’s Outcome Using Sentiment Analysis; data Science in Pharmaceutical Industry; DICOM Metadata Quality Analysis for Mammography Radiation Exposure Characterization; the Use of Social Media in the Recruitment Process; complex Human Emotions in Alzheimer’s Interviews: First Steps; contribution of Social Tagging to Clustering Effectiveness Using as Interpretant the User’s Community; a Dynamic Approach for Template and Content Extraction in Websites; nutriSem: A Semantics-Driven Approach to Calculating Nutritional Value of Recipes; using Machine Learning Models to Predict the Length of Stay in a Hospital Setting; prediction of Length of Stay for Stroke Patients Using Artificial Neural Networks; artificial Intelligence in Service Delivery Systems: A Systematic Literature Review; benefits of Implementing Marketing Automation in Recruitment; data Fusion Model from Coupling Ontologies and Clinical Reports to Guide Medical Diagnosis Process; computational Analysis of a Literary Work in the Context of Its Spatiality; data Extraction and Preprocessing for Automated Question Answering Based on Knowledge Graphs; ontology Learning Approach Based on Analysis of the Context and Metadata of a Weakly Structured Content; Paving the Way for IT Governance in the Public Sector; analysis of Factors Affecting Backers’ Fundraising on Reward-Based Crowdfunding; ICT and Big Data Adoption in SMEs from Rural Areas: Comparison Between Portugal, Spain and Russia; logic-Based Smart Contracts. 8th World Conference on Information Systems and Technologies, WorldCIST 2020  ",Risk management
388,"The good, the bad, and the ugly: Data-driven load profile discord identification in a large building portfolio","Reducing the overall energy consumption and associated greenhouse gas emissions in the building sector is essential for meeting our future sustainability goals. Recently, smart energy metering facilities have been deployed to enable monitoring of energy consumption data with hourly or subhourly temporal resolution. This unprecedented data collection has created various opportunities for advanced data analytics involving load profiles (e.g., building energy benchmarking programs, building-to-grid integration, and calibration of urban-scale energy models). These applications often need preprocessing steps to detect daily load profile discords, such as: 1) outliers due to system malfunctions (the bad) and 2) irregular energy consumption patterns, such as those resulting from holidays (the ugly) compared to normal consumption patterns (the good). However, current preprocessing methods predominantly focus on filtering using statistical threshold values, which fail to capture the contextual discords of daily profiles. In addition, discord detection algorithms in building research are often aimed at finding individual building-level discords, which are not suitable at a large scale. Thus, in this paper, we develop a method for automated load profile discord identification (ALDI) in a large portfolio of buildings (more than 100 buildings). Specifically, ALDI 1) uses the matrix profile (MP) method to quantify the similarities of daily subsequences in time series meter data, 2) compares daily MP values with typical-day MP distributions using the Kolmogorov-Smirnov test, and 3) identifies daily load profile discords in a large building portfolio. We evaluate ALDI using the metering data of both an academic campus and a residential neighborhood. Our results demonstrate that ALDI efficiently discovers measurement errors by system malfunctions and low energy consumption days in the academic campus portfolio, and it detects unique load shape patterns likely driven by occupant behavior and extreme weather conditions in the residential neighborhood. © 2020 Elsevier B.V. The good, the bad, and the ugly: Data-driven load profile discord identification in a large building portfolio Data mining; Discord detection; Load profile; Portfolio analysis; Smart meter Advanced Analytics; Computational complexity; Data Analytics; Data mining; Extreme weather; Gas emissions; Greenhouse gases; Housing; Smart meters; Discord detection; Energy consumption datum; Extreme weather conditions; Kolmogorov-Smirnov test; Load profiles; Low energy consumption; Portfolio analysis; Residential neighborhoods; Energy utilization",Strategic alignment
389,Deep learning for decision making and the optimization of socially responsible investments and portfolio,"A socially responsible investment portfolio takes into consideration the environmental, social and governance aspects of companies. It has become an emerging topic for both financial investors and researchers recently. Traditional investment and portfolio theories, which are used for the optimization of financial investment portfolios, are inadequate for decision-making and the construction of an optimized socially responsible investment portfolio. In response to this problem, we introduced a Deep Responsible Investment Portfolio (DRIP) model that contains a Multivariate Bidirectional Long Short-Term Memory neural network, to predict stock returns for the construction of a socially responsible investment portfolio. The deep reinforcement learning technique was adapted to retrain neural networks and rebalance the portfolio periodically. Our empirical data revealed that the DRIP framework could achieve competitive financial performance and better social impact compared to traditional portfolio models, sustainable indexes and funds. © 2019 Elsevier B.V. Deep learning for decision making and the optimization of socially responsible investments and portfolio Decision support systems; Deep reinforcement learning; Multivariate analytics; Portfolio optimization; Socially responsible investment Decision making; Decision support systems; Electronic trading; Investments; Machine learning; Reinforcement learning; Financial investments; Financial investors; Financial performance; Investment portfolio; Multivariate analytics; Portfolio optimization; Reinforcement learning techniques; Socially responsible investments; Deep learning",Strategic alignment
390,What technology is needed for future offshore development?,"Prior to 2007, the YOU.S. Department of Energy (DOE) upstream oil and gas research program focusedprimarily on onshore applications. In 2000, the DOE published the Offshore Technology Roadmap for theUltra-Deepwater Gulf of Mexico. Then, the Energy Policy Act of 2005 required the DOE to expand itsresearch portfolio to include ultra-deepwater research. DOE has continued this offshore research focus tothe present, and this paper presents an overview of past accomplishments and results, the DOE's currentresearch portfolio, and outlines the potentially key elements of a technology roadmap for the entire OCS. Discussion focuses on key research findings from the DOE ultra-deepwater research portfolio of2007-2013. Then the paper describes the current offshore research portfolio 2014-2019. Finally, thepaper describes the outcomes and insights from key discussions with industry, academia, research and non-government and government stakeholders that could become a frame for a technology research roadmapfor the entire Outer Continental Shelf. DOE research investments in public-private partnerships with industry, academia, research labs, andothers have made an important contribution to the current state-of-the-art in offshore technology-contributions that most people may not realize are tied to previous research investments by DOE. Tracingthese contributions, tracking them back to the Offshore Technology Roadmap for the Ultra-Deepwater Gulfof Mexico published in November 2000, and framing a technology research roadmap for the OCS willdemonstrate the value of public-private partnerships. The information in this paper will both inform and inspire new frontiers of research for the OCS. Asthe USA moves forward with onshore development of unconventional resources, there are features of theDOE onshore research portfolio that may have merit in the OCS. For example, the DOE Field Laboratoryprogram is focused on basin-specific research strategies where new technology can be applied to operatingoilfields and evaluated via the scientific method. Then the data captured can potentially become part offurther research by the DOE National Laboratories including geophysical, geomechanical, geochemical,and data analytics such as machine learning. This DOE program has been very successful onshore, andperhaps there is a place for a comparable multi-disciplinary, multi-partner approach in the OCS. © 2020, Offshore Technology Conference. What technology is needed for future offshore development?  Application programs; Data Analytics; Engineering research; Investments; Offshore oil well production; Resource valuation; National laboratory; Offshore development; Outer continental shelves; Public private partnerships; Technology research; Technology roadmaps; YOU.S. Department of Energy; Unconventional resources; Offshore technology",Strategic alignment
391,MACSPro 2020 - Proceedings of the Conference on Modeling and Analysis of Complex Systems and Processes 2020,The proceedings contain 16 papers. The topics discussed include: modeling the protest-repression nexus; equilibrium and optimality in international trade models under monopolistic competition: the unified approach; an application of linear programming to sociophysics models; compositional conformance checking of nested Petri nets and event logs of multi-agent systems; software system behavior can be analyzed with visual analytics; when laziness leads to stability: approximate equilibria in one-dimensional jurisdiction formation models; why the conservative Basel III portfolio credit risk model underestimates losses?; identifying the topics of Russian political talk shows; real-time vehicle type detection and counting from road camera video; and model of general equilibrium in multisector economy with monopolistic competition and hypergeometric utilities. MACSPro 2020 - Proceedings of the Conference on Modeling and Analysis of Complex Systems and Processes 2020  ,Monitoring and control
392,Scale Can Improve the Clinical Value of Radiology Practices,"Radiology is participating in the recent consolidation trend. Larger practices can invest in the infrastructure and teams to help improve the clinical value of the services they deliver. An example of national practice is provided that leverages its scale to promote clinical best practices aimed at reducing variability in the recommendations radiologists make for common imaging findings. This is accomplished by promoting the culture of learning and collaboration. In some initiatives, developing a machine learning tool to facilitate the application of clinical algorithms at the point of dictation facilitates the adoption of the recommendations. Regular feedback on practice and individual performance promotes improvement in performance and personal satisfaction of the clinicians. Cost savings through the reduction of unnecessary imaging studies or invasive procedures as well as improved outcomes through evidence-based follow-up have been achieved. In some cases, reductions in the rupture rate of abdominal aortic aneurysms have been realized through clinical follow-up programs. Embracing a culture of continuous learning through peer learning can lay the foundation for sharing clinical best practices. Having access to the benefits of scale in the form of investment in data, analytics, project management, and machine learning tools can facilitate the process of creating clinical value for our patients. © 2019 American College of Radiology Scale Can Improve the Clinical Value of Radiology Practices Clinical; practice; scaled; value Article; diagnostic value; health service; human; peer review; protocol compliance; quality control; radiodiagnosis; rating scale; treatment outcome",Strategic alignment
393,A predictive risk model based on social network analysis,"Traditional approaches to forecast business trends are no longer suitable to assure success of organizations. Due continuously product and services complexity, the “invented here” approach, does not fit actual market demands. To overcome this issue, organizations engage in collaborative networks with external partners, universities or institutions. However, despite the advantages of such initiatives, organizations still risk a great deal, if not armed with an efficient risk support model concerning the repeatable dynamic collaborative behaviors (RDCBs) between involved entities. Understanding, and forecasting the influence of RDCBs in project outcomes, is vital to success. This paper proposes a heuristic two-part predictive model to estimate a project's outcome likelihood, by measuring the deviation between the evolution of an ongoing project (actual state), against its planned evolution (desired state), regarding a set of project critical success factors based on the repeatable dynamic collaborative behaviors (RDCBs) across all phases of a project lifecycle, of the participating entities. © 2020 EUROSIS-ETI. A predictive risk model based on social network analysis Business; Decision-making; Forecasting; Optimization; Social Science Life cycle; Modal analysis; Project management; Risk assessment; Collaborative behavior; Collaborative network; Critical success factor; Predictive modeling; Product and services; Project lifecycle; Project outcomes; Traditional approaches; Predictive analytics",Value management
394,An Optimization Methodology to Quantify the Benefit in the Western Energy Imbalance Market Participation,"The Western Energy Imbalance Market (EIM) administered by the California Independent System Operator (CAISO) has extended from two members in 2014 to eight members today, with a reported benefit of over half billion dollars. A key element of this benefit estimate comes from the heuristic methodology the CAISO adopted to calculate the counterfactual dispatch cost savings. Though fast and easy, the heuristic methodology offers little value to the EIM participant. This paper proposes a full optimization methodology from the market participant point of view to accurately quantify the EIM benefit utilizing a combination of publicly and privately available portfolio datasets. Multiple counterfactual scenarios can be constructed easily for in-depth generation portfolio analytics. A simple example will first be presented to understand all elements of the EIM benefit, and then an actual EIM generation portfolio will be tested to validate the proposed methodology. © 2019 IEEE. An Optimization Methodology to Quantify the Benefit in the Western Energy Imbalance Market Participation Electricity markets; Hydroelectric power generation; Linear programming; Optimization; Power generation dispatch Commerce; Electric utilities; Heuristic methods; Hydroelectric power; Linear programming; Optimization; Power markets; Energy imbalances; Full optimization; Generation portfolios; Independent system operators; Market participants; Market participations; Optimization methodology; Power generation dispatch; Electric load dispatching",Financial management
395,A New Cluster Validity Index for Stock Clustering Based on Efficient Frontier,"Clustering is an unsupervised learning method to discover meaningful information by grouping similar objects together. It is a great challenge to valuate the results of stock clustering. In this paper, we propose a specific index IBEF(Index Based on Efficient Frontier) to evaluate the results of stock clustering based on the concept of efficient frontier. IBEF is defined by the difference between two efficient frontier curves. One curve is built by all stocks and the other curve is built by center stock of each cluster. If the clustering result is good, the two curves are close to each other and IBEF value will be small. Our experiments on different correlation coefficients and clustering methods show that IBEF is a proper validity index comparing with other indexes. © 2020 IEEE. A New Cluster Validity Index for Stock Clustering Based on Efficient Frontier Cluster; Cluster validity index; Efficient frontier; Portfolio; Stock Advanced Analytics; Big data; Clustering algorithms; Unsupervised learning; Cluster validity indices; Clustering methods; Clustering results; Correlation coefficient; Efficient frontier; Unsupervised learning method; Validity index; Learning systems",Monitoring and control
396,Business Intelligence Solution in Project Monitoring and Control; [Solucão de Business Intelligence na Monitorizacão e Controlo de Projetos],"Due to the need to control and monitor the performance of projects and the inherent need for project managers to provide feedback on the status of their project(s), this study focuses mainly on the planning of a Business Intelligence solution, as an aid in the monitoring and control of projects. In this sense, the EVM (Earned Value Management) technique is used as a method recommended by PMBOK and it is effective in controlling and monitoring projects, to identify the best indicators and develop the proposed Business Intelligence solution for this research. Based on the questionnaires answered by project managers from different sectors and with different experiences, and based on the literature on EVM and dashboards, it was possible to formulate a Business Intelligence solution that allows project managers to evaluate, through a single screen, the status of their project(s). The developed dashboard refers to a dashboard of tactical typology, since its purpose is to measure progress and optimize the process through indicators of action and result, in summary / detailed form with the purpose of improving the analysis by a BI Portal. © 2020 AISTI. Business Intelligence Solution in Project Monitoring and Control; [Solucão de Business Intelligence na Monitorizacão e Controlo de Projetos] Business Intelligence; Dashboard; Decision Making; EVM; Project Control and Monitoring; Project Management Budget control; Information systems; Information use; Managers; Surveys; Value engineering; Control and monitor; Earned value management; Monitoring and control; Project managers; Project monitoring and control; Single screen; Information analysis",Monitoring and control
397,Big Data in Portfolio Allocation: A New Approach to Successful Portfolio Optimization,"In the classic mean-variance portfolio theory as proposed by Harry Markowitz, the weights of the optimized portfolios are directly proportional to the inverse of the asset correlation matrix. However, most contemporary portfolio optimization research focuses on optimizing the correlation matrix itself, and not its inverse. In this article, the author demonstrates that this is a mistake. Specifically, from the Big Data perspective, she proves that the inverse of the correlation matrix is much more unstable and sensitive to random perturbations than is the correlation matrix itself. As such, optimization of the inverse of the correlation matrix adds more value to optimal portfolio selection than does optimization of the correlation matrix. The author further shows the empirical results of portfolio reallocation under different common portfolio composition scenarios. The technique outperforms traditional portfolio allocation techniques out of sample, delivering nearly 400% improvement over the equally weighted allocation over a 20-year investment period on the S&P 500 portfolio with monthly reallocation. In general, the author demonstrates that the correlation inverse optimization proposed in this article significantly outperforms the other core portfolio allocation strategies, such as equally weighted portfolios, vanilla mean-variance optimization, and techniques based on the spectral decomposition of the correlation matrix. The results presented in this article are novel in the data science space, extend far beyond financial data, and are applicable to any data correlation matrixes and their inverses, whether in advertising, healthcare, or genomics. © 2019 Pageant Media Ltd. Big Data in Portfolio Allocation: A New Approach to Successful Portfolio Optimization Big data/machine learning; Mutual funds/passive investing/indexing; Portfolio theory ",Strategic alignment
398,"7th International KES Conference on Smart Education and e-Learning, KES SEEL 2020","The proceedings contain 53 papers. The special focus in this conference is on Smart Education and e-Learning. The topics include: Implementation of Blended Learning into ESP for Medical Staff; providing an Ethical Framework for Smart Learning: A Study of Students’ Use of Social Media; “Product-Based” Master Program at ASCREEN Interactive Center; Developing a Conceptual Framework for Smart Teaching: Using VR to Teach Kids How to Save Lives; blended Learning Technology Realization Using a Basic Online Course; data Cleaning and Data Visualization Systems for Learning Analytics; Computational Linguistics and Mobile Devices for ESL: The Utilization of Linguistics in Intelligent Learning; personal Generative Libraries for Smart Computer Science Education; the Virtual Machine Learning Laboratory with Visualization of Algorithms Execution Process; EIFEL—A New Approach for Digital Education; The Use of Students’ Digital Portraits in Creating Smart Higher Education: A Case Study of the AI Benefits in Analyzing Educational and Social Media Data; Using Smart Education Together with Design Thinking: A Case of IT Product Prototyping by Students Studying Management; application of Smart Education Technologies on the Disciplines of the Music-Theoretical Cycle in Musical College and University; research on ‘Diteracy’ Measurement as a Smart Literacy Element; internet Resource as a Means of Diagnostics and Support of Artistically Gifted University Students; strategic Management of Smart University Development; concepts of Educational Collaborations and Innovative Directions for University Development: Knowledge Export Educational Programs; project Management as a Tool for Smart University Creation and Development; human Resource Management System Development at Smart University; university Financial Sustainability Assessment Models. 7th International KES Conference on Smart Education and e-Learning, KES SEEL 2020  ",Strategic alignment
399,Portfolio Management by Normal Mean-Variance Mixture Distributions,"According to the empirical evidence, financial returns show leptokurtosis, skewness and heavy-tailness. Regarding this behavior, we apply normal mixture mean variance distributions for portfolio management and allocating best weights for portfolio optimization and efficient frontiers. These distributions are appropriate for portfolio optimization and have a natural multivariate that consists of NIG, VG, NTS, GH and skewed t. Conditional Value-at-Risk (CVaR) is utilized as a measure of risk to evaluate the level of risk and simulated by Monte Carlo method. If we do not have closed density function of distribution, for example NTS distribution, we can use characteristic function with Fourier transformation to compute CVaR and portfolio modeling. Finally, real data in Iran stock market are given to illustrate the effectiveness our model by skewed t distribution. © 2019 IEEE. Portfolio Management by Normal Mean-Variance Mixture Distributions kurtosis; normal mean-variance mixture distributions; portfolio optimization; skewness Advanced Analytics; Financial data processing; Fourier transforms; Investments; Mixtures; Monte Carlo methods; Risk assessment; Value engineering; Characteristic functions; Conditional Value-at-Risk; Efficient frontier; Financial returns; Fourier transformations; Mixture distributions; Portfolio managements; Portfolio optimization; Distribution functions",Risk management
400,"2nd IEOM European International Conference on Industrial Engineering and Operations Management, IEOM 2018","This proceedings contains 404 papers. The 2nd IEOM European International Conference on Industrial Engineering and Operations Management aims at providing ideas and to bridge the gap between the Industrial Engineering and Operations Management theory and its application in solving the most current problems and the latest developments and advancements in the fields of Industrial Engineering and Operations Management. Topics covering industrial issues/applications and theoretical research includes: Business management; Big Data and Analytics; Decision Sciences; E-Business and E-Commerce; Energy and Resource Efficiency; Engineering Economy, Education and Management; Facilities Planning and Management; Global Manufacturing; High Value Manufacturing; Human Factors and Ergonomics; Information Technology and Information Systems; Inventory Management; Knowledge Management; Lean and Six Sigma; Logistics, Transport and Traffic Management; Manufacturing Design and Servitisation; Operations Management and Operations Research; Product design and development; Production Planning and Control; Project Management; Quality Engineering, Control and Management; Reliability and Maintenance; Reverse Logistics and Green Systems; Service Systems and Service Management; Sustainable Operations and Supply Chain Management; Sustainability in Supply Chains and Operations; Sustainability in Manufacturing, Services, Logistics, and Freight transportation; Sustainable Manufacturing; Systems Engineering; Technology Management; Tools for Sustainable Manufacturing and Service Systems Design, Management, and Performance Measurement; Waste Management, etc. The key terms of this proceedings include bio ethanol production, treating brewery wastewater, automated warehouse management system, eco-driving techniques, stochastic optimization, data envelopment analysis application, particle swarm ptimization (PSO), system dynamics modeling, NDN for intelligent transportation systems, HSE performance. 2nd IEOM European International Conference on Industrial Engineering and Operations Management, IEOM 2018  ",Financial management
401,"Engineering project health monitoring: Application of automatic, real-time analytics to PDM systems","Modern engineering work, both project-based and operations, is replete with complexity and variety making the effective development of detailed understanding of work underway difficult, which in turn impacts on management and assurance of performance. Leveraging the digital nature of modern engineering work, recent research has demonstrated the capability and opportunity for implementation of broad-spectrum data analytics for development of detailed management information. Of key benefit is that these analytics may be both real-time and automatic. This paper contextualises such analytics with respect to PDM through exploration of the potential for driving the analytics directly from data typically captured within PDM systems. Through review of twenty-five analytics generated from engineering-based digital assets, this paper examines the subset that may be applied to PDM-driven analysis on systems as-is, examines the coverage of such analytics from the perspective of the potential managerial information and understanding that could be inferred, and explores the potential for maximizing the set of analytics driven from PDM systems through capture of a minimal set of supplementary data. This paper presents the opportunity for integration of detailed analytics of engineering work into PDM systems and the extension of their capability to support project management and team performance. © IFIP International Federation for Information Processing 2018. Engineering project health monitoring: Application of automatic, real-time analytics to PDM systems Analytics; Data analysis; Engineering management Data reduction; Delta sigma modulation; Engineering research; Human resource management; Project management; Real time systems; Research and development management; Analytics; Engineering management; Engineering project; Management information; Modern engineering; Real-time analytics; Recent researches; Supplementary data; Life cycle",Risk management
402,Reservoir performance benchmarking to unlock further development of Malaysian oil fields,"As resource owner and enabler, PETRONAS's Malaysia Petroleum Management (MPM) is entrusted to ensure maximizing recovery efforts from more than 1000 oil reservoirs under production in its portfolio. Performance and recovery from oil reservoirs depends on many factors that can be broadly classified into Reservoir Complexity and how the reservoir has been developed and managed. To undertake a development gap analysis and expectation setting the exercise was undertaken to benchmark reservoir performance against reservoirs of similar complexity. The objective was to take the learnings from better performing reservoirs and explore potential replication in poor performing reservoirs of similar complexity. The main challenge was to establish a single term to define Reservoir Complexity. This term should encompass all the factors like geological, petro physical, rock & fluid etc. that could potentially make the reservoir complex and at the same time also decide on the relative weightage of these parameters posing recovery challenges. Data analytics has been used to accomplish this task and the calibration with reference reservoirs has been achieved. This benchmarking tool can help to internally set targets for all fields where the recoveries have been lower than normally observed, help set EUR numbers for green fields and drive additional development strategies to maximize recoveries in existing fields where they are falling short. When reservoirs of similar complexities are grouped together, they show varying performance indicators viz. recovery factor, decline rates etc. The gap analysis between the reservoirs of similar complexity has helped in identifying poor performing reservoirs and the underlying reasons for underperformance. Learnings from the better performing reservoirs have been incorporated and a detailed action plan has been prepared to improve the performance of these reservoirs. Considering the various ways in which this information can be used, a reservoir complexity benchmark would be a great asset to any major operator or regulator. The workflow has been developed to calculate complexity based on the parameters that are affecting microscopic displacement efficiency, horizontal displacement efficiency and the vertical displacement efficiency. Data analytics has been used to assign weightages to each component posing recovery challenges and derivation of a single number defining complexity on a scale of 0 to 1. This is major improvement on all previous works of this nature attempted in various parts of the world and provides the user with not only the complexity per se but also its distribution. This benchmarking tool has been used for selected fields and has enabled development gap analysis and helped in initiating course correction to unlock more values from the underperforming reservoirs. Copyright 2019, Society of Petroleum Engineers. Reservoir performance benchmarking to unlock further development of Malaysian oil fields  Benchmarking; Data Analytics; Digital storage; Efficiency; Gasoline; Oil field development; Petroleum reservoir engineering; Recovery; Scales (weighing instruments); Benchmarking tools; Development strategies; Horizontal displacements; Microscopic displacement; Performance indicators; Reservoir complexity; Reservoir performance; Vertical displacements; Reservoir management",Monitoring and control
403,Large scale anomaly detection in data center logs and metrics,"Data centers continuously produce largeamounts of data related to their internal operation. This kind of machine-generated data is flowing 24×7×365;however,it isseldom exploited to benefit the health of the processes and the business itself.The information usually comes in two flavors: application events orsystem logs, andperiodic measurementsofsomechanging magnitude (processor load, used memory, etc.).We have, therefore, a mix of structured and unstructured data with a high intrinsic variety, yet containing a high strategic value for those ableto extract it.In this work,we propose adata processing engine foranomaly detection based on a real setup made for an IT services company who wanted to enhance its portfolio of technological solutions. Two were the main challenges we facedas fundamental requirements: makingthe system work in toughbig data environments, and being able to yieldaccurate real-time responses. © 2018 Association for Computing Machinery. Large scale anomaly detection in data center logs and metrics Anomaly Detection; Real-Time Processing; Stream Analytics Computer applications; Computer programming; Anomaly detection; Internal operations; Processing engine; Real time response; Realtime processing; Stream Analytics; Technological solution; Unstructured data; Software architecture",Capacity management
404,A Fuzzy Index Tracking Multi-Objective Approach to Stock Data Analytics,"Index tracking is an passive strategy in portfolio management, it mimics the performance of a benchmark index to construct portfolios for obtaining the average return of the target market. Index tracking has become popular in investors because it possesses the advantages of low cost, high liquidity and lower risk. This paper introduced sensitivity analysis to construct a fuzzy multi-objective index tracking portfolio model with value at risk (SA-IT-VAR-FMOPM) when return rate was set as parabolic fuzzy variable, the sensitivity and VaR factors were considered in the model. An improved particle swarm optimization (IPSO) algorithm was used to search optimal solution for multi-objective problem. To verify the effective of the proposed model, Dow 30 index data were selected to the empirical experiment, the results show the fuzzy multi-objective index tracking portfolio model which considered the sensitivity and VaR factors can obtain more stable portfolio and achieve the average return of target market. © 2018 IEEE. A Fuzzy Index Tracking Multi-Objective Approach to Stock Data Analytics Fuzzy variable; improved particle swarm optimization; index tracking; portfolio model; sensitivity analysis; stock data analytics; value at risk Benchmarking; Commerce; Financial data processing; Investments; Particle swarm optimization (PSO); Reactive power; Risk assessment; Sensitivity analysis; Value engineering; Fuzzy variable; Index tracking; Portfolio model; Stock data; Value at Risk; Target tracking",Monitoring and control
405,Predicting Corporate Venture Capital Investment,"Corporate venture capital (CVC) has been growing rapidly in the past decades. As a critical first step for effective CVC investment, the selection of appropriate portfolio companies is challenging and difficult due to the large number of potential targets and the high uncertainty arising from an investment deal. In this study, we adopt the design science approach and develop a prediction model to support CVC investment decisions by identifying a list of potential investees from a large pool of portfolio companies for a CVC investor. We develop five key features using data science techniques including business proximity, wisdom of crowds in CVC investments, strategic alignment, status differential, and geographic proximity. To evaluate the performance of the proposed model, we plan to conduct experiments on the CrunchBase dataset. Predicting Corporate Venture Capital Investment Corporate venture capital (CVC); CVC investment network; Data science; Prediction model Data Science; Forecasting; Predictive analytics; Corporate venture capital; Geographic proximity; Investment decisions; Potential targets; Prediction model; Status differentials; Strategic alignment; Wisdom of crowds; Investments",Strategic alignment
406,"25th International Conference on Information and Software Technologies, ICIST 2019","The proceedings contain 46 papers. The special focus in this conference is on Information and Software Technologies. The topics include: Fuzzy delphi method with z-numbers; use of chatbots in project management; decision-making algorithms for erp systems in road maintenance work; tabbyxl: Rule-based spreadsheet data extraction and transformation; extending interaction flow modeling language (Ifml) for android user interface components; directed multi-target search based unit tests generation; motivational and goal-oriented viewpoint for architectural modeling of software intensive systems; modeling bimodal social networks subject to recommendation; on similarity measures for a graph-based recommender system; run-time class generation: Algorithms for union of homogeneous and inhomogeneous classes; modelling patterns for business processes; design and implementation of rule execution mechanism for an eguide gamification web service; challenges for automated, model-based test scenario generation; investigation of matrix power asymmetric cipher resistant to linear algebra attack; using r-indiscernibility relations to hide the presence of information for the least significant bit steganography technique; lrc-256, an efficient and secure lfsr based stream cipher; absolutesecure: A tri-layered data security system; visual analytics for cyber security domain: State-of-the-art and challenges; a novel unsupervised learning approach for assessing web services refactoring; comparing static and dynamic weighted software coupling metrics; empirical study on the distribution of object-oriented metrics in software systems; identification of age and gender in pinterest by combining textual and deep visual features; analysis of dispersion and principal component analysis of babblings’ signals from moderate preterm and term infants; a graph-based approach to topic clustering of tourist attraction reviews. 25th International Conference on Information and Software Technologies, ICIST 2019  ",Financial management
407,Predicting Military Construction Project Time Outcomes Using Data Analytics,"Through its Department of Defense (DoD) agencies, and outside contractors, the USA invests billions of dollars each year in military construction (MILCON) projects. Although construction management expertise is gained and significant amount of data are collected from past projects, completing projects on time remains a challenge. This article uses data from 466 MILCON projects to identify key factors that influence project duration and provide a new model to predict project time outcomes. The model generates accurate results and serves as a useful tool in the early phases of a project life cycle. Another key contribution of this study is the employed methodology, which includes the use of available data, targeting of relevant parameters, and development of the predictive model. The contributed methodology is applicable outside of the MILCON domain with the appropriate data set and by targeting the relevant influential factors to create models to predict time outcomes of future projects. © 2018, © 2018 Taylor & Francis. Predicting Military Construction Project Time Outcomes Using Data Analytics Data Analytics; Decision Making & Risk Management; Program & Project Management; Systems Engineering; Military Construction; Predictive Model; Project Duration Data Analytics; Decision making; Information management; Life cycle; Predictive analytics; Project management; Construction projects; Data analytics; Decision making & risk management;; Decision making managements; Military construction; Predictive models; Program & project management;; Programme (project) management; Project duration; Risks management; Risk management",Risk management
408,"3rd Workshop on Mining Data for Financial Applications, MIDAS 2018 and 2nd International Workshop on Personal Analytics and Privacy, PAP 2018 held at 18th European Conference on Machine Learning and Knowledge Discovery in Databases, ECML PKDD 2018","The proceedings contain 12 papers. The special focus in this conference is on Mining Data for Financial Applications. The topics include: Privacy risk for individual basket patterns; exploring students eating habits through individual profiling and clustering analysis; calibrating the mean-reversion parameter in the hull-white model using neural networks; deep factor model: Explaining deep learning decisions for forecasting stock returns with layer-wise relevance propagation; a comparison of neural network methods for accurate sentiment analysis of stock market tweets; a progressive resampling algorithm for finding very sparse investment portfolios; ICIE 1.0: A novel tool for interactive contextual interaction explanations; testing for self-excitation in financial events: A bayesian approach; a web crawling environment to support financial strategies and trend correlation: – extended abstract –; a differential privacy workflow for inference of parameters in the rasch model. 3rd Workshop on Mining Data for Financial Applications, MIDAS 2018 and 2nd International Workshop on Personal Analytics and Privacy, PAP 2018 held at 18th European Conference on Machine Learning and Knowledge Discovery in Databases, ECML PKDD 2018  ",Strategic alignment
409,Hunting High and Low: Visualising Shifting Correlations in Financial Markets,"The analysis of financial assets’ correlations is fundamental to many aspects of finance theory and practice, especially modern portfolio theory and the study of risk. In order to manage investment risk, in-depth analysis of changing correlations is needed, with both high and low correlations between financial assets (and groups thereof) important to identify. In this paper, we propose a visual analytics framework for the interactive analysis of relations and structures in dynamic, high-dimensional correlation data. We conduct a series of interviews and review the financial correlation analysis literature to guide our design. Our solution combines concepts from multi-dimensional scaling, weighted complete graphs and threshold networks to present interactive, animated displays which use proximity as a visual metaphor for correlation and animation stability to encode correlation stability. We devise interaction techniques coupled with context-sensitive auxiliary views to support the analysis of subsets of correlation networks. As part of our contribution, we also present behaviour profiles to help guide future users of our approach. We evaluate our approach by checking the validity of the layouts produced, presenting a number of analysis stories, and through a user study. We observe that our solutions help unravel complex behaviours and resonate well with study participants in addressing their needs in the context of correlation analysis in finance. © 2018 The Author(s) Computer Graphics Forum © 2018 The Eurographics Association and John Wiley & Sons Ltd. Published by John Wiley & Sons Ltd. Hunting High and Low: Visualising Shifting Correlations in Financial Markets CCS Concepts; Human-centered computing → Visual analytics Computation theory; Correlation methods; Financial markets; Investments; Visualization; CCS Concepts; Correlation analysis; Correlation network; Interaction techniques; Interactive analysis; Modern portfolio theories; Multi-dimensional scaling; Visual analytics; Risk assessment",Strategic alignment
410,"4th International Conference on Soft Computing in Data Science, SCDS 2018","The proceedings contain 30 papers. The special focus in this conference is on Soft Computing in Data Science. The topics include: Exploratory analysis of MNIST handwritten digit for machine learning modelling; Improved conditional value-at-risk (CVaR) based method for diversified bond portfolio optimization; ranking by fuzzy weak autocatalytic set; fortified offspring fuzzy neural networks algorithm; forecasting value at risk of foreign exchange rate by integrating geometric brownian motion; fog of search resolver for minimum remaining values strategic colouring of graph; incremental software development model for solving exam scheduling problems; visualization of frequently changed patterns based on the behaviour of dung beetles; applications of machine learning techniques for software engineering learning and early prediction of students’ performance; scalable single-source shortest path algorithms on distributed memory systems; opinion mining for skin care products on twitter; tweet hybrid recommendation based on latent dirichlet allocation; assessing structured examination question using automated keyword expansion approach; improving topical social media sentiment analysis by correcting unknown words automatically; big data security in the web-based cloud storage system using 3d-aes block cipher cryptography algorithm; an empirical study of classifier behavior in rattle tool; clutter-reduction technique of parallel coordinates plot for photovoltaic solar data; data visualization of violent crime hotspots in Malaysia; malaysia election data visualization using hexagon tile grid map; a computerized tool based on cellular automata and modified game of life for urban growth region analysis; simulation study of feature selection on survival least square support vector machines with application to health data. 4th International Conference on Soft Computing in Data Science, SCDS 2018  ",Strategic alignment
411,A Methodology for Characterizing the Correspondence between Real and Proxy Applications,"Proxy applications are a simplified means for stake-holders to evaluate how both hardware and software stacks might perform on the class of real applications that they are meant to model. However, characterizing the relationship between them and their behavior is not an easy task. We present a data-driven methodology for characterizing the relationship between real and proxy applications based on collecting runtime data from both and then using data analytics to find their correspondence and divergence. We use new capabilities for application-level monitoring within LDMS (Lightweight Distributed Monitoring System) to capture hardware performance counter and MPI-related data. To demonstrate the utility of this methodology, we present experimental evidence from two system platforms, using four proxy applications from the current ECP Proxy Application Suite and their corresponding parent applications (in the ECP application portfolio). Results show that each proxy analyzed is representative of its parent with respect to computation and memory behavior. We also analyze communication patterns separately using mpiP data and show that communication for these four proxy/parent pairs is also similar. © 2018 IEEE. A Methodology for Characterizing the Correspondence between Real and Proxy Applications Big data; Performance evaluation; Proxy applications; Workload characterization Big data; Cluster computing; Computer architecture; Hardware; Communication pattern; Distributed monitoring systems; Experimental evidence; Hardware and software; Hardware performance counters; Performance evaluations; Real applications; Workload characterization; Application programs",Risk management
412,Nursing Value User Stories: A Value Measurement Method for Linking Nurse Contribution to Patient Outcomes,"The use of nursing big data sets for value-based measurement is novel. Nursing value measurement depends on the availability of essential data attributes in the electronic health record related to nursing care delivered (what happened, when, and the result seen). Key in measuring value is a standardized structure and format of these attributes for enabling uniform consistent analysis, along with data sets that are sharable and comparable across individuals and groups, time, organization, and practice focus. The foundation of such sharable and comparable data sets would represent at a minimum individual essential nurse care actions and the resulting patient outcome(s). While nurses generate an extraordinary amount of health-related data, healthcare information systems are not designed to collect structured data that reflect the unique attributes of nursing care or support nursing analytic activities that would measure value. More important, the multidimensional features of the nursing process are difficult to untangle and differentiate from other healthcare workers and nonnursing care activities. The complexity of nursing knowledge work has limited the development of nursing data science methods like value measurement and discouraged value versus cost discussions. This article sets out to describe nursing value measurement and an approach that nurse scientists are maximizing through methods adapted from agile project management, including user stories, and business analysis processes to recognize nurses as primary contributors to patient outcomes and value generation. Nursing Value User Story methods deconstruct complex nursing scenarios into user stories that capture nursing actions as standardized data that can be mapped to a common nursing data model. Methods described here are being used in pilot research at Los Angeles Children's Hospital, and results will be available in 2019. © 2019 Wolters Kluwer Health, Inc. All rights reserved. Nursing Value User Stories: A Value Measurement Method for Linking Nurse Contribution to Patient Outcomes Clinical data model; Hospital administration; Nurse cost models; Nurse value; Nursing administration; Nursing user stories; Risk-sharing arrangements; Value-based care Benchmarking; Electronic Health Records; Humans; Models, Nursing; Practice Patterns, Nurses'; article; big data; California; child; controlled study; data science; electronic health record; hospital management; human; information system; nurse; nursing care; nursing knowledge; nursing management; nursing process; scientist; benchmarking; electronic health record; model; nursing practice; standards; statistics and numerical data",Strategic alignment
413,Smart beta floater portfolios in a rising interest rate regime,"Duration risk becomes one of the investors’ most unwanted exposure in their fixed income and credit portfolios in a rising interest rate regime. Since the consensus market analytics is projecting the Fed’s next interest rate hike in December 2017 and around three times in the following year until mid-3% levels by 2019, investors have demonstrated a skewed preference for floating rate notes. In this article, we have investigated risk factor exposures such as size, the value, low risk, and momentum in the universe of 2,588 US dollar denominated investment grade floaters inclusive of rated structured products and plain-vanilla corporate bonds and developed the market-friendly trading policies in a rising interest rate environment. To the best of our understanding, this paper is the first risk factor approach to develop the floater investment strategy. While the factor-based bond pricing papers mainly focused on investable and high yield categories of fixed income corporate bonds, we expanded our investment universe to the structured credit products readily available for the asset allocation decision. A portfolio construction based on a balanced multi-factor protocol shows better risk-return profiles compared to other risk factors assuming the Fed’s subsequent tightening actions. © Rushing Water Publishers Ltd. 2018. Smart beta floater portfolios in a rising interest rate regime Floating rate note; Momentum trade; Risk factors ",Value management
414,Application of Social Media Analytics in the Banking Sector to Drive Growth and Sustainability: A Proposed Integrated Framework,"Large amounts of data sets being generated from social media platforms have led to surge in demand for social media analytics (SMA) use in business operations. Strategic operation of SMA yields a positive impact on marketing activities, customer engagement, risk analysis and assessment product or service design, credit rating of customer profile, customer education and competitive analysis. Banks have started tapping into advanced prescriptive and predictive analytics into a bid to develop insights, managing high costs of compliance and non-compliance such as financial risks and reputational risks thereby generating a significant impact in business operations. Optimisation modelling of business portfolios, products and services offerings are contributing significantly towards achieving a sustainable and profitable growth. This is however, against the background of high volatility and weakening demand for traditional products in the banking industry. Systematic literature review was used in examining how banks are applying social media analytics to improve their business operations. Findings reveals that social media analytics provide concrete solutions which helps to improve revenue streams of banks, providing a guarantee for compliance, survival, sustainability and growth objectives. As a result, a integrated framework is proposed to address the gap existing in literature which lacks a focus on an integrated framework that can assist decision makers on the social media analytics to employ in the banking operations. © 2018 IEEE. Application of Social Media Analytics in the Banking Sector to Drive Growth and Sustainability: A Proposed Integrated Framework Banking industry; Banks; Social media analytics Decision making; Digital storage; Predictive analytics; Product design; Risk analysis; Risk assessment; Sales; Sustainable development; Banking industry; Banks; Integrated frameworks; Large amounts of data; Products and services; Social media analytics; Social media platforms; Systematic literature review; Social networking (online)",Value management
415,Towards a data mining methodology for the banking domain,"Telecoms and financial service industries are leaders in adopting data analytics technologies, practices, and heavily invest into „Big Data? tools and related competence development. However, many of them fail to realize benefits of data-driven decision making and maximize „Big Data? business value due to lack of knowledge on how to frame, approach and tackle complex data analytics projects. Existing data mining methodologies are domain-independent, general, abstract and partially outdated. Several refinements of data mining methodologies have been proposed, but they address specific aspects or tasks and remain fragmented. The goal of this doctoral project is to develop a domain-specific data mining methodology for the financial sector, which (1) represents consolidation of existing body of knowledge, and (2) is validated on the sample of real life data-mining projects. The proposed illustrative case studies approach is based on broad, typical data mining use cases portfolio executed across different geographical regions and business areas of the financial institution. © CEUR-WS. All rights reserved. Towards a data mining methodology for the banking domain Banking; Big data; CRISP-DM; Data mining; Financial services Data mining; Decision making; Finance; Geographical regions; Information systems; Information use; Systems engineering; Banking; Body of knowledge; Competence development; CRISP-DM; Data driven decision; Domain independents; Financial institution; Financial service; Big data",Strategic alignment
416,CRESDA: Extending data landscape of learners,"Current student record management systems in universities focus only on academic activities of learners and are reflected in a formal transcript showing only academic results. To assist learners in whole person development, we proposed a new system model to also record extracurricular activities and their associated achievements with linkage to the corresponding graduate outcomes. In this paper, we proposed a new form of e-portfolio which captures both academic results and non-academic activities; discussed its challenges and resolution; and reviewed a practical implementation, named as CRESDA, in the City University of Hong Kong. By extending data landscape of learners, it provides a platform for learning analytics in future, especially in enhancing the prediction capability of student success. Such an implementation may provide a useful reference to all educational stakeholders. Copyright © 2018 Inderscience Enterprises Ltd. CRESDA: Extending data landscape of learners Achievement; Extracurricular activity; Outcome-based learning; Personal development China; Hong Kong; higher education; implementation process; learning; stakeholder; university sector",Strategic alignment
417,Identifying critical issues in smart city big data project implementation,"Many cities across the globe are adopting smart city initiatives, as smart city holds the promise of better quality of life and equity for city’s residents, more efficient use of city’s infrastructure, and more effective city planning. Big data analytics is the backbone of smart city and the drive engine to achieve smart city’s promises. However, statistics indicate that more than 50% of big data projects fail; they either never finish or do not offer the expected value. Resulting in severe consequences as such projects tends to be expensive and require allocating the organization’s best resources while doing the project. This is even more crucial in the case of smart city, as cities usually have limited budget and resources. This paper conducted literature review and perspectives analysis to identify challenges, which can cause big data projects to fail, with focus on smart city related big data projects. The goal is to offer a list of challenges, that a project manager can consider as an initial list of risks for the upcoming project, and evaluate the city’s readiness against each of them. © 2018 Association for Computing Machinery. Identifying critical issues in smart city big data project implementation Big data; Challenges; Project management; Risk; Smart cities Big data; Budget control; Data Analytics; Digital storage; Project management; Risks; Challenges; Critical issues; Expected values; Literature reviews; Project implementation; Project managers; Projects fail; Quality of life; Smart city",Stakeholder management
418,Ai robo-advisor with big data analytics for financial services,"Robo-Advisors has been growing attraction from the financial industry for offering financial services by using algorithms and acting as like human advisors to support investors making investment decisions. During the investment planning stage, portfolio optimization plays a crucial role, especially for the medium and long-term investors, in determining the allocation weight of assets to achieve the balance between investors expectation return and risk tolerance. The literature on the topic of portfolio optimization has been offering plenty of theoretical and practical guidance for implementing the theory; however, there is a paucity of studies focusing on the applications which are designed for Robo-Advisors. In this research, we proposed a modular system and focused on integrating big data analysis, deep learning method and the Black-Litterman model to generate asset allocation weight. We developed a portfolio optimization module which takes the information from a variety of sources, such as stocks prices, investor profile and the other alternative data, and used them as input to calculate optimal weights of assets in the portfolio. The module we developed could be used as a sub-system for Robe-Advisors, which offers a customized optimal portfolio based on investors preference. © 2018 IEEE. Ai robo-advisor with big data analytics for financial services Big Data Analysis; Black-Litterman; Deep Learning; Financial Technology; Investment Management; Portfolio Optimization; Robo-Advisors Data handling; Deep learning; Financial data processing; Financial markets; Information analysis; Investments; Big Data Analytics; Black-Litterman; Investment decisions; Investment management; Investment planning; Portfolio optimization; Practical guidance; Robo-Advisors; Big data",Strategic alignment
419,General electric uses simulation and risk analysis for silicon carbide production system design,"This article describes a model we developed to manage risk and value for silicon carbide (SiC) manufacturing at General Electric (GE). Our goal is to improve GE's understanding of SiC fabrication design at the New York Power Electronics Manufacturing Consortium (PEMC) facility. Using this model, we determine the production-capacity risk profile of the PEMC facility and identify an equipment portfolio that minimizes the expected production shortfall, while meeting the capital expenditure (CAPEX) budgetary constraints for each year of the planning horizon. We further present selected operational strategies to support the solution to the equipment-portfolio optimization problem. We expect the impact of the analytical findings on the SiC production system design to be an improvement of 67% in mean annual throughput and an increase of less than 1% in CAPEX. © 2019 INFORMS. General electric uses simulation and risk analysis for silicon carbide production system design Analytics; Operations management; Portfolio selection; Production; Risk analysis; Simulation ",Capacity management
420,"6th World Conference on Information Systems and Technologies, WorldCIST 2018","The proceedings contain 146 papers. The special focus in this conference is on Information Systems and Technologies. The topics include: Freemium project management tools: Asana, freedcamp and ace project; agile analytics: Applying in the development of data warehouse for business intelligence system in higher education; satisfaction with e-participation: A model from the Citizen’s perspective, expectations, and affective ties to the place; Coalition-OrBAC: An agent-based access control model for dynamic coalitions; health data analytics: A proposal to measure hospitals information systems maturity; supply chain challenges with complex adaptive system perspective; understanding the adoption of business analytics and intelligence; the planning process integration influence on the efficiency of material flow in production companies; time difference of arrival enhancement with ray tracing simulation; E-Mail client multiplatform for the transfer of information using the SMTP java protocol without access to a browser; legal and economic aspects of virtual organizations; CRUDi framework application – Bank company case study; GRAPHED: A graph description diagram for graph databases; capabilities and work practices - A case study of the practical use and utility; systematic review of the literature, research on blockchain technology as support to the trust model proposed applied to smart places; an architecture for a viable information system; smart Bengali idiomatic translator service using morphological marketing technique; An approach for knowledge extraction from source code (KNESC) of typed programming languages; measuring the quality of humanitarian information products: Insights from the 2015 Nepal earthquake; assessing review reports of scientific articles: A literature review; mexican Spanish affective dictionary. 6th World Conference on Information Systems and Technologies, WorldCIST 2018  ",Strategic alignment
421,Advances and challenges for scalable cloud-based infrastructure for building data analysis and simulation,"In the commercial building sector, retro-commissioned and new constructions alike are implementing the use of highly integrated and connected Building Information Systems (BISs) to use fewer resources, improve occupant health and productivity and reduce life-cycle costs. Even if programs such as Leadership in Energy and Environmental Design (LEED) are becoming more and more popular, single factors such as horizontal integration at the portfolio level can drive this change leading to an increase in simulation and forecasting demand. BISs generate large amounts of data from various sources such as control networks or utilities. This data is critical in applications such as continuous commissioning through Automated Fault Detection and Diagnostics (AFDD) or predictive analytics. Moreover, depending on the analysis constraints such as computational maximum runtime or overall cost as well as results' availability and presentation, current techniques can prove to be challenging to use or integrate. This paper aims to introduce to the building performance simulation's space, techniques from other fields such as computer science and data analytics to help improve quality, reproducibility, scalability of workflows used by this industry and research community. Cloud computing and open source technologies described in this paper can help answer many of the aforementioned challenges. When dealing with big datasets, file formats such as Comma Separated Values (CSV) offer little compression resulting in large files creating unnecessary costs and increased query complexity. File formats used by distributed query engines such as Parquet can offer 98% size reduction while being easily queryable using the Structured Query Language (SQL) at the expense of time compared to relational databases which can be subsequently used as a caching mechanism. Containers, often used for cloud-based applications, can also be used for simulation offering scalable, reproducible and hardware agnostic environment to deploy large scale analysis at the expense of a little performance overhead, less than full - fledged Virtual Machines (VMs). Moreover using Infrastructure-as a-Service (IaaS) can allow significant cost reductions by paying only for what is needed in terms of compute and/or memory. For example, running AFDD on 200+ buildings can cost on the order of less than a US dollar per day which is equivalent to a quarter the cost of running a local server. Web technologies such as Javascript-based User Interfaces (UIs) can as well be used to distribute and access simulation results as well as managing the simulation engine altogether remotely. This enables collaboration of international team members as well as deployment of analytics to international or remote clients. © 2019 Building Simulation Conference Proceedings. All rights reserved. Advances and challenges for scalable cloud-based infrastructure for building data analysis and simulation  Aerospace industry; Cost benefit analysis; Cost reduction; Data Analytics; Digital storage; Distributed database systems; Engines; Fault detection; Large dataset; Life cycle; Office buildings; Open systems; Predictive analytics; Query languages; Simulation platform; User interfaces; Analysis and simulation; Automated fault detection and diagnostics; Building information system; Building performance simulations; Cloud-based applications; Continuous commissioning; Leadership in energy and environmental designs; Structured query languages; Infrastructure as a service (IaaS)",Monitoring and control
422,Many-objective portfolio optimization approach for stormwater management project selection encouraging decision maker buy-in,"Although formal simulation-optimization approaches have been shown to be able to identify near-optimal outcomes for a range of stormwater management problems, stakeholder acceptance of these solutions can be problematic, especially if there is a lack of familiarity with the optimization processes and simulation model used to arrive at these solutions. To address this problem, a portfolio optimization problem formulation is introduced that allows stormwater best management practices (BMPs) to be evaluated by stakeholders before the portfolio selection process. This enables the search space to be constrained before the BMP optimization process, ensuring that model results are transparent and only represent solutions that are trusted by experienced practitioners. This has the effect of reducing reliance on simulation-optimization involving complex stormwater simulation models, and increasing buy-in to the optimization results. The portfolio optimization formulation is applied to a catchment management problem in Australia, using a typical many-objective optimization approach including visualization techniques. © 2018 Many-objective portfolio optimization approach for stormwater management project selection encouraging decision maker buy-in Many-objective optimization; Multi-criteria decision analysis; Portfolio optimization; Stormwater management; Visual analytics; Water sensitive urban design Australia; Catchments; Constrained optimization; Engineering geology; Financial data processing; Storm sewers; Storms; Visualization; Many-objective optimizations; Multi-criteria decision analysis; Portfolio optimization; Storm-water managements; Visual analytics; Water sensitive urban designs; best management practice; catchment; decision making; multicriteria analysis; multiculturalism; optimization; project assessment; stakeholder; stormwater; urban design; water management; Decision making",Strategic alignment
423,Reexamining the procurement management knowledge area utilizing applied data analytics,"Procurement management is one of the major knowledge areas identified in the Project Management Body of Knowledge (PMBOK). While project management is a relatively mature field, data analytics provides a plethora of opportunities to reexamine existing project management knowledge areas under a new scope. Given the rise in prevalence of supply chain management and data analytics, we aim at critically reexamining the procurement management process from the project management perspective. We specifically examine the notion and feasibility of blockchain ledger-keeping to track historical transaction data in a records management system. The goal of this research is to draw upon supplier historical information to evaluate project considerations such that the cost and procurement risks shift away from the procurer (or the buyer) to the supplier (or the seller). Furthermore, we study potential contract structures that support this concept. © 2018 Institute of Industrial Engineers (IIE). All rights reserved. Reexamining the procurement management knowledge area utilizing applied data analytics  Project management; Records management; Supply chain management; Data analytics; Historical information; Knowledge areas; Mature fields; Procurement management; Project Management Body of Knowledge; Project management knowledge; Transaction data; Information management",Risk management
424,Learning analytics for formative assessment in engineering education,"The development of skills in the engineering education is one of the issues that generate greater interest at present. Thanks to Learning Analytics, we found an excellent opportunity to offer a quality competence assessment of our engineering students. Research in Learning Analytics currently focuses on applying these techniques to find out how the student learns and to improve teaching/learning processes. A key aspect in improving these processes is the assessment of general competences, which constitutes key learning in engineering students and has thus been identified as a need that can be met by Learning Analytics. This article presents two related studies conducted at the University of Deusto. The first study wants to show that it is possible to carry out an assessment of the project management competence through the analysis of the data that is obtained when the students interact with certain tools for the management of projects. In this sense, in the first study conducted with 93 students in the academic year 2014–2015, it compares the automatic assessment performed with Learning Analytics and the manual assessment carried out by the teacher. Another objective of this first study is to compare the validity at the time to assess the project management competence of the three technological tools used in the study. In the second study conducted with 227 students in the academic year 2015–2016, an assessment model is designed based on analytical data that is extracted from even more complex technological tools. In this second study the objective is to demonstrate that the use of Learning Analytics assessment to carry out continuous monitoring and provide feedback to the students, directly influences their capacity to manage a project and therefore, leads to an improvement in their results. The model designed in both studies for analysis is described in this paper, in addition to the methodology and research carried out. © 2018 TEMPUS Publications. Learning analytics for formative assessment in engineering education Competence assessment; Engineering education; Formative assessment; Learning analytics; Project management; Self-regulated learning Education computing; Engineering education; Project management; Teaching; Competence assessments; Continuous monitoring; Formative assessment; Learning analytics; Learning in engineering; Project management competence; Self-regulated learning; Teaching/learning process; Students",Monitoring and control
425,"2nd International Conference on Emerging Technologies in Computing, iCETiC 2019","The proceedings contain 24 papers. The special focus in this conference is on Emerging Technologies in Computing. The topics include: Smart Airports: Review and Open Research Issues; context-Aware Indoor Environment Monitoring and Plant Prediction Using Wireless Sensor Network; achieving Fairness by Using Dynamic Fragmentation and Buffer Size in Multihop Wireless Networks; a Data Science Methodology for Internet-of-Things; a Comparison of the Different Types of Risk Perceived by Users that Are Hindering the Adoption of Mobile Payment; proposing a Service Quality Framework for Mobile Commerce; Sentiment Analysis in E-commerce Using SVM on Roman Urdu Text; prediction and Optimization of Export Opportunities Using Trade Data and Portfolio; automatic Speech Recognition in Taxi Call Service Systems; accuracy Comparison of Machine Learning Algorithms for Predictive Analytics in Higher Education; a Discussion on Blockchain Software Quality Attribute Design and Tradeoffs; generic Framework of Knowledge-Based Learning: Designing and Deploying of Web Application; the Bearing of Culture upon Intention to Utilize D-learning Amongst Jordanian University Students: Modernizing with Emerging Technologies; tracking, Recognizing, and Estimating Size of Objects Using Adaptive Technique; Analysis Filling Factor Catalogue of Different Wavelength SODISM Images; building Energy Management System Based on Microcontrollers; an Efficient Peer-to-Peer Bitcoin Protocol with Probabilistic Flooding; economic Impact of Resource Optimisation in Cloud Environment Using Different Virtual Machine Allocation Policies; SOSE: Smart Offloading Scheme Using Computing Resources of Nearby Wireless Devices for Edge Computing Services; Securing Big Data from Eavesdropping Attacks in SCADA/ICS Network Data Streams through Impulsive Statistical Fingerprinting. 2nd International Conference on Emerging Technologies in Computing, iCETiC 2019  ",Financial management
426,"2nd European and Mediterranean Structural Engineering and Construction Conference, EURO-MED-SEC-2 2018","The proceedings contain 62 papers. The special focus in this conference is on European and Mediterranean Structural Engineering and Construction. The topics include: The elements of natural concept in sustainable building skins; impact of covering irrigation canals on evaporation rates in arid areas; prioritizing sidewalk upgrade projects to maximize compliance with accessibility requirements; crowd-sourced visual data collection for monitoring indoor construction in 3d; diversity and knowledge sharing in construction: A mathematical model; simulation-based analytics: Advancing decision support in construction; on the limitations of the earned value management technique to anticipate project delays; on modeling activity crashing and overlapping: A first algorithm; human resource allocation in engineering design companies; the cost impact of front end engineering design (Feed) accuracy for large industrial projects; research trends in evm limitations and proposed improvements; information management technologies for improving earned value quantification; a simplified method for physical progress measurement; an investigation into crane and scaffold safety in construction industry; improving construction education: The role of collaborations between higher institutions and the construction industry; product versus process: A jungle community case study four years later of technology transfer; the impact of posts on interest in a department of construction management facebook page; meches house: The importance of choosing the right beneficiary on a post disaster alternative construction; reforming undergraduate structural engineering education: A leap of faith; The impact of the construction computing software (CCS) ‘candy’ course: Construction management students’ perceptions; multi-method model for infrastructure portfolio management; delay in construction industry: Causes, effects and mitigation measures. 2nd European and Mediterranean Structural Engineering and Construction Conference, EURO-MED-SEC-2 2018  ",Value management
427,"10th International Development Informatics Association Conference, IDIA 2018","The proceedings contain 20 papers. The special focus in this conference is on Development Informatics Association. The topics include: Enablers of egalitarian participation: Case Studies in underserved communities in South Africa. Processes of creativity “Not for the sake of it”; The role of local bricoleurs in sustaining changing ICT4D solutions; building empathy for design thinking in e-Health: A Zimbabwean case study; coming to terms with telemetry: A scoping review; towards a provisional workplace e-learning acceptance framework for developing countries; localize-It: Co-designing a community-owned platform; the role of the marginalized and unusual suspects in the production of digital innovations: Models of innovation in an african context; Investigating business intelligence (BI) maturity in an African developing country: A mozambican study; The project management information system as enabler for ICT4D achievement at capability maturity level 2 and above; It should be there, but it is hard to find: Economic impact of ICT in sub-saharan economies; identifying the constructs and agile capabilities of data governance and data management: A review of the literature; Domestication of ICTs in community savings and credit associations (Stokvels) in the western cape, South Africa; Rethinking ICT4D impact assessments: Reflections from the siyakhula living lab in South Africa; supporting the identification of victims of human trafficking and forced labor in Thailand; youth unemployment in South Africa and the socio-economic capabilities from mobile phones; smartphone paradoxes in working mothers’ pursuit of work-life balance; development outcomes of training for online freelancing in the Philippines. 10th International Development Informatics Association Conference, IDIA 2018  ",Governance
428,Predicting students’ performance in a virtual experience for project management learning,"This work presents a predictive analysis of the academic performance of students enrolled in project management courses in two different engineering degree programs. Data were gathered from a virtual learning environment that was designed to support the specific needs of the proposed learning experience. The analyzed data included individual attributes related to communication, time, resources, information and documentation activity, as well as behavioral assessment. Also, students’ marks on two exams that took place during the first half of the course were considered as input variables of the predictive models. Results obtained using several regression and classification algorithms –support vector machines, random forests, and gradient boosted trees–confirm the usefulness of Educational Data Mining to predict students’ performance. These models can be used for early identification of weak students who will be at risk in order to take early actions to prevent these students from failure. Copyright © 2019 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved Predicting students’ performance in a virtual experience for project management learning Educational Data Mining; Higher Education; Project Management; Students’ Performance Computer aided instruction; Data mining; Decision trees; E-learning; Education computing; Predictive analytics; Project management; Support vector machines; Support vector regression; Academic performance; Behavioral assessment; Classification algorithm; Educational data mining; Engineering degrees; Higher education; Learning experiences; Virtual learning environments; Students",Risk management
429,Application of Business Intelligence instrumental tools for vis-ualization of key performance indicators of an enterprise in tel-ecommunications,"The article represents the experience of practical application of visual analytics tools that allow to make deliberated and reasonable managerial decisions based on the visualization of large volume of information using Business Intelligence tools. The key feature of the pro-posed approach is the development of visual models of key performance indicators (KPI) at the strategic level of management of the enterprise. The developed models are considered as a tool for justification of managerial decisions, building business models of high-tech compa-nies, measuring the efficiency of functioning and evaluating the effectiveness of the develop-ment of the selected activities. The Business Intelligence tool was used to build a number of visual models for a high-tech enterprise in telecommunications. The novelty of the proposed approach lies in the application of intensively implemented in both Russian and Western companies performance management and management decision making technologies which are aimed at improving competitiveness, import substitution, cost reduction and optimiza-tion of business processes. The requirements of a dynamically changing environment, short-ening the products' life cycles, global competition with the necessity lead to create specialized visual situational business models built on the basis of powerful automated systems of busi-ness planning and graphics, data analysis and processing of information arrays. © National Research Nuclear University. All rights reserved. Application of Business Intelligence instrumental tools for vis-ualization of key performance indicators of an enterprise in tel-ecommunications Business Analytics; Business Intelligence; Data Mart; Data Warehouse; Effi-ciency Assessment; Heat Map; Key Performance Indicators; Market Analysis Control Panel; Project management; Sales Management Automation; Benchmarking; Competition; Competitive intelligence; Cost reduction; Data handling; Data warehouses; Information analysis; Life cycle; Managers; Project management; Sales; Visualization; Business analytics; Control panels; Data mart; Effi-ciency Assessment; Heat maps; Key performance indicators; Sales management; Decision making",Monitoring and control
430,A cybersecurity prospective on industry 4.0: Enabler role of identity and access management,"Rapid development of intelligent machinery is expected to be foundational to prospective evolution of Industry 4.0, especially for traditional industries such as the energy sector. Nanodevices, context-aware sensors, and advanced forms of robotics are expected to formulate fully autonomous cyber-physical systems capable of replacing contemporary human-operated machinery used to perform significant construction activities in hydrocarbon facilities projects. For instance, oil & gas pipeline construction projects may transform into autonomous processes through means of such intelligent cyber-physical machines leveraging contextual awareness, data mining, and analytics techniques. Such projects typically present production lifecycle vectors comprising of material procurement, logistics, and customer demand, in consistency with typical Industry 4.0 business structuring. The intelligence introduced within such vectors present significant impacts on cybersecurity factors, including production integrity, availability, and relevant confidentiality. In this paper, we study influencing factors of cybersecurity on prospective Industry 4.0's main subjects: Industrial Internet of Things (IIoT), extending to those playing role in hydrocarbon construction management. We present the status quo in IIoT cybersecurity challenges and mitigations mechanisms and strategies, in sync with potential developments of advanced cyber-physical industrial machines. The relationship of prospective IIoT advances in tandem with possible cybersecurity challenges is explored. Consequently, a gap analysis is conducted to highlight essential cybersecurity controls and whether they are already present or to be developed. We use identified gaps as engineering elements for a suggested Identity and Access Management (IAM) framework capable of: devising appropriate physical and logical controls, meeting predefined business risk profile, and assuring compliance with state or industrial compliance criteria. To qualitatively ensure validity of the framework, we draw similarity of cybersecurity challenges from similar manufacturing disciplines - to infer applicability, and apply our framework to similar challenges in these industries. We ultimately conclude effectiveness of IAM as an enabler safeguard of Industry 4.0 against relevant cybersecurity issues. The summary of our research results is presented as follows: an inventory of major categories of risks applicable to Industry 4.0 cyber-physical subjects, potential gaps in relevant cybersecurity controls, and an IAM framework made of factors designed to address the associated risks. We present a set of effectively implementable blueprints of the IAM framework developed using the Open Group Architecture Framework (TOGAF) technique, a premier methodology in the enterprise architecture modeling. Novelty of our work is primarily stemmed from the idea of targeting the hydrocarbon construction management domain with firm forms of cyber-physical subjects, along with demonstrating roles of IAM in protecting the subjects' intelligent capabilities by enforcing IAM's cybersecurity controls. Our IAM framework will be flexible to adapt to theoretically all roles that intelligent cyber-physical machines can be designed for, and across the entire lifecycle vectors. © 2019, International Petroleum Technology Conference A cybersecurity prospective on industry 4.0: Enabler role of identity and access management  Compliance control; Construction industry; Data mining; Embedded systems; Gasoline; Hydrocarbons; Industry 4.0; Life cycle; Machinery; Project management; Construction activities; Construction management; Construction management domain; Enterprise architecture modeling; Hydrocarbon facilities; Identity and access managements; Open group architecture frameworks; Pipeline construction projects; Cyber Physical System",Capacity management
431,"Proceedings of the International Conference on Industrial Engineering and Operations Management Pretoria, IEOM 2018","This proceedings contains 192 papers. The conference aims to provide a forum to share the ideas and recent developments in the field of Industrial Engineering and Operations Management. The conference topics covering industrial issues/applications and academic research include Artificial Intelligence; Automation and Control; Business Management; Case Studies; Computers and Computing; Construction Management; Cyber Security; Data Analytics and Big Data; Decision Sciences; Defense Management Science; Design and Analysis; E-Business and E-Commerce; E-Manufacturing and RMS (Reconfigurable Manufacturing Systems); Energy; Engineering Economy; Engineering Education; Engineering Management; Entrepreneurship and Innovation; Environmental Engineering; Facilities Planning and Management; Financial Engineering; Healthcare Systems; Human Factors and Ergonomics; IE / OM in Africa; Information Technology and Information Systems; Inventory Management; Lean; Logistics Management; Manufacturing; Material Flow Cost Accounting (MFCA); Modeling and Simulation; Occupational Safety and Health (OSH); Operations Management; Operations Research; Optimization; Product Lifecycle Management (PLM); Production Planning and Management; Project Management; Quality Control and Management; Reliability and Maintenance; Sensors and Sensing Systems; Service Systems and Service Management; Six Sigma; Software Testing and Quality Assurance; Statistics; Supply Chain Management; Sustainability and Green Systems; Sustainability in Supply Chain, Enterprise Operations and Strategies including Food Supply Chain (Special Track, organized by Prof. Jose Arturo Garza-Reyes, Derby Business School, University of Derby, UK and Dr. Vikas Kumar, Bristol Business School, University of the West of England, UK); Sustainable Manufacturing; Systems Engineering; Technology Management; Total Quality Management; Transportation and Traffic; Waste Management; Work Design, Measurement, Standardization and ISO, etc. The key terms of this proceedings include agro waste bio adsorbents, electromagnetic wave attenuation, photocatalysis, application of computer aided technologies, End-of-Life Tyre (EOLT) Management, disaster risk reduction, design optimization, integrating quality management systems, remote monitoring application, water distribution system. Proceedings of the International Conference on Industrial Engineering and Operations Management Pretoria, IEOM 2018  ",Value management
432,Modular grinding stations (MGS) for sustainable growth,"Cement Industry is changing the business paradigm, searching for scaled ways of growth, controlling risks and avoiding high investments. Modular Grinding Stations (MGS) are an innovative solution that allows Cement Producers to improve their scheduling and business intelligence. MGS in Cement Industry has had Momentum since 2011. This matured technology - proven worldwide and improved year over year - is evolving to larger sizes and offering a wide range of possibilities. All the Project Management Stages (Engineering, Procurement, Construction and Operation) are being designed into a virtuous retrofit circuit, with the result of Standardization. Standardized Installations are minimizing Capex deviations and on-site erection costs. MGS have also introduced the concept of Relocatable Installations. Having moveable assets, makes the decision taking easier, shortens the time in the market of production facilities and eases financing and permitting. This technical paper tries to show the advantages and challenges of this MGS technology, showing real case scenarios that illustrate the potentiality of this innovation. © 2018 IEEE. Modular grinding stations (MGS) for sustainable growth Adaptive Scheduling; Business Intelligence; Grinding Plant; Innovation Management; Portable Installation; Reliable Engineering; Risk Analysis; Scaled Investment; Standardized Manufacture; Sustainable Development Cements; Competitive intelligence; Grinding (machining); Information analysis; Investments; Risk analysis; Risk assessment; Scheduling; Sustainable development; Adaptive scheduling; Grinding plants; Grinding stations; Innovation management; Innovative solutions; Production facility; Real case scenarios; Sustainable growth; Cement industry",Risk management
433,Anticipating new directions in contract clinical outsourcing,"Last year - 2017 - nearly 190 merger and acquisition transactions involving contract clinical research service providers were consummated making it one of the most active periods on record. The contract clinical services landscape is undergoing major transformation signaling a profound transition for the global drug development enterprise. Market leading contract research organizations (CROs) are integrating project management and study conduct services with the hope of gleaning scale efficiencies and economics. The top four largest CROs have repositioned themselves to meet demand for higher levels of patient engagement as well as rich, advanced and continuous data and analytics supporting learning health and research systems. At the same time, niche service providers offering specialized capacity and expertise are seeing strong growth in demand. © 2018 Compare Networks, Inc. Anticipating new directions in contract clinical outsourcing  ",Stakeholder management
434,Integration of big-data ERP and business analytics (BA),"Technology advancements in cloud computing, big data systems, No-SQL database, cognitive systems, deep learning, and other artificial intelligence techniques make the integration of traditional ERP transaction data and big data streaming from various social media platforms and Internet of Things (IOTs) into a unified analytics system not only feasible but also inevitable. Two steps are prominent for this integration. The first, coined as forming the big-data ERP, is the integration of traditional ERP transaction data and the big data and the second is to integrate the big-data ERP with business analytics (BA). As ERP implementers and BA users are facing various challenges, managers responsible for this big-data ERP-BA integration are also seriously challenged. To help them deal with these challenges, we develop the SIST model (including Strategic alignment, Intellectual and Social capital integration, and Technology integration) and propose that this integration is an evolving portfolio with various maturity levels for different business functions, likely leading to sustainable competitive advantages. © 2018 Integration of big-data ERP and business analytics (BA) Big data; Business analytics; ERP; Maturity model; Portfolio perspective; Sustainable competitive advantages Cognitive systems; Competition; Data integration; Deep learning; Distributed computer systems; Enterprise resource planning; Integration; Media streaming; Sustainable development; Artificial intelligence techniques; Business analytics; Internet of thing (IoTs); Maturity model; Portfolio perspective; Social media platforms; Sustainable competitive advantages; Technology Integration; Big data",Strategic alignment
435,Achieving decision-making quality and organisational agility in innovation portfolio management in telecommunication 4.0,"Innovation portfolio management (IPM) decision-making capability has become one of the most important organisational competencies to survive and be sustained in the hyper-competitive market of the telecommunications equipment industry. As Telecommunication 4.0 brings up new challenges and complexities created by open innovation and integrated hardware and software, there is a necessity to further demonstrate how organisations leverage networking with partners and exploit internal organisational structural flexibility to fully gain IPM decision-making quality and organisational agility. This study aims to explore how IPM decision-making quality and organisational agility are influenced by a business intelligence system with the capability of providing profound market knowledge and orchestrating the organisations' networking capabilities in leveraging external capabilities with internal factors embedded in a nimble organisational structure that subsequently influences decision-making accuracy. Furthermore, the relationship of antecedents-a business intelligence system, networking capability, and nimble organisational structure-is elaborated in our conceptual research model. Exploratory case studies with four telecommunication equipment companies in Indonesia and a literature review are used to examine the relationship between IPM decision-making quality antecedents. Deriving from information processing theory, dynamic capability, and resource-based theory, the originality of this study lies in how organisations exploit not only internal resources and flexible organisational structure, but also in how it accesses, configures, and leverages external network resources embedded in business partners to come up with full access to IPM decision-making quality in an open innovation era. © 2019 Primrose Hall Publishing Group. Achieving decision-making quality and organisational agility in innovation portfolio management in telecommunication 4.0 agility; business intelligence; decision-making; Innovation portfolio management; networking capability; nimble organisational structure ",Value management
436,"12th International Scientific and Practical Conference on Environment. Technology. Resources, 2019","The proceedings contain 173 papers. The special focus in this conference is on Environment Technology Resources. The topics include: Development of software for design ontological representations of production technologies; the method of automated building of domain ontology; research of human fatigue and measurement parameters for workability assessment; analysis of research trends in agricultural engineering; data science approach for it project management; informational warfare – influence on informational structures; modern algorithms to identify plagiarism; comparative evaluation of the rule based approach to representation of adaptation logics; process-event approach for operational risk estimation; analysis of research trends in the fieldof mechanical engineering; Comparison of algorithms for construction detection using airborne laser scanning and nDSM classification; a method football team model optimization and application of the optimization control; potential benefits of web-based idea management system based on practical evidence; brain connections analysis using graph theory measures; information technology competency management in the financial sector in Latvia; forecasting missing data using different methods for road maintainers; adaptive Kalman filter forecasting for road maintainers; gamification framework for software development project processes; A methodology to diagnose ICT governance process based on ISO/IEC 38500 standard. Case study: Ecuadorian retail organization; development and analysis of authentication method for Iot devices software on the network using blockchain technologies; modeling of the financial system using the concept of vacuum polarization; fuzzy logic procedure for drawing up a psychological profile of learners for better perception in courses; Neural network classification method for aircraft in ISAR images; safety of artificial superintelligence. 12th International Scientific and Practical Conference on Environment. Technology. Resources, 2019  ",Financial management
437,Collaborative workspace for employee engagement leveraging social media architecture,"Employee Engagement had long been a challenge for any organization in the world. The problem has been even worsened due to the enhanced endeavor on digital transformation, where most of the employees are left behind. In addition to that, the new (and young) generations who are heavily social media-oriented face substantial challenges to engage themselves into the workplaces, which are either very much traditional or much advanced to adopt and require substantial skills. The objective of this paper is to present a digital collaborative framework, intelligent digital Eco-System (iDES) for bringing multiple organizations and employees along with their work and communication into one single platform that engages all generations of employees. The framework is built addressing the architecture of a new of way of project management, the ""Open Project Management"" (OPM). Such framework will allow workers to engage in the organization by bringing personal lifestyle of using social network, which should trigger a cultural step change in the workplace. A digital collaborative work-space mimics popular social media platform. In the workspace the employees in the network are connected thru projects, and allows making of attractive dynamic reports by incorporating drawing visuals, complex analytics, cloud applications, videos and visualize their existing documents or presentations or spreadsheets to create their work-image. A project management capability, that allows synergism of traditional and agile approach, is introduced to have a truly team driven project management process for valued tasks, including an effective communication tool with a scheduler, video conferencing, and file sharing. eLearning modules facilitating easy training and knowledge share are also introduced for technology adoptions. Overall, the workspace is designed to have most of the work process within the system, allowing employees to display their work out of their siloed room structure to an open digital space. Such architecture of collaborative work space allows a creation of self-image, like social media, into the workplace, enhancing employee motivation towards work, increases multi generation employee participation in digital transformation and providing a transitional phase to any employee through the cultural step change required to implement digital transformation objectives. The comprehensive capability of performing tasks and engagement into one single place will create substantial amount of data related to human resources which can be utilized for further enhancement of employee engagement and empowerment. The novelty of the digital workspace lies in empowering people in many details and helps driving business by making them a fundamental building block in the digital transformation endeavor. © 2019, Society of Petroleum Engineers Collaborative workspace for employee engagement leveraging social media architecture  Gasoline; Image enhancement; Network architecture; Project management; Social networking (online); Video conferencing; Collaborative framework; Collaborative workspace; Effective communication; Employee participation; Fundamental building blocks; Management capabilities; Project management process; Social media platforms; Human resource management",Financial management
438,A predictive model to identify Kanban teams at risk,"Kanban, which is an agile process methodology as well as a means to implement lean principles, has been growing as a project management framework across a range of domains, including manufacturing, software development and data science. This paper explores, for teams using Kanban, the ability to predict low team performance. The prediction is based on an analytical model that uses specific project metrics that can be collected via the team's visual Kanban board. Specifically, data from 80 teams was used to build and test machine learning models that predict teams at risk for delivering low quality results. The model developed was significantly better than the baseline situation of thinking that all teams were at risk. While this analysis was done within a data science project context, the results are likely applicable across a range of information system projects. © 2019-IOS Press and the authors. All rights reserved. A predictive model to identify Kanban teams at risk data science project management; Kanban; metrics; project management; team performance ",Risk management
439,DataGMA: Data-driven culture creation in shale field operations,"The objective of this project, named DataGMA after Data Governance, Data Management and Data Analytics, is to transform YPF shale operations to a data-driven culture. Recognizing that Data Governance, Management & Analytics (DGM&A) problem solving approach requires a wide interaction between the concerned players is the first step to delineate a sustainable and scalable outcome regarding DGM&A best practices. Increasing data-awareness leads to an active engagement of the whole organization around the data life cycle which, in turn, is the basis of a DGM&A virtuous cycle. This paper presents an integrated approach through a multidisciplinary task force (DGM&A + IT + Upstream Technical Staff) designed not only to tackle the data challenges YPF faces but, and more importantly, to build data awareness and engagement around the data life cycle among the Upstream Technical Team. This combined effort assists the organization in understanding the broader picture of the situation through an exhaustive assessment and an accurate diagnosis. More than 20 initiatives were selected to deliver, with a more consistent, yet flexible, work plan, using different tools as design thinking, agile teams and Business Intelligence. Keeping data-related issues siloed only around the DGM&A influence pulls organizations away from the great opportunity to improve their data governance and exploitation. The implemented approach expanded the process to other areas encouraging a more efficient use of the available resources, both human and material. Crucial elements for the advance and success of this initiative are: strong sponsorship from executive management, champions in every technical discipline (D&C, G&R, Production), project management leadership, data leadership (DGM&A), IT leadership and ad-hoc task-forces for each initiative. A continual assessment engaging the operations community was key to delineate a fit-for-purpose plan which keeps the long-term vision while solving the day-to-day data concerns through reliable quick-wins. All together it stimulates employees' commitment towards YPF goals by increasing their awareness around the ""data-problem"" in an increasingly data-driven environment. A shift in attitude is reached and the beneficiary is the whole organization that now is in the process of having high-quality validated data available and ready to be used whenever and wherever is needed. It certainly creates a competitive advantage. In summary, the tangible results are: missing data was recovered, new data types were created and saved, data was officially safeguarded and made available through new workflows. The cost of no-quality has been revealed with real and consistent figures. This paper describes the data challenges faced in shale field development operations in a corporation and shows a path to deal with this kind of projects in an integrated fashion. Digital Operations do not come out of a box, but processes and lessons learnt can be transferred into similar future projects. © 2019, Society of Petroleum Engineers DataGMA: Data-driven culture creation in shale field operations  Competition; Data Analytics; Information management; Life cycle; Project management; Competitive advantage; Data governances; Digital operation; Executive management; Field development; Integrated approach; Integrated fashion; Project management leadership; Shale",Capacity management
440,How the Center for Public Partnerships and Research Navigates Complex Social Problems to Make a Collective Difference,"The challenge of maximizing the well-being of children, youth, and families is recognizing that change occurs within complex social systems. Organizations dedicated to improving practice, advancing knowledge, and informing policy for the betterment of all must have the right approach, structure, and personnel to work in these complex systems. The University of Kansas Center for Public Partnerships and Research cultivates a portfolio of innovation, research, and data science approaches positioned to help move social service fields locally, regionally, and nationally. Mission, leadership, and smart growth guide our work and drive our will to affect positive change in the world. © 2018 Taylor & Francis. How the Center for Public Partnerships and Research Navigates Complex Social Problems to Make a Collective Difference Collective impact; community engaged scholarship; complexity theory; social services; systems change Academies and Institutes; Adolescent; Capacity Building; Child; Child Abuse; Child Development; Child Welfare; Financing, Organized; Humans; Leadership; Maternal-Child Health Services; Mental Health Services; Organizational Objectives; Personnel Management; Research; Social Work; Students; Systems Theory; Universities; article; human; leadership; social problem; social work; adolescent; capacity building; child; child abuse; child development; child welfare; economics; financial management; maternal child health care; mental health service; organization; organization and management; personnel management; prevention and control; research; social work; student; systems theory; university",Strategic alignment
441,Innovative strategy decision and portfolio assets analysis instrument – Permanent model of on-line reserves and resources monitoring,"The effectiveness of reserves and resources management depends on the full performance about resource base of the Company, its development history, current status and future prospects. Existing methods of data evaluation and analytics are not interconnected between each other and provided in the different systems by the various specialist's background. The volume of information is increasing yearly despite production conditions are getting complicated. Extremal climate conditions, remote territories, unconventional reserves are usual circumstances of actual oil and gas industry. The only method to save the cost production level is enhancing business process efficiency. Historically the Oil and Gas industry has a huge risk of failure – dry well or incorrect development plan may lead to great losses and jeopardize company's sustainability. For the very reason it is very important to increase effectiveness and rationality of the Resource base Management using disparate data source. In response to the complex needs of the oil and gas industry, it became necessary to use a single software solution for strategic decision making based on resource base analysis. Russian and foreign markets analysis showed the lack of software that is fully complies with the business needs. This fact encourage Gazprom Neft to develop its own IT solution for managing the company's reserves and resources: ""Constantly operating reserves and resources model (PDMZiR)"". This is a unique tool that aggregate huge amount of information from different data sources and modern technics for Resource base Management in one platform. PDMZiR can significantly increase the quality of resource base estimations for any Oil and Gas company, improve assets potential, identify and prevent any significant risk, justify recommendations for field development. Copyright 2018, Society of Petroleum Engineers. Innovative strategy decision and portfolio assets analysis instrument – Permanent model of on-line reserves and resources monitoring  Behavioral research; Gas industry; Gasoline; Oil field development; Petroleum reservoir evaluation; Proven reserves; Public utilities; Risk perception; Amount of information; Innovative strategies; Oil and gas companies; Oil and Gas Industry; Resources management; Resources monitoring; Single software solutions; Strategic decision making; Information management",Risk management
442,Improvement of Hedging on Stock Price Volatility Based on Fundamental Securities in Emerging Financial Markets,"The stock price volatility in emerging financial markets could be hedged by some innovative financial engineering methods without derivatives. In this paper, it is explored how to apply the principle of portfolio to hedge stock price volatility risk in emerging financial markets like China. This hedging portfolio involves the stock and a zero-coupon bond that deducting from the former. This portfolio can effectively eliminate stock price volatility and empirical results show that 91.74% of the risk is removed. © 2018 IEEE. Improvement of Hedging on Stock Price Volatility Based on Fundamental Securities in Emerging Financial Markets Bonds; Emerging Markets; Hedging on Stock Price Volatility; Stocks Advanced Analytics; Bonding; Commerce; Emerging markets; Financial engineering; Stock price volatilities; Stocks; Zero coupon bond; Financial markets",Financial management
443,Estimating cost and commercial risks of North Sea decommissioning projects: Lessons learnt for Asia pacific,"The Cost Estimate is an essential document in the preparation of decommissioning cost projects and entails the quantification of commercial and schedule risks. This study lists the Lessons Learnt from the preparation of Cost Estimates for North Sea decommissioning studies. It reviews on a holistic manner the considerations by Estimators on the impact of statutory and contracting regime and the major influences of cost in decommissioning. This study also proposes theoretical cost savings and efficiencies currently being explored in the North Sea. This study was conducted via interviews with a leader in North Sea decommissioning as well as extensive research of journals and web-based articles and releases. The identification of major cost elements which influenced decommissioning cost applied probabilistic analysis using Monte Carlo simulation. Following the processing of data received, the author then applied the results to current conditions in Asia Pacific by considering the related challenges faced (and anticipated) by the region's offshore decommissioning industry. The Lessons Learnt have been summarized into 5 main areas: 1. Development of the Cost Estimate: Areas that have been highlighted as potential hazards include the preparation of the accuracy of estimates and cost consolidation over multiple projects. Suggestions are made to include the application of probabilistic analysis in cost estimates, a process to challenge cost-driving assumptions and adopting integrated Cost & Schedule Risk analysis in the development of the cost estimate. 2. Collaboration and Data Analytics: The report highlights the opportunities that regional collaboration can drive a richer environment for data analytics. In the North Sea, organisations including the Oil and Gas Authority (UK) and the OSPAR commission have been tasked with centralising and disseminating. 3. Statutory Regime: it is imperative to remind ourselves in this region that operators, and contractors, in the North Sea have had to keep up with changes in regulations and ever greater environmental obligations. 4. Contracting Regime: Consideration of Contingent Liabilities and financial responsibility for decommissioning, even those who have sold, relinquished or otherwise disposed of their license is discussed. Contracting Strategies as well as the use of Decision Trees, Project Controls and Project Management in contract preparation and insurances are discussed. 5. Adoption of Circular Economy Principles: Findings in the North Sea suggest that increasing reuse over the recycling of equipment and structures on installations can increase values by between five and seven times. Copyright © 2018, Society of Petroleum Engineers Estimating cost and commercial risks of North Sea decommissioning projects: Lessons learnt for Asia pacific  Cost estimating; Data handling; Decision trees; Decommissioning (nuclear reactors); Digital storage; Environmental regulations; Intelligent systems; Monte Carlo methods; Offshore oil well production; Offshore oil wells; Project management; Regional planning; Risk analysis; Risk assessment; Risk perception; Contracting strategy; Decommissioning project; Financial responsibilities; Multiple projects; Offshore decommissioning; Potential hazards; Probabilistic analysis; Regional collaboration; Cost benefit analysis",Risk management
444,"39th International Conference Information Systems Architecture and Technology, ISAT 2018","The proceedings contain 105 papers. The special focus in this conference is on . The topics include: Simulation-based analysis of penalty function for insurance portfolio with embedded catastrophe bond in crisp and imprecise setups; selecting the efficient market indicators in the trading system on the forex market; impact of the size of equity on corporate liquidity; estimation of the probability of inversion in the test of variable dependencies; the role of business intelligence tools in harvesting collective intelligence; the concept of conjoined management; the simplification of organizational structure: Lessons from product design; information systems reliability and organizational performance; target marketing public libraries’ vital readers: Before; modeling investment decisions in the system of sustainable financing; IT reliability and the results of controlling; user experience for small companies – a case study; enterprise meta-architecture for megacorps of unmanageably great size, speed, and technological complexity; a genetic algorithm to solve the hybrid flow shop scheduling problem with subcontracting options and energy cost consideration; calibration of the risk model for hazards related to the technical condition of the railway infrastructure; Selected aspect of IT tools application in process improvement in industrial laundry services; risk analysis in the appointment of the trucks’ warranty period operation; Rationalization of retooling process with use of SMED and simulation tools; the risk model for hazards generated at level crossings; financial valuation of production diversification in the steel industry using real option theory; efficiency of gradient boosting decision trees technique in polish companies’ bankruptcy prediction. 39th International Conference Information Systems Architecture and Technology, ISAT 2018  ",Financial management
445,"2nd EAI International Conference on Smart Grid and Internet of Things, SGIoT 2018","The proceedings contain 14 papers. The special focus in this conference is on Smart Grid and Internet of Things. The topics include: Effectiveness of Hard Clustering Algorithms for Securing Cyber Space; on Data Driven Organizations and the Necessity of Interpretable Models; a Multi-factor Authentication Method for Security of Online Examinations; evaluation Metrics for Big Data Project Management; ioT Big Data Analytics with Fog Computing for Household Energy Management in Smart Grids; Secured Cancer Care and Cloud Services in IoT/WSN Based Medical Systems; privacy Preserving for Location-Based IoT Services; Smart Home Security Application Enabled by IoT:: Using Arduino, Raspberry Pi, NodeJS, and MongoDB; An MQTT-Based Scalable Architecture for Remote Monitoring and Control of Large-Scale Solar Photovoltaic Systems; A Smart Meter Firmware Update Strategy Through Network Coding for AMI Network; protected Bidding Against Compromised Information Injection in IoT-Based Smart Grid; A Chain Based Signature Scheme for Uplink and Downlink Communications in AMI Networks. 2nd EAI International Conference on Smart Grid and Internet of Things, SGIoT 2018  ",Monitoring and control
446,Digital analog intelligence helps mitigate E&P risk,"Based on the structured and regularized data, DAKS provides insight, intelligence and solutions to support E&P decision-making through its global field and reservoir knowledge base, pioneering classification scheme and powerful set of analytics tools. With the analytics and intelligence, geoscientists, reservoir engineers and portfolio managers will be able to quickly and efficiently expand their own experiences and be more creative to minimize the risks and achieve superior performance. The author will present case studies to demonstrate how DAKS digital intelligence has been applied in identifying prospect critical risks, calibrating subsurface uncertainties (Figure 2), validating reservoir models and development concepts, and mitigating E&P risks with examples from the Gulf of Mexico, South Atlantic Margin Basins, and the North Sea. © 2019 Asia Petroleum Geoscience Conference and Exhibition, APGCE 2019. All rights reserved. Digital analog intelligence helps mitigate E&P risk  Gasoline; Geology; Knowledge based systems; Analytics tools; Classification scheme; Gulf of Mexico; Knowledge base; Portfolio managers; Reservoir engineers; Reservoir models; South Atlantic; Decision making",Risk management
447,A Survey of Internet of Things and Big Data integrated Solutions for Industrie 4.0,"The industrial internet of things (IIoT) is growing at an exponential rate generating massive amounts of industrial data. This data must be leveraged to support business and operational goals. As a result, there is an urgent need for adopting big data technologies to enable data analytics in industrial automation. This paper explores interrelations between IIoT and big data technologies and how they work together to generate business insights from industrial data. Additionally, requirements for cloud-based solutions are derived from the Industrie 4.0 use case scenario value-based-services, focusing on condition monitoring and predictive maintenance services. A survey of selected cloud-based platforms is conducted to examine how these platforms meet the requirements derived from the use case. Results show that existing general cloud platforms should adopt more IIoT applications and platforms, while existing industrial cloud platforms should add big data frameworks to their portfolio. Finally, an architecture for integrating cloud-based IIoT and big data solutions is introduced and issues regarding the use of public cloud for IIoT applications are discussed. © 2018 IEEE. A Survey of Internet of Things and Big Data integrated Solutions for Industrie 4.0 Big Data; Cloud Computing; Industrial Automation; Industrial IoT (IIoT); Industrie 4.0; Internet of Things (IoT) Big data; Cloud computing; Condition monitoring; Data integration; Factory automation; Surveys; Big data technologies; Cloud based platforms; Industrial automation; Industrial IoT (IIoT); Industrie 4.0; Internet of Things (IOT); On condition monitoring; Predictive maintenance services; Internet of things",Financial management
448,The role of sensors and controls in transforming the energy landscape,"The YOU.S. Department of Energy funds a large portfolio of fossil energy R&D projects aimed at transformational improvements in the cost and environmental performance of coal-based power generation while maintaining high reliability standards. The National Energy Technology Laboratory (NETL) manages DOE's Crosscutting Research Program, which leverages on-going trends in disruptive technology to achieve breakthroughs in sensors and controls. Examples include: Advanced manufacturing of embedded sensors with energy harvesting capability; wireless signal transmission and blockchain infrastructure; and the application of data analytics and machine learning to processing distributed sensor signals to create actionable control interfaces. The fossil energy application space requires the development of sensors capable of monitoring key operational parameters (temperature, pressure, and gas compositions) while operating in harsh environments; analytical sensors capable of on-line, real-Time evaluation and measurement to support condition-based monitoring. Controls development centers around self-organizing information networks, algorithms for component lifetime assessment and plant-level economics, and distributed intelligence for process control and decision making. All of this must be accomplished while hardening fossil energy assets. An overview of DOE's Fossil Energy R&D Program with an emphasis on Sensors and Controls will be presented including discussion of technologies and applications being pursued and their status. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only. The role of sensors and controls in transforming the energy landscape Blockchain; Controls; Fossil Energy; Harsh Environments; Power Generation; Reliability; Sensors Blockchain; Control engineering; Decision making; Energy harvesting; Engineering education; Environmental management; Information services; Learning systems; Manufacturing data processing; Nanotechnology; Plants (botany); Power generation; Reliability; Sensors; Coal based power generation; Environmental performance; Fossil energy; Harsh environment; National Energy Technology Laboratory; Technologies and applications; YOU.S. Department of Energy; Wireless signal transmission; Application programs",Monitoring and control
449,"6th World Conference on Information Systems and Technologies, WorldCIST 2018","The proceedings contain 301 papers. The special focus in this conference is on Information Systems and Technologies. The topics include: Freemium project management tools: Asana, freedcamp and ace project; agile analytics: Applying in the development of data warehouse for business intelligence system in higher education; satisfaction with e-participation: A model from the Citizen’s perspective, expectations, and affective ties to the place; Coalition-OrBAC: An agent-based access control model for dynamic coalitions; health data analytics: A proposal to measure hospitals information systems maturity; supply chain challenges with complex adaptive system perspective; understanding the adoption of business analytics and intelligence; the planning process integration influence on the efficiency of material flow in production companies; time difference of arrival enhancement with ray tracing simulation; E-Mail client multiplatform for the transfer of information using the SMTP java protocol without access to a browser; legal and economic aspects of virtual organizations; CRUDi framework application – Bank company case study; GRAPHED: A graph description diagram for graph databases; capabilities and work practices - A case study of the practical use and utility; systematic review of the literature, research on blockchain technology as support to the trust model proposed applied to smart places; an architecture for a viable information system; smart Bengali idiomatic translator service using morphological marketing technique; An approach for knowledge extraction from source code (KNESC) of typed programming languages; measuring the quality of humanitarian information products: Insights from the 2015 Nepal earthquake; assessing review reports of scientific articles: A literature review; mexican Spanish affective dictionary. 6th World Conference on Information Systems and Technologies, WorldCIST 2018  ",Strategic alignment
450,"39th International Conference Information Systems Architecture and Technology, ISAT 2018","The proceedings contain 105 papers. The special focus in this conference is on . The topics include: Simulation-based analysis of penalty function for insurance portfolio with embedded catastrophe bond in crisp and imprecise setups; selecting the efficient market indicators in the trading system on the forex market; impact of the size of equity on corporate liquidity; estimation of the probability of inversion in the test of variable dependencies; the role of business intelligence tools in harvesting collective intelligence; the concept of conjoined management; the simplification of organizational structure: Lessons from product design; information systems reliability and organizational performance; target marketing public libraries’ vital readers: Before; modeling investment decisions in the system of sustainable financing; IT reliability and the results of controlling; user experience for small companies – a case study; enterprise meta-architecture for megacorps of unmanageably great size, speed, and technological complexity; a genetic algorithm to solve the hybrid flow shop scheduling problem with subcontracting options and energy cost consideration; calibration of the risk model for hazards related to the technical condition of the railway infrastructure; Selected aspect of IT tools application in process improvement in industrial laundry services; risk analysis in the appointment of the trucks’ warranty period operation; Rationalization of retooling process with use of SMED and simulation tools; the risk model for hazards generated at level crossings; financial valuation of production diversification in the steel industry using real option theory; efficiency of gradient boosting decision trees technique in polish companies’ bankruptcy prediction. 39th International Conference Information Systems Architecture and Technology, ISAT 2018  ",Financial management
451,"Proceedings of the 2018 Federated Conference on Computer Science and Information Systems, FedCSIS 2018","The proceedings contain 149 papers. The topics discussed include: novel solutions for smart cities - creating air pollution maps based on intelligent sensors; towards a language to support value cocreation: an extension to the ArchiMate modeling framework; enhancing project management for cyber-physical systems development; hybrid ant colony optimization algorithm for workforce planning; group anonymity in security protocols; a new WAF-based architecture for protecting web applications against CSRF attacks in malicious environment; comparative analysis of big data analytics and BI projects; and identifying hidden influences of traffic incidents' effect in smart cities. Proceedings of the 2018 Federated Conference on Computer Science and Information Systems, FedCSIS 2018  ",Capacity management
452,Capabilities and Readiness for Big Data Analytics,"Despite some of the initial hype from marketers and consultants, the use of big data is now firmly established in many organisations worldwide. Big data analytics (BDA) is making use of huge volumes of data from a wide range of structured and unstructured sources. Surveys have however reported a number of barriers to organisational effectiveness with BDA. This research aims to determine what capabilities large organisations require to be ready for a successful BDA initiative. Drawing mainly on relevant results of two published research articles, key informed stakeholders from a large South African telecommunications company were interviewed on this topic. Thematic analysis identified the key themes and sub-themes relating to capabilities needed for the organization to be ready for effective BDA. These proved to be very similar to those given in the earlier research, although a new capability of legal compliance for data protection was now added. © 2019 The Authors. Published by Elsevier B.V. Capabilities and Readiness for Big Data Analytics Big data analytics; business analytics; frameworks; organisational capabilities; organisational readiness; thematic analysis Advanced Analytics; Big data; Data Analytics; Information systems; Information use; Project management; Business analytics; frameworks; Organisational capabilities; Organisational readiness; Thematic analysis; Information management",Strategic alignment
453,Financial Analytics,"Data analytics in finance is a part of quantitative finance. Quantitative finance primarily consists of three sectors in finance—asset management, banking, and insurance. Across these three sectors, there are four tightly connected functions in which quantitative finance is used—valuation, risk management, portfolio management, and performance analysis. Data analytics in finance supports these four sequential building blocks of quantitative finance, especially the first three—valuation, risk management, and portfolio management. © Springer Nature Switzerland AG 2019. Financial Analytics  ",Risk management
454,Strategic Markowitz Portfolio Optimization (SMPO): A Portfolio Return Booster,"In Financial Data Science and more specifically in Investment Analytics, Portfolio Optimization is a very crucial aspect. A portfolio consisting of a collection of securities, has certain weights assigned to each security. Banking on these weights, the overall Portfolio Return and Risk are determined. Investors always try to find the Optimized Portfolio by adjusting the weightage given to each security in the portfolio. In this paper, a diverse, practical and exemplar portfolio, also having tint of similarity among securities, is considered. It has stocks from 8 companies (not from or any particular market indices): General Motors (GM), Ford Motor Company (F), Cognizant (CTS), International Business Machines Corporation (IBM), Apple Technology Company (AAPL), Vivo (VIVO), Under Armour (UAA) and Nike (NKE). The Optimized Portfolio is constructed with adjusted weightage for each company in the portfolio using Strategic Markowitz Portfolio Optimization (SMPO). The obtained Optimized Portfolio yielded a Logarithmic Portfolio Return of 0.04268 at minimum Risk (Standard Deviation) of 0.14951 and maximum possible Logarithmic Return of 0.15873 at a Risk (Standard Deviation) of 0.17938. Using the Markowitz Portfolio Optimization in a strategic manner for such portfolio where there is diversity along with different shades of similarity, can fetch more Optimized Portfolio than obtained by the Classical approach of Markowitz Portfolio Optimization. © 2019 IEEE. Strategic Markowitz Portfolio Optimization (SMPO): A Portfolio Return Booster Financial Data Science; Investment Analytics; Portfolio Optimization; Return; Risk; Strategic Markowitz Portfolio Optimization Business machines; Financial data processing; Microelectronics; Risks; Statistics; Apple technologies; Classical approach; Financial data; Ford motor companies; International business; Portfolio optimization; Return; Standard deviation; Financial markets",Strategic alignment
455,"PICMET 2018 - Portland International Conference on Management of Engineering and Technology: Managing Technological Entrepreneurship: The Engine for Economic Growth, Proceedings","The proceedings contain 235 papers. The topics discussed include: unified business intelligence ecosystem: a project management approach to address business intelligence challenges; dynamics of competition and strategy: a literature review of strategic management models and frameworks; a case study on Fintech in Japan based on keystone strategy; improving systematic literature review with automation and bibliometrics; a framework for building integrative scenarios of autonomous vehicle technology application and impacts, using fuzzy cognitive maps (FCM); proposal of five frameworks for constructing keystone strategy; incumbent firm capacity building in analytics: strategy, structure, and innovation management perspectives; business intelligence and data analytics as a driver of dynamic capability strategic approach; IEC standard revision dynamics: symbiosis between standard and technology; IT governance effectiveness and its influence on innovation product and process; and Nokia phones: from a total success to a total fiasco: a study on why Nokia eventually failed to connect people, and an analysis of what the new home of Nokia phones must do to succeed. PICMET 2018 - Portland International Conference on Management of Engineering and Technology: Managing Technological Entrepreneurship: The Engine for Economic Growth, Proceedings  ",Value management
456,Rating microfinance products consumers using artificial neural networks,"Assessing the loan repayment capacity of a client is the most Obsession of Financial institutions. In fact, loan portfolio management is optimal when incapable clients are identified and dismissed from the outset. This reduces delinquency and radiation, and thus increases the profitability of financial institutions. This paper presents an approach that uses artificial neural networks for the rating of corporate clients offering microfinance services. We started our work with a survey of several microfinance companies to understand closely the problems encountered by using customer-rating tools, and then we have used data analysis tools to explain the results of the survey. After that, we have collected masse of data containing real customer profiles provided by partner companies. Then, we have filtered and studied this data to create a learning database for the artificial neural network-based scoring system. Finally, we have designed an expandable, flexible, versatile and configurable scoring system. © Springer Nature Switzerland AG 2019. Rating microfinance products consumers using artificial neural networks Artificial neural networks; Business intelligence; Data analysis; Microfinance Competitive intelligence; Data handling; Data reduction; Financial data processing; Information analysis; Information systems; Information use; Investments; Surveys; Corporate clients; Customer profiles; Data analysis tool; Financial institution; Learning database; Loan repayments; Microfinance; Scoring systems; Neural networks",Strategic alignment
457,"39th International Conference Information Systems Architecture and Technology, ISAT 2018","The proceedings contain 105 papers. The special focus in this conference is on . The topics include: Simulation-based analysis of penalty function for insurance portfolio with embedded catastrophe bond in crisp and imprecise setups; selecting the efficient market indicators in the trading system on the forex market; impact of the size of equity on corporate liquidity; estimation of the probability of inversion in the test of variable dependencies; the role of business intelligence tools in harvesting collective intelligence; the concept of conjoined management; the simplification of organizational structure: Lessons from product design; information systems reliability and organizational performance; target marketing public libraries’ vital readers: Before; modeling investment decisions in the system of sustainable financing; IT reliability and the results of controlling; user experience for small companies – a case study; enterprise meta-architecture for megacorps of unmanageably great size, speed, and technological complexity; a genetic algorithm to solve the hybrid flow shop scheduling problem with subcontracting options and energy cost consideration; calibration of the risk model for hazards related to the technical condition of the railway infrastructure; Selected aspect of IT tools application in process improvement in industrial laundry services; risk analysis in the appointment of the trucks’ warranty period operation; Rationalization of retooling process with use of SMED and simulation tools; the risk model for hazards generated at level crossings; financial valuation of production diversification in the steel industry using real option theory; efficiency of gradient boosting decision trees technique in polish companies’ bankruptcy prediction. 39th International Conference Information Systems Architecture and Technology, ISAT 2018  ",Financial management
458,APC forum: Governing the wild west of predictive analytics and business intelligence,"CIO's achieve IT and business strategy alignment while effectively managing IT expenses using project portfolio governance tenets advanced in the early 2000's. However, today's IT context is characterized by the proliferation of big data, business intelligence and predictive analytics projects often incubated in business units but that require it is commitment for effective deployment. For CIO's, the IT governance approaches of the past need to be augmented with new methods, exemplars and reference models. We discuss three pillars to IT governance for this new era: a general model of big data and analytical solution deployment, a summary of current industry best practices and a taxonomy of plan, design and execution imperatives. These pillars provide foundation for an Analytics Capability Reference Model that has been validated in industry settings. © 2018 University of Minnesota. APC forum: Governing the wild west of predictive analytics and business intelligence  ",Governance
459,Social media as an information source in finance: evidence from the community of financial market professionals in Poland,"Social media has become a source of information for individuals making decisions in financial institutions worldwide. As a part of Business Intelligence systems, platforms such as Facebook, Twitter, or LinkedIn provide financial market professionals with a magnitude of market data—for example, stock price expectations, customer insights, and market sentiment. Based on 415 survey responses of financial market professionals in Poland (purposive sample), this study examines factors behind social media usage in financial institutions. We found that decision-makers representing these institutions seldom use social media for job-related purposes. However, the professionals from institutions that manage more diversified asset portfolios and hold the most risky assets in the portfolios, are more likely to use social media for information purposes. We also found that the value of assets does not affect the use of social media for information purposes among financial market professionals in Poland. This implies that representatives of certain types of financial institutions are more inclined to use social media for professional purposes. Our study offers an insight into the variables that best explain the decision to monitor social media content by financial market professionals. Thus, it can provide a basis of recommendations aiming to enhance the market for business information. © 2019, Universidad de Huelva. All Rights Reserved. Social media as an information source in finance: evidence from the community of financial market professionals in Poland Decision-making; Financial market professionals; Information needs; Social media ",Value management
460,"8th International Symposium on Business Modeling and Software Design, BMSD 2018","The proceedings contain 35 papers. The special focus in this conference is on Business Modeling and Software Design. The topics include: An Information Security Architecture for Smart Cities; three Categories of Context-Aware Systems; increasing the Visibility of Requirements Based on Combined Variability Management; situational Method Engineering for Constructing Internet of Things Development Methods; towards Blockchain Support for Business Processes; uncover and Assess Rule Adherence Based on Decisions; towards the Component-Based Approach for Evaluating Process Diagram Complexity; Differences Between BPM and ACM Models for Process Execution; general Architectural Framework for Business Visual Analytics; reconciling the Academic and Enterprise Perspectives of Design Thinking; a Causal Explanatory Model of Bayesian-belief Networks for Analysing the Risks of Opening Data; presence Patterns and Privacy Analysis; digitization Driven Design – A Guideline to Initialize Digital Business Model Creation; exploring Barriers in Current Inter-enterprise Collaborations: A Survey and Thematic Analysis; Smart Factory Modelling for SME: Modelling the Textile Factory of the Future; Configuring Supply Chain Business Processes Using the SCOR Reference Model; Strategy-IT Alignment: Assuring Alignment Using a Relation Algebra Method; an Ontology-Based Expert System to Detect Service Level Agreement Violations; multi-sided Platforms for the Internet of Things; towards Context-Aware Vehicle Navigation in Urban Environments: Modeling Challenges; from Strategy to Process Improvement Portfolios and Value Realization: A Digital Approach to the Discipline of Business Process Management; design Options of Store-Oriented Software Ecosystems: An Investigation of Business Decisions; business Process Variability and Public Values; composite Public Values and Software Specifications; monitoring the Software Development Process with Process Mining. 8th International Symposium on Business Modeling and Software Design, BMSD 2018  ",Governance
461,"6th World Conference on Information Systems and Technologies, WorldCIST 2018","The proceedings contain 301 papers. The special focus in this conference is on Information Systems and Technologies. The topics include: Freemium project management tools: Asana, freedcamp and ace project; agile analytics: Applying in the development of data warehouse for business intelligence system in higher education; satisfaction with e-participation: A model from the Citizen’s perspective, expectations, and affective ties to the place; Coalition-OrBAC: An agent-based access control model for dynamic coalitions; health data analytics: A proposal to measure hospitals information systems maturity; supply chain challenges with complex adaptive system perspective; understanding the adoption of business analytics and intelligence; the planning process integration influence on the efficiency of material flow in production companies; time difference of arrival enhancement with ray tracing simulation; E-Mail client multiplatform for the transfer of information using the SMTP java protocol without access to a browser; legal and economic aspects of virtual organizations; CRUDi framework application – Bank company case study; GRAPHED: A graph description diagram for graph databases; capabilities and work practices - A case study of the practical use and utility; systematic review of the literature, research on blockchain technology as support to the trust model proposed applied to smart places; an architecture for a viable information system; smart Bengali idiomatic translator service using morphological marketing technique; An approach for knowledge extraction from source code (KNESC) of typed programming languages; measuring the quality of humanitarian information products: Insights from the 2015 Nepal earthquake; assessing review reports of scientific articles: A literature review; mexican Spanish affective dictionary. 6th World Conference on Information Systems and Technologies, WorldCIST 2018  ",Strategic alignment
462,"1st International conference on Computational Intelligence , Communications, and Business Analytics, CICBA 2017","The proceedings contain 8 papers. The special focus in this conference is on Computational Intelligence , Communications, and Business Analytics. The topics include: A Comparative Study of Bio-inspired Algorithms for Medical Image Registration; different Length Genetic Algorithm-Based Clustering of Indian Stocks for Portfolio Optimization; an Evolutionary Matrix Factorization Approach for Missing Value Prediction; Differential Evolution in PFCM Clustering for Energy Efficient Cooperative Spectrum Sensing; feature Selection for Handwritten Word Recognition Using Memetic Algorithm; a Column-Wise Distance-Based Approach for Clustering of Gene Expression Data with Detection of Functionally Inactive Genes and Noise; detection of Moving Objects in Video Using Block-Based Approach. 1st International conference on Computational Intelligence , Communications, and Business Analytics, CICBA 2017  ",Monitoring and control
463,Guidelines for Evaluating the Completeness of the Portfolio,"Design has been expanding its position. The design has been expanding its position. because, the development of technology, to grow demand that solving problems using design methodologies [9, 13]. As a result, the capacity of individual designers is becoming more important, and the importance of ‘portfolios’ expressing and evaluating their capabilities is increasing. In the industrial society, the portfolio is used as an indicator of competence of designers, but the evaluation method and criteria depend on the subjective view of the evaluator so that the competence of the designer is not objective. Although both designers and evaluators agree on this problem, research on the portfolio itself, as well as the portfolio evaluation area, is very limited [3]. Therefore, this study is based on the priority of the components that should be considered in the evaluation of the completeness of the portfolio by analyzing the components of the portfolio so that the portfolio can be evaluated as an index for evaluating the individual competence of the designer Evaluation guidelines were presented. In order to do this, we conduct surveys and in-depth interviews with designers engaged in the business to understand the needs of the portfolio, and analyze the component data of the portfolio extracted based on the analysis, and prioritize the portfolio components that should be considered for completeness determination This study has significance in that it is presented through data analysis. © 2019, Springer Nature Switzerland AG. Guidelines for Evaluating the Completeness of the Portfolio Artificial intelligence; Data science; Design; Portfolio Artificial intelligence; Data Science; Design; Human computer interaction; Design Methodology; Evaluation guidelines; In-depth interviews; Industrial societies; Portfolio; Portfolio evaluation; Petroleum reservoir evaluation",Risk management
464,An empirical investigation of the relationship between institutional aspect and supply chain strategy in relation to investment policy in Indonesia,"The asset size of institutional investment in Indonesia is only about 6% of its GDP which is an obvious indication of a very weak potential of investors to contribute to country's development plans or its infrastructure development. Hence, most asset managers and institutional investors fail to get any opportunity to work with Big Data analytics or financial supply chain management (SCM) practices. Due to a relatively restricted workflow and a very small volume of data to dispose, they mostly depend upon manual spreadsheets and primitive financial accounting methods. As a result, the data available for investors to make investment decisions are very limited in scope, lacking metrics for analysis. This study investigated the interrelationship of such institutional investors with supply chain practices. The results revealed that manually kept data of Indonesia investors are not properly organized nor are retrievable from their archives. The data of this study was collected through interviews of portfolio and asset managers, brokers and commissioned agents in banks, insurance and stock institutions of Indonesia. The theoretical framework of this study enabled to understand the scope of institutional investments in different scenarios. The findings reveal the evolution of such practices like sustainable SCM and green SCM that have strengthened the institutional investment patterns in Indonesia. © ExcelingTech Pub, UK. An empirical investigation of the relationship between institutional aspect and supply chain strategy in relation to investment policy in Indonesia Banking and insurance; Indonesia; Institutional investment; Supply chain management ",Strategic alignment
465,The Opinion Management Framework: Identifying and addressing customer concerns extracted from online product reviews,"Online product reviews appear in many e-commerce websites and help merchants understand any obstacles experienced by existing customers. Negative reviews can discourage potential customers, especially when such reviews appear with no response from the merchant. After the appearance of an unfavourable review, the merchant is at risk of incurring negative impact on the community of present and future customers, which can harm the business. He or she may be able to deflect this by promptly communicating any planned actions, completing them, and reporting that they are complete. The initial communication is the most urgent. When presented with a set of online reviews, a merchant's predicament is to quickly decide what tasks need to be done, which are the most important, and when each can be completed. In this paper, we describe our Opinion Management Framework that assists a merchant to quickly identify, select, and schedule tasks that can rectify issues mentioned in online reviews. We also describe an interactive web-based prototype that helps the business owner (1) to select a set of tasks with an optimal cost/benefit tradeoff, (2) to ensure that all tasks can be completed within a specific time limit, and (3) to conservatively estimate a completion date for each issue's resolution. © 2017 Elsevier B.V. The Opinion Management Framework: Identifying and addressing customer concerns extracted from online product reviews Actionable analytics; Critical path; Electronic word of mouth; Opinion extraction; Optimal task selection; Project management; Social commerce; Topic extraction Commerce; Project management; Sales; Websites; Actionable analytics; Critical Paths; Electronic word of mouths; Opinion extraction; Social commerces; Task selection; Topic extraction; Data mining",Risk management
466,Causal data science for financial stress testing,"The most recent financial upheavals have cast doubt on the adequacy of some of the conventional quantitative risk management strategies, such as VaR (Value at Risk), in many common situations. Consequently, there has been an increasing need for verisimilar financial stress testings, namely simulating and analyzing financial portfolios in extreme, albeit rare scenarios. Unlike conventional risk management which exploits statistical correlations among financial instruments, here we focus our analysis on the notion of probabilistic causation, which is embodied by Suppes-Bayes Causal Networks (SBCNs); SBCNs are probabilistic graphical models that have many attractive features in terms of more accurate causal analysis for generating financial stress scenarios. In this paper, we present a novel approach for conducting stress testing of financial portfolios based on SBCNs in combination with classical machine learning classification tools. The resulting method is shown to be capable of correctly discovering the causal relationships among financial factors that affect the portfolios and thus, simulating stress testing scenarios with a higher accuracy and lower computational complexity than conventional Monte Carlo simulations. © 2018 Elsevier B.V. Causal data science for financial stress testing Causality; Classification; Decision trees; Graphical models; Stress testing; Suppes-Bayes Causal Networks Classification (of information); Decision trees; Graphic methods; Intelligent systems; Learning systems; Monte Carlo methods; Risk assessment; Risk management; Risk perception; Value engineering; Causal network; Causal relationships; Causality; GraphicaL model; Machine learning classification; Probabilistic graphical models; Statistical correlation; Stress Testing; Finance",Strategic alignment
467,Using analytics to predict project management success,"IT projects normally face time and cost overruns challenges. Predicting which projects will not be completed by the expected end date (time overrun) and within the allotted number of hours (man-hours overrun) helps manage company's employee utilization and avoids overscheduling. This study applied a data analytics framework on IT project management dataset to predict the project completion time and overruns (in man-hours). We used linear regression and classification models to predict projects' performance and analyze the underlying factors causing project delay. Our predictive analysis used 131 variables which included 536 tasks, 138 resources, 69371 employee hours, 72 contractors that were assigned to 434 projects. We also calculated two new variables closeness and betweenness among project team members. Results showed that Decision Tree outperformed SVM, ANN, LDA, and logistic regression in predicting man-hours overrun. In addition, preliminary Social Network Analysis (SNA) indicates that Average-closeness and Average-betweenness did not improve prediction on the overall amount of time and man-hours overrun but improve the prediction on time overrun, but task, resource and contractor assignments variables were significant at p-value of .01. The models we used helps identify key predictors of project performance and provide insights into the company's resource management. © IEOM Society International. Using analytics to predict project management success Data analytics; Decision tree; Logistic regression; Project management; Social network analysis ",Monitoring and control
468,Portfolio Management by Time Series Clustering Using Correlation for Stocks,"Investment diversification and portfolio building has been a great interest for share market investors, so as to minimize risk and maximize profit in a sensitive stock market. This paper gives an inside view of application of clustering for grouping 79 stocks (NSE), which can be used to build a diversified portfolio. Manually trying out different groupings to diversify portfolio is a computationally expensive task. In this paper, the closing price, time series of the stocks have been considered. Common effect due to market has been discounted using partial correlation, and a correlation based dissimilarity measure has been used for clustering. An equal investment strategy has been adopted to compare the portfolio&#x2019;s performance with SENSEX. The empirical results of the portfolios have been studied and presented in details. &#x00A9; 2019, Springer Nature Singapore Pte Ltd. Portfolio Management by Time Series Clustering Using Correlation for Stocks Clustering; Correlation analysis; Time series Advanced Analytics; Artificial intelligence; Commerce; Correlation methods; Financial data processing; Investments; Time series; Time series analysis; Clustering; Correlation analysis; Dissimilarity measures; Investment strategy; Partial correlation; Portfolio managements; Share market; Time series clustering; Financial markets",Strategic alignment
469,Evaluating firm's readiness for big data adoption: A hierarchical decision model,"This paper investigates big data challenges, leading to the development of a Hierarchical Decision Model (HDM) model that can be used by firms to evaluate readiness to adopt big data, and highlight/address probable causes of failure before the project even starts. Hence, increasing the chances of a successful big data adoption that can deliver value to firm and provide insights and analytics that will significantly help in addressing the problems it is built to help solve. The model was evaluated by experts from the industry, and then tested against a hypothetical case, in which Portland State University readiness to implement a big data project to address a main problem facing the university was conducted. Finally, a discussion about the results of the model, experts' evaluation, and case study was offered. © Copyright© (2018) by American Society for Engineering Management (ASEM). All rights reserved. Evaluating firm's readiness for big data adoption: A hierarchical decision model Adoption; Big data; Challenges; Project management Project management; Adoption; Challenges; Data adoption; Data challenges; Hierarchical decision models; Big data",Value management
470,SmartCVC: A novel startup selection method for corporate venture capital,"Corporate venture capital (CVC) has been growing rapidly in the past decades. As a critical first step for effective CVC investment, the selection of appropriate portfolio companies is challenging due to the large number of potential targets and the difficulty in evaluating the fit between CVC and different startups. In this study, we tackle this business problem through data science and develop an innovative prediction model, smartCVC, to recommend a short list of highly promising startups to a given CVC investor. To assess the strategic alignment and fit between a CVC investor and a startup, we adopt topic modeling techniques to analyze the business descriptions of both parties and construct a first-order topical similarity measure. We also construct two second-order topical similarity features from the existing CVC-portfolio investment network to characterize how similar a startup's past CVC investors are to a given CVC investor and how similar a CVC investor's past portfolios are to a given startup. Computational experiments on the CrunchBase dataset suggest that the proposed method achieves a significantly better performance than the baseline method. © International Conference on Information Systems 2018, ICIS 2018.All rights reserved. SmartCVC: A novel startup selection method for corporate venture capital Corporate venture capital (CVC); CVC investment network; Data science; Predictive analytics; Target selection Data Science; Information systems; Information use; Predictive analytics; Computational experiment; Corporate venture capital; Portfolio investment; Potential targets; Selection methods; Similarity measure; Strategic alignment; Target selection; Investments",Strategic alignment
471,"Proceedings of the 3rd EU International Conference on Industrial Engineering and Operations Management, IEOM 2019","This proceedings contains 244 papers. The 3rd IEOM European International Conference aims at providing ideas in the latest developments and advancements in the fields of Industrial Engineering and Operations Management. The conference Topics covering industrial issues/applications and theoretical research, include: Business management; Big Data and Analytics; Decision Sciences; E-Business and E-Commerce; Energy and Resource Efficiency; Engineering Economy, Education & Management; Facilities Planning and Management; Global Manufacturing; High Value Manufacturing; Human Factors and Ergonomics; Information Technology and Information Systems; Inventory Management; Knowledge Management; Lean and Six Sigma; Logistics, Transport and Traffic Management; Manufacturing Design and Servitisation; Operations Management and Operations Research; Product design and development; Production Planning and Control; Project Management; Quality Engineering, Control and Management; Reliability and Maintenance; Reverse Logistics and Green Systems; Service Systems and Service Management; Sustainable Operations and Supply Chain Management; Sustainability in Supply Chains and Operations; Sustainability in Manufacturing, Services, Logistics, and Freight transportation; Sustainable Manufacturing; Systems Engineering; Technology Management; Tools for Sustainable Manufacturing and Service Systems Design, Management, and Performance Measurement; Waste Management, etc. The key terms of this proceedings include OEE Approach, Analytic Hierarchical Process (AHP), artificial neural network, total quality service (TQS) framework, six sigma methodology, organizational management and supply chains, logistic interoperability, Greenhouse Gas (GHG) emissions, Stylistic Design Engineering (SDE), Penalized Poisson-GzLM. © 2019, IEOM Society International. Proceedings of the 3rd EU International Conference on Industrial Engineering and Operations Management, IEOM 2019  ",Financial management
472,Text and Information Analytics for Fully Automated Energy Code Checking,"Energy compliance checking aims to check the compliance of design information embedded in building information models (BIMs) with applicable building energy codes. To fully automate the compliance checking process – without the need to manually code the rules in a compliance checking software, there is a need to automatically process and understand the text in the codes, extract the relevant requirements from the text, and match the extracted requirements to the information in the BIMs. This is challenging because of the text complexities of the codes, including longer provisions, requirement exceptions, hierarchically-complex sentence structures, and different terminologies and levels of detail compared to BIMs. To address these challenges, a set of text and information analytics methods were proposed and implemented in an energy compliance checking prototype. The prototype includes four main modules: text classification, information extraction, semantic information alignment, and compliance checking and reporting. For testing the prototype, a BIM of an educational building was checked for compliance with three energy codes – the 2012 International Energy Conservation Code, the 2013 Building Energy Efficiency Standards (known as the California Energy Code), and the Ontario Building Code Supplementary Standard SB-10. A promising performance of 91.7% recall and 84.6% precision in noncompliance detection was achieved. This study could benefit different participants in the domain of architecture, engineering and construction, by streamlining the energy code compliance process in a fully-automated manner. Future research may test the proposed methods and prototype on other energy topics (e.g., fenestration) and different types of documents (e.g., contract specifications). The challenges for developing a compliance checking method that is fully-automated and generalized across different types of documents could also be studied. © 2019, Springer Nature Switzerland AG. Text and Information Analytics for Fully Automated Energy Code Checking Building Information Modeling (BIMs); Compliance Checking; Energy Code; Information Analysis Method; Non-compliance Detection Architectural design; Automation; Building codes; Classification (of information); Energy efficiency; Energy policy; Historic preservation; Project management; Regulatory compliance; Semantics; Soil structure interactions; Text processing; Architecture , engineering and constructions; Building energy code; Building energy efficiency standards; Compliance checking; Contract specifications; Educational buildings; Semantic information; Text classification; Compliance control",Financial management
473,"Contractual Procedures in the Construction Industry, Seventh Edition","Contractual Procedures in the Construction Industry 7th edition aims to provide students with a comprehensive understanding of the subject, and reinforces the changes that are taking place within the construction industry. The book looks at contract law within the context of construction contracts, it examines the different procurement routes that have evolved over time and the particular aspects relating to design and construction, lean methods of construction and the advantages and disadvantages of PFI/PPP and its variants. It covers the development of partnering, supply chain management, design and build and the way that the clients and professions have adapted to change in the procurement of buildings and engineering projects. This book is an indispensable companion for students taking undergraduate courses in Building and Surveying, Quantity Surveying, Construction Management and Project Management. It is also suitable for students on HND/C courses in Building and Construction Management as well as foundation degree courses in Building and Construction Management. Key features of the new edition include: • A revised chapter covering the concept of value for money in line with the greater emphasis on added value throughout the industry today. • A new chapter covering developments in information technology applications (building information modelling, blockchains, data analytics, smart contracts and others) and construction procurement. • Deeper coverage of the strategies that need to be considered in respect of contract selection. • Improved discussion of sustainability and the increasing importance of resilience in the built environment. Concise descriptions of some the more important construction case laws. © 2018 Allan Ashworth and Srinath Perera. Contractual Procedures in the Construction Industry, Seventh Edition  ",Strategic alignment
474,Digital innovative manufacturing basing on formation of an ecosystem of services and resources,"The article examines development of digital models for industrial enterprises on the basis of formation of virtual twins (avatars) ecosystem using methods of industrial analytics (Big Data) allowing transformation of current processes with respect to criteria of value chains. Novelty of the approach proposed is in development of comprehensive theory and practical mechanisms of processes transformation at separate enterprises using advanced information technologies (processing of large volumes of information, fog computing, digital avatars, blockchain technology, etc.) that will provide fundamental change of business processes at enterprises and technologies of preparation and making of decisions. Cross-disciplinary character of research proposed determined application of new sources of reconsideration of scientific thought development, principles and mechanisms, and also way of economic growth at micro-, meso-, macro- and mega-levels of management. As a part of research, it is proposed to develop mechanisms of implementation and to support initiatives of pilot projects (testbet) for extending the concept of digital factory (plant) (smart factory, virtual factory) regarding interaction of technologies of digital design and modeling. Introduction of results of research proposed in management practice will facilitate general growth of economic and innovative activity of industrial enterprises, reduction of average duration of investment and innovative projects life cycles, and also improvement of their quality. Social significance of the project's planned results involves obtaining accompanying social benefits of their practical introduction which, first of all, consist in projected improvement of living standards of Russia. © 2018 Elsevier Ltd. All rights reserved. Digital innovative manufacturing basing on formation of an ecosystem of services and resources Digital economy; Digital ecosystem; Digital manufacturing; Digital transformation; Digital twin; Digitalization Computation theory; Ecosystems; Fog computing; Industrial economics; Industrial research; Information management; Manufacture; Metadata; Research and development management; Digital economy; Digital ecosystem; Digital manufacturing; Digital transformation; Digital twin; Digitalization; Project management",Strategic alignment
475,HR transformation within the hotel industry: building capacity for change,"Purpose: This study aims to identify recent trends in the strategic repositioning of the human resources (HR) function within the hotel industry, and to explore challenges facing HR professionals as they engage in strategies to develop talent and organisational capability, while adjusting to the shifting boundaries of the HR function. Design/methodology/approach: The study provides a case study investigation based on a qualitative research design. It draws on a series of informal discussions with key informants, in-depth round table discussions with members of the HR function and a rich source of secondary (company specific) data about a recent strategic change initiative. Findings: The study presents a rich picture of the contradictory nature of the strategic repositioning of the HR function, and the role of electronic HR systems in shaping this. It points to the significance of “higher-order” HR capabilities associated with the functions’ capacity to engage in strategies to develop talent and organisational capability. Practical implications: This study points to contradictions and tensions in shifting the focus of the HR function from “operational” to “strategic” management of talent. It offers four practice implications in the areas of continuous professional development, and building HR and line manager skills in dialogue, project management and the use of new technology, talent data and analytics. Originality/value: This study illustrates the deployment of talent management practices within a broader organisational development remit to restructure the business and HR function in pursuit of more efficient and effective people management. © 2018, Emerald Publishing Limited. HR transformation within the hotel industry: building capacity for change e-HRM; Organizational development; Paradox; Strategic HR; Talent management ",Strategic alignment
476,Big data services based on mobile data and their strategic importance,"Telecom companies are sitting on a gold mine because they own a huge amount of data. Once data are unlocked, this can develop competitive advantages and generate new sources of revenue. In the context of increased competitiveness, rapid technological evolution and alignment with global telecoms rules, this approach ensures openness to other related service categories, allowing both the internal transformation of the operator - by operational efficiency or by reducing costs, but also externally - through providing new services as a data provider (i.e. tourism, public services, transportation, etc.). In order to extract the strategic meaning of the data and transform it into actionable information, we propose a detailed analysis of the current state of knowledge regarding Business Intelligence in Telecom (Big Data, Data Analytics, Cloud, and mobile computing), identifying the portfolio of possible areas of application and their impact. This approach involves the identification of structured and unstructured data sets, internal or external to the operator. The primary purpose is to determine the answer to the following questions: 'where are the data used?' or 'is the data used in the most profitable way (for better measurement of organizational performance, avoidance of investment, generation of related service options that can be offered to other interested partners)?' We also need to analyze the architectures or software components for data analysis and business intelligence platforms. These issues will assist existing decisions, capitalizing data richness taking into account the development of telecommunication market. We will define which steps are the necessary for value the Big Data, highlighting their originality in the Romanian and international telecommunication market. © 2018 IEEE. Big data services based on mobile data and their strategic importance actionable information; Big Data; cloud; data analytics; mobility Carrier mobility; Clouds; Commerce; Competition; Data mining; Investments; Metadata; Telecommunication services; actionable information; Business Intelligence platform; Data analytics; International telecommunications; Operational efficiencies; Organizational performance; Technological evolution; Telecommunication market; Big data",Strategic alignment
477,A Data-Driven Residential Transformer Overloading Risk Assessment Method,"Residential transformer population is a critical type of asset that many electric utility companies have been attempting to manage proactively and effectively to reduce unexpected transformer failures and life loss that are often caused by overloading. Within the typical power asset portfolio, the residential transformer asset is often large in population, has the lowest reliability design, lacks transformer loading data, and is susceptible to customer loading behaviors, such as adoption of distributed energy resources and electric vehicles. On the bright side, the availability of more residential service operation data along with the advancement of data analytics techniques has provided a new path to further our understanding of residential transformer overloading risk statistically. This paper develops a new data-driven method that combines transformer temperature rise and insulation life loss simulation model with clustering analysis technique. It quantitatively and statistically assesses the overloading risk of residential transformer population in one area and suggests proper risk management measures according to the assessment results. Multiple application examples for a Canadian utility company have been presented and discussed in detail to demonstrate the applicability and usefulness of the proposed method. © 1986-2012 IEEE. A Data-Driven Residential Transformer Overloading Risk Assessment Method clustering methods; life estimation; Power system reliability; transformers; unsupervised learning Electric losses; Electric transformers; Electric utilities; Energy resources; Housing; Population statistics; Reliability; Risk management; Unsupervised learning; Clustering methods; Distributed Energy Resources; Life estimation; Multiple applications; Power system reliability; Risk assessment methods; Risk management measures; Transformer overloading; Risk assessment",Risk management
478,Quantitative Comparison of Big Data Analytics and Business Intelligence Project Success Factors,"Decision support systems such as big data, business intelligence (BI), and analytics offer firms capabilities to generate new revenue sources, increase productivity and outputs, and gain strategic benefits. However, the field is crowded with terminology that makes it difficult to establish reasonable project scopes and to staff and manage projects. This study clarifies the terminology around data science, computational social science, big data, business intelligence, and analytics, and defines decision support projects. The study uses quantitative methods to empirically classify the project scopes, investigate the similarities and differences between the project types, and identify the critical success factors. The results suggest BI and big data analytics projects are differentiated based on analytics competence, proprietary algorithms, and distinctive business processes. They are significantly different for 19 of the 52 items evaluated. For big data analytics projects, many of the items are correlated with strategic benefits, while for BI projects they are associated with the operational benefits of cost and revenue performance. Project complexity is driven by the project characteristics for BI projects, while the external market drives the complexity of big data analytics projects. These results should inform project sponsors and project managers of the contingency factors to consider when preparing project plans. © 2019, Springer Nature Switzerland AG. Quantitative Comparison of Big Data Analytics and Business Intelligence Project Success Factors Analytics; Big data; Business intelligence; Data science; Project management; Success factors Advanced Analytics; Artificial intelligence; Big data; Competitive intelligence; Data Analytics; Data Science; Decision support systems; Digital storage; Information systems; Information use; Project management; Terminology; Analytics; Business Intelligence projects; Computational social science; Critical success factor; Operational benefits; Project characteristics; Quantitative comparison; Success factors; Information management",Capacity management
479,Managing Urban Complexity: Project and Risk Management and Polycentric and Participatory Governance,"The widespread dissemination of sustainability, the rapid urbanization of the world, and the global rise of ICT are the three most important global trends at play across the urban world today. They will most likely change the way cities can be managed and developed drastically. They are also rendering the tasks of urban management increasingly more challenging on many scales with regard to city development. This implies that the management of urban systems and what they entail in terms of operations, functions, processes, and services in the context of smart sustainable cities require complex interdisciplinary knowledge pertaining not only to project management and multiscale and participatory governance, but also to the administration of ICT and related computational and data analytics processes. These three urban management functions are particularly associated with significant risks and challenges that need to be managed and overcome, respectively, in the process of making decisions as part of the development of smart sustainable cities of the future. However, topical studies on project management, governance, and risk management approach these topics from a general perspective predominantly. From a somewhat specific perspective, the focus in this chapter is rather on these urban management functions in relation to smart sustainable cities as having distinctive characteristics with respect to both the ubiquity presence and massive use of ICT and what this entails in terms of information security risks as well as the complexity of multiscale and participatory governance structures and project management processes. This chapter intends to explore urban and ICT project and related risk management in the context of smart sustainable cities, as well as the various models of governance of their functioning and development. The emphasis in risk management is placed on both urban development and ICT projects as well as information security in relation to the use of cloud computing as an increasingly widely applied solution for big data and context-aware applications. As to governance models, we put emphasis on polycentric, participatory, and big data forms. This is deemed of particular importance to providing insights into workable, practice-oriented solutions for the management of the complexity of smart sustainable cities increasingly being sought by urban planners, strategists, policymakers, and decision-makers. © 2018, Springer International Publishing AG, part of Springer Nature. Managing Urban Complexity: Project and Risk Management and Polycentric and Participatory Governance ICT projects; Multiscale and participatory governance; Project management; Risk management; Smart sustainable cities; Urban development projects; Urban management; Urban sustainability ",Risk management
480,Metrics That Matter: Core Predictive and Diagnostic Metrics for Improved Project Controls and Analytics,"Project progress and performance assessment is critically important to the successful delivery of capital facility projects. However, there is no standardized approach for the selection and use of project control metrics, making it difficult to analyze project progress and performance for transforming data into meaningful insights. This research identified core predictive and diagnostic metrics that may provide actionable insights into a project's actual progress, performance, and forecast at completion. The methodology used for identifying these metrics included a literature review, surveys, expert evaluation utilizing the Delphi method, and statistical validation. The researchers analyzed 44 surveys and collected multiple rounds of responses from 16 subject matter experts to validate the findings. Results indicated there are 20 core metrics, seven validation metrics, seven innovative metrics, and 14 other significant metrics, which can be used for multiple project types, sizes, and contracting strategies. Statistical analyses of the survey data were used to further validate the core metrics and demonstrated that use of more core metrics corresponded with project cost performance and using more diagnostic metrics in projects led to better schedule performance. © 2018 American Society of Civil Engineers. Metrics That Matter: Core Predictive and Diagnostic Metrics for Improved Project Controls and Analytics Cost and schedule; Forecasting; Performance assessment; Project control metrics Cost benefit analysis; Forecasting; Metadata; Project management; Surveys; Capital facilities; Contracting strategy; Cost and schedule; Performance assessment; Project control; Schedule performance; Statistical validation; Subject matter experts; Predictive analytics",Risk management
481,Evaluation Criteria of Project Risk and Decision Making Through Beta Analysis and TOPSIS Towards Achieving Organizational Effectiveness,"Value of a project of an organization is primarily determined by two major factors &#x2013; risk and return. The most important aspect of a business analysis, therefore, lies with the analysis of the risks and their associated returns. The basic objective of an organization is to increase the productivity to grab more market share. But the problem is that market risk is inherent in all projects and, by nature, it is stochastic. It can hardly be avoided but can be mitigated at most through diversification. Through Capital Asset Pricing Model (CAPM), the systematic or un-diversifiable risks can be described and measured by beta, &#x03B2;. In order to mitigate the risk, investments are to be made on a combination of different projects or portfolio of projects rather than a single project. Through &#x03B2;-hedging, a proper hedging strategy can be developed to reduce the systematic risk. But it has also been observed that the concept of CAPM has been plagued by the stochastic nature of the economy. Therefore, in the first part of this work, the systematic risk has been evaluated through time-varying &#x03B2; analysis. According to the results of the hedge performance of individual projects of the portfolio, it will be possible to select/rank the projects according to their risk-return trade-off capacity and in the second part, the Technique for Order Preference using Similarity to Ideal Solution (TOPSIS), one of the most important MCDM techniques, has been merged with CAPM in order to provide a more justified selection procedure of projects considering four more attributes, other than risk, which may confirm a more realistic basis of creating the portfolio for increasing organizational effectiveness. &#x00A9; 2019, Springer Nature Singapore Pte Ltd. Evaluation Criteria of Project Risk and Decision Making Through Beta Analysis and TOPSIS Towards Achieving Organizational Effectiveness &#x03B2; analysis; &#x03B2;-hedging; CAPM; MCDM; Portfolio project; Project risk and return; TOPSIS Advanced Analytics; Artificial intelligence; Competition; Decision making; Economic and social effects; Financial markets; Investments; Stochastic systems; Strategic planning; CAPM; MCDM; Portfolio project; Project risk; TOPSIS; Risk assessment",Risk management
482,Production excellence is the platform to support digital oil field,"Kuwait Oil Company manages the production and export of oil and gas with the associated facilities from different oil fields in the state of Kuwait. Kuwait Oil Company has ambitious growth targets with challenges of expanding portfolio, high water cuts, lower reservoir pressures, new facilities, and complex production processes. In recognition of these challenges, KOC Leadership Committee created the Production Excellence and Planning Team (PE&P) in June 2015. KOC assets are committed to the company's vision to achieve a leading global position in the Oil & Gas industry. Operational Excellence is an element of organizational leadership that stresses the application of a variety of principles, systems, and tools toward the sustainable improvement of key performance metrics. This is only possible through the consistent and transparent view of companywide production, capacity (wells and facilities), constraints and loss numbers. Efficiently deriving actionable insights from the vast amounts of data generated upon a daily basis is a strategic priority for KOC. The ongoing digital transformation across KOC is one lever to attain the step change in performance required to meet KOC's targets. KOC S&EK in 4th Dec 2017 launched the Integrated Operational Excellence Program (IOX) in S&EK for one year and was completed with key lessons learnt. Technology and innovation continues to be the foundation of KOC's success and Business requirements mandate that the company further incorporate the transformational opportunities of the digital revolution, by leveraging key advances in areas such as surface & subsurface modelling, big data, advanced analytics, and the digitally connected workforce into our daily workflows. Building on the powerful Production Excellence Framework and the consistency and transparency it generates, KOC can achieve its ambitions around the Digital Oil Field of the future leading to Business Digital Transformation. Copyright 2019, Society of Petroleum Engineers. Production excellence is the platform to support digital oil field  Advanced Analytics; Gas industry; Oil fields; Business requirement; Digital oil field of the futures; Digital transformation; Kuwait oil companies; Operational excellence; Performance metrics; Reservoir pressures; Sustainable improvement; Petroleum industry",Capacity management
483,What CPI = 0.85 Really Means: A Probabilistic Extension of the Estimate at Completion,"This paper investigates the predictive power of project cost data in earned value management (EVM) as an early indicator of the cost overrun probability in risk management. The predictive power of the cost performance index (CPI) is probabilistically assessed and used (1) to update the cost overrun probability and the estimate at completion (EAC) distribution and (2) to visualize all possible CPI trajectories to project completion. Specifically, this paper presents two decision support tools: a probabilistic EAC (P-EAC) model and a CPI trajectory simulator for visual risk communication. The predictive models were applied to a real project and computational experiments were conducted. The results indicate that a deterministic CPI measurement, for instance, CPI = 0.85 at a 20% completion point, may indicate a wide range of possible cost overrun probabilities from 54 to 100% according to the predictive power of cost data. Improved risk awareness from the proposed analytics can be a vital element for enhanced management visibility and more informed decision-making in project control. © 2018 American Society of Civil Engineers. What CPI = 0.85 Really Means: A Probabilistic Extension of the Estimate at Completion Cost performance index (CPI); Cost risk; Method of moments; Predictive power; Project management; Simulation Budget control; Costs; Decision support systems; Method of moments; Probability distributions; Project management; Risk management; Risk perception; Value engineering; Computational experiment; Cost performance index; Decision support tools; Earned value management; Estimate at completions; Predictive power; Probabilistic extension; Simulation; Cost benefit analysis",Monitoring and control
484,Bringing Advanced Analytics to Manufacturing: A Systematic Mapping,"Advanced analytics has the potential to redefine manufacturing. However, practical implementation is in its infancy. One reason is a lack of management tools that enable decision-makers to choose suitable techniques from advanced analytics for domain-specific problems in manufacturing. This paper uses a systematic mapping review in order to identify seven application areas to which analytics can add substantial value. Each area is then matched with suitable techniques from the field of advanced analytics. The resulting systematic map provides a novel management tool for the purpose of identifying promising analytics projects in manufacturing and thus facilitates decision-making. © IFIP International Federation for Information Processing 2019. Bringing Advanced Analytics to Manufacturing: A Systematic Mapping Advanced analytics; AI applications; Smart manufacturing; Systematic mapping review Decision making; Industrial management; Manufacture; Mapping; Project management; AI applications; Application area; Decision makers; Domain specific; Management tool; Smart manufacturing; Systematic mapping; Systematic maps; Advanced Analytics",Strategic alignment
485,"2nd International Conference on Computational Intelligence, Communication, and Business Analytics, CICBA 2018","The proceedings contain 76 papers. The special focus in this conference is on Computational Intelligence, Communication, and Business Analytics. The topics include: A Lemmatizer Tool for Assamese Language; portfolio Management by Time Series Clustering Using Correlation for Stocks; an Approach Towards Development of a Stem Borer Population Prediction Model Using R Programming; Evaluation Criteria of Project Risk and Decision Making Through Beta Analysis and TOPSIS Towards Achieving Organizational Effectiveness; an Approach Towards Classification of Fruits and Vegetables Using Fractal Analysis; categorization of Bangla Medical Text Documents Based on Hybrid Internal Feature; a Critical Survey of Mathematical Search Engines; removing Irrelevant Features Using Feature Information Map for Unsupervised Learning; target Protein Function Prediction by Identification of Essential Proteins in Protein-Protein Interaction Network; mutual Information &#x2013;The Biomarker of Essential Gene Predictions in Gene-Gene-Interaction of Lung Cancer; Design and Implementation of a Mobile-Based Personal Digital Assistant (MPDA); biometric Template Generation Framework Using Retinal Vascular Structure; graph Theoretical Characterization of Retinal Vascular Network&#x2013;Finding Minimum Cost Spanning Tree; Empirical Analysis of Programmable ETL Tools; design and Implementation of an Improved Data Warehouse on Clinical Data; Dynamic FP Tree Based Rare Pattern Mining Using Multiple Item Supports Constraints; a Meetei Mayek Basic Characters Recognizer Using Deep Features; face Image Retrieval Using Discriminative Ternary Census Transform and Spatial Pyramid Matching; Cloud ERP Adoption Pitfalls and Challenges &#x2013; A Fishikawa Analysis in the Context of the Global Enterprises; an Interactive Practical Approach for Traditional Cryptanalysis of Vigenere Cipher; multilayer Based Improved Priority Scheduling Algorithm in Cloud Environment. 2nd International Conference on Computational Intelligence, Communication, and Business Analytics, CICBA 2018  ",Risk management
486,Automated creation of the pipeline digital twin during construction - Improvement to construction quality and pipeline integrity,"The concept of the digital twin dates all the way back to the 1950's when NASA, GE and other industrial manufacturers started creating abstract digital models of equipment to model their performance in simulations and maintain a record of the asset throughout its life span [1]. Over the years more and more industries have adopted the digital twin paradigm to improve traceability, maintenance, and analytics allowing for improved sustainment of the asset or equipment while reducing various risks identified during life cycle management. It has been found that collectively, the digital twin concept improves the overall net present value of an asset. The oil and gas industry has slowly been adopting the digital twin paradigm of asset life cycle management over the past two decades with the focus on facilities. Recently, field trials were completed to test and evaluate workflows and sensor platforms for the creation of a digital twin for pipelines. The trials resulted in highly accurate pipeline centerlines, weld locations, Depth to Cover (DoC) and ditch geometry capture in digital formats. This paper describes the methodologies used, and the results of an actual construction field trial with a comparison to traditional data collection methods for these attributes. The value of creating a pipeline digital twin during pipeline construction in near-real-time is discussed with an emphasis on the potential benefits to life cycle management and pipeline integrity. Copyright © 2018 ASME Automated creation of the pipeline digital twin during construction - Improvement to construction quality and pipeline integrity  Gas industry; Life cycle; NASA; Offshore oil well production; Project management; Construction fields; Construction improvements; Construction quality; Data collection method; Industrial manufacturers; Life-cycle management; Oil and Gas Industry; Pipeline construction; Offshore pipelines",Strategic alignment
487,Understanding customer voice of project portfolio management software,"Project Portfolio Management (PPM) has gained success in many projects due to its large number of features that covers effective scheduling, risk management, collaboration, and third-party software integrations to mention a few. A broad range of PPM software is available; however, it is essential to select the PPM with minimum usage issues over time. While many companies use surveys and market research to get users feedback, the PPM product software reviews carry the voice of users; the positive and negative sentiments of the PPM software reviews. This paper collected 4,775 reviews of ten PPM software from Capttera.com. Our approach has these phases- text preprocessing, sentiment analysis, summarization, and categorizations. The software reviews are filtered and cleaned, then negative sentiments of user reviews are summarized into a set of factors that identify issues of adopted PPM software. We report the most important issues of PPM software which were related to missing technological features and lack of training. Results using Latent Dirichlet Allocation (LDA) model showed that the top ten common issues are related to software complexity and lack of required features. © 2018 The Science and Information (SAI) Organization Limited. Understanding customer voice of project portfolio management software LDA; Project Portfolio Management (PPM); Sentiment analytics; Software reviews; Text summarization Computer software selection and evaluation; Investments; Market Research; Risk management; Scheduling; Sentiment analysis; Latent Dirichlet allocation; Management software; Negative sentiments; Project portfolio management; Risks management; Scheduling risks; Sentiment analytic; Software reviews; Text Summarisation; Statistics",Risk management
488,Measuring the impact of additional instrumentation on the skill of numerical weather prediction models at forecasting wind ramp events during the first Wind Forecast Improvement Project (WFIP),"The first Wind Forecast Improvement Project (WFIP) was a DOE and NOAA-funded 2-year-long observational, data assimilation, and modeling study with a 1-year-long field campaign aimed at demonstrating improvements in the accuracy of wind forecasts generated by the assimilation of additional observations for wind energy applications. In this paper, we present the results of applying a Ramp Tool and Metric (RT&M), developed during WFIP, to measure the skill of the 13-km grid spacing National Oceanic and Atmospheric Administration/Earth System Research Laboratory (NOAA/ESRL) Rapid Refresh (RAP) model at forecasting wind ramp events. To measure the impact on model skill generated by the additional observations, controlled data-denial RAP simulations were run for six separate 7 to 12-day periods (for a total of 55 days) over different seasons. The RT&M identifies ramp events in the time series of observed and forecast power, matches in time each forecast ramp event with the most appropriate observed ramp event, and computes the skill score of the forecast model penalizing both timing and amplitude errors. Because no unique definition of a ramp event exists (in terms of a single threshold of change in power over a single time duration), the RT&M computes integrated skill over a range of power change (Δp) and time period (Δt) values. A statistically significant improvement of the ramp event forecast skill is found through the assimilation of the special WFIP data in two different study areas, and variations in model skill between up-ramp versus down-ramp events are found. Published 2019. This article is a YOU.S. Government work and is in the public domain in the USA. Wind Energy Published by John Wiley & Sons, Ltd. Measuring the impact of additional instrumentation on the skill of numerical weather prediction models at forecasting wind ramp events during the first Wind Forecast Improvement Project (WFIP) data assimilation; forecasting; ramp events Forecasting; Predictive analytics; Research laboratories; Wind power; Amplitude errors; Data assimilation; Energy applications; Field campaign; Integrated skills; National Oceanic and Atmospheric Administration; Numerical weather prediction models; Ramp events; accuracy assessment; climate prediction; data assimilation; instrumentation; NOAA satellite; numerical method; numerical model; project management; Weather forecasting",Value management
489,Developing a Framework for a Healthcare Data Science Hub; Challenges and Lessons Learned,"'Research through innovation' is the current demand echoing throughout the healthcare industry, healthcare institutions tend to invest heavily in technology. Data Science being the major disruptor across industries is being incepted through establishment of innovation and R&D centers within their respective organizations. Data Science has become a critical component for the healthcare industry, supporting innovative approaches towards advanced clinical practice, clinical research and corporate management, serving to build an intelligent enterprise. Every healthcare institution maintains a good number of technical staffs with IT, Software, data management, BI and analytical capabilities, aiding the institutions to manage report and publish its data in some or the other way, grossly covering most aspects of data science knowingly or unknowingly. Setting up a new entity within the organization by recruitment of staff with Data Science based skill sets would be the first thought to strike the management, which in contrast would end up as disaster when it comes to understanding the organizational culture, processes, infrastructure, platforms, data etc. Hence in order to setup a data science hub, regrouping or realigning some of the existing institutional resources is crucial. With this approach, the Data Science hub would carry out three primary functions. The 'Project Management & Data Sourcing', the 'Data Management & General Analytics' and 'Advanced Analytics'. Current resources can be reorganized within the first two functions, further; it would be about establishing an advanced analytics group within the hub which would perform the Machine learning and AI functions. © 2019 The authors and IOS Press. All rights reserved. Developing a Framework for a Healthcare Data Science Hub; Challenges and Lessons Learned Advanced Analytics; Data Science; Machine Learning Data Science; Delivery of Health Care; Humans; Information Storage and Retrieval; Machine Learning; Software; Advanced Analytics; Clinical research; Data Science; Health care; Human resource management; Learning systems; Machine learning; Medical informatics; Project management; Clinical practices; Corporate management; Healthcare industry; Healthcare institutions; Innovative approaches; Institutional resources; Intelligent enterprise; Organizational cultures; clinical practice; clinical research; conference paper; data science; disaster; health care industry; human; machine learning; organizational culture; skill; software; staff; health care delivery; information retrieval; machine learning; Information management",Governance
490,A Practical Approach of Teaching Digitalization and Safety Strategies in Cyber-Physical Production Systems,"Digitalization strategies in cyber-physical production systems (CPPS) are one of the key factors of Industry 4.0. The topic not only addresses data preparation, real-time data processing, big data analytics, visualization and machine interface design but also cyber security and safety. Especially, unauthorized access to protected (personal or enterprise) data or unauthorized control of production facilities imply risks when it comes to digitalization. Because of the increased complexity of state-of-the-art technologies, educational institutions need to provide practice-oriented teaching methods in learning factories to help engineers of today understand the impact of those developments. In the light of this fact, this paper presents a practical approach of teaching digitalization strategies in CPPS. Planning, implementing and impacts of digitalization strategies are taught on a use-case with human-robot-collaboration. The objective of the use-case is to realize a real-time obstacle avoidance approach for a collaborative application based on a local positioning system. Here, students not only learn how to model the kinematics of a robot and program a robot but also how to design machine interfaces for real-time data transfer and processing as well as impacts of digitalization on safety and security. The implementation of the use-case is part of the TU Wien teaching portfolio and thus part of its learning factory, where students and apprentices have the possibility to experiment and gain experiences by deliberate error simulations. © 2019 The Authors. Published by Elsevier B.V. A Practical Approach of Teaching Digitalization and Safety Strategies in Cyber-Physical Production Systems Cyber-physical production system; digitalization; human-robot-collaboration; safety; security ",Monitoring and control
491,Consumer driven product technology function deployment using social media and patent mining,"The capability of identifying real-time customer needs is critical for manufacturers that provide short life cycle consumer products such as smart phones. Companies need to form research and development (R&D) strategies to improve key functional features for short lifespan products to reflect the adoption of innovative technologies and changing customer expectations. With the pervasive use of the Internet, this research crawls and analyzes the online voice of customers (VoC), overcoming the time lag of offline surveys, to identify and prioritize product functions for deployment using extended quality function deployment (eQFD) models. In this research, the novel analytics of the manufacturer's patent portfolio is added as an additional eQFD dimension to map ranked functional improvements to a manufacturer's R&D capabilities. Thus, a computer supported eQFD system is developed to perform the unique mappings and gap analyses between the VoC, the prioritized product functions, and the manufacturer's patent portfolio. The newly developed eQFD methodology and its novel discoveries are demonstrated in detail using a case study of three smart phones launched during the same time frame. The products include the Samsung Galaxy S7, the Huawei Honor 5X, and the ASUS Zenfone 3. The newly developed methodology is generally applicable to support VoC-centric product function deployment and R&D strategic planning in other domains. © 2018 Elsevier Ltd Consumer driven product technology function deployment using social media and patent mining Data mining; Latent semantic analysis; Patent analysis; Quality function deployment (QFD); Voice of the customer; Web mining Consumer products; Data mining; Patents and inventions; Quality control; Quality function deployment; Sales; Semantics; Smartphones; Social networking (online); Telephone sets; Latent Semantic Analysis; Patent analysis; Quality function deployments (QFD); Voice of the customer; Web Mining; Life cycle",Strategic alignment
492,Deploymentand life cycle management,"The successful implementation of an analytics project relies on strong project management skills and on leveraging the analytic and data insights that are the unique contributions of analytics professionals. This chapter focuses on joining those pieces together in a structured and ordered way, while detailing the special nature, complexities, and challenges of delivering analytics projects. A life cycle is defined as a sequence of phases in the process of developing an analytics model or system. The chapter describes the six major components of the CRoss Industry Standard Process for Data Mining (CRISP-DM) methodology: business understanding, data understanding, data preparation, modeling, evaluation, and deployment. Metrics are numerical measures that represent important summary information about the operations of a business or organization. Successful models must support and provide information that is consistent with the sponsoring organizations business metrics and produce outputs that directly support their calculation and credibility. © 2019 John Wiley and Sons, Inc. All rights reserved. Deploymentand life cycle management Analytics professionals; Analytics project; CRISP-DM; Life cycle management; Project management skills ",Capacity management
493,Quantifying Model Uncertainty and Risk,"In the last two decades, significant progress in science and technology has driven major advances in the modeling of natural hazards and their impact on the built environment. In addition, several major catastrophe events over the same period highlighted important issues that could affect the underwriting and decision-making processes for selecting and managing risk. In many cases, these events had a profound impact on insurance and reinsurance practices, expanding the use of models, analytics, and exposure-data quality analysis to understand the sources of risk and to model loss uncertainty. Furthermore, since the development of earlier versions of catastrophe models, technology and computing power have advanced enormously, allowing modelers to address key elements of model and loss uncertainty in a more systematic way.Most catastrophe models implicitly account for the uncertainty associated with various model components. These uncertainties can be characterized as aleatory or epistemic. The aleatory uncertainty represents the inherent variability in the physical system, which cannot be reduced. In contrast, the epistemic uncertainty relates to the lack of knowledge about the damage and hazard model components (i.e., building characteristics, ground motion, central pressure, etc.) and thus can be reduced with additional information about the system.A complete quantification of the contribution of each model component to the epistemic uncertainty requires a systematic approach and thus involves developing multiple alternative versions of the hazard and damage modules. By developing multiple versions, the modeler can capture the plausible range of parameters that control each model component. The risk model can then propagate those ranges into estimating variability of losses for a given portfolio. However, developing such a bottom-up quantification of the catastrophe model uncertainty is time consuming, and carrying out all the required analyses is also computationally expensive for practical applications. Nevertheless, modelers can now take advantage of the robust computational power developed in recent years to quantify systematically the uncertainty in the estimation of risk. This paper illustrates some practical approaches to capture uncertainties in risk models. The issues and challenges that remain in developing risk models and in quantifying the uncertainty, even with today's advancement in science and engineering, and vastly enriched technological environment, are also addressed in this paper. It draws on specific examples from the authors' experience in modeling earthquake and hurricane perils. © 2018 Elsevier Inc. All rights reserved. Quantifying Model Uncertainty and Risk Catastrophe; Earthquake; Hurricane; Loss; Uncertainty ",Value management
494,Which startup to invest in: a personalized portfolio strategy,"Recent years have witnessed a venture capital boom. By offering capitals and mentoring, venture investors would receive high returns if their portfolio companies successfully exit, namely being acquired or going Initial Public Offering. However, the screening and evaluation of startups for investment largely depends on the investors’ personal experiences, social relationships, and qualitative evaluation on firms. The entrepreneurial finance industry thus has a strong call for the methodologically sound, quantitative study of venture capital deals. Plus, more accessible data and more sophisticated analytics techniques signal the opportunity for methodical decision-making in investing the venture financing market. To this end, in this paper, we aim at developing a personalized portfolio strategy for assisting investors to target the right startups and determine the proper amount of capital to fund. Specifically, we first develop a Probabilistic Latent Factor model to estimate the investment preferences of all the investors in a collaborative way. The model is fitted with not only the historical investment records but also the profiles of the venture capitalists and the startups. Then, we assess the startups’ investment outcomes by regressing the potential returns and risks. We improve the regression performances by nonparametric methods. At last, we use modern portfolio theory to optimize the investment strategy over the startups recommended by the investment preference model. As a result, the investment strategy can yield maximized returns with suppressed potential risks, and meanwhile meet the investment preferences of the venture capitalists. The proposed method is evaluated using data from venture financing markets in USA, and the results show that our method outperforms other state-of-the-art methods on various metrics. © 2016, Springer Science+Business Media New York. Which startup to invest in: a personalized portfolio strategy Financial decision-makers’ preferences; Financial portfolio selection; Investment strategy; Recommender system; Venture capital ",Strategic alignment
495,Unified business intelligence ecosystem: A project management approach to address business intelligence challenges,"The purpose of this paper is to explore a concept that improves the success rate of Business Intelligence projects by looking at it as an ecosystem rather than standalone project. This paper discusses the importance on Business Intelligence (BI) initiatives and today's challenges in implementing them. The concentration is on using proven practices of project management to reduce the failure rates of the Business Intelligence projects by evaluating currently existing Information Technology (IT) project process, Business Intelligence frameworks and a Project Vulnerability process in order to adopt some of the best project management practices that works for Business Intelligence projects. The paper studies currently existing processes and introduces a new concept which henceforth will be termed as 'Unified Business Intelligence Ecosystem' framework (UBIE). This research is based on various project management frameworks, those are proven to increase the efficiency of projects, and far-ranging sources of research on nature of Business Intelligence projects, personal observations and real-world Business Intelligence consulting experience. Research limitations/implications: Business Intelligence is a constantly developing field. Partial standardization and extensive availability and access to Business Intelligence resources has created many definitions and buzz words in the industry. The assumptions described in this research might seem arguable given the reader's experience in this field. Originality/value: The paper presents a different perspective to view and manage Business Intelligence projects. It encourages to interpret Business Intelligence projects as an ecosystem by combining technical, business and management aspect rather than separate, standalone entity. The aim of this research paper is to achieve Business Intelligence success by customizing Project Management (PM) practices to fit the unique need of Business Intelligence projects. Acronyms Business Intelligence-BI Project Management-PM Data Science-DS Information technology-IT Unified Business Intelligence Ecosystem-UBIE Responsibility, Accountability, Consulted, Informed-RACI. © 2018 Portland International Conference on Management of Engineering and Technology, Inc. (PICMET). Unified business intelligence ecosystem: A project management approach to address business intelligence challenges  Economics; Ecosystems; Failure analysis; Project management; Best projects; Business and management; Business Intelligence projects; Business intelligence success; Failure rate; Information technology projects; Project management frameworks; Research papers; Information analysis",Value management
496,Leveraging Big Data from Open Source to Improve Software Project Management,"Software engineering management research has been conducted for many years. Researchers have faced a myriad of challenges in obtaining reliable data and associated metrics, developing practical estimating models, and influencing project improvements within industry. Open source software (OSS) can be leveraged to address these issues. This paper explores how big data from OSS can improve software engineering and management practices. The authors begin to explore a select few metrics to predicting process quality. Improved software engineering management methods are identified. It is expected that these tangible research outcomes would be of keen interest to practitioners. © 1973-2011 IEEE. Leveraging Big Data from Open Source to Improve Software Project Management Big data; predictive models; quality metrics; software automation; software information Big data; Open Data; Open systems; Predictive analytics; Project management; Management practices; Predictive models; Process quality; Quality metrics; Research outcome; Software automation; Software engineering management; Software project management; Open source software",Value management
497,Overestimation in operators budgets and long-term forecasting; A non-operator perspective,"EBN is the Dutch state energy company that is a large non-operating partner of over 10 different operators that produce from more than 200 on- and offshore assets with more than 850 projects defined on them. Estimating budget production, medium and long-term forecasts and its associated operating and capital expenditures are of vital importance to EBN. Larger companies with many assets and even more projects, at varying degree of maturity, have great difficulty to reliably predict an aggregated forecast. Historically, EBN would copy and risk operator data, which led to continuous overestimation of both budget production and longterm forecasts. A straightforward correction method was developed; that consists of two parts: firstly, the budget production is set for all producing assets and projects by assessing technical, subsurface, infrastructural and human factors on the operator's fields and projects performance. Secondly, the medium and longterm forecast is delayed with 1 to 4 years for respective SPE PRMS resource classes ""justified for development"" to ""project unviable"" and the associated project forecasts are risked with a chance of development according to their subclasses of the contingent resource classes. Data analytics on almost 10 years of reserve reporting according to SPE PRMS standards led to a straightforward solution to reduce short and medium-term forecasting error. The short-term absolute average error used to be 8%. Through the implementation of the new method, 7 years ago, the absolute average short-term forecasting error dropped to 4%. The long-term aggregated forecast, obtained by simply copying the operator data, resulted in an overestimation of up to 50% 5 years ahead. The overestimation was reduced to an absolute average error of 23% by an earlier correction method, which only used risking factors on contingent projects, but no time delay. This paper presents a new method, that uses both risking factors and time delays on the realization of projects. The method reduced the error in the long-term forecast to an uncertainty band of a few percent. Various causes for the overestimation were identified. The budget production errors were primarily attributed to wrong uptime predictions. Longterm forecast errors are impacted by the overestimation of the number of executed projects, while the timing and performance of new projects affects both the short and middle term forecasts. The solution presented is the first methodology for EBN that is able to predict aggregated forecasts of hundreds of projects of several operators with an accuracy within a 5% margin over a lengthy period. The described risking factors described, and delay times, are dependent on the portfolio maturity and investment climate. Historic data has to be utilized to determine these factors for your portfolio. Copyright 2019, Society of Petroleum Engineers. Overestimation in operators budgets and long-term forecasting; A non-operator perspective  Budget control; Data Analytics; Electromagnetic wave attenuation; Errors; Investments; Offshore oil well production; Time delay; Capital expenditures; Contingent resources; Correction method; Forecasting error; Investment climate; Long-term forecast; Long-term forecasting; Short-term forecasting; Forecasting",Monitoring and control
498,Big Data Analytics: A Trading Strategy of NSE Stocks Using Bollinger Bands Analysis,"The availability of huge distributed computing power using frameworks like Hadoop and Spark has facilitated algorithmic trading employing technical analysis of Big Data. We used the conventional Bollinger Bands set at two standard deviations based on a band of moving average over 20 minute-by-minute price values. The Nifty 50, a portfolio of blue chip companies, is a stock index of National Stock Exchange (NSE) of India reflecting the overall market sentiment. In this work, we analyze the intraday trading strategy employing the concept of Bollinger Bands to identify stocks that generates maximum profit. We have also examined the profits generated over one trading year. The tick-by-tick stock market data has been sourced from the NSE and was purchased by Amrita School of Business. The tick-by-tick data being typically Big Data was converted to a minute data on a distributed Spark platform prior to the analysis. © 2019, Springer Nature Singapore Pte Ltd. Big Data Analytics: A Trading Strategy of NSE Stocks Using Bollinger Bands Analysis Big data; Bollinger bands; Chartists; Intraday trading; NSE; Spark Commerce; Distributed computer systems; Electric sparks; Electronic trading; Financial markets; Profitability; Algorithmic trading; Big Data Analytics; Blue chip companies; Bollinger bands; Chartists; Intraday trading; Standard deviation; Trading strategies; Big data",Financial management
499,Interpretability of machine learning solutions in industrial decision engineering,"The broad application of machine learning (ML) methods and algorithms in diverse range of organisational settings led to the adoption of legislation, like European Union’s General Data Protection Regulation, which require firm capabilities to explain algorithmic decisions. Currently in the ML literature there does not seem to be a consensus on the definition of interpretability of a ML solution. Moreover, there is no agreement about the necessary level of interpretability of such solution and on how this level can be determined, measured and achieved. In this article, we provide such definitions based on research as well as our extensive experience of building ML solutions for various organisations across industries. We present CRISP-ML, a detailed step-by-step methodology, that provides guidance on creating the necessary level of interpretability at each stage of the solution building process and is consistent with the best practices of project management in the ML settings. We illustrate the versatility and effortless applicability of CRISP-ML with examples across a variety of industries and types of ML projects. © Springer Nature Singapore Pte Ltd. 2019. Interpretability of machine learning solutions in industrial decision engineering Data science methodology; Interpretability in machine learning; Level of interpretability; Machine learning methodology; Model interpretability; Project management Data mining; Laws and legislation; Project management; Broad application; Building process; European union; Firm capabilities; General data protection regulations; Interpretability; Organisational; Science methodologies; Machine learning",Strategic alignment
500,Applied Probabilistic Calculus for Financial Engineering: An Introduction Using R,"Illustrates how R may be used successfully to solve problems in quantitative finance Applied Probabilistic Calculus for Financial Engineering: An Introduction Using R provides R recipes for asset allocation and portfolio optimization problems. It begins by introducing all the necessary probabilistic and statistical foundations, before moving on to topics related to asset allocation and portfolio optimization with R codes illustrated for various examples. This clear and concise book covers financial engineering, using R in data analysis, and univariate, bivariate, and multivariate data analysis. It examines probabilistic calculus for modeling financial engineering-walking the reader through building an effective financial model from the Geometric Brownian Motion (GBM) Model via probabilistic calculus, while also covering Ito Calculus. Classical mathematical models in financial engineering and modern portfolio theory are discussed-along with the Two Mutual Fund Theorem and The Sharpe Ratio. The book also looks at R as a calculator and using R in data analysis in financial engineering. Additionally, it covers asset allocation using R, financial risk modeling and portfolio optimization using R, global and local optimal values, locating functional maxima and minima, and portfolio optimization by performance analytics in CRAN. • Covers optimization methodologies in probabilistic calculus for financial engineering • Answers the question: What does a ""Random Walk"" Financial Theory look like? • Covers the GBM Model and the Random Walk Model • Examines modern theories of portfolio optimization, including The Markowitz Model of Modern Portfolio Theory (MPT), The Black-Litterman Model, and The Black-Scholes Option Pricing Model Applied Probabilistic Calculus for Financial Engineering: An Introduction Using R s an ideal reference for professionals and students in economics, econometrics, and finance, as well as for financial investment quants and financial engineers. © 2017 John Wiley & Sons, Inc. All rights reserved. Applied Probabilistic Calculus for Financial Engineering: An Introduction Using ARE  Brownian movement; Data handling; Economics; Financial data processing; Information analysis; Investments; Multivariant analysis; Statistics; Financial engineering; Financial investments; Geometric Brownian motion; Modern portfolio theories; Multivariate data analysis; Optimization methodology; Portfolio optimization; Probabilistic calculus; Calculations",Strategic alignment
501,What Data Science Means to the Business,"Big data have been associated with some common misconceptions so far, and this chapter will help the reader in identify and understand those fallacies. It is going to be then shown the best data deployment approach, followed by an ideal internal data management process. A four-stages development structure will be provided, in order to assess the big data internal advancements, and a data maturity map will summarize a set of relevant metrics that should be considered for an efficient big data strategy. © 2016, Springer International Publishing Switzerland. What Data Science Means to the Business Data Strategy; Information Life Cycle; Lean Approach; Primitive Stage; Project Management Support Information management; Life cycle; Project management; Data maturity; Data strategy; Information life cycle; Lean approach; Management process; Management support; Primitive stage; Project management support; Big data",Risk management
502,What Does the Volatility Risk Premium Say About Liquidity Provision and Demand for Hedging Tail Risk?,"This article provides a data-driven analysis of the volatility risk premium, using tools from high-frequency finance and Big Data analytics. We argue that the volatility risk premium, loosely defined as the difference between realized and implied volatility, can best be understood when viewed as a systematically priced bias. We first use ultra-high-frequency transaction data on SPDRs and a novel approach for estimating integrated volatility on the frequency domain to compute realized volatility. From that we subtract the daily VIX, our measure of implied volatility, to construct a time series of the volatility risk premium. To identify the factors behind the volatility risk premium as a priced bias, we decompose it into magnitude and direction. We find compelling evidence that the magnitude of the deviation of the realized volatility from implied volatility represents supply and demand imbalances in the market for hedging tail risk. It is difficult to conclusively accept the hypothesis that the direction or sign of the volatility risk premium reflects expectations about future levels of volatility. However, evidence supports the hypothesis that the sign of the volatility risk premium is indicative of gains or losses on a delta-hedged portfolio. © 2016 American Statistical Association. What Does the Volatility Risk Premium Say About Liquidity Provision and Demand for Hedging Tail Risk? Big Data risk analytics; Fourier transform; Integrated volatility; Microstructure noise; Tail risk; Ultra-high-frequency data; Volatility risk premium ",Risk management
503,Implementing asset management for the U.S. Army Corps of Engineers Civil Works,"The YOU.S. Army Corps of Engineers (USACE) Civil Works oversees and administers an asset portfolio with more than $250 billion in capital investments and hundreds of operating projects located in all 50 states, as well as several international river basins. Reliable performance of the nation's investment in infrastructure is essential to the asset portfolio's ability to deliver safe and dependable service. In response to this need, USACE has developed a conceptual strategic investment framework addressing four main areas (maintenance management, operational condition assessments, risk assessment, and portfolio analytics) to provide improved tools and business processes in order to focus strategic investments on the most mission-critical infrastructure assets/components that: a) are in the worst shape/condition and b) have the highest likelihood of failing and impacting mission delivery, which will c) because the highest adverse impact to the public and the nation. This application of a cradle to grave lifecycle approach across all Civil Works infrastructure systems will allow USACE to better prioritize limited funding and make improved investment choices at key decision points throughout the complete life cycle of a project. This effort is aligned with ISO 55000 asset management principles and global best practices, adapted to the unique multi-mission and public features of Civil Works infrastructure. © 2016 Institution of Engineering and Technology. All rights reserved. Implementing asset management for the YOU.S. Army Corps of Engineers Civil Works Asset management; Condition assessment; Integrated water resource management; Life cycle; Maintenance management; Portfolio analytics; Public agency; Risk-informed; System Asset management; Life cycle; Maintenance; Risk assessment; Water management; Condition assessments; Integrated water resource management; Maintenance management; Portfolio analytics; Public agencies; System; Investments",Strategic alignment
504,Developing and rewarding teachers as educators and scholars: remarkable progress and daunting challenges,"Context: This article describes the scholarly work that has addressed the fifth recommendation of the 1988 World Conference on Medical Education: ‘Train teachers as educators, not content experts alone, and reward excellence in this field as fully as excellence in biomedical research or clinical practice’. Progress: Over the past 30 years, scholars have defined the preparation needed for teaching and other educator roles, and created faculty development delivery systems to train teachers as educators. To reward the excellence of educators, scholars have expanded definitions of scholarship, defined educator roles and criteria for judging excellence, and developed educator portfolios to make achievements visible for peer review. Despite these efforts, the scholarship of discovery continues to be more highly prized and rewarded than the scholarship of teaching. These values are deeply embedded in university culture and policies. Challenges: To remedy the structural inequalities between researchers and educators, a holistic approach to rewarding the broad range of educational roles and educational scholarship is needed. This requires strong advocacy to create changes in academic rewards and support policies, provide a clear career trajectory for educators using learning analytics, expand programmes for faculty development, support health professions education scholarship units and academies of medical educators, and create mechanisms to ensure high standards for all educators. © 2017 John Wiley & Sons Ltd and The Association for the Study of Medical Education Developing and rewarding teachers as educators and scholars: remarkable progress and daunting challenges  Curriculum; Education, Medical; Faculty, Medical; Humans; Organizational Culture; Reward; Staff Development; Teaching; achievement; career; clinical practice; human; human experiment; learning; medical education; medical research; peer review; reward; scientist; teacher; university; curriculum; medical school; organizational culture; personnel management; procedures; reward; standards; teaching",Strategic alignment
505,Many-objective optimization and visual analytics reveal key trade-offs for London's water supply,"In this study, we link a water resource management simulator to multi-objective search to reveal the key trade-offs inherent in planning a real-world water resource system. We consider new supplies and demand management (conservation) options while seeking to elucidate the trade-offs between the best portfolios of schemes to satisfy projected water demands. Alternative system designs are evaluated using performance measures that minimize capital and operating costs and energy use while maximizing resilience, engineering and environmental metrics, subject to supply reliability constraints. Our analysis shows many-objective evolutionary optimization coupled with state-of-the art visual analytics can help planners discover more diverse water supply system designs and better understand their inherent trade-offs. The approach is used to explore future water supply options for the Thames water resource system (including London's water supply). New supply options include a new reservoir, water transfers, artificial recharge, wastewater reuse and brackish groundwater desalination. Demand management options include leakage reduction, compulsory metering and seasonal tariffs. The Thames system's Pareto approximate portfolios cluster into distinct groups of water supply options; for example implementing a pipe refurbishment program leads to higher capital costs but greater reliability. This study highlights that traditional least-cost reliability constrained design of water supply systems masks asset combinations whose benefits only become apparent when more planning objectives are considered. © 2015 The Authors. Many-objective optimization and visual analytics reveal key trade-offs for London's water supply Evolutionary multi-objective optimization; Infrastructure system design; Multi-criteria decision-making; Trade-off analysis and visualization; Water resources planning Cost engineering; Costs; Decision making; Desalination; Design; Economic and social effects; Groundwater; Multiobjective optimization; Operating costs; Optimization; Recharging (underground waters); Reliability; Reservoirs (water); Systems analysis; Visualization; Wastewater reclamation; Water conservation; Water management; Water supply; Water supply systems; Evolutionary multiobjective optimization; Infrastructure systems; Multi criteria decision making; Trade-off analysis; Water resources planning; Water resources",Value management
506,Conditional value at risk-based portfolio optimization using metaheuristic approaches,"Of late, the field of portfolio optimization (the process of selecting the proportions of diverse assets existing within a portfolio and building the portfolio to be the best in relation to some criterion) in the modern capital market has assumed paramount importance as a field in business intelligence thanks to the evolution of the multiobjective optimization of the market risk-return paradigm. The downside risk can be ascertained with a very common method within a portfolio known as value at risk (VaR), which, in turn, can be explicated as the pth percentile of return of a defined portfolio during the termination of the planning skyline. The conditional value at risk (CVaR) is a more robust expedient for determining the defined unit of risk of a portfolio in volatile market conditions. The soft computing paradigm is efficient in handling real-life uncertainties. It entails several tools and techniques, namely, neural networks, the concept of fuzzy logic, and evolutionary computation measures. Applications of three different soft computing-based metaheuristic approaches to risk minimization leading to portfolio optimization using particle swarm optimization (PSO), ant colony optimization (ACO), and differential evolution (DE) techniques centering on optimizing the CVaR measure under different market conditions based on several objectives and constraints are reported in this chapter. The proposed approaches are proven to be reliable on a collection of several financial instruments as compared to their VaR counterparts. The results obtained show encouraging avenues in determining optimal portfolio returns. © 2017 by Taylor & Francis Group, LLC. Conditional value at risk-based portfolio optimization using metaheuristic approaches  Commerce; Electronic trading; Fuzzy logic; Fuzzy neural networks; Multiobjective optimization; Particle swarm optimization (PSO); Risk assessment; Risk perception; Soft computing; Value engineering; Ant Colony Optimization (ACO); Conditional Value-at-Risk; Differential Evolution; Meta-heuristic approach; Optimal portfolios; Portfolio optimization; Risk minimization; Tools and techniques; Ant colony optimization",Risk management
507,Business intelligence in the context of integrated care systems (ICS): Experiences from the ICS “gesundes kinzigtal” in germany,"Patients generate various data with every contact to the health care system. In integrated care systems (ICS) these fragmented patient data sets of the various health care players can be connected. Business intelligence (BI) technologies are seen as valuable tools to gain insights and value from these huge volumes of data. However so far there are just sparse experiences about BI used in the integrated care (IC) context. Therefore the aim of this article is to describe how a BI solution can be implemented practically in an ICS and what challenges have to be met. By the example of a BI best practice model-the ICS Gesundes Kinzigtal-it will be shown that data from various data sources can be linked in a Data Warehouse, prepared, enriched and used for management support via a BI front-end: starting with the project preparation and development via the ongoing project management up to a final evaluation. Benefits for patients, care providers, the ICS management company and health insurers will be characterised as well as the most crucial lessons learned specified. © Springer International Publishing Switzerland 2016. Business intelligence in the context of integrated care systems (ICS): Experiences from the ICS “gesundes kinzigtal” in germany  Data warehouses; Health care; Hospital data processing; Information analysis; Management science; Project management; Best practices; Data-sources; Gain insight; Health-care system; Integrated care; Management support; Patient data; Project preparation; Intelligent control",Risk management
508,Introducing cybernomics: A unifying economic framework for measuring cyber risk,"This is the first in a series of papers on the risk measures and unifying economic framework encompassing the cross-disciplinary field of “Cybernomics”. This is also the first academic paper to formally propose measurement units for cyber risk. In this paper, multidisciplinary methodologies are used to apply proven risk measurement methods in finance and medicine to define novel risk units central to cybernomics. Leveraging established risk units – MicroMort (MM) for measuring medical risk and Value-at-Risk (VaR) for measuring market risk – BitMort (BM) and hekla (named after an Icelandic volcano) are defined as cyber risk units. Risk calculation methods and examples are introduced in this paper to measure cost-effectiveness of control factors, articulate an entity's “willingness-to-pay” (risk pricing) for cyber risk reduction, cyber risk limit, and cyber risk appetite. Built around BM and hekla, cybernomics integrates cyber risk management and economics to study the requirements of a databank in order to improve risk analytics solutions for: 1) the valuation of digital assets; 2) the measurement of risk exposure of digital assets; and 3) the capital optimization for managing residual cyber risk. Establishing adequate, holistic and statistically robust data points on the entity, portfolio and global levels for the development of a cybernomics databank are essential for the resilience of our shared digital future. This paper explains the need to establish data schemes such as International Digital Asset Classification (IDAC) and International Classification of Cyber Incidents (ICCI). © 2016 Elsevier Ltd Introducing cybernomics: A unifying economic framework for measuring cyber risk Cyber risk unit; Cybernomics; Economic modelling; Enterprise risk management; Risk analytics Cost effectiveness; Economics; Risk management; Units of measurement; Value engineering; Cross-disciplinary; Cyber risk unit; Cybernomics; Economic framework; Economic modelling; Enterprise risk management; Risk calculation; Willingness to pay; Risk assessment",Risk management
509,Alliance Portfolio Management: A Model Based on Dynamic Capabilities,"Alliances established by firms are increasing since three decades and these firms have to manage an important alliance portfolio. Researches have demonstrated that alliances contribute to the improvement of the firm’s performance via savings in coordination costs, access to new resources and competencies, the development of new activities and new markets, or the reinforcement of the competitive position. The increasing contribution of the alliances to the turnover and the organization of the activities of the firm make the portfolio as a key strategic asset. Our research question relates to the definition of an integrating model which takes the multidimensional nature of alliance portfolio management into consideration. In an attempt to improve it, our objective is to suggest a modeling of the portfolio management based on recognized and complementary corpuses: the resource-based approach and the evolutionary model. Specifically, we develop an emerging approach based on the concept of dynamic capabilities (Teece et al., Strateg Manag J 18:509–533, 1997) using business intelligence, networking, alliance management, and absorptive capabilities. The creation of an “alliance unit” plays a crucial role in the development of the alliance portfolio management capabilities. This model aims to optimize the composition and the management of the alliance portfolio to improve the value creation linked to the alliance strategy and the firm performance so that it obtains a specific advantage. © Springer International Publishing AG 2017. Alliance Portfolio Management: A Model Based on Dynamic Capabilities Alliance Unit; Business Intelligence; Dynamic Capability; Management Capability; Portfolio Management ",Strategic alignment
510,BCG analysis as an innovative it application in industrial companies,"Long-term and strategic cooperation between industry and universities is one of the basic tools for achieving the desired competitiveness of companies. The research team from the Institute of Economics and Control Systems HGF VSB-TUO in cooperation with industrial companies in the region creates an innovative system for processing of strategic analyses of industrial companies, which assists managers in their decision making when solving current needs. One of analyses, which this system includes, is BCG analysis. This is a known method which faces a number of obstacles in practice. The paper shows how the research team proceeded in the integration of BCG into a comprehensive system, and how the negative characteristics of this analysis were removed. BCG analysis as an innovative it application in industrial companies BCG analysis; Bcg matrix; Business intelligence applications; Industrial companies; Innovation; Market analysis; Portfolio analysis; School managers; Strategic analysis; Strategic management Competition; Decision making; Economic and social effects; Economics; Human resource management; Industrial management; Industrial research; Information management; Innovation; Management science; Managers; BCG analysis; BCG matrixes; Business intelligence applications; Industrial companies; Market analysis; Portfolio analysis; Strategic analysis; Strategic management; Industrial economics",Strategic alignment
511,Reducing preventable harm: Observations on minimizing bloodstream infections,"Purpose - The purpose of this paper is to provide a practical framework that health care organizations could use to decrease preventable healthcare-acquired harms. Design/methodology/approach - An existing theory of how hospitals succeeded in reducing rates of central line-associated bloodstream infections was refined, drawing from the literature and experiences in facilitating improvement efforts in thousands of hospitals in and outside the USA. Findings - The following common interventions were implemented by hospitals able to reduce and sustain low infection rates. Hospital and intensive care unit (ICU) leaders demonstrated and vocalized their commitment to the goal of zero preventable harm. Also, leaders created an enabling infrastructure in the way of a coordinating team to support the improvement work to prevent infections. The team of hospital quality improvement and infection prevention staff provided project management, analytics, improvement science support, and expertise on evidence-based infection prevention practices. A third intervention assembled Comprehensive Unit-based Safety Program teams in ICUs to foster local ownership of the improvement work. The coordinating team also linked unit-based safety teams in and across hospital organizations to form clinical communities to share information and disseminate effective solutions. Practical implications - This framework is a feasible approach to drive local efforts to reduce bloodstream infections and other preventable healthcare-acquired harms. Originality/value - Implementing this framework could decrease the significant morbidity, mortality, and costs associated with preventable harms. © Emerald Publishing Limited. Reducing preventable harm: Observations on minimizing bloodstream infections Bloodstream infections; Improvement science; Patient safety; Preventable harm; Quality improvement Bacteremia; Catheter-Related Infections; Humans; Patient Care Team; Patient Safety; Quality Improvement; bacteremia; Catheter-Related Infections; human; organization and management; patient care; patient safety; total quality management",Value management
512,NPD Risk Management: Proposed implementation to increase new product success,"Innovation is one of the cornerstones of European manufacturing industry. For small and medium-sized (SMEs) enterprises NPD projects are very often a 'win or lose' game. Due to the limited availability of resources, the risk of failure in a single project can endanger the survival of the whole company. SMEs have to deal with the following problems when undertaking the development of new products: (1) lack of information, (2) lack of a risk management methodology, and (3) lack of decision aiding tools. This paper recognises the need of acquiring suitable information and focuses on Risk Management as a critical process to increase the success of New Product Development. A staged implementation plan composed of 3 steps is proposed: Business Intelligence System, Risk Management System and Optimisation of Decision Making Process. Currently implemented systems and steps are commented, as well as future action points, in order to increase the competitiveness of companies. © 2006 IEEE. NPD Risk Management: Proposed implementation to increase new product success Business Intelligence; Industrial Case; New Product Development; Project Management; Risk Management Competitive intelligence; Decision support systems; Industrial management; Information analysis; Management science; Product development; Project management; Risk management; Business intelligence systems; Decision making process; European manufacturing; Following problem; Management methodologies; New product development; New product success; Risk management systems; Decision making",Risk management
513,Differences in ERP Value between Iberian Manufacturing and Services SMEs,"Enterprise Resource Planning (ERP) system literature reports little research on the specificities of an industry analysis. Based on a theoretical model we assess ERP Value between Manufacturing and Services industries in Small and Medium Enterprises (SMEs) across the Iberian region (Portugal and Spain). The empirical test was conducted through structural equation modeling, using data from 261 firms. Results show that Firm size, Analytics and Collaboration contribute to ERP Value in those industries, with Analytics being more important for the Services industry. © 2017 The Authors. Published by Elsevier B.V. Differences in ERP Value between Iberian Manufacturing and Services SMEs Enterprise Resource Planning (ERP); ERP use; ERP Value; Manufacturing; Resource-Based View; Services; SMEs; Technology-Organizational-Environment Information management; Information systems; Manufacture; Project management; Resource allocation; Enterprise resource planning (ERP); Enterprise resource planning systems; Resource-based view; Services; Small and medium enterprise; SMEs; Structural equation modeling; Theoretical modeling; Enterprise resource planning",Stakeholder management
514,"IFIP WG 5.7 International Conference on Advances in Production Management Systems, APMS 2016","The proceedings contain 112 papers. The special focus in this conference is on Advances in Production Management Systems. The topics include: Logical decision-making method relating to innovation management; hierarchical clustering based on reports generated by scriptlattes; using logic concepts on six sigma; formal information model for representing production resources; a communication procedure between tactical and operational levels in spare parts supply chains; digital factories for capability modeling and visualization; learning analytics deployment at an university; environmental support for dilution of pollutants from broiler production and aquaculture in brazil; combining genetic algorithm with constructive and refinement heuristics for solving the capacitated vehicle routing problem; container crane controller with the use of a neurofuzzy network; improving process management in a water treatment plant using control modelling; dynamic seed genetic algorithm to solve job shop scheduling problems; strategic portfolios for the integral design of value-added networks; workforce planning models for distribution center operations; an application of operations research for reducing fuel costs; adaptive configuration of the organization in manufacturing startup companies; support policies and collective efficiency in a furniture cluster; the importance of timely feedback to interactivity in online education; assessment of structural qualities of production systems; comparing techniques for selecting automation technology; theoretical framework of performance indicators with BSC for the private higher education institution; the identification of the professional profile that uses canvas approach and sustainable development within enterprise architecture. IFIP WG 5.7 International Conference on Advances in Production Management Systems, APMS 2016  ",Strategic alignment
515,Big Data—How to Get Started,"This chapter focuses on the organizational and people aspect of the big-data challenge, there are obviously technical obstacles to overcome as well. Institutions embark on big-data journeys for many reasons. For some, it is a continuation of a successful “little data” Analytics 1.0 effort. Leveraging existing domain maturity as a component in the selection of the big-data use case can help reduce risk and improve time to delivery. Big data initiatives can be daunting but are absolutely amenable to the tenets of good project management. The role of little data and big data strategically and the institution’s resource commitment will ultimately determine the scale and capabilities of the analytics environment. Data warehousing has always been difficult, because leaders within an organization want to approach warehousing and analytics as just another technology or application buy. Many organizations continue to approach warehousing as just another application, and therein lies the root of failure. © 2017 by Taylor & Francis Group, LLC. Big Data—How to Get Started  ",Value management
516,Estimating the value of decisions relating to managing and developing software-intensive products and projects,"The software industry's current decision-making relating to product/project management and development is largely done in a value neutral setting, in which cost is the primary driver for every decision taken. However, numerous studies have shown that the primary critical success factor that differentiates successful products/projects from failed ones lie in the value domain. Therefore, to remain competitive, innovative and to grow, companies must change from cost-based decision-making to value-based decision-making where the decisions taken are the best for that company's overall value creation. Our vision to tackle this problem and to provide a solution for value estimation is to employ a combination of qualitative and machine learning solutions where a probabilistic model encompassing the knowledge from different stakeholders will be used to predict the overall value of a given decision relating to product management and development. This vision drives the goal of a 3-year research project funded by the Finnish Funding Agency for Technology and Innovation (Tekes), with the participation of several industry partners. © 2015 ACM. Estimating the value of decisions relating to managing and developing software-intensive products and projects Bayesian network; Software product and project management; Value-based decision making; Value-based software engineering Bayesian networks; Costs; Digital storage; Predictive analytics; Project management; Software engineering; Critical success factor; Funding agencies; Probabilistic modeling; Product management; Software industry; Software products; Value based software engineering; Value-based; Decision making",Strategic alignment
517,Understanding alliance portfolios using visual analytics,"In an increasingly global and competitive business landscape, firms must collaborate and partner with others to ensure survival, growth, and innovation. Understanding the evolutionary composition of a firm's relationship portfolio and the underlying formation strategy is a difficult task given the multidimensional, temporal, and geospatial nature of the data. In collaboration with senior executives, we iteratively determine core design requirements and then design and implement an interactive visualization system that enables decision makers to gain both systemic (macro) and detailed (micro) insights into a firm's alliance activities and discover patterns of multidimensional relationship formation. Our system provides both sequential and temporal representation modes, a rich set of additive cross-linked filters, the ability to stack multiple alliance portfolios, and a dynamically updated activity state model visualization to inform decision makers of past and likely future relationship moves. We illustrate our tool with examples of alliance activities of firms listed on the S&P 500. A controlled experiment and real-world evaluation with practitioners and researchers reveals significant evidence of the value of our visual analytic tool. Our design study contributes to design science by addressing a known problem (i.e., alliance portfolio analysis) with a novel solution (interactive, pixel-based multivariate visualization) and to the rapidly emerging area of data-driven visual decision support in corporate strategy contexts. We conclude with implications and future research opportunities. © 2017 ACM. Understanding alliance portfolios using visual analytics Alliances; Strategy decision support; Visual analytics Data visualization; Decision making; Decision support systems; Alliances; Design and implements; Interactive visualization systems; Multivariate visualization; Research opportunities; Strategy decision; Temporal representations; Visual analytics; Visualization",Strategic alignment
518,Quantifying the benefits of investment portfolio optimisation versus prioritisation for asset intensive organisations,"Historically, many organisations have made investment decisions using a conventional ranking and prioritisation process. When prioritising, a fixed score is determined for each investment. This can be a measure of the investment's financial return (e.g. Net Present Value), or possibly a measure of the risk that the investment will mitigate for the organisation (risk score). During prioritisation, the portfolio of investments is ranked by that fixed score, and then those investments that can be executed within the budgetary constraints are selected. Mathematical optimisation using linear programming can improve on prioritisation results and achieve higher value outcomes whilst honouring multiple constraints (e.g. financial, service level, resources, timing, inter-project dependencies, and risk tolerances). Whilst individual organisations have reported significant benefits of using optimisation techniques these results are naturally specific to their operating context. This study seeks to generalise these results and quantify the value obtained by using optimisation techniques on portfolios of asset investments. © 2016 Institution of Engineering and Technology. All rights reserved. Quantifying the benefits of investment portfolio optimisation versus prioritisation for asset intensive organisations Asset investment planning & management; Decision analytics; Optimisation; Prioritisation; Quantified benefits of asset management Asset management; Budget control; Linear programming; Risk assessment; Budgetary constraints; Decision analytics; Investment planning; Investment portfolio; Mathematical optimisation; Optimisation techniques; Optimisations; Prioritisation; Investments",Strategic alignment
519,Healthcare analytics in oncology: A framework to improve competitive advantage on healthcare,"Digitization is a reality that is transforming society and creates new challenges. Consequently businesses are forced to rethink their strategies and business models to explore new opportunities based on synergy with information technologies (IT). The research addresses a real problem of global concern and provides a high practical value for healthcare business and chronic patients. In healthcare management, the key to build healthcare delivery system for the twenty-first century is to evolve towards personal health management that applies the full power of IT to improve patient care. We propose to build a methodology and a framework to personal health management for chronic patients with oncologic diseases. Resulting from this advancement, healthcare organization will be capable to offer innovative services with holistic patient-centered orientation, and patients will receive tools to take active responsibility for healthcare self-management. The framework is based on intellectual capital and knowledge management concepts. A review of literature shows the role of IT alignment to obtain competitive advantages. Next, we studied the patients' responses as co-creator of service innovation in healthcare. Finally, we applied concepts of personal health management and healthcare analytics. The methodology and framework has been tested through qualitative methods applied to various case studies of cancer patients. To design the framework and healthcare services, we interviewed experts on healthcare. The first case of study is in progress revealing interesting results for healthcare services and exhibits an excellent experience for patients. The model permits strategical healthcare management on personalized perspective, and will be of particular interest for expert specialists, portfolio of healthcare services and their resolving capacity. This approach fulfils an identified need in healthcare towards a user-centered attention, quality management and continuity in healthcare management, improvement of use of material, as well as human resources and optimization in use of them. © The Authors, 2017. Healthcare analytics in oncology: A framework to improve competitive advantage on healthcare Digital health; Healthcare analytics; Intellectual capital management; Patient-centered care; Personal health management; Strategic healthcare management Competition; Health; Information management; Knowledge management; Network function virtualization; Quality management; Competitive advantage; Health-care managements; Healthcare delivery; Healthcare organizations; Healthcare services; Intellectual capital; Patient-centered care; Personal health managements; Health care",Strategic alignment
520,"OpenServ4P - Open, intelligent services for production systems; [OpenServ4P: Offene, intelligente Services für die Produktion]","OpenServ4P – Open, Intelligent Services for Production Systems. The use of sensors at various points in the value chain leads to a high volume of process, plant and quality data which results in a high, untapped potential with regard to the planning and control of the production processes and their components. At the same time, the developments in the field of Industrie 4.0 and Big Data Analytics have resulted in methods that enable the extraction, processing and analysis of large databases. The project OpenServ4P therefore aims to decouple tools for the planning and control of production processes from the actual user. For this purpose, an internet-based platform is developed, that offers various planning and control methods “as a service”. In addition, the following three services will be developed as part of the project: Industrial real-time production planning and control, adaptive risk management, and predictive maintenance and quality. Furthermore, the industrial applicability is proven within a real production scenario. © 2016, Carl Hanser Verlag. All rights reserved. OpenServ4P - Open, intelligent services for production systems; [OpenServ4P: Offene, intelligente Services für die Produktion]  Big data; Data mining; Production control; Project management; Quality control; Risk management; Adaptive risk management; Intelligent Services; Large database; Planning and control; Predictive maintenance; Production process; Production system; Real-time production; Process control",Risk management
521,Organizational considerations initiating a big data and analytics implementation,"Purpose: Organizations are beginning to realize the potential benefits of big data and harnessing all of the data they are creating. However, a major impediment for many organizations is understanding where to start in big data and analytics implementation. In many respects, starting a successful implementation is not much different from any other project managed within the organization. The major stumbling block is knowing what questions to ask to get things going. This paper aims to help libraries and information organizations that are considering big data and analytics implementation to begin their journey by following a checklist of eight aspects to be considered in the development of a big data and analytics strategy. Design/methodology/approach: The eight aspects to consider in big data and analytics implementation were developed using a combination of existing project management common knowledge, consultant recommendations and real-life experiences. Findings: Organizations considering big data and analytics implementation need to explore aspects related to the data they have, what organizational problems they are trying to solve, how data governance will work in the new environment, as well as how they will define success in terms of their implementation. These are in addition to the technical issues one would normally expect in a systems implementation. Originality/value: While there have been many articles written about the implementation of big data and analytics in organizations, most of these focus on technical issues rather than managerial and organizational concerns. In addition, none of these other articles have been from the perspective of library and information science. In this article, the focus is specifically on how information professionals may approach this problem. © 2016, © Emerald Group Publishing Limited. Organizational considerations initiating a big data and analytics implementation Analytics; Analytics project implementation; Big data; Big data business concerns; Big data project implementation; Organizational issues related to big data and analytics in the information professions Information services; Management science; Project management; Societies and institutions; Analytics; Data business; Design/methodology/approach; Information organization; Information professionals; Information professions; Library and information science; Project implementation; Big data",Governance
522,An empirical study into social success factors for agile software development,"Though many warn that Agile at larger scale is problematic or at least more challenging than in smaller projects, Agile software development seems to become the norm, also for large and complex projects. Based on literature and qualitative interviews, we constructed a conceptual model of social factors that may be of influence on the success of software development projects in general, and of Agile projects in particular. We also included project size as a candidate success factor. We tested the model on a set of 40 projects from 19 Dutch organizations, comprising a total of 141 project members, Scrum Masters and product owners. We found that project size does not determine Agile project success. Rather, value congruence, degree of adoption of Agile practices, and transformational leadership proved to be the most important predictors for Agile project success. © 2015 IEEE. An empirical study into social success factors for agile software development Agile Software Development; predictive model; social success factors; transformational leadership; value congruence Predictive analytics; Project management; Agile software development; Predictive modeling; Success factors; Transformational leadership; value congruence; Software design",Capacity management
523,Critical infrastructure risk in NHS England: predicting the impact of building portfolio age,"ABSTRACT: NHS Trusts in England must adopt appropriate levels of continued investment in routine and backlog maintenance if they are to ensure critical backlog does not accumulate. This paper presents the current state of critical backlog maintenance within the National Health Service (NHS) in England through the statistical analyses of 115 Acute NHS Trusts. It aims to find empirical support for a causal relationship between building portfolio age and year-on-year increases in critical backlog. It makes recommendations for the use of building portfolio age in strategic asset management. The current trend across this sample of NHS Trusts may be typical of the whole NHS built asset portfolio and suggests that most Trusts need to invest between 0.5 and 1.5 per cent of income (depending upon current critical backlog levels and Trust age profile) to simply maintain critical backlog levels. More robust analytics for building age, condition and risk-adjusted backlog maintenance are required. Copyright © 2015 Vilnius Gediminas Technical University (VGTU) Press. Critical infrastructure risk in NHS England: predicting the impact of building portfolio age Asset management; Backlog; Healthcare; Performance; Service life planning; Value ",Risk management
524,"10th International Conference on Management Science and Engineering Management, ICMSEM 2016","The proceedings contain 138 papers. The special focus in this conference is on Intelligent Systems, Logistics Engineering, Information Technology, Risk Management, Computing Methodology, Project Management, Industrial Engineering and Decision Making Systems. The topics include: A condition monitoring system for blades of wind turbine maintenance management; on interactive learning and mutual trust within the innovation network; a study of urban natural disaster vulnerability assessment based on PCA-TOPSIS method; hotelling model based dynamic pricing of three sides; artificial intelligence for concentrated solar plant maintenance management; slotting optimization of warehousing system based on the Hungarian method; research on evaluation of regional inclusive innovation capacity based on catastrophe progression method; some estimations for the mathematical expectation of renewal-reward process with nonnegative rewards; on general form of tanh method and its application to medical problems; stages and processes of self change of exercise behavior; toward an integrative model of change; multivariable analysis for advanced analytics of wind turbine management; generating distributions through convolution of characteristic functions; bi-objective integer programming of hospitals under dynamic electricity price; longitudinal joint model for instrument and person memories in a quality of life study; asymmetric information effect on transshipment reporting strategy; metasynthesis-based intelligent big data processing paradigm; size effect, neighbour effect and peripheral effect in cross-border tax games; intensified water treatment methods; company z storehouse center layout optimization; a bid evaluation method for multi-attribute online reverse auction and green and lean model for business sustainability. 10th International Conference on Management Science and Engineering Management, ICMSEM 2016  ",Value management
525,Assessing skills and capacity for informatics: Activities most commonly performed by or for local health departments,"Objective: To describe the informatics activities performed by and for local health departments. Design: Analysis of data from the 2015 Informatics Capacity and Needs Assessment Survey of local health departments conducted by the Jiann-Ping Hsu College of Public Health at Georgia Southern University in collaboration with the National Association of County &City Health Officials. Participants: 324 local health departments. Main Outcome Measure(s): Informatics activities performed at or for local health departments in use and analysis of data, system design, and routine use of information systems. Results: A majority of local health departments extract data from information systems (69.5%) and use and interpret quantitative (66.4%) and qualitative (55.1%) data. Almost half use geographic information systems (45.0%) or statistical or other analytical software (39.7%). Local health departments were less likely to perform project management (35.8%), business process analysis and redesign (24.0%), and developing requirements for informatics system development (19.7%). Local health departments were most likely to maintain or modify content of a Web site (72.1%). A third of local health departments (35.8%) reported acting as ""super users"" for their information systems. A significantly higher proportion of local health departments serving larger jurisdictions (500 000+) and those with shared governance reported conducting informatics activities. Conclusion: Most local health department informatics activities are completed by local health department staff within each department or a central department, but many state health departments also contribute to informatics at the local level. Larger local health departments and those with shared governance were more likely to perform informatics activities. Local health departments need effective leadership, a skilled workforce, strong partnerships, and policies that foster implementation of health information systems to successfully engage in informatics. Local health departments also face important training needs, including data analytics, project management, and geographical information systems, so they can adapt to the increasing availability of electronic data and changes in technology. © Copyright 2016 Wolters Kluwer Health, Inc. All rights reserved. Assessing skills and capacity for informatics: Activities most commonly performed by or for local health departments Informatics; Local health departments; Public health ",Governance
526,A framework for mining enterprise risk and risk factors from text documents,"Any real world events or trends that can affect the company's growth trajectory can be considered as Risk. There has been a growing need to automatically identify, extract and analyze risk related statements from news events. In this demonstration, we will present a risk analytics framework that processes enterprise project management reports in the form of textual data and news documents and classify them into valid and invalid risk categories. The framework also extracts information from the text pertaining to the different categories of risks like, their possible because and impacts. Accordingly, we have used machine learning based techniques and studied different linguistic features like n-gram, POS, dependency, future timing, uncertainty factors in texts and their various combinations. A manual annotation study from management experts using risk descriptions collected for a specific organization was conducted to evaluate the framework. The evaluation showed promising results for automated risk analysis and identification. © COLING 2016 - 26th International Conference on Computational Linguistics, Proceedings of COLING 2016: System Demonstrations. A framework for mining enterprise risk and risk factors from text documents  Computational linguistics; Learning systems; Project management; Risk analysis; Enterprise project management; Growth trajectories; Linguistic features; Manual annotation; Mining enterprise; Risk categories; Text document; Uncertainty factors; Risk assessment",Risk management
527,Concept for development project management by aid of predictive analytics,"Manufacturing companies in high wage countries strive towards shortened development and innovation cycles at decreased costs in order to strengthen their competitive advantage. These goals can be achieved by efficient development projects. However, approaches aiming at designing efficient development processes such as the value stream analysis only analyze development projects retrospectively as well as periodically and therefore do not continuously improve the efficiency of the respective projects themselves. Therefore, a concept is needed to anticipate deviations from the target process and thus inefficiencies within development projects by aid of predictive analytics. To derive a predictive analytics model, neural networks are applied to identify the impact of deviation indicators on the efficiency dimensions time, costs and quality of an activity. Upon reversion, it is possible to monitor the deviation indicators and use the respective indicator values as input for the neural networks. Based on the identified impact of the indicator on the efficiency dimensions, the neural network is able to predict the final values of an activity in terms of time, cost and quality. By comparing the predicted values with the defined target values, the deviation can be determined and preventive measures can be implemented to eliminate inefficiencies. © 2016 Portland International Conference on Management of Engineering and Technology, Inc. Concept for development project management by aid of predictive analytics  Competition; Costs; Efficiency; Project management; Competitive advantage; Development process; Development project; Indicator values; Innovation cycles; Manufacturing companies; Preventive measures; Value stream analysis; Predictive analytics",Monitoring and control
528,Data-driven demand response characterization and quantification,"Analysis of load behavior in demand response (DR) schemes is important to evaluate the performance of participants. Very few real-world experiments have been carried out and quantification and characterization of the response is a difficult task. Nevertheless it will be a necessary tool for portfolio management of consumers in a DR framework. In this paper we develop methods to quantify and characterize the amount of DR in a load. The contribution to the aggregated load from each household is quantified on a daily basis, showing the potential variability of the response in time. Clustering on the average values and standard deviation of the contribution regroups households with the same average response. Independent Component Analysis (ICA) is used to characterize different DR delivery profiles. © 2017 IEEE. Data-driven demand response characterization and quantification Demand Response (DR); DR characterization; DR quantification; energy analytics; smart grid Financial data processing; Independent component analysis; Investments; Smart power grids; Demand response; DR quantification; energy analytics; Independent component analysis(ICA); Portfolio managements; Real world experiment; Smart grid; Standard deviation; Characterization",Capacity management
529,Resilience Analytics with Application to Power Grid of a Developing Region,"Infrastructure development of volatile regions is a significant investment by international government and nongovernment organizations, with attendant requirements for risk management. Global development banks may be tasked to manage these investments and provide a channel between donors and borrowers. Moreover, various stakeholders from the private sector, local and international agencies, and the military can be engaged in conception, planning, and implementation of constituent projects. Emergent and future conditions of military conflict, politics, economics, technology, environment, behaviors, institutions, and society that stress infrastructure development are prevalent, and funding mechanisms are vulnerable to fraud, waste, and abuse. This article will apply resilience analytics with scenario-based preferences to identify the stressors that most influence a prioritization of initiatives in the electric power sector of Afghanistan. The resilience in this article is conceived in terms of the degree of disruption of priorities when stressors influence the preferences of stakeholders, and ultimately a prioritization of initiatives. The ancillary results include an understanding of which initiatives contribute most and least across strategic criteria and which criteria have the most impact for the analysis. The article concludes with recommendations for risk monitoring and risk management of the portfolio of stressors through the life cycle and horizon of grid capacity expansion. © 2016 Society for Risk Analysis Resilience Analytics with Application to Power Grid of a Developing Region Afghanistan; energy security; infrastructure systems; programmatic risk; risk and development; systems acquisition; systems engineering Afghanistan; Economics; Energy security; International cooperation; Investments; Life cycle; Risk management; Systems engineering; Afghanistan; Capacity expansion; Electric power sector; Infrastructure development; Infrastructure systems; Non-government organizations; Programmatic risks; Systems acquisition; developing world; electrical power; energy budget; infrastructural development; nongovernmental organization; prioritization; project management; risk assessment; smart grid; stakeholder; adult; Afghanistan; army; article; conception; economics; fraud; funding; international cooperation; investment; life cycle; non-governmental organization; private sector; risk management; stress; Electric power transmission networks",Risk management
530,Using learning analytics to assess project management skills on engineering degree courses,"Learning analytics is a field of study that has been evolving since the outset in attempting to meet various needs. The use of learning analytics techniques has helped us ascertain the level of students' participation and their degree of satisfaction in order to learn how they use resources or identify students at risk. Research currently focuses on applying these techniques to find out how the student learns and to improve teaching/learning processes. A key aspect in improving these processes is the assessment of general competences, which constitutes key learning in engineering students and has thus been identified as a need that can be met by learning analytics. An experiment was conducted on 93 students from different engineering groups at the University of Deusto with a view to assessing the extent to which students have developed the project management competence, using learning analytics techniques. The model designed for analysis is described in this paper, in addition to the methodology and research carried out. Results have shown that by combining an automatic analysis and exploratory learning analytics techniques, conclusions can effectively be drawn about the extent to which a given student has developed a competence based on data obtained via use of a technological tool. © 2016 ACM. Using learning analytics to assess project management skills on engineering degree courses Engineering education; Formative assessment; Learning analytics; Project management Ecology; Ecosystems; Education; Education computing; Management science; Project management; Students; Teaching; Degree of satisfaction; Exploratory learning; Formative assessment; Learning analytics; Learning in engineering; Project management competence; Project management skills; Technological tools; Engineering education",Monitoring and control
531,The evolution of Global Libraries’ performance measurement and impact assessment systems,"Purpose – The purpose of this paper is to describe the evolution of a common approach to impact assessment across the Global Libraries (GL) portfolio of grants. It presents an overview of two systems, the Performance Metrics (PMs) and the Common Impact Measurement System (CIMS). By providing a standard set of definitions and methods for use across countries, these systems enable grantees to collect data that can be compared and aggregated for the purpose of collective learning, improvement, accountability, and advocacy. Design/methodology/approach – The PMs offer a standard methodology to collect library project performance management data, whereas the CIMS is a standard survey of public library users. The paper describes how the PM and CIMS data are being visualized and used, with examples of findings and lessons learned. Findings – The paper cites examples of the type of PM and CIMS data available, with a focus on employment, gender, and case studies from Botswana and Indonesia. These highlights illustrate how libraries’ user demographics differ from other types of public internet access venues and how libraries can contribute to strong employment and growth. Research limitations/implications – The measurement systems rely on different partners collecting data for the same metrics across different countries; while each grantee adheres to a standard methodology, small procedural, and methodological differences are inevitable. Future research could focus on conducting similar studies elsewhere, outside the cohort of countries in the GL portfolio of grants. Practical implications – The paper offers insights and lessons for library agencies or institutions interested in implementing a common measurement system. Recognizing that few library projects have the resources to track a comprehensive set of indicators, a case study is presented about how smaller initiatives can adapt these systems to their needs. Social implications – The indicators described in this paper enable public libraries to shift their focus from services provided to the outcomes they help individuals and communities realize, potentially increasing the potency of their programming and advocacy. Originality/value – Common measurement systems are not new, but their application in the public library field is novel, as is the Data Atlas, a platform grantees use to compare results across metrics, track progress, and conduct advocacy. © 2015, Authors. Published by Emerald Group Publishing Limited. The evolution of Global Libraries’ performance measurement and impact assessment systems Business intelligence; Impact; Library analytics; Library outcomes; Measurement systems; Public libraries ",Value management
532,Fundamental analysis and technical analysis integrated system for stock filtration,"Fundamentals and technical investigation is a technique which enhances decision making for stock investors. The fundamental analysis includes looking at any information, other than the trading patterns of the stock itself, which can affect the cost and the perceived value of a stock. Technical analysis is an exchanging apparatus utilized to assess securities and endeavor to forecast their future development by breaking down insights accumulated from exchanging action, such as price movement and volume. This system utilizes data mining techniques to analyze various stock information and the factors to create a logical decision model. This helps new and inexperienced investors to make less errors as well as make the stock market more approachable to the general community. This research focuses on a high performance stock selection using the fundamental analysis of individual stocks, which is reflected in the financial statements. Ten criteria calculated from stock financial statement reports are proposed for the analysis. The 10 years of historical fundamental information on organizations recorded in Thailand stock exchange were clustered into three groups and used the fundamental criteria to classify the interesting return stocks selection. The multilayer perceptron neural network is used in the training process to verify the clustering results. For the technical analysis, experimental results reveal that the exponential moving average technique is the most favorable and thus being selected to apply in our system. To affirm the productivity of the suggested stocks, an experiment using information from the system’s decision model based on the Stock Exchange of Thailand data in the year 2015 is conducted. After the activities from 5000 simulated portfolios, the average returns of the ports are positive. In fact, the ports gain almost three times higher than the average market yield. This indicates the efficiency of the system’s stock filtering and decision making capabilities. © 2016, Springer Science+Business Media New York. Fundamental analysis and technical analysis integrated system for stock filtration Business intelligence; Clustering; Decision support; MLP neural network; Stock trading Classification (of information); Clustering algorithms; Commerce; Competitive intelligence; Data mining; Decision making; Decision support systems; Filtration; Financial markets; Investments; Clustering; Decision supports; Exponential moving averages; Financial statements; MLP neural networks; Multi-layer perceptron neural networks; Stock Exchange of Thailand; Stock trading; Electronic trading",Financial management
533,"2nd International Conference on Decision Support System Technology, ICDSST 2016","The proceedings contain 15 papers. The special focus in this conference is on DSS Applications Addressing Sustainability and Societal Challenges. The topics include: A case study on social sustainability in agriculture; an operations research-based morphological analysis to support environmental management decision-making; searching for cost-optimized strategies; enhancing antenatal clinics decision-making through the modelling and simulation of patients flow by using a system dynamics approach; fuzzy inference approach to uncertainty in budget preparation and execution; detectability based prioritization of interdependent supply chain risks; scaling issues in MCDM portfolio analysis with additive aggregation; a knowledge based system for supporting sustainable industrial management in a clothes manufacturing company based on a data fusion model; knowledge management as an emerging field of business intelligence research; updating business intelligence and analytics maturity models for new developments; a novel collaborative approach for business rules consistency management; knowledge sharing and innovative corporate strategies in collaborative relationships; developing innovative tool to enhance the effectiveness of decision support system and an exploratory study of text mining approach to attain cognitive map based on citizen survey data. 2nd International Conference on Decision Support System Technology, ICDSST 2016  ",Strategic alignment
534,Improving software project outcomes through predictive analytics: Part 1,"The complex and emergent behavior of software systems makes information and data key components of this unpredictable environment. The use of a data-driven approach to identify and to accurately predict the sources of software project delays, cost overruns, failures, or successes may prove a significant contribution to the fields of systems engineering, software development and project management. Software project failures are pervasive and despite the research, failures still persist. This paper deals with the systems mindset in addressing failure to introduce a software-specific predictive analytics model that accurately predicts software project outcomes of failure or success. The use of an evidence-based approach to identify software project failure factors will result in better understanding of these phenomena that will ultimately improve software project success rates and minimize risks in systems engineering efforts. © 1973-2011 IEEE. Improving software project outcomes through predictive analytics: Part 1 predictive analytics; project management; software engineering; software failure; systems engineering Cost engineering; Information management; Predictive analytics; Project management; Safety engineering; Software engineering; Systems engineering; Cost overruns; Data-driven approach; Emergent behaviors; Evidence-based; Software failure; Software project; Software systems; Unpredictable environments; Software design",Risk management
535,High-frequency financial statistics through high-performance computing,"Financial statistics covers a wide array of applications in the financial world, such as (high-frequency) trading, risk management, pricing and valuation of securities and derivatives, and various business and economic analytics. Portfolio allocation is one of the most important problems in financial risk management. One most challenging part in portfolio allocation is the tremendous amount of data and the optimization procedures that require computing power beyond the currently available desktop systems. In this article, we focus on the portfolio allocation problem using high-frequency financial data, and propose a hybrid parallelization solution to carry out efficient asset allocations in a large portfolio via intra-day highfrequency data.We exploit a variety of HPC techniques, including parallel R, Intelr Math Kernel Library, and automatic offloading to Intelr Xeon Phi coprocessor in particular to speed up the simulation and optimization procedures in our statistical investigations. Our numerical studies are based on high-frequency price data on stocks traded in New York Stock Exchange in 2011. The analysis results show that portfolios constructed using high-frequency approach generally perform well by pooling together the strengths of regularization and estimation from a risk management perspective.We also investigate the computation aspects of large-scale multiple hypothesis testing for time series data. Using a combination of software and hardware parallelism, we demonstrate a high level of performance on highfrequency financial statistics. © Springer International Publishing Switzerland 2016. High-frequency financial statistics through high-performance computing  Commerce; Electronic trading; Finance; Frequency estimation; Investments; Risk assessment; Risk management; Risk perception; Statistical tests; Statistics; Time series analysis; Financial risk management; High performance computing; Hybrid parallelization; Multiple hypothesis testing; New York Stock Exchange; Optimization procedures; Simulation and optimization; Software and hardwares; Financial markets",Risk management
536,A simulation analytics approach to dynamic risk monitoring,"Simulation has been widely used as a tool to estimate risk measures of financial portfolios. However, the sample paths generated in the simulation study are often discarded after the estimate of the risk measure is obtained. In this article, we suggest to store the simulation data and propose a logistic regression based approach to mining them. We show that, at any time and conditioning on the market conditions at the time, we can quickly estimate the portfolio risk measures and classify the portfolio into either low risk or high risk categories. We call this problem dynamic risk monitoring. We study the properties of our estimators and classifiers, and demonstrate the effectiveness of our approach through numerical studies. © 2016 IEEE. A simulation analytics approach to dynamic risk monitoring  Risk perception; Dynamic risks; Financial portfolio; High-risk categories; Logistic regressions; Market condition; Portfolio risks; Simulation data; Simulation studies; Risk assessment",Risk management
537,An agile approach for academic analytics: a case study,"Purpose: The purpose of this paper is to describe an agile approach to academic analytics that is currently being applied on one of the campuses of a leading higher educational institution in the Caribbean. This agile approach enables the rapid development of a strategic analytics roadmap and proof-of-concept analytics applications for the institution. Design/methodology/approach: The approach was developed using Design Science which involves the development and rigorous evaluation of an artifact. The agile approach is the artifact and the design evaluation was done using the observational method of primary cases studies where the artifact is studied in depth in a business environment, in this case this was a leading higher educational institution in the Caribbean. Findings: The final output, the roadmap, highlights the importance of a balanced portfolio of analytics initiatives, relevant and tailored to the institution’s specific context that includes technology and applications projects, as well as capacity building, organizational structures and policy initiatives. Research limitations/implications: The approach that was used and the specific techniques proposed can be extended by other researchers and in so doing will increase the body of research as it relates to agile analytics. Practical implications: The approach will be beneficial to educational institutions that are considering how best to harness the strategic value of its data. The analytics roadmap will allow the institution to be clear about the path they should take to maximize their investments in analytics initiatives. Originality/value: A number of existing well-accepted research techniques have been synthesized in the development and application of this agile approach. The approach and final roadmap consider the institution’s readiness for and understanding of what is involved in analytics before investing significant resources in its adoption. © 2017, © Emerald Publishing Limited. An agile approach for academic analytics: a case study Academic analytics; Agile; Analytics roadmap; Higher educational institutions ",Strategic alignment
538,IBM predicts cloud computing demand for sports tournaments,"The rapid growth of the Internet and of mobile and other smart technologies has generated increased demand on digital platforms, which are supported by enterprise cloud-computing capabilities. To support IBM's leadership in analytics, mobile, and cloud technologies, a small team within IBM Global Technology Services (GTS) developed a system that uses advanced analytics to address the dynamic and unpredictable Web traffic patterns produced by a digital-enterprise workload, while driving greater operational efficiencies in computing and labor resources. Current cloud platforms are reactive; that is, they require human intervention to scale computing resources to meet demand. To address this shortcoming, the GTS team developed the Predictive Cloud Computing (PCC) system. PCC uses multiple advanced analytical techniques, such as novel numerical analysis techniques, discrete-event simulation, and advanced forecasting to produce models that forecast Internet traffic demands in near real time, allocating computing resources as needed. In 2014, GTS applied the PCC system across tennis and golf sporting tournaments reducing our cloud-computing hours by about 50 percent, while driving a reduction in labor through automation. The PCC system continues to expand IBM's technology base; since its inception, it has resulted in 16 patent filings, strengthening IBM's analytics patent portfolio and overall brand. © 2016 INFORMS. IBM predicts cloud computing demand for sports tournaments Big data; Cloud computing; Forecasting; Predictive modeling; Social analytics; Sports; Stream computing ",Financial management
539,Revitalising energy capital project development and execution strategies: Lessons from the energy sector,"This study sought to examine project delivery challenges currently being faced by energy clients and to determine how they could achieve value creation through better alignment of project delivery processes. There are important lessons to be learned from the energy sector on how to deal with the unique operational and project challenges. Four exploratory focus groups were held with twenty senior project management practitioners, to better understand the greatest needs and project management processes in the energy sector. A formal deductive approach was used to examine and evaluate existing and future energy project delivery processes. From the qualitative data, participants recognised the need to introduce science-based project techniques such as system dynamics and project predictive analytics in project management processes. Participants further noted that comprehensive innovative project delivery processes and analytical approaches are required to cope with the increasing scale and complexity of energy capital projects. © 2015 Taylor & Francis. Revitalising energy capital project development and execution strategies: Lessons from the energy sector capacity planning; energy project delivery; predictive project analytics; Process modelling; system dynamics Energy policy; Information management; System theory; Capacity planning; Energy project; predictive project analytics; Process modelling; System Dynamics; Project management",Strategic alignment
540,The Need for a Modular Approach to IT Enablement,"With the explosion of currently trending IT such as social, mobile, analytics, cloud, and the Internet of Things, managing IT complexity has become one of the most significant impediments to obtaining the full leverage that IT can provide to business. In this context, maintaining the alignment of IT with business capabilities and operating models to support the execution of business strategy has become more challenging than ever. The authors discuss how a modular approach to the analysis, design, and implementation of IT solutions can become a sustainable strategy for IT-business alignment. © 2016 IEEE. The Need for a Modular Approach to IT Enablement information technology; Internet of Things; project management; software engineering Information technology; Internet of things; Project management; Software engineering; Business strategy; IT solution; IT-business alignments; Modular approach; Operating models; Sustainable strategies; Internet",Governance
541,Leveraging internet of things technologies and equipment data for an integrated approach to service planning and execution,"The euphoria over the scorching pace of Internet of Things (IoT) market is on the rise as organizations have realized the economic value of the data that will be generated from billions of connected devices. Within an enterprise environment, this provides large number of opportunities across business and functional processes. This paper focuses on equipment after-sales operations as one of the business processes that can leverage IoT for digital transformation. It demonstrates that an IoT platform based approach and using predictive analytics as a foundational element, can help achieve smarter and profitable after-sales service and support portfolio. © 2015 IEEE. Leveraging internet of things technologies and equipment data for an integrated approach to service planning and execution equipment after-sales; installed base; Internet of Things; IoT platform; operational technology; predictive analytics; service and support Predictive analytics; Sales; After-sales; Digital transformation; Enterprise environment; installed base; Internet of Things (IOT); Internet of things technologies; IoT platform; Operational technologies; Internet of things",Governance
542,Effective information management - A - big- data driven road map for enterprise decision making,"Simple data analytics reveal basic insights; more sophisticated analytics, applied to data that has been pooled into a ""data lake"" with data from external and enterprise sources allows utilities to unearth deeper insights that will help to optimize performance. Because of the growing volume, complexity and strategic importance of asset management data, it is no longer desirable or even feasible for each department/ unit/division/function within a utility to manage this data by itself, or to build its own data analytics capabilities. To get the most out of the new data resources, utilities are creating dedicated data groups that are potentially embedded within the core asset management program team to consolidate data collection, aggregation and analytics. Three trends have emerged in the data management realm - cloud computing, mobile computing, and explosion of data. Utilities are collecting more data than ever before. However, the challenge facing utilities is their inability to convert all the data into meaningful & usable information. Over the past year, self-service business intelligence tools have provided the necessary capabilities for utility staff to process and analyze data to produce meaningful insights. Advances in technology have revolutionized data and performance reporting so that users (with limited IT development expertise) can perform data mining and develop high impact visuals for performance reporting. Water and Wastewater Utilities are implementing Business Intelligence (BI) frameworks to track and report key asset management performance indicators and other data analytics. Benefits of this business intelligence reporting framework include: 1. Eliminates the reliance on core IT developers to develop and manage reporting frameworks as BI is now integrated with common applications, putting the non-IT user in a position to perform complex data analysis and develop aesthetically-pleasing visualizations 2. Significantly reduces development cost and level of effort 3. Through the concept of data ""lakes"", data models can be constructed using data from various sources (CMMS, GIS, SCADA, project management, financial and customer information systems) with ease 4. Eliminates the extensive costs and need for complex and disparate system integration that is typically required to connect data for effective performance reporting 5. Reduces the time to develop high impact visualizations to hours or days, rather than weeks, months, and years 6. Complete transferability to mobile devices for use at meetings and workshops This presentation will discuss the utility management business intelligence frameworks that have been implemented by utilities for effective integration, tracking and reporting of various data within their organization. The main purpose of this paper is to discuss the value generated by implementing data management and business intelligence through data analytics and how business intelligence aligns with the 3 data trends (cloud, mobile, and explosion of data). Copyright © 2017 Water Environment Federation. Effective information management - A - big- data driven road map for enterprise decision making Big Data; Business Intelligence; Cloud Computing; Data Analytics; Data Governance; Data Life Cycle; Data Management; Data-Driven Decision Making; Information Management; Mobile Asset management; Big data; Cloud computing; Competitive intelligence; Data integration; Data mining; Decision making; Human resource management; Lakes; Life cycle; Mobile cloud computing; Project management; SCADA systems; Visualization; Data analytics; Data driven decision; Data governances; Data life cycle; Mobile; Information management",Financial management
543,Modeling and qualitative evaluation of a management canvas for big data applications,"A reference model for big data management is proposed, together with a methodology for business enterprises to bootstrap big data projects. Similar to the business model canvas for marketing management, the big data management (BDM) canvas is a template for developing new (or mapping existing) big data applications, strategies and projects. It subdivides this task into meaningful fields of action. The BDM canvas provides a visual chart that can be used in workshops iteratively to develop strategies for generating value from data. It can also be used for project planning and project progress reporting. The canvas instantiates a big data reference meta-model, the BDM cube, which provides its meta-structure. In addition to developing and theorizing the proposed data management model, two case studies on pilot applications in companies in Switzerland and Austria provide a qualitative evaluation of our approach. Using the insights from expert feedback, we provide an outlook for further research. © Copyright 2017 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved. Modeling and qualitative evaluation of a management canvas for big data applications Big data; Case study; Data management; Pilot application; Project management; Reference model Data Science; Information management; Iterative methods; Marketing; Project management; Big data applications; Business enterprise; Business modeling; Marketing management; Pilot applications; Project planning; Qualitative evaluations; Reference modeling; Big data",Strategic alignment
544,Big Data Analytics in Support of the Decision Making Process,"Information is a key success factor influencing the performance of decision makers, specifically the quality of their decisions. Nowadays, sheer amounts of data are available for organizations to analyze. Data is considered the raw material of the 21st century, and abundance is assumed with today's 15 billion devices [aka Things!] already connected to the Internet. Accordingly, solutions need to be studied and provided in order to handle and extract value and knowledge from these datasets. Furthermore, decision makers need to be able to gain valuable insights from such rapidly changing data of high volume, velocity, variety, veracity, and value by using big data analytics. This paper aims to research how big data analytics can be integrated into the decision making process. Accordingly, using a design science methodology, the ""Big - Data, Analytics, and Decisions"" (B-DAD) framework was developed in order to map big data tools, architectures, and analytics to the different decision making phases. The ultimate objective and contribution of the framework is using big data analytics to enhance and support decision making in organizations, by integrating big data analytics into the decision making process. Consequently, an experiment in the retail industry was administered to test the framework. Accordingly, results showed added value when integrating big data analytics into the decision making process. Big Data Analytics in Support of the Decision Making Process B-DAD framework; Big data analytics; decision making; design science Data integration; Decision making; Design; Information management; Information systems; Project management; Data analytics; Decision makers; Decision making process; Design science; Design science methodologies; High volumes; Key success factors; Retail industry; Big data",Strategic alignment
545,Acceptance factors for using a big data capability and maturity model,"Big data is an emerging field that combines expertise across a range of domains, including software development, data management and statistics. However, it has been shown that big data projects suffer because they often operate at a low level of process maturity. To help address this gap, the Diffusion of Innovation Theory is used as a theoretical lens to identify factors that might drive an organization to try and improve their process maturity. Specifically, thirteen acceptance factors for teams to use (or not use) a Big Data CMM are identified. These results suggest that a positive perception exists with respect to relative advantage, compatibility and observability factors, and a negative perception exists with respect to perceived complexity. While more work is required to refine the list of factors, this insight can help guide the improvement of big data team processes. © 2017 Proceedings of the 25th European Conference on Information Systems, ECIS 2017. All rights reserved. Acceptance factors for using a big data capability and maturity model Big Data; Data Science; Project Management Digital storage; Information management; Information systems; Information use; Project management; Software design; Data Science; Diffusion of innovation theory; Maturity model; Process maturity; Team process; Big data",Strategic alignment
546,Testing analytics on software variability,"Software testing is a tool-driven process. However, there are many situations in which different hardware/software components are tightly integrated. Thus system integration testing has to be manually executed to evaluate the system's compliance with its specified requirements and performance. There could be many combinations of changes as different versions of hardware and software components could be upgraded and/or substituted. Occasionally, some software components could even be replaced by clones. The whole system after each component change demands to be re-tested to ensure proper system behavior. For better utilization of resources, there is a need to prioritize the past test cases to test the newly integrated systems. We propose a way to facilitate the use of historical testing records of the previous systems so that a testcase portfolio can be developed, which intends to maximize testing resources for the same integrated product family. As the proposed framework does not consider much of internal software complexity, the implementation costs are relatively low © 2015 IEEE. Testing analytics on software variability Clones; Risk management; Software project management; Software testing; Variability testing Cloning; Project management; Risk management; Software testing; Hardware and software components; Hardware/software components; Implementation cost; Integrated products; Software complexity; Software project management; Software variabilities; Utilization of resources; Integration testing",Financial management
547,Business intelligence capabilities as facilitators to achieve organizational agility,"Today's business environment is characterized by fast and unexpected changes, many of which are driven by technological advancement. In such environment, the ability to respond effectively and adapt to the new requirements is not only desirable but essential to survive. Comprehensive and quick understanding of intricacies of market changes facilitates firm's faster and better response. Two concepts contribute to the success of this scenario; organizational agility and business intelligence (BI). As of today, despite BI's capabilities to foster organizational agility and consequently improve organizational performance, a clear link between BI and organizational agility has not been established. In this paper we argue that BI solutions have the potential to be facilitators for achieving agility. We aim at showing how BI capabilities can help achieve agility at operational, portfolio, and strategic levels. Business intelligence capabilities as facilitators to achieve organizational agility  Information systems; Business environments; Market changes; Organizational agility; Organizational performance; Strategic level; Technological advancement; Information analysis",Governance
548,Fixed income analytics: Bonds in high and low interest rate environments,"This book analyses and discusses bonds and bond portfolios. Different yields and duration measures are investigated. The transition from a single bond to a bond portfolio leads to the equation for the internal rate of return. Its solution is analyzed and compared to different approaches proposed in the financial industry. The impact of different yield scenarios on a model bond portfolio is illustrated. Market and credit risk are introduced as independent sources of risk. Different concepts for assessing credit markets are described. Lastly, an overview of the benchmark industry is offered and an introduction to convertible bonds is given. This book is a valuable resource not only for students and researchers but also for professionals in the financial industry. © Springer International Publishing AG 2017. Fixed income analytics: Bonds in high and low interest rate environments Bond analytics; Credit market; Internal return rate; Risk market; Straight bonds ",Value management
549,Putting asset data at the heart of organisational decision-making using an Integrated Workplace Management System,"With the advent of “Big Data” and the “Internet of Things”, traditional approaches to asset management are being continually challenged and superseded by innovative technology solutions. These enable organisations to improve operational efficiency, strengthen stakeholder relations and reduce costs. Given current economic pressures, the coincidence of technological and economic pressures is driving a step change in how organisations operate. To adopt to the changing technology landscape, organisations need to have a clear strategic approach around how they manage their information. A core component of this can be an Integrated Workplace Management Systems (IWMS), which provides a single technology platform that organisations can firstly use to create accurate records of their reference data (which include buildings, assets, people, and contracts). Secondly, organisations can use the integral process engines within the IWMS to manage business processes (for example contract management, preventative / reactive maintenance tasks, project management, facilities assessments, strategic planning, and environmental reporting). Organisations that have implemented an IWMS have experienced significant operational benefits, through more efficient business processes, better stakeholder relationships, reduced operating costs, improved productivity and clearly demonstrable legal compliance. With a single data repository, transparent reference data, and comprehensive analytics, organisations can embrace a new generation of key performance indicators to command differential advantage in their competitive environments. However, implementing an IWMS requires a strategic approach to implementation, which must align to business process change and strong data governance. It should be noted that whilst this paper has been produced by employees of IBM, who provide IWMS capabilities and associated consulting services, the paper and its associated presentation are not a sales pitch for IBM solutions. There are many technology platforms and capabilities that align to this paper's description of an IWMS and the associated challenges and benefits. © 2016 Institution of Engineering and Technology. All rights reserved. Putting asset data at the heart of organisational decision-making using an Integrated Workplace Management System Asset management; Assets; Big data; BIM; Integrated workplace management system; Internet of Things; IoT; IWMS; Maintenance; Portfolio management; Predictive analytics; Real estate; Strategy; Systems Architectural design; Asset management; Behavioral research; Benchmarking; Big data; Computer systems; Cost reduction; Financial data processing; Information management; Information services; Internet of things; Investments; Maintenance; Operating costs; Predictive analytics; Project management; Assets; IWMS; Portfolio managements; Real estate; Strategy; Workplace managements; Decision making",Strategic alignment
550,Resilience analytics and the public sector management of disasters,"Catastrophe models, originally developed to help insurers and reinsurers price catastrophe risks, manage insurance portfolios and ensure long-term solvency, are increasingly being employed to help countries and cities plan for a wide range of potential disasters. Going forward we can expect to find a new focus of expertise in the use of catastrophe models and resilience analytics within city (and national) governments. This will require a programme to collect and maintain detailed GIS information on all the exposure categories across the city: including buildings, infrastructure, people, economic activity and sustainability. We can expect that a single catastrophe model application, harnessing the probabilistic hazard capabilities developed for insurance, will then be providing information to three separate classes of users in city and national administration: the Chief Resilience Officer (CRO), the Chief Financial Officer (CFO) and the Disaster Response Manager (DRM). Each will require a customized set of applications. The analytical knowledge on risk that will be gained in this process will inspire new forms of risk reduction and risk transfer, as well as highlight how resilience analytics can come to drive 'risk-based government'. © 2016 CURRAN-CONFERENCE. All rights reserved. Resilience analytics and the public sector management of disasters Catastrophe model; Exposure data generation; Risk based government; Urban resilience Digital storage; Disasters; Economics; Analytical knowledge; Catastrophe model; Chief financial officers; Economic activities; Exposure data; Public sector management; Risk-based; Urban resilience; Risk management",Risk management
551,Updating business intelligence and analytics maturity models for new developments,"Recent developments such as real-time, social, predictive and cloud business intelligence and analytics (BI&A) introduce extra ways for organisations to obtain insight and business value from an expanded range of data. Organisations have struggled with the strategy, implementation, and measurement of their BI&A efforts, and a series of business intelligence maturity models (BIMMs) has been introduced to identify strengths and weaknesses of their BI&A situation, and assist remedial action. These BIMMs are however seen to be incomplete and outdated and do not accommodate recent BI&A developments. This study suggests how BIMMs should be modified to cater for these developments. Existing BIMMs were examined, and interviews conducted with BI&A professionals knowledgeable about BIMMs and recent BI&A changes. Findings suggested that existing BIMM dimensions should be modified in various ways to cater for the recent changes in BI&A. In addition, project management was identified as a new BIMM dimension. © Springer International Publishing Switzerland 2016. Updating business intelligence and analytics maturity models for new developments Analytics; Big data; Business intelligence; Decision support; Maturity models ",Value management
552,Preventive controlling of product development projects by aid of predictive analytics-identifying hot spots within the deviation probability map,"This paper presents a concept to anticipate deviations from the target process and thus inefficiencies within development projects by aid of predictive analytics. It is stated that predictive analytics approaches can be adapted to predict deviations in development projects, comparable to the anticipation of crimes. Deviations in terms of time, costs and quality are seen as a result of waste and therefore a dimension for inefficiencies. In this context the deviation probability map is introduced as a part model of the superordinate methodology allowing the intuitive identification of deviation hot spots and enhancing preventive controlling of development projects. © 2015 IEEE. Preventive controlling of product development projects by aid of predictive analytics-identifying hot spots within the deviation probability map development process; predictive analytics; project controlling; value stream optimization Project management; Development process; Development project; Hot spot; Probability maps; Product development projects; Project controlling; Value streams; Predictive analytics",Strategic alignment
553,Ask the engineers: Exploring repertory grids and personal constructs for software data analysis,"Maturity in software projects is often equated with data-driven predictability. However, data collection is expensive and measuring all variables that may correlate with project outcome is neither practical nor feasible. In contrast, a project engineer can identify a handful of factors that he or she believes influence the success of a project. The challenge is to quantify engineers' insights in a way that is useful for data analysis. In this exploratory study, we investigate the repertory grid technique for this purpose. The repertory grid technique is an interview-based procedure for eliciting 'constructs' (e.g., Adhering to coding standards) that individuals believe influence a worldly phenomenon (e.g., What makes a high-quality software project) by comparing example elements from their past (e.g., Projects they have worked on). We investigate the relationship between objective metrics of project performance and repertory grid constructs elicited from eight software engineers. Our results show correlations between the engineers' subjective constructs and the objective project outcome measures. This suggests that repertory grids may be of benefit in developing models of project outcomes, particularly when project data is limited. © 2015 IEEE. Ask the engineers: Exploring repertory grids and personal constructs for software data analysis practitioners; repertory grids; software data analytics Data Analytics; Data handling; Engineers; Information analysis; Project management; Software engineering; Exploratory studies; High-quality software; practitioners; Project engineers; Project performance; Repertory grid technique; Repertory grids; Software data; Professional aspects",Risk management
554,Industrial-Scale Ad Hoc Risk Analytics Using MapReduce,"Modern reinsurance companies hold portfolios consisting of thousands of reinsurance contracts covering millions of individually insured locations. To ensure capital adequacy and for fine-grained financial planning, these companies carry out large-scale Monte Carlo simulations to estimate the probabilities that the losses incurred due to catastrophic events such as hurricanes, earthquakes, etc. exceed certain critical values. This is a computationally intensive process that requires the use of parallelism to answer risk queries over a portfolio in a timely manner. We present a system that uses the MapReduce framework to evaluate risk analysis queries on industrial-scale portfolios efficiently. In contrast to existing production systems, this system is designed to support arbitrary ad hoc queries an analyst may pose while achieving a performance that is very close to that of highly optimized production systems, which often only support evaluating a limited set of risk metrics. For example, a full portfolio risk analysis run consisting of a 1,000,000-trial simulation, with 1,000 events per trial, and 3,200 risk transfer contracts can be completed on a 16-node Hadoop cluster in just over 20 min. MapReduce is an easy-to-use parallel programming framework that offers the flexibility required to develop the type of system we describe. The key to nearly matching the performance of highly optimized production systems was to judiciously choose which parts of our system should depart from the classical MapReduce model and use a combination of advanced features offered by Apache Hadoop with carefully engineered data structure implementations to eliminate performance bottlenecks while not sacrificing the flexibility of our system. © 2016, Springer International Publishing Switzerland. Industrial-Scale Ad Hoc Risk Analytics Using MapReduce Distribute File System; Loss Distribution; MapReduce Framework; Query Engine; Risk Metrics Computer software; Intelligent systems; Monte Carlo methods; Parallel programming; Query processing; Risk analysis; Risk assessment; Distribute file systems; Industrial scale; Loss distribution; Map-reduce; Mapreduce frameworks; Optimized production; Performance; Production system; Query engines; Risk metric; MapReduce",Strategic alignment
555,Monetizing risk helps tulsa optimize capital investment,"The Tulsa Metropolitan Utility Authority (TMUA) in Oklahoma has demonstrated that asset management using risk cost analytics is proving valuable in determining the most cost effective portfolio of projects to include in a five-year capital improvement plan. This solution is helping TUMA in balancing performance, cost, and risk to manage infrastructure in the most cost-effective manner. The vision of TUMA is to ensure long-term sustainability of Tulsa's water and wastewater systems by providing services that are cost-effective and resource-efficient while supporting community goals. Monetizing risk helps tulsa optimize capital investment  Cost effectiveness; Costs; Capital improvement plans; Capital investment; Cost effective; Long-term sustainability; Oklahoma; Resource-efficient; Risk costs; Water and wastewater; Investments",Risk management
556,Contribution of talent analytics in change management within project management organizations the case of the French aerospace sector,"Big data analytics is emerging as an important tool that has the capability to transform the way firms conduct talent management (TM) into the so-called talent analytics. Drawing on the talent management literature, a model of work in organizations project management and a case study conducted in the French aerospace industry, this paper aims to explore the potential benefits of talent analytics in the context of change management within project management organizations. Our findings, implications and future research directions are presented and discussed. © 2017 The Authors. Published by Elsevier B.V. Contribution of talent analytics in change management within project management organizations the case of the French aerospace sector aerospace; case study; talent analytics; Talent management Aerospace industry; Big data; Information management; Information systems; Societies and institutions; aerospace; Aerospace sectors; Change management; Future research directions; Potential benefits; Project management organization; talent analytics; Talent management; Project management",Strategic alignment
557,The dynamic model and prediction research on electricity forward price in competitive power market,"In competitive electricity market, the accurate forecast of electricity forward price can provide guidance for bidding strategies of market participants and reference for power generation project, which will optimize the power suppliers' investment portfolio in significant measure. More importantly, participants are risk averse, and they can stabilize their income by electricity forward forecasting. Electricity forward price is affected by real-time electricity price, interest rate, the influence of load demand or other factors, which makes it difficult to establish an accurate mathematical model for a comprehensive description. Aiming at this point, the Wavelet Neural Network (WNN) prediction algorithm is proposed for dynamic model establishment and prediction research of the electricity forward price, which is obtained from the Nordic electricity market in this paper. The electricity forward prices from January to April, 2014 are predicted, and compared with the Grey prediction model, the accuracy of WNN algorithm is verified. ©, 2015, Binary Information Press. All right reserved. The dynamic model and prediction research on electricity forward price in competitive power market Competitive electricity market; Electricity forward price forecasting; Wavelet neural network Dynamic models; Electric industry; Electronic trading; Forecasting; Investments; Neural networks; Predictive analytics; Competitive electricity markets; Competitive power markets; Forward price; Grey prediction model; Modeling and predictions; Power generation projects; Prediction algorithms; Wavelet neural networks; Power markets",Financial management
558,Supporting the engineering of cyber-physical production systems with the AutomationML analyzer,"The engineering phase of Cyber-Physical Production Systems (CPPS) is a multi-disciplinary process in which representatives of diverse engineering disciplines collaborate to deliver a complex CPPS. To ensure optimal project management as well as to avoid risks of inconsistencies between engineering models created by engineers from different disciplines, support is needed for integrating and subsequently analyzing diverse engineering data. AutomationML is an emerging data exchange format for engineering data which makes the first step towards the easier exchange of engineering data. Yet, there is a lack of tool support for integrating, making sense of and analyzing AML files. In this paper, we explore the use of Semantic Web and Linked Data technologies to provide extended functionality on top of AML that allows advanced data analytics on engineering data such as intuitive browsing of interlinked engineering models and queries for project-wide verification and validation activities. As a result of these investigations, we present the AutomationML Analyzer prototypical implementation to showcase some of the functionalities made possible by Semantic Web and Linked Data technologies in this context. © 2016 IEEE. Supporting the engineering of cyber-physical production systems with the AutomationML analyzer AutomationML; CPPS Engineering; Data integration; Industrie4.0; Linked Data; Semantic Web Data handling; Electronic data interchange; Project management; Semantic Web; AutomationML; Data exchange format; Engineering disciplines; Industrie4.0; Linked datum; Multi-disciplinary process; Prototypical implementation; Verification-and-validation; Data integration",Financial management
559,Business intelligence: Finding nuggets in the Noise,"Portfolio companies readily acknowledge data as a critical commodity in today's intensively competitive business climate. The common theme: ""The more data we own, the more intelligent, 'fact-based' decisions we can make."" The implication: ""Your portfolio investments are on the right track to identifying the 'nuggets in the noise' that will unlock increased EBITDA and exit value."" The facts tell a different story. The emergence of business intelligence as a tool and a process can help identify EBITDA growth nuggets right now. And, despite what you may have heard, a smartly designed business intelligence application does not have to be costly or operationally disruptive. Business intelligence: Finding nuggets in the Noise  ",Value management
560,Web-based integrated project controls system,"The early detection of overruns in construction industry is essential for effective project management. Inefficient and wasteful project reporting systems contribute in project failure as it deprives project management team from taking immediate corrective actions. Reporting and dashboard tools allow multiple data source extraction and processing, which facilitate producing integrated web-based visualized report in near-realtime to be available for project parties. This paper presents a web-based platform that supports real estate developers through integrating cost data with visualized progress information. The platform development tool is ORACLE Business Intelligence Enterprise (BIE). A specially designed relational database was developed to store progress and cost data and to produce the visualized progress layout using mapping techniques and dashboard design. Cost data is imported from a customized cost reporting system with a distinct database. The developed platform creates a practical integration between cost reporting and facilitates data reporting in visualized and graphical representation. The platform has been tested and applied to actual case study to demonstrate and verify its ease of use and capabilities. The case study project is a residential community composed of villas, building and supporting infrastructure works in addition to community building in the Kingdom of Saudi Arabia with 53,800 m2 land area and a 39,526 m2 built up area. The developed model supports project controls more thoroughly to make better business decision. Web-based integrated project controls system Cost reporting; Integrated reporting; Project control and ORACLE business intelligence; Web-based reporting Construction industry; Data integration; Human resource management; Information analysis; Robotics; Websites; Graphical representations; Integrated reporting; Kingdom of Saudi Arabia; Multiple data sources; Platform development; Project control; Residential communities; Web based; Project management",Monitoring and control
561,Probabilistic Approach to Predicting Risk in Software Projects Using Software Repository Data,"Although the factors that need to be focused on for a successful software project appear to be difficult to define, risk management has become one of the key activities for achieving such success because significant risk is involved in each software development phase. Software project failures are often a result of insufficient and ineffective risk information regarding the future. To overcome this, software risk prediction should be performed in advance to allow project managers insight into providing more valuable information for decision making, such as scope coverage, resource allocation, and schedule changes. In this research, we propose a risk prediction model from the perspective of quality using a software repository. We evaluated the risk threat level by mapping some defect attributes that exist in the defect lifecycle, defined their risk threat transition states, and applied a Markov chain for predicting the potential risk level. We evaluated the proposed approach using practical real-industry mobile software projects. The experimental results confirm that our approach is applicable to software threat risk estimation. © 2015 World Scientific Publishing Company. Probabilistic Approach to Predicting Risk in Software Projects Using Software Repository Data project management; software engineering; software repository; Software risk Defects; Forecasting; Life cycle; Markov chains; Predictive analytics; Project management; Risk management; Risk perception; Software engineering; Mobile softwares; Probabilistic approaches; Risk information; Risk prediction models; Schedule changes; Software repositories; Software risks; Software threats; Software design",Risk management
562,"Proceedings - IEEE International Enterprise Distributed Object Computing Workshop, EDOCW","The proceedings contain 19 papers. The topics discussed include: automating the provisioning and integration of analytics tools with data resources in industrial environments using OpenTOSCA; aligning service level agreements with service-oriented enterprise architecture; a system organic architecture based on dynamic functional architecture modeling; towards integration methods of product-it into enterprise architectures; decision management for micro-granular digital architecture; defining enterprise architecture: a systematic literature review; a case study of stakeholder concerns on EAM; bimodal enterprise architecture management: the emergence of a new EAM function for a BizDevOps-based fast IT; automatic design of secure enterprise architecture: work in progress paper; introducing a coordination perspective to enterprise architecture management research; key performance indicators for a capability-based application portfolio management; services as activities: towards a unified definition for (public) services; evaluation of the risk and security overlay of ArchiMate to model information system security risks; exploring the role of enterprise architecture models in the modularization of an ontology network: a case in the public security domain; and in-depth modeling of the UNIX operating system for architectural cyber security analysis. Proceedings - IEEE International Enterprise Distributed Object Computing Workshop, EDOCW  ",Governance
563,Maximizing the U.S. Army's future contribution to global security using the Capability Portfolio Analysis Tool (CPAT),"Recent budget reductions have posed tremendous challenges to the YOU.S. Army in managing its portfolio of ground combat systems (tanks and other fighting vehicles), thus placing many important programs at risk. To address these challenges, the Army and a supporting team developed and applied the Capability Portfolio Analysis Tool (CPAT) to optimally invest in ground combat modernization over the next 25-35 years. CPAT provides the Army with the analytical rigor needed to help senior Army decision makers allocate scarce modernization dollars to protect soldiers and maintain capability overmatch. CPAT delivers unparalleled insight into multiple-decade modernization planning using a novel multiphase mixed-integer linear programming technique and illustrates a cultural shift toward analytics in the Army's acquisition thinking and processes. CPAT analysis helped shape decisions to continue modernization of the $10 billion Stryker family of vehicles (originally slated for cancellation) and to strategically reallocate over $20 billion to existing modernization programs by not pursuing the Ground Combat Vehicle program as originally envisioned. More than 40 studies have been completed using CPAT, applying operations research methods to optimally prioritize billions of taxpayer dollars and allowing Army acquisition executives to base investment decisions on analytically rigorous evaluations of portfolio trade-offs. © 2016 INFORMS. Maximizing the YOU.S. Army's future contribution to global security using the Capability Portfolio Analysis Tool (CPAT) Decision support; Fleet scheduling; Ground combat systems; Mixed-integer linear programming; Portfolio optimization; YOU.S. Army ",Strategic alignment
564,Collaborative creation with customers for predictive maintenance solutions on hitachi IoT platform,"OVERVIEW: Through the proliferation of sensors, smart machines, and instrumentation, industrial operations are generating ever increasing volumes of data of many different types and our customers are demanding solutions that provide business value over this collected data. In our interactions with customers across verticals, we have discovered that there is an urgent need for predictive maintenance solutions that meet customer demands. The reason for the appeal of predictive maintenance solutions is their ability to increase equipment availability, reduce the cost of unexpected failures and make operations more predictable. Hitachi offers a portfolio of data analytics technologies to address predictive maintenance use cases in a variety of verticals and in this paper we present an overview of our work in this area. Collaborative creation with customers for predictive maintenance solutions on hitachi IoT platform  ",Capacity management
565,Usage patterns and data quality: A case study of a national type-1 diabetes study,The Environmental Determinants of Islet Auto-immunity (ENDIA) project is Australia's largest study into the causes of Type-1 Diabetes (T1D). The ENDIA study is supported by a Cloud-based software platform including a clinical registry comprising extensive longitudinal information on families at risk of having a child that might go on to develop T1D. This registry includes both demographic and clinical information on families and children as well as the environmental factors that might influence the onset of T1D. A multitude of samples are obtained through the study and used to support a diverse portfolio of bioinformatics data analytics. The quality of the data in the registry is essential to the overall success of the project. This paper presents a Cloud-based log-analytics platform that supports the detailed analysis of patterns of usage of the registry by the clinical centres and collaborators involved. We explore the impact that the usage patterns have on the overall data quality. We also consider ways of improving data quality by mothers entering their own data through targeted mobile applications that have been developed for dietary data collection. © 2017 Association for Computing Machinery. Usage patterns and data quality: A case study of a national type-1 diabetes study Auditing; Cloud; Log analysis; Type-1 diabetes Clouds; Computer programming; Auditing; Bioinformatics data; Clinical information; Environmental determinants; Environmental factors; Log analysis; Mobile applications; Software platforms; Computer applications,Value management
566,Uncover the Truly Maximum Profit Opportunity of a Prospective M&A Deal: Next Generation M&A Financial Analysis Capability,"By integrating three analytic techniques already in use, separately, for merger and acquisition (M&A) financial analysis, it will be demonstrated that an optimized income statement (OIS) represents a “next generation” financial analysis capability for M&A and private equity portfolio management. The three techniques currently in use are (1) mixed integer and linear math programming (MILP) for least-cost supply chain design, (2) predictive analytics for sizing and allocating sales and marketing expenditures more profitably, and (3) activity-based costing (ABC). ABC data availability can substantially reduce the cost and time required to implement the MILP portion of an OIS model. It also improves the profitability of the M&A analysis beyond that available with OIS analyses conducted without ABC detail. © 2016 Wiley Periodicals, Inc. © 2016 Wiley Periodicals, Inc. Uncover the Truly Maximum Profit Opportunity of a Prospective M&A Deal: Next Generation M&A Financial Analysis Capability  ",Strategic alignment
567,Development and application of a stage-gate process to reduce the unerlying risks of it service projects,"Recently IT service projects have increased to introduce new technology as like data analytics, IoT (Internet of Things), cloud and mobile computing and to change or improve business process of finance, manufacturing, service, and government and public organizations, but lots of projects were failed due to cost-overrun, schedule delay, and fail to pass user acceptance test on time, and fail to align company’s objective and strategy. There are several critical factors of project failure, for example, incorrect project cost estimation, lack of enterprise-wide risk management, unfair contract agreement, and missing or incomplete user requirements, and low quality level of design and development, and lack of user participation or cooperation for user requirement definitions and user acceptance test. To prevent the critical risk factors of project, the risks should be identified and assessed during project lifecycle, and report to project governance board, and the project governance board should make the Go/No-Go decision at the end of each project stage. The purpose of this thesis is to develop the project SGP(Stage-Gate Process) for enterprise-wide risk management structure to reduce the project failure rate, and for helping to achieve the company’s objective and strategy, and then to verify the effectiveness of the project SGP through application of the SGP to actual IT service projects. We can aware the SGP is very useful to reduce the failure rate of project through preventing the costoverrun, schedule delay, and failure to pass for user acceptance test. The SGP is consisting of assessment of deliverable by project management office and quality assurance, and Go/No-Go decision making based on quality criteria by executives for enterprise-wide risk and quality management at the end of each project stage. And we confirmed the effectiveness of SGP through FGI (Focus Group Interview), the result show that the SGP is very useful to manage cost, risk, and quality, but the effectiveness of SGP is dependent on company’s project governance structure and process, and project governance board’s attention and support to the SGP process. © 2005-2016 JATIT & LLS. All rights reserved. Development and application of a stage-gate process to reduce the unerlying risks of it service projects It service project; Project governance board; Project management office; Project stakeholder; Risk management; Stage-gate process ",Risk management
568,A descriptive analytics tool for improving project human resource management: The importance of discerning a project team member's sentiments,"Recent research studies indicate a gap in research regarding descriptive and diagnostic analytics across project management knowledge areas. As a result there is mismatch between the information needed by project managers for good decision making and what the current tools provide. This research study purports that a project management intelligence (PMInt) tool which is aimed at assisting project manager to make informed decisions attempts to close this gap. Furthermore, the research proposes additional functional capabilities for the PMInt tool which enables it to fully provide project managers with insightful information (such as trends and patterns of sentiments over time, duration of expressed sentiments) and not just inform managers about sentiment polarity, namely, positive or negative or neutral. The proposed improvements have been incorporated in the prototype of the PMInt tool. The preliminary test results obtained from testing the prototype indicate that the tool will provide project managers with insightful information which will assist project managers to arrive at informed decisions regarding project team members' concerns and views and hence help in stabilizing project teams. © 2017 IEEE. A descriptive analytics tool for improving project human resource management: The importance of discerning a project team member's sentiments Descriptive analytics; PMInt; Project management; Sentiments; Software project Decision making; Managers; Personnel; Project management; Descriptive analytics; Functional capabilities; Informed decision; PMInt; Project human resource management; Project management knowledge; Sentiments; Software project; Human resource management",Strategic alignment
569,An investigation into how corporate real estate in the financial services industry can add value through alignment and methods of performance measurement,"Purpose-The purpose of this study is to evaluate corporate real estate (CRE) performance measurement and how value can be added to the core business. Design/methodology/approach-An analysis is made of the appropriate literature and primary research conducted via interviews with 11 senior professionals from three globally renowned companies, one global financial organisation and two corporate advisory firms. Findings-The findings from this research provide evidence that CRE can be used to add value to the core business, both in the physical and behavioural environment. By aligning aims and objectives with the business, continually conducting portfolio analytics, encompassing size, cost, space, retention and productivity, value can be added, maximising shareholder worth. Research limitations/implications-The main conclusions drawn from this study are that CRE can add value to the business. The role of corporate real estate asset managers (CREAMs) needs to change from the physical environment to the behavioural environment, working to increase productivity, which can have greatest impact on shareholder value. Originality/value-This paper provides evidence to suggest that CRE ’s role is not only to manage property but should be broadened to add value to the organisation by aligning CRE strategy with the corporate strategy. Closer interactions with human resource and information technology are required to enhance productivity, via relationship management, perhaps outsourcing to provide best in industry expertise.CREAMscan shape the future of office space, by demanding carbon neutral properties. This paper recommends that further research should be conducted on the measurement of intangibles, like productivity and corporate social responsibility, and how they can be used to add value and sustainable saves. © Emerald Group Publishing Limited. An investigation into how corporate real estate in the financial services industry can add value through alignment and methods of performance measurement Adding value; Alignment; Case study; CREAM; Performance measurement; Strategy ",Strategic alignment
570,The right tone of VOS: Improving the argument for local community solar,"This paper describes an alternative to the typical value-of-solar (VOS) analytic approach for supporting utility acquisition of local, distributed solar, relative to centralized solar resources. The specific context is resource acquisition for a community-solar program. The utility in this case could acquire (by ownership or power contract) solar from a centralized solar project for a relatively low cost, or it could include a portfolio of local, commercial-scale solar projects with higher ""sticker price,"" but strategic benefits. This case sheds light on the utility's internal-stakeholder debate and on the limitations of detailed bottom-up VOS analysis for some kinds of utility solar decisions. The recommended approach involves building a qualitative, strategic argument, which focuses on relatively few calculated values - three in this case, including strategic-design improvement, reduced transmission costs, and customer-retention value. In other cases, other values or ranges of values might be used. The objective is to apply analytics sparingly, to facilitate better decision-making under highly changeable technology, market, and policy conditions. © 2016. The Authors. Published by International Solar Energy Society. The right tone of VOS: Improving the argument for local community solar Community solar; DER; Distributed solar; Strategic solar; Utility solar; Value of solar; VOS Decision making; Solar energy; Community solar; Distributed solar; Strategic solar; Utility solar; Value of solar; Costs",Stakeholder management
571,"Adapting agile practices for data warehousing, business intelligence, and analytics","Business surveys indicate that fewer than 30% of data warehousing and business intelligence (DW/BI) projects meet the stated goals of the budget, schedule, and quality. Agile methods have been suggested as a possible solution, but because of the large size of the typical DW/BI project, it may be difficult to apply the agile values and principles. In this article, the following research questions are raised: Can agile practices be adapted for DW/BI development? What factors influence agile DW/BI development? Six semi-structured interviews were conducted using a questionnaire. The interview transcripts were coded using the grounded theory approach. Eight categories emerged from the analysis: business value, project management, agile development, shared understanding, technological capability, top management commitment, complexity, and organizational culture. Based on the categories, a research framework is proposed. The findings reveal that agile methods are suited for only certain aspects of DW/BI projects and need to be augmented with project management practices. Copyright © 2017, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited. Adapting agile practices for data warehousing, business intelligence, and analytics Agile; Analytics; Business Intelligence; Data Warehousing Budget control; Competitive intelligence; Computer supported cooperative work; Data warehouses; Information analysis; Project management; Surveys; Warehouses; Agile; Analytics; Grounded theory approach; Organizational cultures; Project management practices; Semi structured interviews; Technological capability; Top management commitment; Management science",Strategic alignment
572,ACM International Conference Proceeding Series,The proceedings contain 29 papers. The topics discussed include: assets and liabilities portfolio optimal model based on ES controlled interest rate risk; weight optimization model based on the maximum discriminating power of credit evaluation result; high embedded dimension stock data prediction model based on chaotic prediction model; research on retirement pension; research on the risk spillover effect of china's commercial banks based on quantile regression; bibliometrics analysis of complex networks research; an investigation on online shopping behaviors in the singles day: the role of gender; market structure and competitive behavior of china's P2P lending industry; the effect of service recovery on repurchase intention in online context: the moderation of perceived justice and psychological contract; the like economy: the impact of interaction between artists and fans on social media in art market; analysis of evaluating dimensions of Chinese express hotels based on user generated content; a transitivity analysis of on-line product descriptions-a case study of book product descriptions;identifying consumer buying behavior differences through market basket analysis in multiple outlet types; applications of civil information modeling (CIM) for constructability review in railway construction projects; study on application of PSO with time window for user recommendation in research social networks; and establishment of business intelligence and big data analysis for higher education. ACM International Conference Proceeding Series  ,Risk management
573,Exploring the use of financial capacity as a predictor of construction company corporate performance: Evidence from South Africa,"Purpose – The purpose of this paper is to examine the resilient operational variables that impact the corporate performance of construction companies in the South African construction industry and to explore whether financial capacity can be used as a predictor of construction company performance in the context of the South African construction industry. Design/methodology/approach – The operational variables of construction companies that impact their corporate performance were identified through an in-depth review of the extant literature. A combination of convenience and snowball sampling techniques were used in identifying 185 building and civil engineering construction companies based in four provinces of South Africa and registered in Grades 2-6 of the Construction Industry Development Board (CIDB) contractor grading register. The data used in the study were collected from this cohort of respondents through the use of structured questionnaires. At the end of the study period, 62 valid responses representing a response rate of 33.5 per cent were received. Data collected were analyzed using descriptive and inferential statistics. Findings – The findings of this study indicate that there is a significant positive relationship between the financial capital and net assets of construction companies and their corporate performance in terms of turnover. The data collected did not support any significant relationship between other operating financial variables, such as Return on Capital Employed and profitability and financial performance. Research limitations/implications – A predictive model for predicting the financial performance of firms was developed from the data collected. The implication of this is that the more financial capital possessed by a construction company, the more the company’s financial performance in terms of turnover. The CIDB can use financial capacity as a measure when grading contractors, as a good number of contractors are not performing. The predictive model developed could be adopted by the CIDB as an instrument for predicting the corporate financial performance of construction companies that seek to be listed on their contractor grading register. Originality/value – This research will be of significance to researchers and members of the research community in providing new knowledge as well as to contractors in enabling them to understand the importance of having financial capital. It is also of importance to the CIDB in their quest for contractor and construction industry development. Further research to validate the results obtained in this study using a larger sample size across more provinces of South Africa will form the basis of future studies. © 2015, Emerald Group Publishing Limited. Exploring the use of financial capacity as a predictor of construction company corporate performance: Evidence from South Africa Capability; Construction industry; Construction management; Corporate performance; Entrepreneurship; Financial capital; Financing; Human resources management; Information and knowledge management; Modeling; Net assets; Turnover Construction; Construction industry; Contractors; Grading; Knowledge management; Models; Predictive analytics; Project management; Surveys; Capability; Construction management; Corporate performance; Entrepreneurship; Financial capital; Financing; Human resources management; Information and knowledge managements; Net assets; Turnover; Finance",Strategic alignment
574,Why is Empowerment Important in Big Data Analytics?,"Big data analytics with its intricate insights is enabling service providers to better gauge customer needs. It is equally delivering information about the competitive landscape of services to customers. The frontline employees (FLEs) responsible for managing the diversified needs of these 'informed customers' face multiple challenges. The FLEs need not only information about their products/ services but also about markets and customers. A systematic review of the extant literature of big data and FLEs has helped to understand that FLEs need empowerment to adapt their services in high contact big data driven services. Empowerment as a concept is well known in management and psychology literature. The empowerment construct has predominantly remained to consisting of a single, and in a few cases, of multiple items. To facilitate effective service delivery in high contact big data driven services, FLEs need empowerment on multiple levels and there exists a significant gap in the literature about these constituent dimensions. This paper, synthesizing the relevant scholarly work, proposes a conceptual model for the empowerment construct. In doing so, this paper makes an important theoretical and managerial contribution towards the understanding of FLEs' empowerment and its relevance in high contact data driven services. © 2017 The Authors. Published by Elsevier B.V. Why is Empowerment Important in Big Data Analytics? big data analytics; big data analytics capability; data driven services; empowerment; frontline employees; service adaptation Information management; Information systems; Project management; Sales; Data analytics; Data driven; empowerment; Frontline; Service adaptation; Big data",Strategic alignment
575,Analytical Product Release Planning,"As part of any incremental and iterative development, release planning is the process of assigning features to upcoming releases (or iterations) such that the overall product evolution is optimized. Analytical product release planning refers to the application of analytical methods in this process, thereby utilizing the diversity of data available from internal and external sources of information. In this chapter, information needs for release planning are outlined and a taxonomy of release planning problems is given. The paradigm of Open Innovation is introduced as a new way to elicit and gain access to relevant data related to product objectives, features and their dependencies, customers and changing priorities, as well as product values and market trends. Analytical Open Innovation (AOI) is the integration of Open Innovation with (a portfolio of) analytical methods which could be used in different problems of a semi-wicked nature such as planning and design. This chapter studies the usage of AOI in the context of release planning (RP). The respective approach called ""AOI@RP"" is taking advantage of gathering and generating data and relating data into well-defined aspects of the problem and combining analytical methods to address the solution. The usage of AOI is studied in more detail for two of the concrete release planning problems given in the taxonomy: (1) Release planning in the presence of advanced feature dependencies and synergies detected from morphological analysis; (2) continuous what-to-release planning in consideration of ongoing trial feature evaluation. An illustrative case study is used as proof of concept to the proposed solution methodology. © 2015 Elsevier Inc. All rights reserved. Analytical Product Release Planning Case study; Data analytics; Decision support; Open innovation; Release planning Innovation; Taxonomies; Data analytics; Decision supports; Iterative development; Morphological analysis; Open innovation; Planning and design; Release planning; Solution methodology; Iterative methods",Strategic alignment
576,Quantifying Model Uncertainty and Risk,"In the last two decades, significant progress in science and technology has driven major advances in the modeling of natural hazards and their impact on the built environment. In addition, several major catastrophe events over the same period highlighted important issues that could affect the underwriting and decision-making processes for selecting and managing risk. In many cases, these events had a profound impact on insurance and reinsurance practices, expanding the use of models, analytics, and exposure-data quality analysis to understand the sources of risk and to model loss uncertainty. Furthermore, since the development of earlier versions of catastrophe models, technology and computing power have advanced enormously, allowing modelers to address key elements of model and loss uncertainty in a more systematic way. Most catastrophe models implicitly account for the uncertainty associated with various model components. These uncertainties can be characterized as aleatory or epistemic. The aleatory uncertainty represents the inherent variability in the physical system, which cannot be reduced. In contrast, the epistemic uncertainty relates to the lack of knowledge about the damage and hazard model components (i.e., building characteristics, ground motion, central pressure, etc.) and thus can be reduced with additional information about the system. A complete quantification of the contribution of each model component to the epistemic uncertainty requires a systematic approach and thus involves developing multiple alternative versions of the hazard and damage modules. By developing multiple versions, the modeler can capture the plausible range of parameters that control each model component. The risk model can then propagate those ranges into estimating variability of losses for a given portfolio. However, developing such a bottom-up quantification of the catastrophe model uncertainty is time consuming, and carrying out all the required analyses is also computationally expensive for practical applications. Nevertheless, modelers can now take advantage of the robust computational power developed in recent years to quantify systematically the uncertainty in the estimation of risk. This paper illustrates some practical approaches to capture uncertainties in risk models. The issues and challenges that remain in developing risk models and in quantifying the uncertainty, even with today's advancement in science and engineering, and vastly enriched technological environment, are also addressed in this paper. It draws on specific examples from the authors' experience in modeling earthquake and hurricane perils. © 2018 Elsevier Inc. All rights reserved. Quantifying Model Uncertainty and Risk Catastrophe; Earthquake; Hurricane; Loss; Uncertainty ",Value management
577,Speeding up Risk Analyses of U.S. Flood Insurance Loss Data Using the Diffusion Map,"In the insurance industry, catastrophe risk analysis using catalogs of catastrophic events is a major component for quantifying financial risks of an insurance portfolios. To ensure an accurate quantification of risk, particularly for rare, strong catastrophic events, large sizes of catalogs are simulated and used for computing loss estimates location-by-location and event-by-event, but this is computationally intensive. In this paper, we propose to speed up the risk computation by taking a data analytic approach to compress the catalog-specifically, using dimension reduction and clustering. To address the nonlinear geometry of the loss data from the YOU.S. Flood model, we used a nonlinear dimension reduction technique, the diffusion map. Combined with clustering, we show that it yields accurate catalog compression and produces a realistic representation of hydrometeorological patterns over the entire country. Finally, we discuss how clustering results must be refined to ensure fidelity in retaining the most important catastrophic events, and how in real life, a risk manager can utilize our results to make informed risk management decisions. Speeding up Risk Analyses of YOU.S. Flood Insurance Loss Data Using the Diffusion Map Catastrophe modeling; Diffusion map; Insurance risk analytics; Nonlinear dimension reduction; Spectral clustering Clustering algorithms; Diffusion; Disasters; Flood insurance; Floods; Insurance; Reduction; Risk assessment; Risk management; Catastrophe model; Diffusion maps; Insurance risk; Nonlinear dimension; Spectral clustering; Risk analysis",Risk management
578,Factors determining South African construction workers' prejudice towards and discrimination against HIV+ persons,"Stigma and fear of discrimination are powerful deterrents against human immunodeficiency virus (HIV) antibody testing. Prejudice and discrimination against people living with acquired immune deficiency syndrome (AIDS) are deterrents to their willingness to disclose their status and thereby avail themselves of treatment. Stigma impairs employees' general well-being, affecting their work capacity, and thus directly impacting upon project management success. Little is known about construction workers' prejudice towards and discrimination against HIV+ persons, and the determinants thereof. A field-administered questionnaire survey gathered data from 512 site-based construction employees in the Western Cape, South Africa. Following bivariate and regression analyses of the survey response data, a structural equations model comprising demographic factors, lifestyle risk, substance use (alcohol consumption and drug use), knowledge about HIV/AIDS, and attitudinal fear of testing is posited to explain prejudice towards and discrimination against HIV+ persons. The results indicate the following: (1) education, HIV/AIDS knowledge, attitudinal fear of testing, and prejudice towards HIV+ persons are determinants of discrimination against HIV+ persons; (2) education, HIV/AIDS knowledge, and attitudinal fear of testing are determinants of prejudice towards HIV+ persons; (3) AIDS knowledge is predicted by level of education and ethnicity; and (4) attitudinal fear of testing is predicted by level of education, HIV/AIDS knowledge, and extent of risky lifestyle. The research confirms current knowledge about the multivariate determination of prejudice towards and discrimination against HIV+ persons. It advances this understanding by identifying and emphasizing the central role played by education and subject-specific information in such determination. Specifically, it offers a clearer picture of the direct influence of educational background and AIDS knowledge in determining both the cognitive dimension of prejudice and the behavioral manifestation of this through discrimination. This has clear implications for construction firms in their efforts at HIV/AIDS intervention management. As much of this AIDS knowledge is determined by cultural beliefs and values, there exists the need to design programs that directly and sensitively address these cultural beliefs and their likely impact on prejudice and discrimination. Likewise, prior education is also critical, both in terms of amplifying or mitigating the impact of cultural beliefs and in terms of the potential comprehension by workers of such structured media and awareness campaigns. Failure to account for these two factors and their influence on cognition and behavior would result in interventions of diminished impact and success. © 2015 American Society of Civil Engineers. Factors determining South African construction workers' prejudice towards and discrimination against HIV+ persons Construction workers; Discrimination; Fear of testing; Human immunodeficiency virus/acquired immune deficiency syndrome (HIV/AIDS); Labor and personnel issues; Predictive modeling; Prejudice; Stigma Construction industry; Human resource management; Personnel testing; Predictive analytics; Project management; Regression analysis; Surveys; Viruses; Construction workers; Discrimination; HIV/AIDS; Personnel issues; Predictive modeling; Prejudice; Stigma; Diseases",Capacity management
579,The journey from epidemiology to health analytics,"She Will Health has had an Epidemiology team for over thirty years that, historically, has focused on traditional occupational epidemiology in the United States. She Will Health epidemiologists have published more than fifty papers using the She Will Health Surveillance System (HSS), a database comprised of demographic, work history, and absence data for YOU.S.-based employees. In the 2000s, epidemiologists began to distribute health reports highlighting health risk factors and absenteeism rates and to design wellness interventions in YOU.S. operating sites. The current team of epidemiologists has skill sets that are interdisciplinary, reaching across businesses and transcending traditional epidemiology into a global program of great breadth. Their portfolio now includes holistic approaches to wellness, community health, and global measurement strategies. Epidemiologists lead the analyses of the positive psychology-based Resilience Program implemented in office settings globally. As the Program expands, epidemiologists are working closely with operating sites' leadership to ensure the enhanced program's applicability to their workers and appropriate selection of business outcomes. She Will's Care for People is a global initiative to provide an environment in which workers in camps and construction sites can perform their best; epidemiologists developed its analytics strategy and met with leaders from across the business to ensure appropriateness and usefulness of proposed data collection. The transition of the team to a global discipline has been recognized by the Vice President of Health and the Global Leadership Team. Under their guidance, the team re-branded to Health Analytics to represent its breadth and global reach. A long-term strategy, mission, and vision are being created by the team to demonstrate its scope and its strong relationship with businesses across She Will. The group is likely to grow from a YOU.S.-centric team delivering classic occupational epidemiology to a global discipline focused on realizing Health's value in partnership with She Will business across human performance dimensions. The new team will be the first scientific discipline in She Will Health to re-brand itself and expand globally after a long history in only the YOU.S. The current members have played important roles in the growth of the team's scope, its global recognition, and its transition into a new discipline. Copyright 2016, Society of Petroleum Engineers. The journey from epidemiology to health analytics  Epidemiology; Safety engineering; Shells (structures); Social aspects; Construction sites; Global leadership; Global measurements; Health surveillances; Long-term strategy; Occupational epidemiologies; Positive psychology; Scientific discipline; Health risks",Change Management
580,Suboptimal business intelligence implementations: Understanding and addressing the problems,"Purpose – The purpose of this paper is to examine the failures of business intelligence (BI) implementations and to understand why they fail as well as what action can be taken to ensure implementation success. Design/methodology/approach – The paper is based on a literature review of academic journals and case studies relating to BI, and the success and failure of the implementation of such projects. It focuses on four areas of BI projects to measure success: return on investment, non-concrete measures, project management measures and user satisfaction. The literature provides insights into what factors contribute to the success of a BI implementation and what factors contribute to the failure. Once the failures can be ascertained, a strategic approach to remedying the failure is discussed. Findings – Implementation failure specifically relating to BI is a rarely discussed topic. This paper provides an understanding of why BI implementations fail and how organisations can ensure, prior to implementing such a solution, the considerations that need to be made to ensure that success is achieved from a technological, organisational and process perspective. Originality/value – The paper uses empirical evidence from the literature to provide an understanding of why BI implementations fail. The factors contributing to BI failure are examined along with insights into how to succeed with a BI implementation. © Emerald Group Publishing Limited Suboptimal business intelligence implementations: Understanding and addressing the problems Business intelligence; Business strategy; Project failure; Project implementation; Suboptimal implementation Competitive intelligence; Project management; Business strategy; Design/methodology/approach; Implementation success; Project failures; Project implementation; Return on investments; Strategic approaches; Suboptimal implementations; Information analysis",Strategic alignment
581,The Social Justice Collaboratorium: Illuminating Research Pathways between Social Justice and Library and Information Studies,"Purpose This chapter introduces an initiative of the Spectrum Doctoral Fellows to build an online resource that engages the Library and Information Studies (LIS) community in a discussion of social justice initiatives within the field. This tool further develops a social justice framework that raises awareness of and integrates social justice methodologies into LIS curricula and library practices. This framework facilitates community building and the empowerment of the populations they serve. Methodology/approach Using an iterative approach to user-centered design, the Social Justice Collaboratorium (SJC) development process consists of input from a community of engaged users to inform the wireframe, prototype, testing, and development phases. This includes gathering substantial qualitative and quantitative data such as surveys of LIS faculty, practitioners and students, as well as tracking web analytics once the tool is live. Practical implications The SJC allows for the confluence of research, resources, networks, best practices, and LIS school models in a centralized medium. Designed for LIS practitioners, faculty, staff, and students, as well as those interested in project management, resource development, and collaborative work, the SJC supports different approaches to social justice in LIS. Originality/value The SJC will be accessible to a distributed community of social justice LIS scholars, practitioners, students, and activists. Contributions from the community of users throughout every stage of the development process ensures participation, stewardship, and intentionality. In this way, the SJC will be a transformative tool for the LIS community as a vehicle for promoting equity and social change. © 2016 by Emerald Group Publishing Limited. The Social Justice Collaboratorium: Illuminating Research Pathways between Social Justice and Library and Information Studies collaborative work; collaboratorium; resource development; Social justice; spectrum; user-centered design ",Value management
582,Disaster recovery planning and business continuity for informaticians,"Purpose: The purpose of this paper is to address the problems associated with informatics and analytics projects that are developed in an “organic” manner. As such, this often circumvents formal project management principles and practices. The decision to do this is determined by organizational factors; however, even in an informal environment, ensuring that adequate disaster recovery and business continuity plans are in place for all mission-critical applications is vital to ensure the long-term survival prospects of an organization in the event of a disaster. Design/methodology/approach: By adapting the principles and techniques of traditional disaster recovery and business continuity planning, an informatician can develop plans that integrate the requirements of their projects into a larger, organization-wide plan to recover from incidents and ensure continuity of business operations. Findings: The use of disaster recovery planning and business continuity planning can help ensure the long-term viability of informatics and analytics projects within an organization. Originality/value: Most business continuity planning is focused on projects that are formally developed and relatively large in scale. This paper applies these principles and practices to informatics and analytics projects that are developed informally and managed casually. Thorough an example, the point that more traditional disaster recovery and continuity practices can and should be applied in this less-formal environment is demonstrated. © 2017, © Emerald Publishing Limited. Disaster recovery planning and business continuity for informaticians BCP and BIRA; Business continuity planning; Business impact and risk analysis for informatics; Disaster recovery planning; DRP; Informatics project planning; Risk prioritization in informatics projects Management science; Project management; Recovery; Risk analysis; Risk assessment; BCP and BIRA; Business continuity planning; Disaster recovery planning; Informatics; Project planning; Disasters",Risk management
583,Digital revolution: How digital technologies will transform E&P business models in Asia-Pacific,"Disruptions, both to what the industry does and how it does it, have the potential to transform the oil and gas (O&G) industry. 3 main trends will impact future exploration and production (E&P) business models, namely: (i) lower oil price environment resulting in shrinking margins; (ii) changing energy mix towards natural gas and non-hydrocarbons; and (iii) increased level of connectivity that is driving digital technology adoption and automation. Digital is the latest buzzword, but what are the implications for an industry that is not new to digital technology? The increasing pace of innovation is driving opportunity for the sector to re-imagine their business models. Digital transformations could unlock USD 1.6-2.5 trillion for the industry, its customers, and wider society (World Economic Forum, 2017). Currently, only 6% of companies can be described as ""Digital High Performers"" – those who have successfully altered existing business models to capture opportunities for sustainable top-line growth (Accenture Digital, 2017). This paper identifies potential business models for the upstream sector that are built upon technologies such as advanced real-time 3D seismic studies, dynamic portfolio and margin management, well & facilities standardisation, and predictive analytics for flow assurance. Becoming a Digital High Performer requires a mindset shift, which results in completely different lifecycle processes (from workflow-driven to decision time-driven) and organisational structures (from rigid partnerships to a fluid capability ecosystem). Extracting resources faster and cheaper will replace maximising total recovery as a strategic goal; companies that achieve this have the potential to increase returns on invested capital by 3-10 times current levels (Refined, 2016). The path to capturing the future of O&G requires the right investments in technology and people, paired with flawless execution and governance. Economic scenarios suggest that the combined impact of technology and socio-economic factors could accelerate peak oil demand to as early as 2030 (World Energy Council, 2016). This increases the urgency for energy companies to rethink their strategic direction and how they generate competitive advantage in a rapidly changing world. © 2017, Society of Petroleum Engineers. Digital revolution: How digital technologies will transform E&P business models in Asia-Pacific  Behavioral research; Competition; Economics; Investments; Petroleum engineering; Petroleum industry; Predictive analytics; Rigid structures; Competitive advantage; Digital technologies; Digital transformation; Exploration and productions; Organisational structure; Socio-economic factor; Strategic direction; World Energy Council; Industrial economics",Financial management
584,"19th European Conference on Applications of Evolutionary Computation, EvoApplications 2016","The proceedings contain 53 papers. The special focus in this conference is on EvoBAFIN and EvoBIO. The topics include: Enhanced multiobjective population-based incremental learning with applications in risk treaty optimization; genetic programming with memory for financial trading; improving fitness functions in genetic programming for classification on unbalanced credit card data; evolving classification models for prediction of patient recruitment in multicentre clinical trials using grammatical evolution; portfolio optimization, a decision-support methodology for small budgets; evolutionary multiobjective optimization for portfolios in emerging markets; on combinatorial optimisation in analysis of protein-protein interaction and protein folding networks; a multi-objective genetic programming biomarker detection approach in mass spectrometry data; automating biomedical data science through tree-based pipeline optimization; bicliques in graphs with correlated edges; hybrid biclustering algorithms for data mining; discovering potential clinical profiles of multiple sclerosis from clinical and pathological free text data with constrained non-negative matrix factorization; application of evolutionary algorithms for the optimization of genetic regulatory networks; a hybrid discrete artificial bee colony algorithm for the multicast routing problem; evolving coverage optimisation functions for heterogeneous networks using grammatical genetic programming; joint topology optimization, power control and spectrum allocation for intra-vehicular multi-hop sensor networks using dandelion-encoded heuristics; a heuristic crossover enhanced evolutionary algorithm for clustering wireless sensor network; a variable local search based memetic algorithm for the load balancing problem in cloud computing and reducing efficiency of connectivity-splitting attack on newscast via limited gossip. 19th European Conference on Applications of Evolutionary Computation, EvoApplications 2016  ",Strategic alignment
585,Software Risk Modeling by Clustering Project Metrics,"Together with the development and integration of software technologies, an increase in the complexity of the software development environment has made identifying software risks challenging. Identifying software risks, which is a critical activity in project management, is challenging because numerous factors may affect software projects. In this paper, an approach to identify software-risk items is proposed in which data collected from past software projects are mined to construct software-risk models. The prediction models obtained can be used to identify potential software risks for subsequent software projects. The advantage of the proposed approach is that the software-risk models can be constructed at an early stage in software projects to facilitate the planning of methods to mitigate software risks. The proposed approach is applied to a business project to demonstrate how software risk items can be identified. © 2015 World Scientific Publishing Company. Software Risk Modeling by Clustering Project Metrics clustering; data mining; Software process; software risk Data mining; Predictive analytics; Project management; Risk assessment; Risk management; clustering; Critical activities; Prediction model; Software development environment; Software process; Software project; Software risks; Software technology; Software design",Risk management
586,Health Information Management: Changing with Time,"Objective: With the evolution of patient medical records from paper to electronic media and the changes to the way data is sourced, used, and managed, there is an opportunity for health information management (HIM) to learn and facilitate the increasing expanse of available patient data. Methods: This paper discusses the emerging trends and lessons learnt in relation with the following four areas: 1) data and information governance, 2) terminology standards certification, 3) International Classification of Diseases, 11th edition (ICD-11), and 4) data analytics and HIM. Results: The governance of patient data and information increasingly requires the HIM profession to incorporate the roles of data scientists and data stewards into its portfolio to ensure data analytics and digital transformation is appropriately managed. Not only are terminology standards required to facilitate the structure and primary use of this data, developments in Canada in relation with the standards, role descriptions, framework and curricula in the form of certification provide one prime example of ensuring the quality of the secondary use of patient data. The impending introduction of ICD-11 brings with it the need for the HIM profession to manage the transition between ICD versions and country modifications incorporating changes to standards and tools, and the availability and type of patient data available for secondary use. Conclusions: In summary, the health information management profession now requires abilities in leadership, data, and informatics in addition to health information science and coding skills to facilitate the expanding secondary use of patient data. Georg Thieme Verlag KG Stuttgart. Health Information Management: Changing with Time  Health Information Management; International Classification of Diseases; Statistics as Topic; Vocabulary, Controlled; controlled vocabulary; International Classification of Diseases; medical information system; statistics; trends",Monitoring and control
587,A business intelligence instrument for detection and mitigation of risks related to projects financed from structural funds,"The Structural Funds, in order to produce the intended effects, must use their specific management tools, for achieving the strategic objectives, outcome indicators and elements of added value set by each EU member state. The project portfolios must be managed properly. If risks of a project become contagious for other projects, we are witnessing a phenomenon that can compromise the chance that a program financed by Structural Funds to be well carried out. In this paper it is introduced an algorithm to reduce the project implementation risk and an IT interface is designed to serve as a control system, for the permanent measurement and monitoring of the risk indicators, in order to facilitate decision-making and prediction. © 2018, Bucharest University of Economic Studies. All rights reserved. A business intelligence instrument for detection and mitigation of risks related to projects financed from structural funds Dashboard; Implementation risk; IT interface; Portfolio risk management; Structural funds ",Risk management
588,"5th International Symposium on Computational and Business Intelligence, ISCBI 2017","The proceedings contain 30 papers. The topics discussed include: development of computer vision based obstacle detection and human tracking on smart wheelchair for disabled patient; determining direction of moving object using object tracking for smart wheelchair controller; Siamese-twin random projection neural network with bagging trees tuning for unsupervised binary image hashing; GLCM and its application in pattern recognition; tomato ripeness clustering using 6-means algorithm based on v-channel OTSU segmentation; onward movement detection and distance estimation of object using disparity map on stereo vision; optimal placement of mesh routers in a wireless mesh network with mobile mesh clients using simulated annealing; maximum-minimum temperature prediction using fuzzy random auto-regression time series model; fuzzy random auto-regression time series model in enrollment university forecasting; classification of dental diseases using CNN and transfer learning; optimizing k-means text document clustering using latent semantic indexing and pillar algorithm; TPC: an automatically generated comprehensive English-Persian parallel corpus; portfolios optimization with coherent risk measures in fuzzy asset management; the nexus between R&D, innovation and profitability of indigenous oil firms: a structural equilibrium model approach; and a framework for identifying and evaluating technologies of interest for effective business strategy: using text analytics to augment technology forecasting. 5th International Symposium on Computational and Business Intelligence, ISCBI 2017  ",Strategic alignment
589,Predictive safety analytics: Inferring aviation accident shaping factors and causation,"This paper illustrates the development of an object-oriented Bayesian network (OOBN) to integrate the safety risks contributing to an in-flight loss-of-control aviation accident. With the creation of a probabilistic model, inferences about changes to the states of the accident shaping or causal factors can be drawn quantitatively. These predictive safety inferences derive from qualitative reasoning to conclusions based on data, assumptions, and/or premises, and enable an analyst to identify the most prominent causal factors leading to a risk factor prioritization. Such an approach facilitates a mitigation portfolio study and assessment. The model also facilitates the computation of sensitivity values based on perturbations to the estimates in the conditional probability tables. Such computations lead to identifying the most sensitive causal factors with respect to an accident probability. This approach may lead to vulnerability discovery of emerging causal factors for which mitigations do not yet exist that then informs possible future R&D efforts. To illustrate the benefits of an OOBN in a large and complex aviation accident model, the in-flight loss-of-control accident framework model is presented. © 2014, E-flow Taylor and Francis. All rights reserved. Predictive safety analytics: Inferring aviation accident shaping factors and causation accident causation; aviation safety risk; object-oriented Bayesian network Aircraft accidents; Bayesian networks; Risk perception; Safety factor; Accident causation; Aviation safety; Conditional probability tables; Loss-of-control accidents; Object-oriented Bayesian networks; Probabilistic modeling; Qualitative reasoning; Vulnerability discovery; Predictive analytics",Monitoring and control
590,Predicting delays in software projects using networked classification,"Software projects have a high risk of cost and schedule overruns, which has been a source of concern for the software engineering community for a long time. One of the challenges in software project management is to make reliable prediction of delays in the context of constant and rapid changes inherent in software projects. This paper presents a novel approach to providing automated support for project managers and other decision makers in predicting whether a subset of software tasks (among the hundreds to thousands of ongoing tasks) in a software project have a risk of being delayed. Our approach makes use of not only features specific to individual software tasks (i.e. local data) - as done in previous work - but also their relationships (i.e. networked data). In addition, using collective classification, our approach can simultaneously predict the degree of delay for a group of related tasks. Our evaluation results show a significant improvement over traditional approaches which perform classification on each task independently: achieving 46% - 97% precision (49% improved), 46% - 97% recall (28% improved), 56% - 75% F-measure (39% improved), and 78% - 95% Area Under the ROC Curve (16% improved). © 2015 IEEE. Predicting delays in software projects using networked classification Machine Learning; Networked classification; Risk management; Software analytics Artificial intelligence; Automation; Cost engineering; Decision making; Forecasting; Learning systems; Project management; Risk management; Area under the ROC curve; Automated support; Collective classifications; Cost and schedule; Engineering community; Evaluation results; Software project management; Traditional approaches; Software engineering",Risk management
591,"11th IFIP TC 3 World Conference on Computers and Education, WCCE 2017","The proceedings contain 68 papers. The special focus in this conference is on Computers and Education. The topics include: The use of tablets in secondary schools and its relationship with computer literacy; Learners’ experiences in a multicultural remote collaborative learning environment: A case of ICT4D course; collaborative postgraduate studies in higher education: A case study of South Africa; Scaling a model of teacher professional learning – harnessing MOOCS to recreate deep learning conversations; evaluating acceptance of a haptic learning resource from various perspectives; modelling e-learner comprehension within a conversational intelligent tutoring system; The value of project management education for IT professionals; learning analytics for formative purposes; digital safety and responsible use within a primary school ecosystems community in Aotearoa/New Zealand; feature based sentiment analysis for evaluating the mobile pedagogical affordances of apps; how interactives can change learnability of science concepts for young children – re-positioning them as learners ‘who can and did’; Primary school students’ choices in writing opinion essays: Using ICT combined with self-regulated strategies; Towards a framework for developing the emotional intelligence of secondary school students through the use of VLEs; enhancing learning in a virtual environment: Qualities of learning in different learning modes; online teacher education: Transforming teachers’ knowledge for teaching with digital technologies; Understanding the best way to embed ICT in teacher education; DIYLab as a way for student teachers to understand a learning process; determinants of mobile learning in indigenous/cultural contexts: The Phenomenon in Canadian first nations; ontology-based backward learning support system; preface. 11th IFIP TC 3 World Conference on Computers and Education, WCCE 2017  ",Monitoring and control
592,The importance of E-portfolios for effective student-facing learning analytics,"The field of Academic Analytics offers considerable potential to Higher Education institutions (HEIs), the academic staff who work for them and, most importantly, the students they teach. This approach to data-led decision-making is starting to have an influence and impact on what is arguably the core business of Higher Education: student learning. As well as being nascent, Learning Analytics is, potentially at least, a very broad area of inquiry and development; the field, necessarily, therefore has significant gaps. It is also just one of a large number of changes and developments that are affecting the way that Higher Education operates. These changes include such things as the introduction of standards-based assessment and outcomes-based education, and the identification and warranting of core competencies and capabilities of university graduates. It is also happening at a time when the affordances of a wide variety of eLearning tools are introducing new possibilities and opportunities to the pedagogy of Higher Education in ways that are demonstrably challenging traditional approaches to teaching and learning, something Sharpe and Oliver famously refer to as the ‘trojan mouse’ (Sharpe and Oliver In Designing courses for e-learning. Rethinking Pedagogy for a Digital Age, Designing and delivering e-learning, pp. 41-51, 2007, p. 49). This chapter considers the role that one such eLearning tool-the e-portfolio-can play in the implementation of a student-facing Learning Analytics strategy in this ambitious new approach to conceptualising, facilitating, structuring, supporting and assuring student learning achievement. © Springer Nature Singapore Pte Ltd. 2017. The importance of E-portfolios for effective student-facing learning analytics Assessment Analytics; Assessment and feedback; E-portfolios; Learning analytics; Self-regulated learning ",Strategic alignment
593,Data Plan: Accessing analytics is essential when optimizing health care property portfolios,"[No abstract available] Data Plan: Accessing analytics is essential when optimizing health care property portfolios  Datasets as Topic; Decision Making; Financial Management, Hospital; Health Facilities; Humans; Maintenance and Engineering, Hospital; Risk Assessment; decision making; financial management; health care facility; hospital service; human; information processing; risk assessment",Risk management
594,Survey on advisor intelligence through purchase patterns and sales analytics,"In mutual fund, an individual or a firm that is in the business of giving advice about securities to clients is an investment advisor. Investment advisers are individuals or firms that receive compensation for giving advice on investing in stocks, bonds, mutual funds, or exchange-traded funds. Investment advisors manage portfolios of securities. Advisors can use new cognitive and analytics capabilities to better understand their clients and needs and have a stronger ability to deepen relationships with a better portfolio. In this paper, we analyze data points for each advisor, and distinguish the best prospects, obtain insight into their experience and credentials, and learn about their portfolio, in other words, to recognize the pattern of portfolio of the advisors. Such analysis helps the sales people to sell the fund company products to the suitable advisors based on the nature of the product they want to sell. This is done by investigating what kind of products advisors have been buying, and what kind of products they might be looking for. This helps to increase the sales of the products as sales people will be reaching the appropriate advisors. © 2017 The Authors. Survey on advisor intelligence through purchase patterns and sales analytics Big data analytics; Correlation; Evolutionary algorithm; Investment advisor; Leader-follower; Pattern recognition evolutionary algorithm; human; intelligence; investment; leadership; pattern recognition",Strategic alignment
595,Digital fitness: Four principles for successful development of digital initiatives,"Digital initiatives such as big data analytics at Progressive Insurance can drive competitive advantage. However, digital initiatives often fail to meet their objectives in terms of budget, time, or performance. Why is this the case and what can be done about it? Complementing existing studies of software development methodologies, project management techniques, and the role of the CIO, we focus on the role of the non-CIO organizational leader in successful development of digital initiatives. Synthesizing our own original research with existing scholarship, we introduce four principles for successful development of digital initiatives: generate business value, maintain strategic alignment with organizational and business strategies, leverage technology trajectories, and apply digital economics. Understanding the four principles raises the digital fitness of organizational leaders, thereby reducing risk and raising the chances of success in digital initiatives. © 2015 IEEE. Digital fitness: Four principles for successful development of digital initiatives Digital economics; Digital fitness; IT business value; Strategic alignment; Technology trajectories Advanced Analytics; Budget control; Competition; Data Analytics; Digital storage; Health; Planning; Project management; Business strategy; Competitive advantage; Digital fitness; IT business values; Management techniques; Software development methodologies; Strategic alignment; Technology trajectory; Software design",Strategic alignment
596,Proactive Construction Project Controls via Predictive Visual Data Analytics,"This paper presents a new visual production management system for proactive project controls on construction sites. Taking advantage from the unprecedented growth of visual data on construction sites, the system continuously reconstructs and visualizes reality in form of 4D point clouds and maps it directly within 4D BIM. By putting schedule tasks and project performance data in a visual context for the entire team and mapping Reality to Plan, the system communicates ""who does what work in what location"", tracks actual progress and productivity at the weekly work plan and the look ahead schedule, and infers top locations at risk for potential delays. To validate the system and its impact on field reporting and project controls practices, a case study is conducted on a building construction project. Results demonstrate that such a system improves coordination and communication by providing transparency in project execution and helps project teams improve reliability of their short term plans. © 2017 ASCE. Proactive Construction Project Controls via Predictive Visual Data Analytics  Architectural design; Data Analytics; Human resource management; Predictive analytics; Building construction projects; Construction projects; Construction sites; Production management systems; Project control; Project execution; Project performance; Visual context; Project management",Strategic alignment
597,The financial reward for environmental performance in the energy sector,"This article studies the financial reward for environmental performance of firms in the energy sector. Because of their substantial impact on environment, energy sector firms convey a particular status in the environmental-financial performance question, as compared with firms outside this sector. We use the environmental scores compiled by Kinder, Lyndenberg, and Domini Research and Analytics to construct two portfolios that differ in their environmental performance. We find that, between 2000 and 2011, energy sector firms with good environmental performance financially outperform energy sector firms with poor environmental performance. A portfolio strategy with a long (short) position in energy sector firms with good (poor) environmental performance generates an annual abnormal return of 9.624% after correcting for market, size, book-to-market and momentum risks. For firms outside the energy sector, the performance of the two portfolios is statistically insignificant. Using the VIX index, we also show that the market does not reward environmental performance of energy sector firms in periods of high financial uncertainty. © The Author(s) 2016. The financial reward for environmental performance in the energy sector Energy sector; Environmental performance; Stock performance; Uncertainty Commerce; Finance; Abnormal returns; Energy sector; Environmental performance; Financial performance; Financial rewards; Portfolio strategies; Stock performance; Uncertainty; electricity industry; environmental economics; finance; stock market; Environmental management",Financial management
598,"Data integration- Jefferson County’s journey to compile, track, and report on key performance indicators (KPI)","Jefferson County Alabama currently owns, operates and maintains approximately 3,100 miles of gravity sewer, 176 pump stations, and 7 major WWTPs. However, before 1996, the County only owned and operated 570 miles of trunk sewers and 33 pump stations. All other assets were owned and operated by 21 municipalities. A 1996 Consent Decree required the County to take over all of these systems and to eliminate overflows. Many of these systems had not been maintained properly and were not constructed to proper standards. In subsequent years, the County spent a significant amount of money making system improvements and, with a downturn in the economy went into bankruptcy in 2011. The significant reduction in capital funding and the need to manage the system through the bankruptcy created a unique situation that motivated the County to take a customized Asset Management approach based on early successes and measuring outcomes to continue to improve the system and address overflows during both wet and dry weather. Simple data analytics reveal basic insights; more sophisticated analytics, applied to data that has been pooled into a “data lake” with data from external and enterprise sources, unearth deeper insights that will help water and wastewater utilities optimize their performance. Because of the growing volume, complexity and strategic importance of asset management data, it is no longer desirable or even feasible for each departmental unit/division/function within a utility to manage this data by itself, or build its own data analytics capability. To get the most out of the new data resources, utilities are creating dedicated data groups that are potentially embedded within the core asset management program team that consolidate data collection, aggregation and analytics. Therefore, an additional responsibility of the asset management program is for making data and insights available across various organizational functions and business units. Recently, advances in technology have revolutionized data and performance reporting in that users (with limited IT development expertise) can perform data mining and develop high impact visuals for performance reporting. Jefferson County has implemented Microsoft Business Intelligence (BI) to track and report key performance indicators. (Figure presented.) Key project benefits include: • Eliminates the reliance of core IT developers to develop and manage reporting frameworks as BI is now integrated with common applications such as Excel putting the non-IT user in a position to perform complex data analysis and develop visualizations • Significantly reduced development cost and level of effort • Through the concept of data “lakes”, data models may be constructed using data from various sources (CMMS, GIS, SCADA, project management, financial and customer information systems) relatively easily • Eliminates the extensive costs and the need for complex and disparate system integration that is typically required to connect data for effective performance reporting • What would take months to develop high impact visualizations can now be developed in hours or weeks. • Completely transferrable to mobile devices for mobile use at meetings/workshops. The setup of the program and the prioritization process coincided with and assisted with the removal the County from Bankruptcy. The result was a detailed plan of spending, which included specified spending in the collection system, rate increases and debt service payments. With the County out of Bankruptcy and a prioritized program framework in place the County needed to be able to implement their CIP without the ability to hire new staff. What was needed was a robust program management plan with strict controls, key performance metrics, periodic review, and robust schedule management. Multiple Key Performance Indicators are in place, to ensure both effective implementation of the CIP, and the collection system performance. This system promotes the goal of effectively managing projects, as well as ensuring that the right projects are selected to minimize SSOs. Monthly program reports track key performance indicators. This presentation and paper will cover the key aspects of the development and implementation of robust data analytics. Jefferson County has implemented Microsoft Business Intelligence (BI) data analytics tools to track and report on the performance of KPIs. Copyright ©2016 Water Environment Federation. Data integration- Jefferson County’s journey to compile, track, and report on key performance indicators (KPI)  Asset management; Benchmarking; Coordinate measuring machines; Cost benefit analysis; Data Analytics; Data integration; Data mining; Economics; Human resource management; Lakes; Project management; Sanitary sewers; SCADA systems; Visualization; Core asset management; Customer information systems; Effective performance; Key performance indicators; Organizational functions; Performance reporting; Prioritization process; Water and wastewater; Information management",Financial management
599,Executive Roundtable Series: Driving Higher ROI and Organizational Change,"Many organizations are investing in analytics programs but encountering challenges in getting sufficient ROI, or need to make new investments but have not put forth an adequate business case. This article in the Executive Roundtable Series aims to help organizations get greater value from their analytics programs and to understand how to increase maturity across the organization, especially in the fast-moving areas of big data and digital customer data. The full video is available at http://youtu.be/85pYr2Cg3Fk. © 2015 IEEE. Executive Roundtable Series: Driving Higher ROI and Organizational Change Big data; big data; Data anallytics; Data analysis; data analytics; Project management; project management; Standards organizations Data reduction; Project management; Societies and institutions; Business case; Customer data; Data anallytics; Data analytics; Organizational change; Big data",Strategic alignment
600,Preliminary Requirements and Architecture Definition for Integration of PLM and Business Intelligence Systems,"With the advance of information systems and business intelligence technologies, new possibilities and functionalities to measure, monitor and control processes have emerged in the research area and in the market. In the context of PLM system, not only KPI for strategic goals can be measure and indicators for decision but also operational metrics link to product, project and process to manage agility of companies. © IFIP International Federation for Information Processing 2014. Preliminary Requirements and Architecture Definition for Integration of PLM and Business Intelligence Systems Agile methods; Business Intelligence; Performance Measurement; PLM; Project Management Competitive intelligence; Industrial management; Knowledge based systems; Project management; Agile methods; Business intelligence systems; Monitor and control; Performance measurements; PLM systems; Strategic goals; Information analysis",Strategic alignment
601,Many-objective de Novo water supply portfolio planning under deep uncertainty,"This paper proposes and demonstrates a new interactive framework for sensitivity-informed de Novo planning to confront the deep uncertainty within water management problems. The framework couples global sensitivity analysis using Sobol' variance decomposition with multiobjective evolutionary algorithms (MOEAs) to generate planning alternatives and test their robustness to new modeling assumptions and scenarios. We explore these issues within the context of a risk-based water supply management problem, where a city seeks the most efficient use of a water market. The case study examines a single city's water supply in the Lower Rio Grande Valley (LRGV) in Texas, using a suite of 6-objective problem formulations that have increasing decision complexity for both a 10-year planning horizon and an extreme single-year drought scenario. The de Novo planning framework demonstrated illustrates how to adaptively improve the value and robustness of our problem formulations by evolving our definition of optimality while discovering key tradeoffs. © 2011 Elsevier Ltd. Many-objective de Novo water supply portfolio planning under deep uncertainty Decision support; Many-objective decision analytics; Multiobjective evolutionary algorithms; Risk; Robust decision making; Sensitivity analysis; Uncertainty Rio Grande Valley; Texas; United States; Decision support systems; Evolutionary algorithms; Risks; Sensitivity analysis; Water management; Decision supports; Many-objective decision analytics; Multi objective evolutionary algorithms; Robust decisions; Uncertainty; algorithm; decision support system; risk assessment; sensitivity analysis; uncertainty analysis; variance analysis; water management; water supply; Water supply",Risk management
602,An analysis of the effectiveness of the M&A strategy of a diversified company (unilever group case study),"The development of the company via the procedure of Mergers and Acquisitions (M&A) can have the negative impact on its appreciation by business groups, analytics and investors and it can be estimated as value destruction. This problem is relevant today because each diversified corporate structure (the diversified company) is a unique phenomenon. Therefore for the diversified companies, developing by the M&A strategy, the question about value creation or destruction and the market perception of their corporate conception demand to be studied in the framework of the individual approach. This study deals with an analysis of the M&A strategy utilized by Unilever Group, as well as with issues relating to identifying the factors defining the value of a diversified company. To these ends, there are particular methods that can be applied to Unilever Group. Because the structure of this diversified company is rather complex, it makes sense to determine whether the company has been accurately valued by the market (if it is overvalued/undervalued), as well as to define the way the market responds to M&A transactions effected by this company. This study includes an estimation of the effectiveness of Unilever Group's mergers and acquisitions strategy, aimed at creating the optimum business portfolio within the diversified corporate structure (company) by how it affects value of the company. The general hypothesis assumes that diversification does not have a destructive effect on the value of an international multi-business company that builds its portfolio based on the success of certain brands and business areas. © Medwell Journals, 2015. An analysis of the effectiveness of the M&A strategy of a diversified company (unilever group case study) Company value; Diversified company; Expected rate of return on invested capital; Merger and acquisition transactions; Strategy effectiveness ",Risk management
603,ICEIS 2014 - Proceedings of the 16th International Conference on Enterprise Information Systems,The proceedings contain 210 papers. The topics discussed include: possibilistic interorganizational workflow net for the recovery problem concerning communication failures; distributed knowledge management architecture and rule based reasoning for mobile machine operator performance assessment; machine learning techniques for topic spotting; fuzzy DEMATEL model for evaluation criteria of business intelligence; an evolutionary algorithm for graph planarization by vertex deletion; evaluating artificial neural networks and traditional approaches for risk analysis in software project management - a case study with PERIL dataset; using visualization and text mining to improve qualitative analysis; DC2DP: a dublin core application profile to design patterns; domain ontology for time series provenance; video stream transmodality; assisting speech therapy for autism spectrum disorders with an augmented reality application; and adding semantic relations among design patterns. ICEIS 2014 - Proceedings of the 16th International Conference on Enterprise Information Systems  ,Risk management
604,"18th Americas Conference on Information Systems 2012, AMCIS 2012, Volume 5","The proceedings contain 572 papers. The special focus in this conference is on Information Systems. The topics include: Health diagnosis of communities of practices (CoPs); open source alternatives for business intelligence; identifying business process activity mappings by optimizing behavioral similarity; hanging with the right crowd; a taxonomy of web-based inbound open innovation initiatives; database intrusion detection: defending against the insider threat; an empirical study of the GIGO axiom in satisficing decisions; analysis of probabilistic news recommender systems; the influence of technology characteristics on privacy calculus; instant messaging privacy in the clouds; towards a component-based description of business models; two-sided cybermediary platforms; an integrative analysis of transactional e-government web usage; facebook usage in government-a case study of information content; deriving business value from asymmetric penalty-reward perspectives of IS users; create attention to attract attention-viral marketing of digital music in social networks; towards a framework for transforming business models into business processes; factors affecting perceived satisfaction with a BPM tool; hypercompetition in the Erp industry; structural flaws in the ethics of technology; ethical considerations for virtual worlds; personality, gender and careers in information technology; on IT control weaknesses in auditors' reports on internal control; effect of the SOX act on IT governance; the influence of general sustainability attitudes and value congruence on consumer behavior; user privacy in mobile advertising; system learning of user interactions; deploying mission critical learning management system using open source software; theorizing the dual role of information technology in technostress research; boundary spanning in business process management; ERP usability and the mangle of practice; direct manipulation tablet apps for education; comparing graphical and tangible user interfaces for a tower defense game; visual perception model for online target marketing; eye movements, perceptions, and performance; individual relationships with technology; internal audit function response to ERP systems implementation; auditing journal entries using self-organizing map; consumer perceptions of the adoption of electronic personal health records; investigating the reciprocal relationships within health virtual communities; designing and visualising healthcare delivery systems; how multinational firms use IT to manage their global operations; conflict, value diversity, and performance in virtual teams; mobile ICT and knowledge sharing in underserved communities; agile decision making framework to support mobile microloans for unbanked customers; the analysis of the telecommunications industry in Thailand; the business value of knowledge management; providing information feedback to bidders in online multi-unit combinatorial auctions; spatial modeling using agents; using probabilistic ontologies for video exploration; new directions, new challenges, and new understandings; function-based categorization of online product information types; exploring antecedents of habit on social network service; personality correlation analysis and applications in social networks; identifying experts in virtual forecasting communities; black males in IT higher education in the USA; technology features, empowering perceptions, and voicing behavior on microblog; information security policy compliance; a preliminary taxonomy for software failure impact; an examination of the success of post-merger IT integration; an analysis of and perspective on the information security maturity model; geographic information systems and the nonprofit sector; effectiveness of shallow hierarchies for document stores; a methodology for the development of web-based information systems; balanced resource allocation; demand response in smart grids; decision support for electric vehicle charging; the expectations for faculty in Latin America; mastering the social IT/Business Alignment Challenge; supply chain resource planning systems; towards a research framework for VLBA operation management; integrating enterprise system's 3rd wave into IS curriculum; a two-tier data-centric framework for flexible business process management; engagement in online communities; organisational semiotics methods to assess organisational readiness for internal use of social media; social media in the workplace; economics of pair programming revisited; social traps of agile methods; metadata exploitation in large-scale data migration projects; collaboratively assessing information quality on the web; reputation management in social commerce communities; E-Business adoption research; a preliminary information theory of difference; replacement of project manager during IT projects-a research agenda; a simulation study of project management and collaborative information technologies; the role of business information visualization in knowledge creation; effects of narrative structure and salient decision points in role playing games; adoption of pervasive e-health solutions; security practices and regulatory compliance in the healthcare industry; the role of demographic characteristics in health care strategic security planning; tailoring software process capability/maturity models for telemedicine systems; understanding dynamic collaboration in teleconsultation; the pathway to enterprise mobile readiness; investigating the role of social media and social capital; exploring 311-driven changes in city government; preventing the gradual decline of shared service centers; developing a conceptual framework for evaluating public sector transformation in the digital era; the impact of cultural differences on cloud computing ecosystems in YOU.S. and China; an examination of the impact of service climate on service productivity in the organizational context; information systems facilitating groundwater sustainability management; keeping electronic medical records secure and portable; the emerging role of robotics in home health care; information quality assessment technique to evaluate the information exchange; boundary dialogues in user-centric innovation; towards a meditation brain state model using electroencephalographic data; design method requirements for agile system of systems; design and evaluation of a socially enhanced classroom blog to pomote student learning in higher education; it is not all about the music: user preference for musicians on facebook; knowledge seeking and knowledge sharing in a nonprofit organizational partner network: a social network analysis; the mediating role of adaptive personalization in online shopping; exploring the temporal nature of sociomateriality from a work system perspective; sociomateriality as radical ontology; information security management; meeting global business information requirements with enterprise resource planning; knowledge sharing in social networking sites for e-collaboration; applying cognitive principles of similarity to data integration-the case of SIAM; reference model in design science research to gather and model information; impact of online content on attitudes and buying intentions; prospect theory and information security investment decisions; using domain knowledge to facilitate cyber security analysis; conceptualizing data security threats and countermeasures in the E-Discovery process with misuse cases; an empirical analysis of an individual's 360 degree protection from file and data loss; analysis of eBook lending: a game-theory approach; facilitating consumers' evaluation of experience goods and the benefits for vendors; three-factor Model vs. Two-Factor Model; automating enterprise architecture documentation using an enterprise service bus; the influence of role models on students' decisions to pursue the IS major; teaching ""people networking"" skills for CIS students; a case of bias in teaching, grading, and plagiarism; a relational view of accounting information sharing; reporting capabilities, financial closing time and effects on cost of equity capital; reflecting on the role of IT and IT research in healthcare; social media around the world; understanding the effects of freeriding in team dynamics; password policy effects on entropy and recall: research in progress; the role of individual characteristics on insider abuse intentions; building a methodology to assess the e-Government transformation success; optimizing freight delivery for less-than-truckload transportation; the influence of perceived information and network characteristics on the attitude towards information overload; information disclosure and generational differences in social network sites; trasactive memory systems virtual team training model; the case of open government and teaching and learning in a virtual world. 18th Americas Conference on Information Systems 2012, AMCIS 2012, Volume 5  ",Strategic alignment
605,"19th Americas Conference on Information Systems, AMCIS 2013, Volume 3","The proceedings contain 438 papers. The special focus in this conference is on Information Systems. The topics include: A cloud-based service for affordable cost analysis; a conceptual examination of distrusting beliefs in older adults about the internet; a consulting model of global service learning; a contingent model of project organization and management; a decision making model for the adoption of cloud computing in Jamaican organizations; a foundation of a first-person perspective systems analysis; a framework for collaborative augmented reality applications; a framework for enterprise social media guidelines; a framework to analyze E-government OSS adoption benefits; a framework to support practitioners in evaluating business-IT alignment models; a measure for assessing the adequacy of DDOS defenses; a systematic classification and analysis of NFRs; about the need for semantically enriched reference models; adopting agile methods for follow-the-sun software development; an analysis of product uncertainty and seller uncertainty; aggregating, analyzing, and diffusing natural disaster information; ameliorating ERP workflow using a sociomaterial lens; an active learning approach to teaching undergraduate introduction to MIS course; an agile approach to systems analysis and design teaching and learning; an empirical study of designing simplicity for mobile application interaction; an examination of IT initiative portfolio characteristics and investment allocation; an exploration of organizational capabilities for emergency response; an icon taxonomy for semi-literate communities; an information security model and its validation; an inquiry into mental models of web interface design; an investigation of the effect of IT occupational subculture on the relationship between knowledge sharing and IT diffusion in organizations; an organizing framework for literacy; an understanding of the impact of gamification on purchase intentions; antecedents and effects of computer self-efficacy on social networking adoption among Asian online users; architecture and implementation of a decision support system for software industry business models; assessing internet source credibility; attitude change process toward ERP systems using the elaboration likelihood model; barriers to mission-critical open source software adoption by organizations; behaviour analysis of distributed systems under time change constraints; blogging as a liminal space; high-value impact through multidisciplinary design science programs of research; breaking the norm - on the determinants of informational nonconformity in online social networks; bringing together BPM and social software; building context-aware access control in enterprise ontologies; capitalizing on social media analysis - insights from an online review on business models; carbon footprint of IT-services - a comparative study of energy consumption for offline and online storage usage; career paths training for the first year students in information systems science-motivational view; chains of control in agile software development; challenges in offshore outsourcing relationship management - a Peruvian perspective; challenges of blind students and IT-based mitigation strategies; cloudifying desktops - a taxonomy for desktop virtualization; CMC influence on voluntarily collaborating knowledge workers' perception of equivocal tasks; collective learning paradigm for rapidly evolving curriculum; a hybrid personalized movie recommender based on perceived similarity; common patterns of cloud business models; comparing ABET-accredited IS undergraduate programs and the ACM 2010IS model curriculum; comprehensive access control for data warehouses; conceptualizing the impact of social capital on knowledge creation; configuring value creation processes for global service; control-related motivations and information security policy compliance; corporate enactments of social control across social media affordances; creative innovativeness with information systems (IS) and its role in quality IS usage; moving from the philosophical to the empirical in the search for causal explanations; critical success factors for ERP system upgrades - the case of a German large-scale enterprise; cultural impact on E-service use in Saudi Arabia; current state of the digital deception studies in IS; customer involvement in organizational innovation - toward an integration concept; data modeling in the cloud; decision support using linked, social, and sensor data; design, evaluation and impact of educational olfactory interfaces; designing decision support systems at the interface between complex and complicated domains; determinants and consequences of herding in P2P lending markets; determining microblogging effectiveness for capturing quality knowledge; developing a governance model for successful business process standardization; developing targeted text messages for enhancing medication adherence; development of a change readiness scale for electronic medical record systems implementation at hospitals; differences between FCM and fuzzy ANP techniques in the process of organizational change readiness assessment; digital service flexibility and performance of credit unions; drive a website performance using web analytics; drivers of cultural differences in information system adoption - a case study; dynamic model to assess organisational readiness during information system implementation; effects of persuasive claims on desirability and impulse purchase behavior; effects of RFID technology on profitability and efficiency in retail supply chains; efficacy of communication support in collaborative online shopping; efficiency and device versatility of graphical and textual passwords; emotion and memory in technology adoption and diffusion; empowering organizations through customer knowledge acquisition; enabling collaboration in virtual manufacturing enterprises with cloud computing; engagement of information technology professionals with their work; enhancing privacy using community driven recommendations; enhancing service lifecycle management - costing as part of service descriptions; enterprise app stores for mobile applications - development of a benefits framework; enterprise architecture software tool support for small and medium-sized enterprises; enterprise systems implementation success in the shakedown phase; environmental pressure on software as a service adoption; estimating the quality of data using provenance; evaluating advanced forms of social media use in government; evaluating cooperation in IT teams using a fuzzy multicriteria sorting method; evaluating the information systems women network (ISWN) mentoring program; evaluating the performance of government IT projects in the Caribbean; examining high performance teams in information systems projects; examining personal information privacy-protective responses (IPPR) with the use of smart devices; examining the use of social media in customer co-creation; exploration of risk management process usage levels and their relationship to project outcomes; exploring subscription renewal intention of operational cloud enterprise systems - a stakeholder perspective; exploring the aesthetic effects of the golden ratio in the design of interactive products; exploring the factors influencing the usage intention of facebook fan page - a preliminary study; exploring the factors that influence social computing intentions; exploring the impact of online reviews with brand equity for online software purchasing behavior; extending successful ebusiness models to the mobile internet; extracting product features from online consumer reviews; eye gazing behaviors in online deception; facilitating collaboration and peer learning through anchored asynchronous online discussions; facilitating conflict resolution of models for automated enterprise architecture documentation; facilitating the adoption of public services using high definition video; factor analysis of critical success factors for data quality; factors of password-based authentication; factors that affect information and communication technology adoption by small businesses in China; feasibility analysis of an assessment model of knowledge acquisition in virtual environments; fostering efficiency in information systems support for product-service systems in the manufacturing industry; framing group norms in virtual communities; global ERP implementations and harmonization of practices in multinational corporations; potentials and challenges to the public health sector of of developing countries; growth of an organizational field for infrastructure; harnessing anomalous preferences of anonymous users for lean information systems development; hidden or implicit contextual factors influencing user participation in online production communities; identification of driving forces in service innovations; illuminating organizing vision careers through case studies; impact of geospatial reasoning ability and perceived task-technology fit on decision-performance; impact of strategic alignment on IT outsourcing success in a complex service setting; impact of unified communications on communication, relationship building and performance; impact of users' cognitive responses on user satisfaction in online community; impediments to enterprise system implementation across the system lifecycle; in search of insights for institutionalization of telemedicine in the health care system in Ethiopia. 19th Americas Conference on Information Systems, AMCIS 2013, Volume 3  ",Strategic alignment
606,"International Conference on Information Systems, ICIS 2012, Volume 5","The proceedings contain 308 papers. The special focus in this conference is on Information Systems. The topics include: Heterogeneity in IT landscapes and monopoly power of firms; community ecology for innovation concept; peer influence in a very large social network; assessing value in an online network of products; optimal pricing with positive network effects; research on viral marketing propagating oriented to marketing context; repurchase intentions of information technology; a multi-level analysis of the impact of health information technology on hospital performance; institutional logics of adoption and use of health information technology; post-acceptance of electronic medical records; socio-technical attachments and IT change; comparing the predictive ability of PLS and covariance models; a note of caution on covariance-equivalent models in information systems; differential effects of omitting formative indicators; being innovative about service innovation; estimating demand for applications in the new mobile economy; continuance of professional social networking sites; role of online social networks in job search by unemployed individuals; knowledge contribution in online network of practice; the dual role of IS specificity in governing software as a service; relational antecedents of information visibility in value networks; summarization of corporate risk factor disclosure through topic modeling; a hybrid method for cross-domain sentiment classification using multiple sources; designing intelligent expert systems to cope with liars; IS offshore project risk, contracts and team structure; vendor and client project managers; technology desirability; the role of green IS in developing eco-effectiveness; the impact of IT-enabled manufacturing capabilities on plant profitability; inter-industry IT spillovers after the dot-com bust; effects of undesired online video advertising choice on user behavior and attitude; unfulfilled obligations in recommendation agent use; understanding the formation of trust in IT artifacts; user decisions among digital piracy and legal alternatives for film and music; a probabilistic generative model for latent business networks mining; the effects of information disclosure policy on the diffusion of security attacks; interactivity of social media and online consumer behavior; strategies for establishing service oriented design in organizations; alternative genres of is research; ICT standardization strategies and service innovation in health care; technical support and IT capacity demand; online social networks as a source and symbol of stress; the effects of social network usage on organizational identification; using the Kano model to identify attractive user-interface software components; developing a theory of multitasking behavior; an empirical investigation of information systems departments' configurations; exploring the adaptation of enterprise systems implementation methodology; out of the box and onto the stage; an organizing vision perspective on green IS development; the emergence of sustainability as the new dominant logic; social influence and defaults in peer-to-peer lending networks; multi-screen strategy for selling mobile content to customers; the emergent role of IT capabilities; reducing price uncertainty through demand side management; factors affecting perceived persuasiveness of a behavior change support system; surfacing schemas from firms' informational engagements; optimal design of consumer review systems; measuring product type with dynamics of online product review variance; different effects on retailers of online product reviews; measurement of multitasking with focus shift analysis; an exploration of the impact of information privacy invasion; measuring mobile users' concerns for information privacy; examining the rationality of location data disclosure through mobile devices; towards a design theory for software project risk management systems; identifying optimal IT portfolios to promote healthcare quality; designing a web-based application to support peer instruction for very large groups; principles for knowledge creation in collaborative design science research; estimating optimal recommendation set sizes for individual consumers; predicting participation in social media sites by analyzing user participation patterns; a hidden Markov model for conversion rate dynamics in online retail; flow experience and continuance intention toward online learning; design and evaluation of a didactical service blueprinting method for large scale lectures; knowledge sharing and maturation in circles of trust; a model-driven method for the systematic literature review of qualitative empirical research; procedurally transparent design science research; advancing task elicitation systems - an experimental evaluation of design principles; the knowing-doing gap in research methods and what we should do about it; efficient and flexible management of enterprise information systems; granularity metrics for IT services; from observed outcomes to measurable performance; the effect of third party investigation on pay-per-click advertising; identifying factors of e-government acceptance - a literature review; effects of cultural cognitive styles on users' evaluation of website complexity; a sustainability model of green IT initiatives; towards a typology of green IS strategies; digital access, political networks and the diffusion of democracy; the effect of free access on the diffusion of scholarly ideas; an empirical examination of cultural biases in interpersonal economic exchange; effect of business intelligence and IT infrastructure flexibility on organizational agility; news recommender systems with feedback; a multi-theoretical framework for social network-based recommendation; improving coverage of design in information systems education; perceived IT security risks of cloud computing: conceptualization and scale development; capability leapfrogging in the Japanese IT services industry; predatory coercion in social media and protection of children online - a critical discourse analysis approach; encouraging collaborative idea-building in enterprise-wide innovation challenges; analyzing the impact of social media strategies on viewer engagement; supporters in deed - studying online support provision from the perspective of social capital; membership overlap and inter-community collaboration; developing customer agility through information management; keyword search patterns in sponsored link advertisements; the hidden effects of opening bids in online auctions; increasing dynamic capabilities through virtualized grid-in-cloud solutions; design principles for heterogeneity decisions in enterprise architecture management; IT stereotyping and the CEO-CIO headlock; exploring the role of un-enacted projects in IT project portfolio management; the difference of determinants of mobile data services' adoption and continuance - a longitudinal study; a study of effectiveness and satisfaction level of cloud CRM users in Taiwan's enterprises; investigating intelligent agents in a 3D virtual world; an empirical study of a two-sided model of fraudulent exchange; perceptual and conceptual effects of incidental exposure to web ads; self-disclosure on online social networks; critical success factors of location-based services; affect and online privacy concerns; the impact of training and social norms on information security compliance; security management in cross-organizational settings; the effect of customers' emotion on service recovery strategy in IT service failures; the influence of IS affordances on work practices in health care; multi-level knowledge transfer in software development outsourcing projects; organizational performance with environmental knowledge intensity; process visibility - towards a conceptualization and research themes; towards an oil crisis early warning system based on absolute news volume; constructing workflow models from agent profiles; formulating effective coordination strategies in agile global software development teams; the transmission of control in information systems projects; the structuration of task-oriented communication in innovative virtual teams and social media in the entrainment of contention to innovation. International Conference on Information Systems, ICIS 2012, Volume 5  ",Value management
607,Accelerating Value-at-Risk estimation on highly parallel architectures,"Values of portfolios in modern financial markets may change precipitously with changing market conditions. The utility of financial risk management tools is dependent on whether they can estimate Value-at-Risk (VaR) of portfolios on-demand when key decisions need to be made. However, VaR estimation of portfolios uses the Monte Carlo method, which is a computationally intensive method often run as an overnight batch job. With the proliferation of highly parallel computing platforms such as multicore CPUs and manycore graphics processing units (GPUs), teraFLOPS of computation capability is now available on a desktop computer, enabling the VaR of large portfolios with thousands of risk factors to be computed within only a fraction of a second. Achieving such performance in practice requires the assimilation of expertise in the following three areas: (i) application domain; (ii) statistical analytics; and (iii) parallel computing. This paper demonstrates that these areas of expertise inform optimization perspectives that, when combined, lead to 127×speedup on our CPU-based implementation and 538×speedup on our GPU-based implementation. Copyright © 2011 John Wiley & Sons, Ltd. Accelerating Value-at-Risk estimation on highly parallel architectures D.1.3 concurrent programming [parallel programming]; G.3 probability and statistics [probabilistic algorithms (including Monte Carlo)]; J.4 social and behavioral sciences [economics] Behavioral research; Commerce; Computer graphics; Graphics processing unit; Monte Carlo methods; Multicore programming; Parallel programming; Program processors; Risk management; Risk perception; Value engineering; Concurrent programming; Financial risk management; Highly parallels; Market condition; Multi-core cpus; Probabilistic algorithm; Social and behavioral science; Value at Risk; Parallel architectures",Value management
608,3DRC: A novel technique in evaluating stakeholders project views,"In recent years there has been a growth in size and complexity of the projects managed by public or private organizations. This leads to increased probability of project failures, frequently due to the difficulty and the ability to achieve the objectives such as on-time delivery, cost containment, expected quality achievement. In particular, one of the most common causes of project failure is the very high degree of uncertainty that affects the expected performance of the project, especially when different stakeholders with divergent aims and goals are involved in the project. In this paper we present a new visualization technique, called 3DRC, that addresses the prevention and proactive handling of the potential controversies among project stakeholders. The approach is based on the 3D radar charts, to allow easier and more immediate analysis and management of the project views giving a contribution in reducing the project uncertainty and, consequently, the risk of project failure. © 2013 ACM. 3DRC: A novel technique in evaluating stakeholders project views 3D radar charts; controversy resolution; information visualization; multi-dimensional data; project management; spatial-temporal data; visual analytics Communication; Information systems; Project management; Radar; Risk assessment; Three dimensional; Uncertainty analysis; 3d radars; Information visualization; Multidimensional data; Spatial-temporal data; Visual analytics; Three dimensional computer graphics",Risk management
609,"Blending technology, human potential, and organizational reality: Managing big data projects in public contexts","To make the deeply rooted layers of catalyzing technology and optimized modelling gain their true value for education, healthcare or other public services, it is necessary to prepare well the Big Data environment in which the Big Data will be developed, and integrate elements of it into the project approach. It is by integrating and managing these non-technical aspects of project reality that analytics will be accepted. This will enable data power to infuse the organizational processes and offer ultimately real added value. This chapter will she would light on complementary actions required on different levels. It will be analyzed how this layered effort starts by a good understanding of the different elements that contribute to the definition of an organization's Big Data ecosystem. It will be explained how this interacts with the management of expectations, needs, goals and change. Lastly, a closer look will be given at the importance of portfolio based big picture thinking. © 2016 by IGI Global. All rights reserved. Blending technology, human potential, and organizational reality: Managing big data projects in public contexts  Blending; Added values; Data environment; Human potential; Organizational process; Project approach; Public services; Technical aspects; Big data",Financial management
610,Predictive Analytics for Modeling UAS Safety Risk,"This paper illustrates the development of an Object-Oriented Bayesian Network (OOBN) to integrate the safety risks contributing to a notional ""lost link"" scenario for a small UAS (sUAS). This hypothetical case investigates the possibility of a ""lost link"" for the sUAS during the bridge inspection mission leading to a collision of the sUAS with the bridge. Hazard causal factors associated with the air vehicle, operations, airmen and the environment may be combined in an integrative safety risk model. With the creation of a probabilistic risk model, inferences about changes to the states of the mishap shaping or causal factors can be drawn quantitatively. These predictive safety inferences derive from qualitative reasoning to conclusions based on data, assumptions, and/or premises and enable an analyst to identify the most prominent causal factor clusters. Such an approach also supports a mitigation portfolio study and assessment. An OOBN approach facilitates decomposition at the subsystem level yet enables synthesis at a higher-order systems level. It is essentially a System of Systems (so is) approach that fosters the integration of sub-nets of risk factors. Such a study provides insight into the integration of UAS into the National Airspace System (NAS) that may be used to eventually inform type design, airworthiness, certifications, safety analyses and risk assessments, and operational requirements. Copyright © 2013 SAE International. Predictive Analytics for Modeling UAS Safety Risk  Artificial intelligence; Bayesian networks; Risk perception; Safety factor; Systems engineering; Unmanned vehicles; Bridge inspection; Higher-order systems; National airspace system; Object-oriented Bayesian networks; Operational requirements; Predictive analytics; Probabilistic risk; Qualitative reasoning; Risk assessment",Risk management
611,Smarter financial management - An answer to the missing link between key performance indicators and budgeting decisions for smarter cities,"Most government entities (cities, schools, etc) use traditional accounting systems and related financial tools to plan budgets. Some introduce more advanced technologies, such as ERP systems to perform this task. However both tend to focus on budgeting from a purely financial perspective and are incapable of linking the impact of budgeting decisions with the strategic objectives trying to be achieved. Even ERP systems typically focus on gathering the data and comparing it with the budget, not on the models underlying the budget itself. To address this gap, we introduce outcome based budgeting for cities and local governments, which takes a radically different approach to the budgeting process. Instead of departments, it looks at the government entities as the service systems they are. Every service a government entity provides has both a financial model, as well as a performance model. These models are linked so that the performance impacts of financial decisions can be investigated and studied. Individual service systems are also linked together into a larger service system, allowing the big picture for each financial decision to be seen. This approach offers better insight to decision makers and could help them prioritize projects for budgeting. It might even make the process politically less contentious by offering transparency and rationale to staff and citizens alike. Impacts that were once invisible are brought to light, allowing for smarter, fact-based decision making by the government leaders. Once adopted, outcome based budgeting opens up opportunities for various analytics, as services are well defined and have concrete models. These analytics allow for better decision-support via what-if scenario planning, and can over time help in project prioritization. © 2012 IEEE. Smarter financial management - An answer to the missing link between key performance indicators and budgeting decisions for smarter cities Analytics; Budget allocation optimization; Business ecosystem; Outcome based budgeting; Project management; Smarter cities Benchmarking; Decision making; Decision support systems; Finance; Project management; Accounting system; Advanced technology; Analytics; Budget allocation; Budgeting process; Business ecosystem; Concrete model; Decision makers; ERP system; Financial decisions; Financial managements; Financial models; Financial tools; Government entities; Government leaders; Individual service; Key performance indicators; Local government; Performance impact; Performance Model; Project prioritization; Service systems; Smarter cities; Strategic objectives; What-if scenarios; Budget control",Strategic alignment
612,"The World spatiotemporal analytics and mapping project (WSTAMP): Further progress in discovering, exploring, and mapping spatiotemporal patterns across the world's largest open source data sets","Spatiotemporal (ST) analytics applied to major data sources such as the World Bank and World Health Organization has shown tremendous value in shedding light on the evolution of cultural, health, economic, and geopolitical landscapes on a global level. WSTAMP engages this opportunity by situating analysts, data, and analytics together within a visually rich and computationally rigorous online analysis environment. Since introducing WSTAMP at the First International Workshop on Spatiotemporal Computing, several transformative advances have occurred. Collaboration with human computer interaction experts led to a complete interface redesign that deeply immerses the analyst within a ST context, significantly increases visual and textual content, provides navigational crosswalks for attribute discovery, substantially reduce mouse and keyboard actions, and supports user data uploads. Secondly, the database has been expanded to include over 16,000 attributes, 50 years of time, and 200+ nation states and redesigned to support non-annual, non-national, city, and interaction data. Finally, two new analytics are implemented for analyzing large portfolios of multi-attribute data and measuring the behavioral stability of regions along different dimensions. These advances required substantial new approaches in design, algorithmic innovations, and increased computational efficiency. We report on these advances and inform how others may freely access the tool. © Authors 2017. The World spatiotemporal analytics and mapping project (WSTAMP): Further progress in discovering, exploring, and mapping spatiotemporal patterns across the world's largest open source data sets Analytics; Data mining; Global; Spatio-temporal; Tool; Visualization ",Capacity management
613,Web portals for financial analytics: How effective are they from the end-users' perspective,"As more investors and traders globally manage their own stock portfolio without the help of human brokers, there is an increasing need to acquire and use financial knowledge and financial data analytics to ensure that a self-maintained financial portfolio is soundly managed. There are a growing number of special web portals that provide financial analytics services for investors and traders who demand detailed analyses of their stocks and other financial derivatives. The objective of this paper is to examine how end-users value the overall usefulness of web portals that provide financial analytics services and capabilities. This research endeavors to identify different unique features of financial analytics web portals, and ask users which of these features prove to be highly useful for their needs in analyzing when to buy, hold, and sell stocks. Copyright © 2013, IGI Global. Web portals for financial analytics: How effective are they from the end-users' perspective End-users; Financial analytics services; Financial data analytics; Investors; Web portals Commerce; Data Analytics; Financial markets; End users; Financial analytics services; Financial data; Financial derivatives; Financial portfolio; Investors; Stock portfolio; Unique features; Portals",Strategic alignment
614,Towards a social networks model for online learning & performance,"In this study, we develop a theoretical model to investigate the association between social network properties, ""content richness"" (CR) in academic learning discourse, and performance. CR is the extent to which one contributes content that is meaningful, insightful and constructive to aid learning and by social network properties we refer to its structural, position and relationship attributes. Analysis of data collected from an e-learning environment shows that rather than performance, social learning correlates with properties of social networks: (i) structure (density, inter-group and intra-network communication) and (ii) position (efficiency), and (iii) relationship (tie strength). In particular, individuals who communicate with internal group members rather than external members express higher tendencies of ""content richness"" in social learning. The contribution of this study is three-fold: (i) a theoretical development of a social network based model for understanding learning and performance, which addresses the lack of empirical validation of current models in social learning; (ii) the construction of a novel metric called ""content richness"" as a surrogate indicator of social learning; and (iii) demonstration of how the use of social network analysis and computational text-mining approaches can be used to operationalize the model for studying learning and performance. In conclusion, a useful implication of the study is that the model fosters understanding social factors that influence learning and performance in project management. The study concludes that associations between social network properties and the extent to which interactions are ""content-rich"" in eLearning domains cannot be discounted in the learning process and must therefore be accounted for in the organizational learning design. Towards a social networks model for online learning & performance Connectivism; E-learning; Group learning; Individual learning; Learning analytics; Performance; Situated learning; Social learning; Social networks; Strength of weak ties; Structural holes ",Monitoring and control
615,Agile analytics in the age of big data,"The world of data and analytics is rapidly changing. As data volume and complexity increase, executives charged with providing analytics capabilities for the enterprise face challenges that require a more intentional approach to analytics governance. Seth Earley talks with Partha Srinivasa, CIO of HCC Insurance Holdings, to help explain the current information environment and the growing importance of information governance. © 2014 IEEE. Agile analytics in the age of big data analytics; big data; data analysis; information governance; information technology; project management Data reduction; Information technology; Project management; analytics; Data volume; Information environment; information governance; Big data",Governance
616,"18th Americas Conference on Information Systems 2012, AMCIS 2012, Volume 6","The proceedings contain 572 papers. The special focus in this conference is on Information Systems. The topics include: Health diagnosis of communities of practices (CoPs); open source alternatives for business intelligence; identifying business process activity mappings by optimizing behavioral similarity; hanging with the right crowd; a taxonomy of web-based inbound open innovation initiatives; database intrusion detection: defending against the insider threat; an empirical study of the GIGO axiom in satisficing decisions; analysis of probabilistic news recommender systems; the influence of technology characteristics on privacy calculus; instant messaging privacy in the clouds; towards a component-based description of business models; two-sided cybermediary platforms; an integrative analysis of transactional e-government web usage; facebook usage in government-a case study of information content; deriving business value from asymmetric penalty-reward perspectives of IS users; create attention to attract attention-viral marketing of digital music in social networks; towards a framework for transforming business models into business processes; factors affecting perceived satisfaction with a BPM tool; hypercompetition in the Erp industry; structural flaws in the ethics of technology; ethical considerations for virtual worlds; personality, gender and careers in information technology; on IT control weaknesses in auditors' reports on internal control; effect of the SOX act on IT governance; the influence of general sustainability attitudes and value congruence on consumer behavior; user privacy in mobile advertising; system learning of user interactions; deploying mission critical learning management system using open source software; theorizing the dual role of information technology in technostress research; boundary spanning in business process management; ERP usability and the mangle of practice; direct manipulation tablet apps for education; comparing graphical and tangible user interfaces for a tower defense game; visual perception model for online target marketing; eye movements, perceptions, and performance; individual relationships with technology; internal audit function response to ERP systems implementation; auditing journal entries using self-organizing map; consumer perceptions of the adoption of electronic personal health records; investigating the reciprocal relationships within health virtual communities; designing and visualising healthcare delivery systems; how multinational firms use IT to manage their global operations; conflict, value diversity, and performance in virtual teams; mobile ICT and knowledge sharing in underserved communities; agile decision making framework to support mobile microloans for unbanked customers; the analysis of the telecommunications industry in Thailand; the business value of knowledge management; providing information feedback to bidders in online multi-unit combinatorial auctions; spatial modeling using agents; using probabilistic ontologies for video exploration; new directions, new challenges, and new understandings; function-based categorization of online product information types; exploring antecedents of habit on social network service; personality correlation analysis and applications in social networks; identifying experts in virtual forecasting communities; black males in IT higher education in the USA; technology features, empowering perceptions, and voicing behavior on microblog; information security policy compliance; a preliminary taxonomy for software failure impact; an examination of the success of post-merger IT integration; an analysis of and perspective on the information security maturity model; geographic information systems and the nonprofit sector; effectiveness of shallow hierarchies for document stores; a methodology for the development of web-based information systems; balanced resource allocation; demand response in smart grids; decision support for electric vehicle charging; the expectations for faculty in Latin America; mastering the social IT/Business Alignment Challenge; supply chain resource planning systems; towards a research framework for VLBA operation management; integrating enterprise system's 3rd wave into IS curriculum; a two-tier data-centric framework for flexible business process management; engagement in online communities; organisational semiotics methods to assess organisational readiness for internal use of social media; social media in the workplace; economics of pair programming revisited; social traps of agile methods; metadata exploitation in large-scale data migration projects; collaboratively assessing information quality on the web; reputation management in social commerce communities; E-Business adoption research; a preliminary information theory of difference; replacement of project manager during IT projects-a research agenda; a simulation study of project management and collaborative information technologies; the role of business information visualization in knowledge creation; effects of narrative structure and salient decision points in role playing games; adoption of pervasive e-health solutions; security practices and regulatory compliance in the healthcare industry; the role of demographic characteristics in health care strategic security planning; tailoring software process capability/maturity models for telemedicine systems; understanding dynamic collaboration in teleconsultation; the pathway to enterprise mobile readiness; investigating the role of social media and social capital; exploring 311-driven changes in city government; preventing the gradual decline of shared service centers; developing a conceptual framework for evaluating public sector transformation in the digital era; the impact of cultural differences on cloud computing ecosystems in YOU.S. and China; an examination of the impact of service climate on service productivity in the organizational context; information systems facilitating groundwater sustainability management; keeping electronic medical records secure and portable; the emerging role of robotics in home health care; information quality assessment technique to evaluate the information exchange; boundary dialogues in user-centric innovation; towards a meditation brain state model using electroencephalographic data; design method requirements for agile system of systems; design and evaluation of a socially enhanced classroom blog to pomote student learning in higher education; it is not all about the music: user preference for musicians on facebook; knowledge seeking and knowledge sharing in a nonprofit organizational partner network: a social network analysis; the mediating role of adaptive personalization in online shopping; exploring the temporal nature of sociomateriality from a work system perspective; sociomateriality as radical ontology; information security management; meeting global business information requirements with enterprise resource planning; knowledge sharing in social networking sites for e-collaboration; applying cognitive principles of similarity to data integration-the case of SIAM; reference model in design science research to gather and model information; impact of online content on attitudes and buying intentions; prospect theory and information security investment decisions; using domain knowledge to facilitate cyber security analysis; conceptualizing data security threats and countermeasures in the E-Discovery process with misuse cases; an empirical analysis of an individual's 360 degree protection from file and data loss; analysis of eBook lending: a game-theory approach; facilitating consumers' evaluation of experience goods and the benefits for vendors; three-factor Model vs. Two-Factor Model; automating enterprise architecture documentation using an enterprise service bus; the influence of role models on students' decisions to pursue the IS major; teaching ""people networking"" skills for CIS students; a case of bias in teaching, grading, and plagiarism; a relational view of accounting information sharing; reporting capabilities, financial closing time and effects on cost of equity capital; reflecting on the role of IT and IT research in healthcare; social media around the world; understanding the effects of freeriding in team dynamics; password policy effects on entropy and recall: research in progress; the role of individual characteristics on insider abuse intentions; building a methodology to assess the e-Government transformation success; optimizing freight delivery for less-than-truckload transportation; the influence of perceived information and network characteristics on the attitude towards information overload; information disclosure and generational differences in social network sites; trasactive memory systems virtual team training model; the case of open government and teaching and learning in a virtual world. 18th Americas Conference on Information Systems 2012, AMCIS 2012, Volume 6  ",Strategic alignment
617,Modeling a business intelligence system for investment projects,"The phases of evaluation and selection of investment projects require marketing, technical and financial information analysis before the decisions are made. In order to meet strict legal environmental and fiscal regulations it is necessary to assess the environmental and social impact. These analysis may indicate alternative sites, models, technologies, and methods of implementation as solutions for executing the project, which often involve higher costs, and because delays in the feasibility analysis and project selection. The purpose of this empirical research is to reduce the effort and duration of the selection and evaluation of projects and project portfolio optimization, increasing the efficiency and accuracy of budget implementation and controlling in accordance with the investment strategy of the company. The results consist of a set of requirements for a specific business intelligence system, designed to improve the operational flow and the performance of the planning process, and an analysis of significant factors underlying performance differences between projected and actual achievements, to define the parameters of the model. © 2015,Academy of Economic Studies, All right reserved. Modeling a business intelligence system for investment projects Budgeting; Business intelligence; Investments; Model; Projects; Requirements ",Strategic alignment
618,Accessing learning opportunities:The development and impact of an innovative Web-based progression tool,"This article profiles a Web-based tool developed with the support of two lifelong learning networks (LLNs) to address issues around higher education progression amongst under-represented groups across the East Midlands, including vocational learners and adults returning to education. Having introduced the site and explored its development, the article assesses its impact by considering the level and nature of usage. Whilst meeting its key objectives in terms of visitor numbers, this evaluation also addresses the quality of visits made to the site and the positive feedback received from surveyed users. The underlying reasons for the site's success are then investigated. With attention turning to the site's sustainability this exploration also considers the lessons that can be learned from developing and managing an online information and advice tool. In doing so, it is suggested that the findings will be of value to others involved in advising learners, given the increasing emphasis placed upon the Internet as a medium for the provision of information, advice and guidance. © 2012 Copyright Further Education Research Association. Accessing learning opportunities:The development and impact of an innovative Web-based progression tool average page views; good practice case study; information and advice; progression maps; progression tool; project management; stakeholder engagement; user feedback; visitor numbers; vocational learners; Web analytics; website marketing ",Strategic alignment
619,"10th International Workshop on Databases in Networked Information Systems, DNIS 2015","The proceedings contain 21 papers. The special focus in this conference is on Big Data Analysis, Knowledge Management, Business Data Analytics and Visualization. The topics include: Discovering chronic-frequent patterns in transactional databases; high utility rare itemset mining over transaction databases; synthetic evidential study as primordial soup of conversation; interactive tweaking of text analytics dashboards; covariance structure and systematic risk of market index portfolio; moving from relational data storage to decentralized structured storage system; comparing infrastructure monitoring with cloud stack compute services for cloud computing systems; a large sky survey project and the related big data analysis; query languages for domain specific information from PTF astronomical repository; mining business process logs for root because analysis of anomalous incidents; modeling personalized recommendations of unvisited tourist places using genetic algorithms and a decentralised approach to computer aided teaching via interactive documents. 10th International Workshop on Databases in Networked Information Systems, DNIS 2015  ",Financial management
620,"19th Americas Conference on Information Systems, AMCIS 2013, Volume 5","The proceedings contain 438 papers. The special focus in this conference is on Information Systems. The topics include: A cloud-based service for affordable cost analysis; a conceptual examination of distrusting beliefs in older adults about the internet; a consulting model of global service learning; a contingent model of project organization and management; a decision making model for the adoption of cloud computing in Jamaican organizations; a foundation of a first-person perspective systems analysis; a framework for collaborative augmented reality applications; a framework for enterprise social media guidelines; a framework to analyze E-government OSS adoption benefits; a framework to support practitioners in evaluating business-IT alignment models; a measure for assessing the adequacy of DDOS defenses; a systematic classification and analysis of NFRs; about the need for semantically enriched reference models; adopting agile methods for follow-the-sun software development; an analysis of product uncertainty and seller uncertainty; aggregating, analyzing, and diffusing natural disaster information; ameliorating ERP workflow using a sociomaterial lens; an active learning approach to teaching undergraduate introduction to MIS course; an agile approach to systems analysis and design teaching and learning; an empirical study of designing simplicity for mobile application interaction; an examination of IT initiative portfolio characteristics and investment allocation; an exploration of organizational capabilities for emergency response; an icon taxonomy for semi-literate communities; an information security model and its validation; an inquiry into mental models of web interface design; an investigation of the effect of IT occupational subculture on the relationship between knowledge sharing and IT diffusion in organizations; an organizing framework for literacy; an understanding of the impact of gamification on purchase intentions; antecedents and effects of computer self-efficacy on social networking adoption among Asian online users; architecture and implementation of a decision support system for software industry business models; assessing internet source credibility; attitude change process toward ERP systems using the elaboration likelihood model; barriers to mission-critical open source software adoption by organizations; behaviour analysis of distributed systems under time change constraints; blogging as a liminal space; high-value impact through multidisciplinary design science programs of research; breaking the norm - on the determinants of informational nonconformity in online social networks; bringing together BPM and social software; building context-aware access control in enterprise ontologies; capitalizing on social media analysis - insights from an online review on business models; carbon footprint of IT-services - a comparative study of energy consumption for offline and online storage usage; career paths training for the first year students in information systems science-motivational view; chains of control in agile software development; challenges in offshore outsourcing relationship management - a Peruvian perspective; challenges of blind students and IT-based mitigation strategies; cloudifying desktops - a taxonomy for desktop virtualization; CMC influence on voluntarily collaborating knowledge workers' perception of equivocal tasks; collective learning paradigm for rapidly evolving curriculum; a hybrid personalized movie recommender based on perceived similarity; common patterns of cloud business models; comparing ABET-accredited IS undergraduate programs and the ACM 2010IS model curriculum; comprehensive access control for data warehouses; conceptualizing the impact of social capital on knowledge creation; configuring value creation processes for global service; control-related motivations and information security policy compliance; corporate enactments of social control across social media affordances; creative innovativeness with information systems (IS) and its role in quality IS usage; moving from the philosophical to the empirical in the search for causal explanations; critical success factors for ERP system upgrades - the case of a German large-scale enterprise; cultural impact on E-service use in Saudi Arabia; current state of the digital deception studies in IS; customer involvement in organizational innovation - toward an integration concept; data modeling in the cloud; decision support using linked, social, and sensor data; design, evaluation and impact of educational olfactory interfaces; designing decision support systems at the interface between complex and complicated domains; determinants and consequences of herding in P2P lending markets; determining microblogging effectiveness for capturing quality knowledge; developing a governance model for successful business process standardization; developing targeted text messages for enhancing medication adherence; development of a change readiness scale for electronic medical record systems implementation at hospitals; differences between FCM and fuzzy ANP techniques in the process of organizational change readiness assessment; digital service flexibility and performance of credit unions; drive a website performance using web analytics; drivers of cultural differences in information system adoption - a case study; dynamic model to assess organisational readiness during information system implementation; effects of persuasive claims on desirability and impulse purchase behavior; effects of RFID technology on profitability and efficiency in retail supply chains; efficacy of communication support in collaborative online shopping; efficiency and device versatility of graphical and textual passwords; emotion and memory in technology adoption and diffusion; empowering organizations through customer knowledge acquisition; enabling collaboration in virtual manufacturing enterprises with cloud computing; engagement of information technology professionals with their work; enhancing privacy using community driven recommendations; enhancing service lifecycle management - costing as part of service descriptions; enterprise app stores for mobile applications - development of a benefits framework; enterprise architecture software tool support for small and medium-sized enterprises; enterprise systems implementation success in the shakedown phase; environmental pressure on software as a service adoption; estimating the quality of data using provenance; evaluating advanced forms of social media use in government; evaluating cooperation in IT teams using a fuzzy multicriteria sorting method; evaluating the information systems women network (ISWN) mentoring program; evaluating the performance of government IT projects in the Caribbean; examining high performance teams in information systems projects; examining personal information privacy-protective responses (IPPR) with the use of smart devices; examining the use of social media in customer co-creation; exploration of risk management process usage levels and their relationship to project outcomes; exploring subscription renewal intention of operational cloud enterprise systems - a stakeholder perspective; exploring the aesthetic effects of the golden ratio in the design of interactive products; exploring the factors influencing the usage intention of facebook fan page - a preliminary study; exploring the factors that influence social computing intentions; exploring the impact of online reviews with brand equity for online software purchasing behavior; extending successful ebusiness models to the mobile internet; extracting product features from online consumer reviews; eye gazing behaviors in online deception; facilitating collaboration and peer learning through anchored asynchronous online discussions; facilitating conflict resolution of models for automated enterprise architecture documentation; facilitating the adoption of public services using high definition video; factor analysis of critical success factors for data quality; factors of password-based authentication; factors that affect information and communication technology adoption by small businesses in China; feasibility analysis of an assessment model of knowledge acquisition in virtual environments; fostering efficiency in information systems support for product-service systems in the manufacturing industry; framing group norms in virtual communities; global ERP implementations and harmonization of practices in multinational corporations; potentials and challenges to the public health sector of of developing countries; growth of an organizational field for infrastructure; harnessing anomalous preferences of anonymous users for lean information systems development; hidden or implicit contextual factors influencing user participation in online production communities; identification of driving forces in service innovations; illuminating organizing vision careers through case studies; impact of geospatial reasoning ability and perceived task-technology fit on decision-performance; impact of strategic alignment on IT outsourcing success in a complex service setting; impact of unified communications on communication, relationship building and performance; impact of users' cognitive responses on user satisfaction in online community; impediments to enterprise system implementation across the system lifecycle; in search of insights for institutionalization of telemedicine in the health care system in Ethiopia. 19th Americas Conference on Information Systems, AMCIS 2013, Volume 5  ",Strategic alignment
621,Visual models' system of project management,"This article describes the project management problems complex and according visual methods applied in order to solve them. A systematic approach to project management and complex objects visualization problem solving is proposed. The article considers the problem of business graphics as modern techniques of computational geometry and scientific visualization, that aims the formation of a geometric model of the project initial data. Methods of visual analytics became popular in recent years and are used as a basic approaches to solve problems in the project management area, to those practitioners pay a lot of attention. Application of visual and instrumental analysis, using original software developed by the authors is proposed. The technological sequence in order to obtain the project management problem visual solution for managers is given; moreover, recommendations for the use of visual analytics methods to determine the necessary intervention in the problem situation arising in the process of project management are given. Decision making software tools for the project management tasks based on graphical analysis of the problem situation and on formulating recommendations for the decision making person are presented in the paper. Visual models of structural and risk analysis, calendar, resources, investments and financial planning are presented.This work is the author's review of methods of visual analysis in the sphere of project management. © 2014, National Research Nuclear University. All rights reserved. Visual models' system of project management Business graphics; Business intelligence; Geometrization of business data; Graphical representation; Problem situation; Project management; Project planning; Project risk analysis; Project schedule; Schedule; Visual analysis; Visual model of the project Application programs; Competitive intelligence; Computational geometry; Data visualization; Investments; Management science; Problem solving; Project management; Risk analysis; Risk assessment; Scheduling; Visualization; Business data; Graphical representations; Problem situation; Project planning; Project schedules; Visual analysis; Visual model; Decision making",Strategic alignment
622,On satisfying the android OS community: User feedback still central to developers' portfolios,"End-users play an integral role in identifying requirements, validating software features' usefulness, locating defects, and in software product evolution in general. Their role in these activities is especially prominent in online application distribution platforms (OADPs), where software is developed for many potential users, and for which the traditional processes of requirements gathering and negotiation with a single group of end-users do not apply. With such vast access to end-users, however, comes the challenge of how to prioritize competing requirements in order to satisfy previously unknown user groups, especially with early releases of a product. One highly successful product that has managed to overcome this challenge is the Android Operating System (OS). While the requirements of early versions of the Android OS likely benefited from market research, new features in subsequent releases appear to have benefitted extensively from user reviews. Thus, lessons learned about how Android developers have managed to satisfy the user community over time could usefully inform other software products. We have used data mining and natural language processing (NLP) techniques to investigate the issues that were logged by the Android community, and how Google's remedial efforts correlated with users' requests. We found very strong alignment between end-users' top feature requests and Android developers' responses, particularly for the more recent Android releases. Our findings suggest that effort spent responding to end-users' loudest calls may be integral to software systems' survival, and a product's overall success. © 2015 IEEE. On satisfying the android OS community: User feedback still central to developers' portfolios Android OS; Mining software repositories; NLP; Open source; Software analytics; Users feedback Android (operating system); Application programs; Data mining; Market Research; Open source software; Open systems; End-users; Mining software; Mining software repository; Open-source; Product evolution; Software analytic; Software features; Software products; Software repositories; User feedback; Natural language processing systems",Governance
623,"19th Americas Conference on Information Systems, AMCIS 2013, Volume 4","The proceedings contain 438 papers. The special focus in this conference is on Information Systems. The topics include: A cloud-based service for affordable cost analysis; a conceptual examination of distrusting beliefs in older adults about the internet; a consulting model of global service learning; a contingent model of project organization and management; a decision making model for the adoption of cloud computing in Jamaican organizations; a foundation of a first-person perspective systems analysis; a framework for collaborative augmented reality applications; a framework for enterprise social media guidelines; a framework to analyze E-government OSS adoption benefits; a framework to support practitioners in evaluating business-IT alignment models; a measure for assessing the adequacy of DDOS defenses; a systematic classification and analysis of NFRs; about the need for semantically enriched reference models; adopting agile methods for follow-the-sun software development; an analysis of product uncertainty and seller uncertainty; aggregating, analyzing, and diffusing natural disaster information; ameliorating ERP workflow using a sociomaterial lens; an active learning approach to teaching undergraduate introduction to MIS course; an agile approach to systems analysis and design teaching and learning; an empirical study of designing simplicity for mobile application interaction; an examination of IT initiative portfolio characteristics and investment allocation; an exploration of organizational capabilities for emergency response; an icon taxonomy for semi-literate communities; an information security model and its validation; an inquiry into mental models of web interface design; an investigation of the effect of IT occupational subculture on the relationship between knowledge sharing and IT diffusion in organizations; an organizing framework for literacy; an understanding of the impact of gamification on purchase intentions; antecedents and effects of computer self-efficacy on social networking adoption among Asian online users; architecture and implementation of a decision support system for software industry business models; assessing internet source credibility; attitude change process toward ERP systems using the elaboration likelihood model; barriers to mission-critical open source software adoption by organizations; behaviour analysis of distributed systems under time change constraints; blogging as a liminal space; high-value impact through multidisciplinary design science programs of research; breaking the norm - on the determinants of informational nonconformity in online social networks; bringing together BPM and social software; building context-aware access control in enterprise ontologies; capitalizing on social media analysis - insights from an online review on business models; carbon footprint of IT-services - a comparative study of energy consumption for offline and online storage usage; career paths training for the first year students in information systems science-motivational view; chains of control in agile software development; challenges in offshore outsourcing relationship management - a Peruvian perspective; challenges of blind students and IT-based mitigation strategies; cloudifying desktops - a taxonomy for desktop virtualization; CMC influence on voluntarily collaborating knowledge workers' perception of equivocal tasks; collective learning paradigm for rapidly evolving curriculum; a hybrid personalized movie recommender based on perceived similarity; common patterns of cloud business models; comparing ABET-accredited IS undergraduate programs and the ACM 2010IS model curriculum; comprehensive access control for data warehouses; conceptualizing the impact of social capital on knowledge creation; configuring value creation processes for global service; control-related motivations and information security policy compliance; corporate enactments of social control across social media affordances; creative innovativeness with information systems (IS) and its role in quality IS usage; moving from the philosophical to the empirical in the search for causal explanations; critical success factors for ERP system upgrades - the case of a German large-scale enterprise; cultural impact on E-service use in Saudi Arabia; current state of the digital deception studies in IS; customer involvement in organizational innovation - toward an integration concept; data modeling in the cloud; decision support using linked, social, and sensor data; design, evaluation and impact of educational olfactory interfaces; designing decision support systems at the interface between complex and complicated domains; determinants and consequences of herding in P2P lending markets; determining microblogging effectiveness for capturing quality knowledge; developing a governance model for successful business process standardization; developing targeted text messages for enhancing medication adherence; development of a change readiness scale for electronic medical record systems implementation at hospitals; differences between FCM and fuzzy ANP techniques in the process of organizational change readiness assessment; digital service flexibility and performance of credit unions; drive a website performance using web analytics; drivers of cultural differences in information system adoption - a case study; dynamic model to assess organisational readiness during information system implementation; effects of persuasive claims on desirability and impulse purchase behavior; effects of RFID technology on profitability and efficiency in retail supply chains; efficacy of communication support in collaborative online shopping; efficiency and device versatility of graphical and textual passwords; emotion and memory in technology adoption and diffusion; empowering organizations through customer knowledge acquisition; enabling collaboration in virtual manufacturing enterprises with cloud computing; engagement of information technology professionals with their work; enhancing privacy using community driven recommendations; enhancing service lifecycle management - costing as part of service descriptions; enterprise app stores for mobile applications - development of a benefits framework; enterprise architecture software tool support for small and medium-sized enterprises; enterprise systems implementation success in the shakedown phase; environmental pressure on software as a service adoption; estimating the quality of data using provenance; evaluating advanced forms of social media use in government; evaluating cooperation in IT teams using a fuzzy multicriteria sorting method; evaluating the information systems women network (ISWN) mentoring program; evaluating the performance of government IT projects in the Caribbean; examining high performance teams in information systems projects; examining personal information privacy-protective responses (IPPR) with the use of smart devices; examining the use of social media in customer co-creation; exploration of risk management process usage levels and their relationship to project outcomes; exploring subscription renewal intention of operational cloud enterprise systems - a stakeholder perspective; exploring the aesthetic effects of the golden ratio in the design of interactive products; exploring the factors influencing the usage intention of facebook fan page - a preliminary study; exploring the factors that influence social computing intentions; exploring the impact of online reviews with brand equity for online software purchasing behavior; extending successful ebusiness models to the mobile internet; extracting product features from online consumer reviews; eye gazing behaviors in online deception; facilitating collaboration and peer learning through anchored asynchronous online discussions; facilitating conflict resolution of models for automated enterprise architecture documentation; facilitating the adoption of public services using high definition video; factor analysis of critical success factors for data quality; factors of password-based authentication; factors that affect information and communication technology adoption by small businesses in China; feasibility analysis of an assessment model of knowledge acquisition in virtual environments; fostering efficiency in information systems support for product-service systems in the manufacturing industry; framing group norms in virtual communities; global ERP implementations and harmonization of practices in multinational corporations; potentials and challenges to the public health sector of of developing countries; growth of an organizational field for infrastructure; harnessing anomalous preferences of anonymous users for lean information systems development; hidden or implicit contextual factors influencing user participation in online production communities; identification of driving forces in service innovations; illuminating organizing vision careers through case studies; impact of geospatial reasoning ability and perceived task-technology fit on decision-performance; impact of strategic alignment on IT outsourcing success in a complex service setting; impact of unified communications on communication, relationship building and performance; impact of users' cognitive responses on user satisfaction in online community; impediments to enterprise system implementation across the system lifecycle; in search of insights for institutionalization of telemedicine in the health care system in Ethiopia. 19th Americas Conference on Information Systems, AMCIS 2013, Volume 4  ",Strategic alignment
624,Many-objective robust decision making for water supply portfolio planning under deep uncertainty,"Portfolios of market-based instruments have been shown to improve the reliability of water supplies, using simulations that utilize a single best estimate of distributions of data to evaluate performance. However, the estimates of problem information and likelihoods could be incorrect, especially when planning for climate change, which can modify streamflow availability, or projecting the trajectories of future water demands. These conditions are termed deep uncertainty, in which decision makers cannot fully conceptualize or agree upon the full range of risks to their system. This presentation will advance a new interactive framework that combines robust decision making (RDM) with many-objective optimization using evolutionary algorithms (MOEA) to confront deep uncertainty for water planning. The framework is demonstrated using a case study that examines a single city's water supply in the Lower Rio Grande Valley (LRGV) in Texas, USA. We use a MOEA to develop a tradeoff set of water supply portfolios for the LRGV, and develop a suite of values for key uncertainties using RDM that represent an ensemble of ""states of the world"". Each solution is tested under the ensemble of plausible future states of the world, with interactive visualizations being used to identify robust solutions for the system. Scenario discovery methods that use statistical data mining algorithms are then used to identify what assumptions and system conditions strongly control the cost-effectiveness, efficiency, and reliability of the robust alternatives. The results suggest that combining robust decision making, many-objective optimization, and visual analytics can dramatically improve risk-based planning decisions. Many-objective robust decision making for water supply portfolio planning under deep uncertainty Interactive visual analytics; Many-objective optimization; Robust decision making; Water supply ",Value management
625,"The MSH Effective and Efficient Utilization Committee: An Approach to Improving Care Quality, Value and Patient Experience","Hospitals are faced with reduced funding but must deliver high-quality care, and increase service capacity (MOHLTC 2013). In response, Mount Sinai Hospital (MSH) has established an organizational approach that engages clinicians and front-line caregivers. MSH's Effective and Efficient Utilization Committee (EEUC) focuses on cross-departmental collaboration to maximize resources, accessibility and quality, while minimizing cost. The challenges were tackled by team coordination guided by senior administration and project management review of evidence-based data supported by health analytics. Over a three-year period, the EEUC has helped reduce wait times, increase service accessibility, provide cost savings and enhance patient experience in several areas.  The MSH Effective and Efficient Utilization Committee: An Approach to Improving Care Quality, Value and Patient Experience  Advisory Committees; Delivery of Health Care; Efficiency, Organizational; Hospital Administration; Hospitals; Humans; Ontario; Quality Improvement; Quality of Health Care; advisory committee; Canada; health care delivery; health care quality; hospital; hospital management; human; organization and management; standards; total quality management",Risk management
626,"International Conference on Information Systems, ICIS 2012, Volume 5","The proceedings contain 308 papers. The special focus in this conference is on Information Systems. The topics include: Heterogeneity in IT landscapes and monopoly power of firms; community ecology for innovation concept; peer influence in a very large social network; assessing value in an online network of products; optimal pricing with positive network effects; research on viral marketing propagating oriented to marketing context; repurchase intentions of information technology; a multi-level analysis of the impact of health information technology on hospital performance; institutional logics of adoption and use of health information technology; post-acceptance of electronic medical records; socio-technical attachments and IT change; comparing the predictive ability of PLS and covariance models; a note of caution on covariance-equivalent models in information systems; differential effects of omitting formative indicators; being innovative about service innovation; estimating demand for applications in the new mobile economy; continuance of professional social networking sites; role of online social networks in job search by unemployed individuals; knowledge contribution in online network of practice; the dual role of IS specificity in governing software as a service; relational antecedents of information visibility in value networks; summarization of corporate risk factor disclosure through topic modeling; a hybrid method for cross-domain sentiment classification using multiple sources; designing intelligent expert systems to cope with liars; IS offshore project risk, contracts and team structure; vendor and client project managers; technology desirability; the role of green IS in developing eco-effectiveness; the impact of IT-enabled manufacturing capabilities on plant profitability; inter-industry IT spillovers after the dot-com bust; effects of undesired online video advertising choice on user behavior and attitude; unfulfilled obligations in recommendation agent use; understanding the formation of trust in IT artifacts; user decisions among digital piracy and legal alternatives for film and music; a probabilistic generative model for latent business networks mining; the effects of information disclosure policy on the diffusion of security attacks; interactivity of social media and online consumer behavior; strategies for establishing service oriented design in organizations; alternative genres of is research; ICT standardization strategies and service innovation in health care; technical support and IT capacity demand; online social networks as a source and symbol of stress; the effects of social network usage on organizational identification; using the Kano model to identify attractive user-interface software components; developing a theory of multitasking behavior; an empirical investigation of information systems departments' configurations; exploring the adaptation of enterprise systems implementation methodology; out of the box and onto the stage; an organizing vision perspective on green IS development; the emergence of sustainability as the new dominant logic; social influence and defaults in peer-to-peer lending networks; multi-screen strategy for selling mobile content to customers; the emergent role of IT capabilities; reducing price uncertainty through demand side management; factors affecting perceived persuasiveness of a behavior change support system; surfacing schemas from firms' informational engagements; optimal design of consumer review systems; measuring product type with dynamics of online product review variance; different effects on retailers of online product reviews; measurement of multitasking with focus shift analysis; an exploration of the impact of information privacy invasion; measuring mobile users' concerns for information privacy; examining the rationality of location data disclosure through mobile devices; towards a design theory for software project risk management systems; identifying optimal IT portfolios to promote healthcare quality; designing a web-based application to support peer instruction for very large groups; principles for knowledge creation in collaborative design science research; estimating optimal recommendation set sizes for individual consumers; predicting participation in social media sites by analyzing user participation patterns; a hidden Markov model for conversion rate dynamics in online retail; flow experience and continuance intention toward online learning; design and evaluation of a didactical service blueprinting method for large scale lectures; knowledge sharing and maturation in circles of trust; a model-driven method for the systematic literature review of qualitative empirical research; procedurally transparent design science research; advancing task elicitation systems - an experimental evaluation of design principles; the knowing-doing gap in research methods and what we should do about it; efficient and flexible management of enterprise information systems; granularity metrics for IT services; from observed outcomes to measurable performance; the effect of third party investigation on pay-per-click advertising; identifying factors of e-government acceptance - a literature review; effects of cultural cognitive styles on users' evaluation of website complexity; a sustainability model of green IT initiatives; towards a typology of green IS strategies; digital access, political networks and the diffusion of democracy; the effect of free access on the diffusion of scholarly ideas; an empirical examination of cultural biases in interpersonal economic exchange; effect of business intelligence and IT infrastructure flexibility on organizational agility; news recommender systems with feedback; a multi-theoretical framework for social network-based recommendation; improving coverage of design in information systems education; perceived IT security risks of cloud computing: conceptualization and scale development; capability leapfrogging in the Japanese IT services industry; predatory coercion in social media and protection of children online - a critical discourse analysis approach; encouraging collaborative idea-building in enterprise-wide innovation challenges; analyzing the impact of social media strategies on viewer engagement; supporters in deed - studying online support provision from the perspective of social capital; membership overlap and inter-community collaboration; developing customer agility through information management; keyword search patterns in sponsored link advertisements; the hidden effects of opening bids in online auctions; increasing dynamic capabilities through virtualized grid-in-cloud solutions; design principles for heterogeneity decisions in enterprise architecture management; IT stereotyping and the CEO-CIO headlock; exploring the role of un-enacted projects in IT project portfolio management; the difference of determinants of mobile data services' adoption and continuance - a longitudinal study; a study of effectiveness and satisfaction level of cloud CRM users in Taiwan's enterprises; investigating intelligent agents in a 3D virtual world; an empirical study of a two-sided model of fraudulent exchange; perceptual and conceptual effects of incidental exposure to web ads; self-disclosure on online social networks; critical success factors of location-based services; affect and online privacy concerns; the impact of training and social norms on information security compliance; security management in cross-organizational settings; the effect of customers' emotion on service recovery strategy in IT service failures; the influence of IS affordances on work practices in health care; multi-level knowledge transfer in software development outsourcing projects; organizational performance with environmental knowledge intensity; process visibility - towards a conceptualization and research themes; towards an oil crisis early warning system based on absolute news volume; constructing workflow models from agent profiles; formulating effective coordination strategies in agile global software development teams; the transmission of control in information systems projects; the structuration of task-oriented communication in innovative virtual teams and social media in the entrainment of contention to innovation. International Conference on Information Systems, ICIS 2012, Volume 5  ",Value management
627,Data challenges in high-performance risk analytics,"Risk Analytics is important to quantify, manage and analyse risks from the manufacturing to the financial setting. In this paper, the data challenges in the three stages of the high-performance risk analytics pipeline, namely risk modelling, portfolio risk management and dynamic financial analysis is presented. © 2012 IEEE. Data challenges in high-performance risk analytics data management; high-performance computing; risk analytics; risk management; risk modelling Information management; Risk management; Data challenges; Financial analysis; High-performance computing; Portfolio risk management; Risk modelling; Risk assessment",Risk management
628,"International Conference on Information Systems, ICIS 2013, Volume 1","The proceedings contain 320 papers. The special focus in this conference is on Information Systems. The topics include: Computing requirements in open source software projects; a comparison of nonlinear pricing preference models for digital services; a computational approach to detecting and assessing sustainability-related communities in social media; a conceptualisation of management situations relevant for collaborative is research projects; a conceptualization of complexity in IS-driven organizational transformations; information security collective efficacy and vulnerability; a cross-sectional and temporal analysis of information consumption on twitter; a measurement model for investigating digital natives and their organisational behaviour; a model of product design and information disclosure investments; a problem-solving perspective on governance and product design in open source software projects; a qualitative approach to examine technology acceptance; a sensemaking approach to tablet users accommodating practices; a systems approach to countermeasures in credibility assessment interviews; aligning business and it strategies in multi-business organizations; an empirical examination of users information hiding in a crowdfunding context; an empirical investigation of a user-centric typology of innovation for evolving information systems products; an exploration of group information security compliance; an information quality evaluation framework of object tracking systems; an ontology artifact for information systems sentiment analysis; visualising perspectives of business and information systems; attention trade-off between two types of user contributions; the effects of user- and marketer-generated content on purchase decision processes; boundary spanning through enterprise social software; investment management systems and new institutional logics for regulatory compliance; bundling of software products and services to fight against piracy; capturing multi-stakeholder needs in customer-centric cloud service design; an empirical analysis of the impact of formal controls in strategic outsourcing contracts; cloud computing providers unrealistic optimism regarding it security risks; cognitive biases in new technology appropriation; collective intelligence in market-based social decision making; comparing peer influences in large social networks an empirical study on caller ringback tone; computer mediated social ties as predictors of SNS usage continuance; conceptualizing unexpected events in IT projects; coordinating one-to-many concurrent negotiation for service provision; counteracting the negative effect of form auto-completion on the privacy calculus; creating engaging student to student engagement in an online class; unpacking the building blocks of crowdsourcing collaboration processes; defining boundaries of web ads perceptual fluency effect; designing business models for platform as a service; designing e-services for an ageing society; designing the modern ICT curriculum; developing a procedure model for business process standardization; developing and measuring IS scales using item response theory; digital traces of information systems; discursive meaning creation in crowdfunding; driving innovation and knowledge management using crowdsourcing; interplay of EA design factors, strategy types, and environments; effective sentiment analysis of corporate financial reports; eliminating failure by learning from it systematic review of is project failure; navigating the ambiguity of materiality through collective design; enhancing shared understanding in collaborative online shopping; evaluating a new pattern development process for interface design; evaluating the quality of technology-mediated learning services; evolving the modular layered architecture in digital innovation; executive doctorate programs and the role of the information systems discipline; exploring foundations for using simulations in is research; exploring the customer perspective of agile development; exploring the effect of arousal and valence on mouse interaction; fit between knowledge transfer complexity and media capability; the impact of information pricing strategy on the pattern and effectiveness of word-of-mouth via social media; from open source to commercial software development - the community based software development model; some thoughts on technology implementation practice; a point of concern to IS use models' validity; how channel choice and service failure influence customer satisfaction; how customers trust mobile marketing applications; how E-consumers integrate diverse recommendations from multiple sources; how team cohesion leads to attitude change in the context of ERP learning; understanding reconfigurations of information systems and organizations in the Norwegian health sector; identifying patterns of idea diffusion in innovator networks; the missing link between it affordances and institutions for better health care in developing countries; impacts of creolization on trust and knowledge sharing in it-enabled global services sourcing; impacts of IT acceptance and resistance behaviors; implications of monitoring mechanisms on bring your own device (BYOD) adoption; improved medication compliance through health IT; improving medical decision-making using electronic health record systems; improving the semantics of conceptual-modeling grammars; inciting networks effects through platform authority; individual values for protecting identity in social networks; individualization of information systems - analyzing antecedents of IT consumerization behavior; information architecture for healthcare organizations; its antecedents and mediating effects on security compliant behavior; information system strategy for opportunity discovery and exploitation; information systems development outsourcing; institutionalization and the effectiveness of enterprise architecture management; internet use and well-being of young adults; investigating mobile messaging in healthcare organizations; different firm types, different alignment configurations; developing valid measures through CFA-based MTMM analysis; IT-enabled performative spaces in gender segregated work; knowledge contribution motivators an expectation-confirmation approach; legitimating user participation in mature organisations- exploring social media adoption in a financial services organization; an analysis of the impact of mobile micro-blogging on communication and decision-making; mobile app portfolio management and developers performance; mobile applications and access to personal information; mobile commerce in the new tablet economy; multi-agent based information systems for patient coordination in hospitals; network analysis for predicting academic impact; network diversity and social cohesion in creative performance; on the importance of organisational culture and structure in business process maturity; one-way mirrors and weak-signaling in online dating; online and offline sales channels for enterprise software; online health information use by disabled people; optimal information technology service pricing and capacity decision under service-level agreement; optimal location of charging stations in smart cities; organizational learning and the error of fixed strategies in IT innovation investment evaluation; pathways to value from business analytics; patients adherence to health advice on virtual communities; peer-based quality assurance in information systems development; their effect on extracurricular work behaviors among IT professionals; success and reciprocity on crowdfunding platforms; predictive validity and formative measurement in structural equation modeling; preventive adoption of information security behaviors; understanding factors contributing to the escalation of software maintenance costs; privacy controls and content sharing patterns of online social network users; psychological empowerment of patients with chronic diseases; purposive selection and the quality of qualitative IS research; quality-adjusted consumer surplus for online labor markets with asymmetric information; quantifying the dynamic sales impact of location-based mobile promotion technologies; is-enabled political decision support with scenario analyses for the substitution of fossil fuels; information technologies and the possibility for imagination; roles of trust in privacy assurance and perceived disease severity on personal health information disclosure; selecting project management methodologies for business intelligence projects a value based approach; service failure complaints identification in social media; role of social distance and social norms in online referral incentive systems; social media and citizen social movement process for political change; software developers' online chat as an intra-firm mechanism for sharing ephemeral knowledge. International Conference on Information Systems, ICIS 2013, Volume 1  ",Value management
629,Maleku: An evolutionary visual software analysis tool for providing insights into software evolution,"Software maintenance is a complex process that requires the understanding and comprehension of software project details. It involves the understanding of the evolution of the software project, hundreds of software components and the relationships among software items in the form of inheritance, interface implementation, coupling and cohesion. Consequently, the aim of evolutionary visual software analytics is to support software project managers and developers during software maintenance. It takes into account the mining of evolutionary data, the subsequent analysis of the results produced by the mining process for producing evolution facts, the use of visualizations supported by interaction techniques and the active participation of users. Hence, this paper proposes an evolutionary visual software analytics tool for the exploration and comparison of project structural, interface implementation and class hierarchy data, and the correlation of structural data with metrics, as well as socio-technical relationships. Its main contribution is a tool that automatically retrieves evolutionary software facts and represent them using a scalable visualization design. © 2011 IEEE. Maleku: An evolutionary visual software analysis tool for providing insights into software evolution  Project management; Visualization; Class hierarchies; Complex Processes; Interaction techniques; Mining process; Scalable visualization; Socio-technical relationships; Software analysis; Software component; Software Evolution; Software project; Structural data; Computer software maintenance",Strategic alignment
630,"How value is created from data: Experiences from the integrated health care system, “gesundes Kinzigtal” (healthy Kinzigtal)","The networking of the various players in integrated health care leads to extensive data. Using this to create added value is associated with a number of challenges. Based on a best practice model Gesundes Kinzigtal it can be shown that data from various sources can be linked and processed in a data warehouse, and made available to management via a business intelligence front end: from project preparation through ongoing project management to evaluation after project completion. The resulting advantages for patients, doctors, management companies and health insurance companies are described and associated with the main project. © 2014 medhochzwei Verlag GmbH, Heidelberg and 2016 Walter de Gruyter GmbH, Berlin/Boston. How value is created from data: Experiences from the integrated health care system, “gesundes Kinzigtal” (healthy Kinzigtal)  Data warehouses; Health insurance; Insurance; Management science; Project management; Added values; Best practices; Front end; Insurance companies; Integrated health care system; Project completion; Project preparation; Health care",Governance
631,"International Conference on Information Systems, ICIS 2013, Volume 4","The proceedings contain 320 papers. The special focus in this conference is on Information Systems. The topics include: Computing requirements in open source software projects; a comparison of nonlinear pricing preference models for digital services; a computational approach to detecting and assessing sustainability-related communities in social media; a conceptualisation of management situations relevant for collaborative is research projects; a conceptualization of complexity in IS-driven organizational transformations; information security collective efficacy and vulnerability; a cross-sectional and temporal analysis of information consumption on twitter; a measurement model for investigating digital natives and their organisational behaviour; a model of product design and information disclosure investments; a problem-solving perspective on governance and product design in open source software projects; a qualitative approach to examine technology acceptance; a sensemaking approach to tablet users' accommodating practices; a systems approach to countermeasures in credibility assessment interviews; aligning business and it strategies in multi-business organizations; an empirical examination of users' information hiding in a crowdfunding context; an empirical investigation of a user-centric typology of innovation for evolving information systems products; an exploration of group information security compliance; an information quality evaluation framework of object tracking systems; an ontology artifact for information systems sentiment analysis; visualising perspectives of business and information systems; attention trade-off between two types of user contributions; the effects of user- and marketer-generated content on purchase decision processes; boundary spanning through enterprise social software; investment management systems and new institutional logics for regulatory compliance; bundling of software products and services to fight against piracy; capturing multi-stakeholder needs in customer-centric cloud service design; an empirical analysis of the impact of formal controls in strategic outsourcing contracts; cloud computing providers' unrealistic optimism regarding it security risks; cognitive biases in new technology appropriation; collective intelligence in market-based social decision making; comparing peer influences in large social networks - an empirical study on caller ringback tone; computer mediated social ties as predictors of SNS usage continuance; conceptualizing unexpected events in IT projects; coordinating one-to-many concurrent negotiation for service provision; counteracting the negative effect of form auto-completion on the privacy calculus; creating engaging student to student engagement in an online class; unpacking the building blocks of crowdsourcing collaboration processes; defining boundaries of web ads' perceptual fluency effect; designing business models for platform as a service; designing e-services for an ageing society; designing the modern ICT curriculum; developing a procedure model for business process standardization; developing and measuring IS scales using item response theory; digital traces of information systems; discursive meaning creation in crowdfunding; driving innovation and knowledge management using crowdsourcing; interplay of EA design factors, strategy types, and environments; effective sentiment analysis of corporate financial reports; eliminating failure by learning from it -systematic review of is project failure; navigating the ambiguity of materiality through collective design; enhancing shared understanding in collaborative online shopping; evaluating a new pattern development process for interface design; evaluating the quality of technology-mediated learning services; evolving the modular layered architecture in digital innovation; executive doctorate programs and the role of the information systems discipline; exploring foundations for using simulations in is research; exploring the customer perspective of agile development; exploring the effect of arousal and valence on mouse interaction; fit between knowledge transfer complexity and media capability; the impact of information pricing strategy on the pattern and effectiveness of word-of-mouth via social media; from open source to commercial software development - the community based software development model; some thoughts on technology implementation practice; a point of concern to IS use models' validity; how channel choice and service failure influence customer satisfaction; how customers trust mobile marketing applications; how E-consumers integrate diverse recommendations from multiple sources; how team cohesion leads to attitude change in the context of ERP learning; understanding reconfigurations of information systems and organizations in the Norwegian health sector; identifying patterns of idea diffusion in innovator networks; the missing link between it affordances and institutions for better health care in developing countries; impacts of creolization on trust and knowledge sharing in it-enabled global services sourcing; impacts of IT acceptance and resistance behaviors; implications of monitoring mechanisms on bring your own device (BYOD) adoption; improved medication compliance through health IT; improving medical decision-making using electronic health record systems; improving the semantics of conceptual-modeling grammars; inciting networks effects through platform authority; individual values for protecting identity in social networks; individualization of information systems - analyzing antecedents of IT consumerization behavior; information architecture for healthcare organizations; its antecedents and mediating effects on security compliant behavior; information system strategy for opportunity discovery and exploitation; information systems development outsourcing; institutionalization and the effectiveness of enterprise architecture management; internet use and well-being of young adults; investigating mobile messaging in healthcare organizations; different firm types, different alignment configurations; developing valid measures through CFA-based MTMM analysis; IT-enabled performative spaces in gender segregated work; knowledge contribution motivators - an expectation-confirmation approach; legitimating user participation in mature organisations- exploring social media adoption in a financial services organization; an analysis of the impact of mobile micro-blogging on communication and decision-making; mobile app portfolio management and developers' performance; mobile applications and access to personal information; mobile commerce in the new tablet economy; multi-agent based information systems for patient coordination in hospitals; network analysis for predicting academic impact; network diversity and social cohesion in creative performance; on the importance of organisational culture and structure in business process maturity; one-way mirrors and weak-signaling in online dating; online and offline sales channels for enterprise software; online health information use by disabled people; optimal information technology service pricing and capacity decision under service-level agreement; optimal location of charging stations in smart cities; organizational learning and the error of fixed strategies in IT innovation investment evaluation; pathways to value from business analytics; patients' adherence to health advice on virtual communities; peer-based quality assurance in information systems development; their effect on extracurricular work behaviors among IT professionals; success and reciprocity on crowdfunding platforms; predictive validity and formative measurement in structural equation modeling; preventive adoption of information security behaviors; understanding factors contributing to the escalation of software maintenance costs; privacy controls and content sharing patterns of online social network users; psychological empowerment of patients with chronic diseases; purposive selection and the quality of qualitative IS research; quality-adjusted consumer surplus for online labor markets with asymmetric information; quantifying the dynamic sales impact of location-based mobile promotion technologies; is-enabled political decision support with scenario analyses for the substitution of fossil fuels; information technologies and the possibility for imagination; roles of trust in privacy assurance and perceived disease severity on personal health information disclosure; selecting project management methodologies for business intelligence projects - a value based approach; service failure complaints identification in social media; role of social distance and social norms in online referral incentive systems; social media and citizen social movement process for political change; software developers' online chat as an intra-firm mechanism for sharing ephemeral knowledge; sources of power and CIO influence and their impact; statistical modeling of nanotechnology knowledge diffusion networks; systems of transfiguration and the adoption of IT under surveillance; taking a new-generation manager perspective to develop interface designs; task-technology fit for low-literate consumers; team adaptability in agile information systems development; technology, interoperability, and provision of public safety networks; the architecture of generativity in a digital ecosystem; the design of a network-based model for business performance prediction; the design of a tangible user interface for a real-time strategy game; the differences between recommender technologies in their impact on sales diversity; the dynamics of IS adaptation in multinational corporations; the emergence of social media as boundary objects in crisis response; the establishment of social IT sourcing organizations. International Conference on Information Systems, ICIS 2013, Volume 4  ",Strategic alignment
632,Collaboration and social networking in higher education,"This paper presents an exploratory analysis of the experience of educational innovation in the configuration of a social learning network in a subject for the Education degree course at the University of Santiago de Compostela (Spain). This innovation is based on the premise of student-centered teaching (independent learning, self-regulated, authentic and breaking boundaries between formal and informal areas) enriched with collaborative activities. The study aims to analyze the intensity and relevance of the student́s contributions in this collaborative framework. We used learning analytics tools with two types of techniques: social network analysis (SNA) andinformation extraction, to measure the intensity, centrality and relevance of collaboration among students. The results obtained allow us to confirm: 1) The consistency and coherence between the pedagogical approach and the option of using a social network in university education; 2) A dense network with a high level of interaction, a moderate degree of centrality and a low centralization index (structure moves away from star), with a group with the capacity to influence the rest (degree of betweenness); 3) High level of relevance to the content analyzed; 4) The usefulness of learning analytics techniques to guide teacher decision-making. Collaboration and social networking in higher education Collaboration; E-portfolio; Innovation; Learning; Learning analytics; Personal learning environments; Social networking; University teaching ",Monitoring and control
633,The Handbook of News Analytics in Finance,"The Handbook of News Analytics in Finance is a landmark publication bringing together the latest models and applications of News Analytics for asset pricing, portfolio construction, trading and risk control. The content of the Hand Book is organised to provide a rapid yet comprehensive understanding of this topic. Chapter 1 sets out an overview of News Analytics (NA) with an explanation of the technology and applications. The rest of the chapters are presented in four parts. Part 1 contains an explanation of methods and models which are used to measure and quantify news sentiment. In Part 2 the relationship between news events and discovery of abnormal returns (the elusive alpha) is discussed in detail by the leading researchers and industry experts. The material in this part also covers potential application of NA to trading and fund management. Part 3 covers the use of quantified news for the purpose of monitoring, early diagnostics and risk control. Part 4 is entirely industry focused; it contains insights of experts from leading technology (content) vendors. It also contains a discussion of technologies and finally a compact directory of content vendor and financial analytics companies in the marketplace of NA. The book draws equally upon the expertise of academics and practitioners who have developed these models and is supported by two major content vendors - RavenPack and Thomson Reuters - leading providers of news analytics software and machine readable news. The book will appeal to decision makers in the banking, finance and insurance services industry. In particular: asset managers; quantitative fund managers; hedge fund managers; algorithmic traders; proprietary (program) trading desks; sell-side firms; brokerage houses; risk managers and research departments will benefit from the unique insights into this new and pertinent area of financial modelling. © 2011 John Wiley & Sons Ltd. The Handbook of News Analytics in Finance  ",Strategic alignment
634,The quest for unbiased rating of the intellectual capital: Value-based analytical detection and fuzzy-quantitative appraisal. Introducing the intellectual capital ontology analytics,"The purpose of this paper is to provide a candidate solution for a rational and unbiased quantitative appraisal of the intellectual capital (IC) and of its rating. This study is intended to fill a gap in IC evaluation and in the assessment of intangible assets (IAs), due to the qualitative nature of the taxonomy-based approaches traditionally used and to the discretionality that their application implies. This goal is achieved by the means of a procedural approach that mitigates the risk of subjectivity in IC taxonomy association and, at the same time, provides some tools for analysing the set of IAs included in the configuration of the IC. It also returns findings that are comparable with other organisations and can be used for unbiased benchmarking. Furthermore the meta-knowledge embedded in its outputs can be used for a more informed and rational resource allocation and management, investment strategy and business development. Copyright © 2014 Inderscience Enterprises Ltd. The quest for unbiased rating of the intellectual capital: Value-based analytical detection and fuzzy-quantitative appraisal. Introducing the intellectual capital ontology analytics Asset portfolio; Business development; Business evaluation; Fuzzy AHP; Fuzzy set theory; Intangible assets; Intellectual capital; Investment decisions; Knowledge management; MADM; Ontology; Resource assessment; SRs; Strategic decisions; Strategic resources; Taxonomy; Value creation ",Strategic alignment
635,The string prediction models as invariants of time series in the forex market,"In this paper we apply a new approach of string theory to the real financial market. The models are constructed with an idea of prediction models based on the string invariants (PMBSI). The performance of PMBSI is compared to support vector machines (SVM) and artificial neural networks (ANN) on an artificial and a financial time series. A brief overview of the results and analysis is given. The first model is based on the correlation function as invariant and the second one is an application based on the deviations from the closed string/pattern form (PMBCS). We found the difference between these two approaches. The first model cannot predict the behavior of the forex market with good efficiency in comparison with the second one which is, in addition, able to make relevant profit per year. The presented string models could be useful for portfolio creation and financial risk management in the banking sector as well as for a nonlinear statistical approach to data optimization. © 2013 Elsevier B.V. All rights reserved. The string prediction models as invariants of time series in the forex market Finance forex market; Financial forecasting; Nonlinear statistics; String theory; Trading strategy Commerce; Electronic trading; Finance; Forecasting; Risk management; String theory; Support vector machines; Time series; Correlation function; Financial forecasting; Financial risk management; Financial time series; Forex markets; Nonlinear statistics; Statistical approach; Trading strategies; Predictive analytics",Strategic alignment
636,Harnessing the power of data in government through analytics,"The Australian government collects and uses a large volume of data from its clients. Much of these data are used to facilitate the provision of services. Data custodians maintain the integrity of records as they record entitlements, decisions and liabilities. Strict regimes exist to protect privacy, commercial sensitivities and potential misuse of such data. Much of these data holdings are in data-stovepipes. While new project management practices (such as Agile and Lean Startup) are gaining support in the private sector, there are few instances of these being applied in government. This environment makes the job of teams seeking to leverage new and emerging analytics capabilities challenging. New and innovative approaches are needed to maximise the value of analytics capabilities. This presentation will offer some thoughts on approaches that can deliver positive outcomes based on recent developments in the Department of Immigration and Border Protection. The departments experience demonstrates that large organisation can develop and deploy affordable, new analytics capabilities into its core systems infrastructure in a relatively short period. © 2013, Australian Computer Society, Inc. Harnessing the power of data in government through analytics  Project management; Border protection; Core systems; Innovative approaches; Large volumes; New projects; Private sectors; Short periods; Data mining",Strategic alignment
637,Validated wind power plant modelling for accurate KPI benchmarks,"A validated Wind Power Plant Simulation model is used as an independent reference for the generation of wind farm performance indicators. SynaptiQ Wind is a multi-user, multi-technology software platform developed by 3E for wind portfolio management, reporting and business intelligence. A measurement campaign using a LiDAR, mounted on the Offshore High-Voltage Station in one of the Belgian offshore wind farms has provided high-quality data that enabled the validation of the Wind Power Plant Simulation model, and in particular the selection and calibration of the wake modelling used. Throughout the operational phase of a wind farm, independent wind speed measurements are often not available. The Wind Power Plant Simulations (WPPS) model provides a benchmark for the expected yields of the wind farm independent of any measurements. Key Performance Indicators can then be defined that are independent of the resource performance. This way SynaptiQ Wind enables monitoring focusing on system performance, and quickly identifies the root because of underperforming assets. Validated wind power plant modelling for accurate KPI benchmarks  Benchmarking; Electric utilities; Exhibitions; Financial data processing; Investments; Offshore wind farms; Reactive power; Key performance indicators; Measurement campaign; Operational phase; Performance indicators; Portfolio managements; Software platforms; Underperforming assets; Wind speed measurement; Computer simulation",Monitoring and control
638,ICEIS 2014 - Proceedings of the 16th International Conference on Enterprise Information Systems,The proceedings contain 210 papers. The topics discussed include: possibilistic interorganizational workflow net for the recovery problem concerning communication failures; distributed knowledge management architecture and rule based reasoning for mobile machine operator performance assessment; machine learning techniques for topic spotting; fuzzy DEMATEL model for evaluation criteria of business intelligence; an evolutionary algorithm for graph planarization by vertex deletion; evaluating artificial neural networks and traditional approaches for risk analysis in software project management - a case study with PERIL dataset; using visualization and text mining to improve qualitative analysis; DC2DP: a dublin core application profile to design patterns; domain ontology for time series provenance; video stream transmodality; assisting speech therapy for autism spectrum disorders with an augmented reality application; and adding semantic relations among design patterns. ICEIS 2014 - Proceedings of the 16th International Conference on Enterprise Information Systems  ,Risk management
639,Navigating the housing downturn and financial crisis: Home appreciation and equity accumulation among community reinvestment homeowners,"Homeownership has historically been considered an effective wealth-creation mechanism for low-income households. These households typically hold a very large proportion of their wealth in their homes. In addition' they also historically have greatly benefited from home price appreciation' not only because of overall positive market trends but also because of the high leverage provided by high loan-to-value mortgages. These factors have made homeownership the primary driving factor in wealth creation for many low-income homeowners (Belsky and Calder 2004; Stegman' Quercia' and Davis 2007). However' as recent economic trends have illustrated on a national scale' such lopsided wealth portfolio allocation has a potentially enormous downside. After doubling between 1995 and 2007' housing prices have experienced an unprecedented national decline' having fallen 14 percent' according to the Federal Housing Finance Agency. The homeownership rate' which peaked at 69.2 percent earlier in 2004' fell to 67.1 percent in the second quarter of 2010. Moreover' First American CoreLogic' a data and analytics consultancy' reports that over 11 million residential properties' or 24 percent of all residential properties with a mortgage' were ""underwater"" (that is' the property owners owed more than the value of these houses) by the first quarter of 2010. Of course' the recession has not been limited to the housing market. The Dow Jones Industrial Average (DJIA) fell 53 percent between October 2007 and March 2009. But housing traditionally represents a highly leveraged investment' especially for low-income households' and' if not offset by sufficient investments in assets that have uncorrelated investment returns' can quickly and easily destroy household wealth. In addition to the possibility of direct loss of their initial investments' low-income homeowners potentially face additional costs from negative equity' foreclosure' and reduced access to credit. Moreover' they typically have few resources with which to weather such additional costs. It is natural' then' to ask how low-income homeowners have fared during the decline of the housing market that began in the spring of 2006 and during the financial crisis that ensued in the fall of 2008. In particular' it makes sense to revisit the question of when-or even whether-housing is a viable investment for these borrowers. Previous research (Ding' Quercia' and Ratcliffe 2008) has addressed how the question of viability can depend in large part on the quality of the loan product. Low mortgage default rates are clearly a prerequisite to successful wealth creation through homeownership' and traditional loan products' such as 30-year prime fixed-rate mortgages' significantly outperform subprime loans for comparable borrowers. This chapter examines solely the follow-up question of equity gains given the reality that house prices do not always rise. A multivariate model is used to analyze price appreciation among low-income homeowners' since it depends upon the timing of loan origination and local area conditions' such as the percentage of the population in poverty' the unemployment rate' and the median age of nearby houses. Results from this model are presented for five time periods around the peak of the housing bubble' providing an overall picture of how the relationships among the price appreciation rate and these correlates have changed over time and relate to changes in the underlying economy. Copyright © 2011 University of Pennsylvania Press. All rights reserved. Navigating the housing downturn and financial crisis: Home appreciation and equity accumulation among community reinvestment homeowners  ",Value management
640,Research on the forecasting of construction accidents with the cubic exponential smoothing method,"Construction accidents occur frequently and because great loss to construction enterprises, the whole industry and the society. Therefore, it is significant to forecast the death tolls of construction engineering accidents to provide decisive reference for development of Chinese construction accident prevention. Being compared with other forecasting method, exponential smoothing method is relatively simple, convenient, and does not need huge volume of historical data. The cubic exponential smoothing model is set up in this paper, and the death tolls of the housing construction and municipal engineering accident from 1988 to 2012 are chosen to predict the construction accidents. The forecasting value result is very close to the original value from 1999 to 2012, which confirms it is reasonable and feasible to use this model to forecast the death tolls of construction engineering accidents. Then, the forecasting values can be predicted in 2013 and 2014, which are 527 and 421. The cubic exponential smoothing method is a reliable predictive model, and it can forecast death tolls in short term. The forecast result can provide reference to apply effective measures and ways to improve the serious construction accidents situation now. © Springer-Verlag Berlin Heidelberg 2014. Research on the forecasting of construction accidents with the cubic exponential smoothing method Accident forecasting; Construction accidents; The cubic exponential smoothing model Accidents; Municipal engineering; Predictive analytics; Project management; Construction accidents; Construction engineering; Construction enterprise; Exponential smoothing method; Exponential smoothing model; Forecasting methods; Housing construction; Predictive modeling; Forecasting",Capacity management
641,High performance risk aggregation: Addressing the data processing challenge the Hadoop MapReduce way,"Monte Carlo simulations employed for the analysis of portfolios of catastrophic risk process large volumes of data. Often times these simulations are not performed in real-time scenarios as they are slow and consume large data. Such simulations can benefit from a framework that exploits parallelism for addressing the computational challenge and facilitates a distributed file system for addressing the data challenge. To this end, the Apache Hadoop framework is chosen for the simulation reported in this paper so that the computational challenge can be tackled using the MapReduce model and the data challenge can be addressed using the Hadoop Distributed File System. A parallel algorithm for the analysis of aggregate risk is proposed and implemented using the MapReduce model in this paper. An evaluation of the performance of the algorithm indicates that the Hadoop MapReduce model offers a framework for processing large data in aggregate risk analysis. A simulation of aggregate risk employing 100,000 trials with 1000 catastrophic events per trial on a typical exposure set and contract structure is performed on multiple worker nodes in less than 6 minutes. The result indicates the scope and feasibility of MapReduce for tackling the computational and data challenge in the analysis of aggregate risk for real-time use. © 2013 ACM. High performance risk aggregation: Addressing the data processing challenge the Hadoop MapReduce way data processing; hadoop mapreduce; high-performance analytics; risk aggregation; risk analysis Cloud computing; Computer simulation; Computer software; Data handling; Data processing; File organization; Monte Carlo methods; Risk analysis; Risk assessment; Catastrophic event; Catastrophic risks; Computational challenges; Distributed file systems; Hadoop distributed file systems; Hadoop MapReduce; high-performance analytics; Risk aggregations; Aggregates",Risk management
642,"9th International Conference on Knowledge Management in Organizations, KMO 2014","The proceedings contain 39 papers. The special focus in this conference is on Knowledge Management in Organizations. The topics include: Big data in land records management in Kenya; managers' interactions and their effect on productivity; identification of motivational factors influencing the return of researchers focusing on the Slovak environment; developing start-up ecosystem in small cities; knowledge management model as a factor of educative quality; antecedents of empowerment and commitment to service quality in the Chinese hotel industry; developing data analytics to improve services in a mechanical engineering company; predicting grades based on students' online course activities; what government subsidiary projects can learn from each other; an exploration of potential multilevel antecedents and consequences; the influence of theory-practice gap on knowledge transfer at the point of clinical placement; knowledge management tools and their role in doctoral studies; a multiple domain analysis and systems modelling intelligence architecture; increasing user engagement using innovative poll first content delivery method; identifying the knowledge needs of Japanese engineers; towards a knowledge transfer 2.0 process with an impact on a companies of social business; mobile learning platforms to assist individual knowledge management; laboratory information management systems -challenges of implementation; identifying the images of desirable middle managers; e-portfolio development through activity theory in action research; integration between IT governance and balanced scorecard; analysing the relationship between components of knowledge orientation strategy in Chilean enterprises; detecting zero-day attacks using contextual relations; understanding relationship between security culture and knowledge management and knowledge creation process as communication - connecting SECI and activity theory via cascading modes of communication. 9th International Conference on Knowledge Management in Organizations, KMO 2014  ",Governance
643,Aspects of data quality that cause impact on business intelligence systems,"Considered the subject that gives visibility to data governance, 'data quality' integrates the portfolio of information governance disciplines prescribed by the strategic planning of information technology of the Sicoob and, in this sense, many resulting actions have been initiated, including the qualification of the data for the business intelligence (BI). This paper presents the activities of planning data quality actions needed to the analytical environment. It also presents a list of problems identified in the customer registration form and how this can affect the financial institutions in generating managerial reports, in the customer relationship and marketing campaigns, in the offer of products, among others. © 2013 IEEE. Aspects of data quality that because impact on business intelligence systems Data Governance; Data Quality; Data Quality Problems Information technology; Management science; Public relations; Business intelligence systems; Customer relationships; Data governances; Data quality; Financial institution; Marketing campaign; Planning data; Data reduction",Value management
644,Virtual project teaming: Incorporation of immersive environments and role-played case study assessments,"The purpose of this article is to present a role-play based approach to virtual project teaming and assessment that provides graduate students with collaborative working spaces, a context within which students can interact with their peers, and a structured approach for facilitators to evaluate the efforts of these virtual teams. Content and analytics for this study stem from data collected over the course of three semesters while offering a graduate-level Technology Project Management course. To offer this online course, several virtual working spaces were utilized to present course lectures and discussions, and to provide virtual in-world team skills training. Discussion includes the incorporation of virtual role-play centered on the use of a contextually appropriate case study that spans a period 10 of the 14 weeks of the semester-long course. The article also addresses both the structure of the course delivery. The case study and role play structure, and challenges faced in maximizing the immersive capabilities of these types of virtual environments all while ensuring that content delivery and required course assessments were being realized. A summary of student survey data and discussion of lessons learned is also incorporated. © American Society for Engineering Education, 2013. Virtual project teaming: Incorporation of immersive environments and role-played case study assessments  Curricula; Engineering education; Project management; Research; Students; Virtual reality; Collaborative working; Content delivery; Course assessment; Graduate students; Immersive environment; Management course; Structured approach; Technology projects; E-learning",Value management
645,Combining life cycle assessment with data science to inform portfolio-level value-chain engineering: A Case Study at PepsiCo Inc.,"Life cycle assessment (LCA)-based analyses of company value chains can inspire profound modifications to products' design, material procurement, manufacturing, energy/water use, distribution, use, and disposal. However, such modifications often create trade-offs, improving some aspects while worsening others. How can firms decide whether or not to carry out such modifications? Or prioritize between different options to choose the one delivering the most competitive advantage? Typically, firms' metrics fall into two groups: (1) product-level metrics across the life cycle, including up- and downstream of facilities (e.g., product carbon footprints); and (2) facility-level metrics (e.g., plants' annual energy cost). Neither is sufficient for firm-wide cost-benefit analyses of modifications that affect multiple products and value-chain stages. Whereas facility-level metrics do not capture up- and downstream effects-where often most cost and environmental impacts originate-life cycle methodologies are currently not mature enough to be applied at the scale of entire product portfolios. We present a pilot system of key performance indicators (KPIs) that evaluate 3,337 products across 211 brands and five countries of PepsiCo, Inc. KPIs are firm-wide, annual figures (environmental, operational, and financial) across the value chain (cradle to grave) and can be determined at any level (single product, brands, or regions). Uncertainty analysis is included. In addition to KPIs for base cases, the system characterizes KPI impacts for any considered modifications (what-if scenarios). In a detailed case study, we present background about how and why PepsiCo used the system to evaluate all aspects of a strategic value-chain modification. For 7 of the 211 brands, this resulted in avoiding an 8% increase in greenhouse gas emissions and a 7% to 10% increase in procurement costs. It also saved PepsiCo an estimated ∼200 years full-time equivalent employee time (or alternatively ∼US$30 million in LCA consultant fees) had the LCAs of the 3,337 SKUs been carried out by traditional methods. This cost efficiency of the KPI system enables considering environmental impacts with more-traditional business metrics side by side. As a result, environmental impacts can be considered on a routine basis as part of integrated strategy and business planning. We discuss implementation considerations of the KPI methodology and future improvements. © 2014, Yale University. Combining life cycle assessment with data science to inform portfolio-level value-chain engineering: A Case Study at PepsiCo Inc. Enterprise resource planning (ERP); Industrial ecology; Key performance indicator (KPI); Life cycle assessment (LCA); Product design; Supply chain management Benchmarking; Carbon footprint; Competition; Cost benefit analysis; Costs; Data Science; Economic and social effects; Efficiency; Enterprise resource management; Enterprise resource planning; Environmental impact; Gas emissions; Greenhouse gases; Planning; Product design; Supply chain management; Uncertainty analysis; Competitive advantage; Enterprise resource planning (ERP); Full time equivalents; Industrial ecology; Key performance indicators; Life Cycle Assessment (LCA); Material procurement; Product carbon footprints; Life cycle",Financial management
646,"International Conference on Information Systems, ICIS 2013, Volume 5","The proceedings contain 320 papers. The special focus in this conference is on Information Systems. The topics include: Computing requirements in open source software projects; a comparison of nonlinear pricing preference models for digital services; a computational approach to detecting and assessing sustainability-related communities in social media; a conceptualisation of management situations relevant for collaborative is research projects; a conceptualization of complexity in IS-driven organizational transformations; information security collective efficacy and vulnerability; a cross-sectional and temporal analysis of information consumption on twitter; a measurement model for investigating digital natives and their organisational behaviour; a model of product design and information disclosure investments; a problem-solving perspective on governance and product design in open source software projects; a qualitative approach to examine technology acceptance; a sensemaking approach to tablet users' accommodating practices; a systems approach to countermeasures in credibility assessment interviews; aligning business and it strategies in multi-business organizations; an empirical examination of users' information hiding in a crowdfunding context; an empirical investigation of a user-centric typology of innovation for evolving information systems products; an exploration of group information security compliance; an information quality evaluation framework of object tracking systems; an ontology artifact for information systems sentiment analysis; visualising perspectives of business and information systems; attention trade-off between two types of user contributions; the effects of user- and marketer-generated content on purchase decision processes; boundary spanning through enterprise social software; investment management systems and new institutional logics for regulatory compliance; bundling of software products and services to fight against piracy; capturing multi-stakeholder needs in customer-centric cloud service design; an empirical analysis of the impact of formal controls in strategic outsourcing contracts; cloud computing providers' unrealistic optimism regarding it security risks; cognitive biases in new technology appropriation; collective intelligence in market-based social decision making; comparing peer influences in large social networks - an empirical study on caller ringback tone; computer mediated social ties as predictors of SNS usage continuance; conceptualizing unexpected events in IT projects; coordinating one-to-many concurrent negotiation for service provision; counteracting the negative effect of form auto-completion on the privacy calculus; creating engaging student to student engagement in an online class; unpacking the building blocks of crowdsourcing collaboration processes; defining boundaries of web ads' perceptual fluency effect; designing business models for platform as a service; designing e-services for an ageing society; designing the modern ICT curriculum; developing a procedure model for business process standardization; developing and measuring IS scales using item response theory; digital traces of information systems; discursive meaning creation in crowdfunding; driving innovation and knowledge management using crowdsourcing; interplay of EA design factors, strategy types, and environments; effective sentiment analysis of corporate financial reports; eliminating failure by learning from it -systematic review of is project failure; navigating the ambiguity of materiality through collective design; enhancing shared understanding in collaborative online shopping; evaluating a new pattern development process for interface design; evaluating the quality of technology-mediated learning services; evolving the modular layered architecture in digital innovation; executive doctorate programs and the role of the information systems discipline; exploring foundations for using simulations in is research; exploring the customer perspective of agile development; exploring the effect of arousal and valence on mouse interaction; fit between knowledge transfer complexity and media capability; the impact of information pricing strategy on the pattern and effectiveness of word-of-mouth via social media; from open source to commercial software development - the community based software development model; some thoughts on technology implementation practice; a point of concern to IS use models' validity; how channel choice and service failure influence customer satisfaction; how customers trust mobile marketing applications; how E-consumers integrate diverse recommendations from multiple sources; how team cohesion leads to attitude change in the context of ERP learning; understanding reconfigurations of information systems and organizations in the Norwegian health sector; identifying patterns of idea diffusion in innovator networks; the missing link between it affordances and institutions for better health care in developing countries; impacts of creolization on trust and knowledge sharing in it-enabled global services sourcing; impacts of IT acceptance and resistance behaviors; implications of monitoring mechanisms on bring your own device (BYOD) adoption; improved medication compliance through health IT; improving medical decision-making using electronic health record systems; improving the semantics of conceptual-modeling grammars; inciting networks effects through platform authority; individual values for protecting identity in social networks; individualization of information systems - analyzing antecedents of IT consumerization behavior; information architecture for healthcare organizations; its antecedents and mediating effects on security compliant behavior; information system strategy for opportunity discovery and exploitation; information systems development outsourcing; institutionalization and the effectiveness of enterprise architecture management; internet use and well-being of young adults; investigating mobile messaging in healthcare organizations; different firm types, different alignment configurations; developing valid measures through CFA-based MTMM analysis; IT-enabled performative spaces in gender segregated work; knowledge contribution motivators - an expectation-confirmation approach; legitimating user participation in mature organisations- exploring social media adoption in a financial services organization; an analysis of the impact of mobile micro-blogging on communication and decision-making; mobile app portfolio management and developers' performance; mobile applications and access to personal information; mobile commerce in the new tablet economy; multi-agent based information systems for patient coordination in hospitals; network analysis for predicting academic impact; network diversity and social cohesion in creative performance; on the importance of organisational culture and structure in business process maturity; one-way mirrors and weak-signaling in online dating; online and offline sales channels for enterprise software; online health information use by disabled people; optimal information technology service pricing and capacity decision under service-level agreement; optimal location of charging stations in smart cities; organizational learning and the error of fixed strategies in IT innovation investment evaluation; pathways to value from business analytics; patients' adherence to health advice on virtual communities; peer-based quality assurance in information systems development; their effect on extracurricular work behaviors among IT professionals; success and reciprocity on crowdfunding platforms; predictive validity and formative measurement in structural equation modeling; preventive adoption of information security behaviors; understanding factors contributing to the escalation of software maintenance costs; privacy controls and content sharing patterns of online social network users; psychological empowerment of patients with chronic diseases; purposive selection and the quality of qualitative IS research; quality-adjusted consumer surplus for online labor markets with asymmetric information; quantifying the dynamic sales impact of location-based mobile promotion technologies; is-enabled political decision support with scenario analyses for the substitution of fossil fuels; information technologies and the possibility for imagination; roles of trust in privacy assurance and perceived disease severity on personal health information disclosure; selecting project management methodologies for business intelligence projects - a value based approach; service failure complaints identification in social media; role of social distance and social norms in online referral incentive systems; social media and citizen social movement process for political change; software developers' online chat as an intra-firm mechanism for sharing ephemeral knowledge; sources of power and CIO influence and their impact; statistical modeling of nanotechnology knowledge diffusion networks; systems of transfiguration and the adoption of IT under surveillance; taking a new-generation manager perspective to develop interface designs; task-technology fit for low-literate consumers; team adaptability in agile information systems development; technology, interoperability, and provision of public safety networks; the architecture of generativity in a digital ecosystem; the design of a network-based model for business performance prediction; the design of a tangible user interface for a real-time strategy game; the differences between recommender technologies in their impact on sales diversity; the dynamics of IS adaptation in multinational corporations; the emergence of social media as boundary objects in crisis response; the establishment of social IT sourcing organizations. International Conference on Information Systems, ICIS 2013, Volume 5  ",Strategic alignment
647,Churn prediction for high-value players in casual social games,"Predicting when players will leave a game creates a unique opportunity to increase players' lifetime and revenue contribution. Players can be incentivized to stay, strategically cross-linked to other games in the company's portfolio or, as a last resort, be passed on to other companies through in-game advertisement. This paper focuses on predicting churn for highvalue players of casual social games and attempts to assess the business impact that can be derived from a predictive churn model. We compare the prediction performance of four common classification algorithms over two casual social games, each with millions of players. Furthermore, we implement a hidden Markov model to explicitly address temporal dynamics. We find that a neural network achieves the best prediction performance in terms of area under curve (AUC). In addition, to assess the business value of churn prediction, we design and implement an A/B test on one of the games, using free in-game currency as an incentive to retain players. Test results indicate that contacting players shortly before the predicted churn event substantially improves the effectiveness of communication with players. They further show that giving out free in-game currency does not significantly impact the churn rate or monetization of players. This suggests that players can only be retained by remarkably changing their gameplay experience ahead of the churn event and that cross-linking may be the more effective measure to deal with churning players. © 2014 IEEE. Churn prediction for high-value players in casual social games A/B evaluation; churn prediction; freemium; hidden Markov model; neural networks; social casual games Economics; Forecasting; Hidden Markov models; Intelligent computing; Neural networks; A/B evaluation; Casual games; Churn predictions; Classification algorithm; Design and implements; Freemium; Gameplay experiences; Prediction performance; Predictive analytics",Financial management
648,"International Conference on Information Systems, ICIS 2012, Volume 3","The proceedings contain 308 papers. The special focus in this conference is on Information Systems. The topics include: Heterogeneity in IT landscapes and monopoly power of firms; community ecology for innovation concept; peer influence in a very large social network; assessing value in an online network of products; optimal pricing with positive network effects; research on viral marketing propagating oriented to marketing context; repurchase intentions of information technology; a multi-level analysis of the impact of health information technology on hospital performance; institutional logics of adoption and use of health information technology; post-acceptance of electronic medical records; socio-technical attachments and IT change; comparing the predictive ability of PLS and covariance models; a note of caution on covariance-equivalent models in information systems; differential effects of omitting formative indicators; being innovative about service innovation; estimating demand for applications in the new mobile economy; continuance of professional social networking sites; role of online social networks in job search by unemployed individuals; knowledge contribution in online network of practice; the dual role of IS specificity in governing software as a service; relational antecedents of information visibility in value networks; summarization of corporate risk factor disclosure through topic modeling; a hybrid method for cross-domain sentiment classification using multiple sources; designing intelligent expert systems to cope with liars; IS offshore project risk, contracts and team structure; vendor and client project managers; technology desirability; the role of green IS in developing eco-effectiveness; the impact of IT-enabled manufacturing capabilities on plant profitability; inter-industry IT spillovers after the dot-com bust; effects of undesired online video advertising choice on user behavior and attitude; unfulfilled obligations in recommendation agent use; understanding the formation of trust in IT artifacts; user decisions among digital piracy and legal alternatives for film and music; a probabilistic generative model for latent business networks mining; the effects of information disclosure policy on the diffusion of security attacks; interactivity of social media and online consumer behavior; strategies for establishing service oriented design in organizations; alternative genres of is research; ICT standardization strategies and service innovation in health care; technical support and IT capacity demand; online social networks as a source and symbol of stress; the effects of social network usage on organizational identification; using the Kano model to identify attractive user-interface software components; developing a theory of multitasking behavior; an empirical investigation of information systems departments' configurations; exploring the adaptation of enterprise systems implementation methodology; out of the box and onto the stage; an organizing vision perspective on green IS development; the emergence of sustainability as the new dominant logic; social influence and defaults in peer-to-peer lending networks; multi-screen strategy for selling mobile content to customers; the emergent role of IT capabilities; reducing price uncertainty through demand side management; factors affecting perceived persuasiveness of a behavior change support system; surfacing schemas from firms' informational engagements; optimal design of consumer review systems; measuring product type with dynamics of online product review variance; different effects on retailers of online product reviews; measurement of multitasking with focus shift analysis; an exploration of the impact of information privacy invasion; measuring mobile users' concerns for information privacy; examining the rationality of location data disclosure through mobile devices; towards a design theory for software project risk management systems; identifying optimal IT portfolios to promote healthcare quality; designing a web-based application to support peer instruction for very large groups; principles for knowledge creation in collaborative design science research; estimating optimal recommendation set sizes for individual consumers; predicting participation in social media sites by analyzing user participation patterns; a hidden Markov model for conversion rate dynamics in online retail; flow experience and continuance intention toward online learning; design and evaluation of a didactical service blueprinting method for large scale lectures; knowledge sharing and maturation in circles of trust; a model-driven method for the systematic literature review of qualitative empirical research; procedurally transparent design science research; advancing task elicitation systems - an experimental evaluation of design principles; the knowing-doing gap in research methods and what we should do about it; efficient and flexible management of enterprise information systems; granularity metrics for IT services; from observed outcomes to measurable performance; the effect of third party investigation on pay-per-click advertising; identifying factors of e-government acceptance - a literature review; effects of cultural cognitive styles on users' evaluation of website complexity; a sustainability model of green IT initiatives; towards a typology of green IS strategies; digital access, political networks and the diffusion of democracy; the effect of free access on the diffusion of scholarly ideas; an empirical examination of cultural biases in interpersonal economic exchange; effect of business intelligence and IT infrastructure flexibility on organizational agility; news recommender systems with feedback; a multi-theoretical framework for social network-based recommendation; improving coverage of design in information systems education; perceived IT security risks of cloud computing: conceptualization and scale development; capability leapfrogging in the Japanese IT services industry; predatory coercion in social media and protection of children online - a critical discourse analysis approach; encouraging collaborative idea-building in enterprise-wide innovation challenges; analyzing the impact of social media strategies on viewer engagement; supporters in deed - studying online support provision from the perspective of social capital; membership overlap and inter-community collaboration; developing customer agility through information management; keyword search patterns in sponsored link advertisements; the hidden effects of opening bids in online auctions; increasing dynamic capabilities through virtualized grid-in-cloud solutions; design principles for heterogeneity decisions in enterprise architecture management; IT stereotyping and the CEO-CIO headlock; exploring the role of un-enacted projects in IT project portfolio management; the difference of determinants of mobile data services' adoption and continuance - a longitudinal study; a study of effectiveness and satisfaction level of cloud CRM users in Taiwan's enterprises; investigating intelligent agents in a 3D virtual world; an empirical study of a two-sided model of fraudulent exchange; perceptual and conceptual effects of incidental exposure to web ads; self-disclosure on online social networks; critical success factors of location-based services; affect and online privacy concerns; the impact of training and social norms on information security compliance; security management in cross-organizational settings; the effect of customers' emotion on service recovery strategy in IT service failures; the influence of IS affordances on work practices in health care; multi-level knowledge transfer in software development outsourcing projects; organizational performance with environmental knowledge intensity; process visibility - towards a conceptualization and research themes; towards an oil crisis early warning system based on absolute news volume; constructing workflow models from agent profiles; formulating effective coordination strategies in agile global software development teams; the transmission of control in information systems projects; the structuration of task-oriented communication in innovative virtual teams and social media in the entrainment of contention to innovation. International Conference on Information Systems, ICIS 2012, Volume 3  ",Value management
649,Fixed Income Securities and Derivatives Handbook: Second Edition,"The definitive guide to fixed-come securities-revised to reflect today's dynamic financial environment. The Second Edition of the Fixed-Income Securities and Derivatives Handbook offers a completely updated and revised look at an important area of today's financial world. In addition to providing an accessible description of the main elements of the debt market, concentrating on the instruments used and their applications, this edition takes into account the effect of the recent financial crisis on fixed income securities and derivatives. As timely as it is timeless, the Second Edition of the Fixed-Income Securities and Derivatives Handbook includes a wealth of new material on such topics as covered and convertible bonds, swaps, synthetic securitization, and bond portfolio management, as well as discussions regarding new regulatory twists and the evolving derivatives market. Offers a more detailed look at the basic principles of securitization and an updated chapter on collateralized debt obligations. Covers bond mathematics, pricing and yield analytics, and term structure models. Includes a new chapter on credit analysis and the different metrics used to measure bond-relative value. Contains illustrative case studies and real-world examples of the topics touched upon throughout the book. Written in a straightforward and accessible style, Moorad Choudhry's new book offers the ideal mix of practical tips and academic theory within this important field. © 2010, 2005 Moorad Choudhry. All rights reserved. Fixed Income Securities and Derivatives Handbook: Second Edition  ",Strategic alignment
650,"Proceedings of the 8th European Conference on Information Management and Evaluation, ECIME 2014","The proceedings contain 46 papers. The topics discussed include: towards a culture of engagement leveraging the enterprise social network; holistic personas and reflective concepts for software engineers; investigating the application of the IT-CMF in maturing strategic business-IT alignment; chief information officers and information systems failure: towards a new research agenda; implementation and benefits of real-time business intelligence; applying quality function deployment method for business architecture alignment; ERP systems and organizational learning: where do we stand? a literature review; critical organizational challenges in delivering business value from IT: the perspective of Lebanese CIOs; understanding the roles of enterprise architects in a proactive enterprise development context; some Ps are missing in the governance of projects, programs and portfolios (PPP); and challenges and success factors for collaborative business process modelling. Proceedings of the 8th European Conference on Information Management and Evaluation, ECIME 2014  ",Governance
651,Improving the prediction of petroleum reservoir characterization with a stacked generalization ensemble model of support vector machines,"The ensemble learning paradigm has proved to be relevant to solving most challenging industrial problems. Despite its successful application especially in the Bioinformatics, the petroleum industry has not benefited enough from the promises of this machine learning technology. The petroleum industry, with its persistent quest for high-performance predictive models, is in great need of this new learning methodology. A marginal improvement in the prediction indices of petroleum reservoir properties could have huge positive impact on the success of exploration, drilling and the overall reservoir management portfolio. Support vector machines (SVM) is one of the promising machine learning tools that have performed excellently well in most prediction problems. However, its performance is a function of the prudent choice of its tuning parameters most especially the regularization parameter, C. Reports have shown that this parameter has significant impact on the performance of SVM. Understandably, no specific value has been recommended for it. This paper proposes a stacked generalization ensemble model of SVM that incorporates different expert opinions on the optimal values of this parameter in the prediction of porosity and permeability of petroleum reservoirs using datasets from diverse geological formations. The performance of the proposed SVM ensemble was compared to that of conventional SVM technique, another SVM implemented with the bagging method, and Random Forest technique. The results showed that the proposed ensemble model, in most cases, outperformed the others with the highest correlation coefficient, and the lowest mean and absolute errors. The study indicated that there is a great potential for ensemble learning in petroleum reservoir characterization to improve the accuracy of reservoir properties predictions for more successful explorations and increased production of petroleum resources. The results also confirmed that ensemble models perform better than the conventional SVM implementation. © 2014 Elsevier B.V. All rights reserved. Improving the prediction of petroleum reservoir characterization with a stacked generalization ensemble model of support vector machines Permeability; Porosity; Regularization parameter; Stacked generalization ensemble; Support vector machines Decision trees; Forecasting; Gasoline; Learning systems; Mechanical permeability; Parameterization; Petroleum industry; Petroleum prospecting; Petroleum reservoir engineering; Porosity; Predictive analytics; Support vector machines; Correlation coefficient; Geological formation; Industrial problem; Machine learning technology; Petroleum reservoir characterization; Petroleum resources; Regularization parameters; Stacked generalization; Reservoir management",Monitoring and control
652,"19th Americas Conference on Information Systems, AMCIS 2013, Volume 1","The proceedings contain 438 papers. The special focus in this conference is on Information Systems. The topics include: A cloud-based service for affordable cost analysis; a conceptual examination of distrusting beliefs in older adults about the internet; a consulting model of global service learning; a contingent model of project organization and management; a decision making model for the adoption of cloud computing in Jamaican organizations; a foundation of a first-person perspective systems analysis; a framework for collaborative augmented reality applications; a framework for enterprise social media guidelines; a framework to analyze E-government OSS adoption benefits; a framework to support practitioners in evaluating business-IT alignment models; a measure for assessing the adequacy of DDOS defenses; a systematic classification and analysis of NFRs; about the need for semantically enriched reference models; adopting agile methods for follow-the-sun software development; an analysis of product uncertainty and seller uncertainty; aggregating, analyzing, and diffusing natural disaster information; ameliorating ERP workflow using a sociomaterial lens; an active learning approach to teaching undergraduate introduction to MIS course; an agile approach to systems analysis and design teaching and learning; an empirical study of designing simplicity for mobile application interaction; an examination of IT initiative portfolio characteristics and investment allocation; an exploration of organizational capabilities for emergency response; an icon taxonomy for semi-literate communities; an information security model and its validation; an inquiry into mental models of web interface design; an investigation of the effect of IT occupational subculture on the relationship between knowledge sharing and IT diffusion in organizations; an organizing framework for literacy; an understanding of the impact of gamification on purchase intentions; antecedents and effects of computer self-efficacy on social networking adoption among Asian online users; architecture and implementation of a decision support system for software industry business models; assessing internet source credibility; attitude change process toward ERP systems using the elaboration likelihood model; barriers to mission-critical open source software adoption by organizations; behaviour analysis of distributed systems under time change constraints; blogging as a liminal space; high-value impact through multidisciplinary design science programs of research; breaking the norm - on the determinants of informational nonconformity in online social networks; bringing together BPM and social software; building context-aware access control in enterprise ontologies; capitalizing on social media analysis - insights from an online review on business models; carbon footprint of IT-services - a comparative study of energy consumption for offline and online storage usage; career paths training for the first year students in information systems science-motivational view; chains of control in agile software development; challenges in offshore outsourcing relationship management - a Peruvian perspective; challenges of blind students and IT-based mitigation strategies; cloudifying desktops - a taxonomy for desktop virtualization; CMC influence on voluntarily collaborating knowledge workers' perception of equivocal tasks; collective learning paradigm for rapidly evolving curriculum; a hybrid personalized movie recommender based on perceived similarity; common patterns of cloud business models; comparing ABET-accredited IS undergraduate programs and the ACM 2010IS model curriculum; comprehensive access control for data warehouses; conceptualizing the impact of social capital on knowledge creation; configuring value creation processes for global service; control-related motivations and information security policy compliance; corporate enactments of social control across social media affordances; creative innovativeness with information systems (IS) and its role in quality IS usage; moving from the philosophical to the empirical in the search for causal explanations; critical success factors for ERP system upgrades - the case of a German large-scale enterprise; cultural impact on E-service use in Saudi Arabia; current state of the digital deception studies in IS; customer involvement in organizational innovation - toward an integration concept; data modeling in the cloud; decision support using linked, social, and sensor data; design, evaluation and impact of educational olfactory interfaces; designing decision support systems at the interface between complex and complicated domains; determinants and consequences of herding in P2P lending markets; determining microblogging effectiveness for capturing quality knowledge; developing a governance model for successful business process standardization; developing targeted text messages for enhancing medication adherence; development of a change readiness scale for electronic medical record systems implementation at hospitals; differences between FCM and fuzzy ANP techniques in the process of organizational change readiness assessment; digital service flexibility and performance of credit unions; drive a website performance using web analytics; drivers of cultural differences in information system adoption - a case study; dynamic model to assess organisational readiness during information system implementation; effects of persuasive claims on desirability and impulse purchase behavior; effects of RFID technology on profitability and efficiency in retail supply chains; efficacy of communication support in collaborative online shopping; efficiency and device versatility of graphical and textual passwords; emotion and memory in technology adoption and diffusion; empowering organizations through customer knowledge acquisition; enabling collaboration in virtual manufacturing enterprises with cloud computing; engagement of information technology professionals with their work; enhancing privacy using community driven recommendations; enhancing service lifecycle management - costing as part of service descriptions; enterprise app stores for mobile applications - development of a benefits framework; enterprise architecture software tool support for small and medium-sized enterprises; enterprise systems implementation success in the shakedown phase; environmental pressure on software as a service adoption; estimating the quality of data using provenance; evaluating advanced forms of social media use in government; evaluating cooperation in IT teams using a fuzzy multicriteria sorting method; evaluating the information systems women network (ISWN) mentoring program; evaluating the performance of government IT projects in the Caribbean; examining high performance teams in information systems projects; examining personal information privacy-protective responses (IPPR) with the use of smart devices; examining the use of social media in customer co-creation; exploration of risk management process usage levels and their relationship to project outcomes; exploring subscription renewal intention of operational cloud enterprise systems - a stakeholder perspective; exploring the aesthetic effects of the golden ratio in the design of interactive products; exploring the factors influencing the usage intention of facebook fan page - a preliminary study; exploring the factors that influence social computing intentions; exploring the impact of online reviews with brand equity for online software purchasing behavior; extending successful ebusiness models to the mobile internet; extracting product features from online consumer reviews; eye gazing behaviors in online deception; facilitating collaboration and peer learning through anchored asynchronous online discussions; facilitating conflict resolution of models for automated enterprise architecture documentation; facilitating the adoption of public services using high definition video; factor analysis of critical success factors for data quality; factors of password-based authentication; factors that affect information and communication technology adoption by small businesses in China; feasibility analysis of an assessment model of knowledge acquisition in virtual environments; fostering efficiency in information systems support for product-service systems in the manufacturing industry; framing group norms in virtual communities; global ERP implementations and harmonization of practices in multinational corporations; potentials and challenges to the public health sector of of developing countries; growth of an organizational field for infrastructure; harnessing anomalous preferences of anonymous users for lean information systems development; hidden or implicit contextual factors influencing user participation in online production communities; identification of driving forces in service innovations; illuminating organizing vision careers through case studies; impact of geospatial reasoning ability and perceived task-technology fit on decision-performance; impact of strategic alignment on IT outsourcing success in a complex service setting; impact of unified communications on communication, relationship building and performance; impact of users' cognitive responses on user satisfaction in online community; impediments to enterprise system implementation across the system lifecycle; in search of insights for institutionalization of telemedicine in the health care system in Ethiopia. 19th Americas Conference on Information Systems, AMCIS 2013, Volume 1  ",Strategic alignment
653,Financial risk modelling in vehicle credit portfolio,"Luxury cars are a segment of vehicles which are usually bought by people with a higher purchasing power. Still, majority of people make this luxury investment through vehicle finance services. The people from this segment tend to have a good credit record and thus are granted credit by vehicle finance service providers. Despite the good credit record and high purchasing power, a certain amount of risk is associated with these credit portfolios. This study deals with the analysis of a data set comprising of opulent vehicle credit portfolios characterized by relevant variables. It aims at assessing the risk associated with these portfolios and finally presents a predictive model which highlights the important variables and depicts the combination of those variables that classify a client under defaulter or non-defaulter. The study starts with the use of conventional statistical techniques and subsequently presents machine learning approach using three different decision tree classifiers. © 2014 IEEE. Financial risk modelling in vehicle credit portfolio Credit Risk; Decision Tree Classifiers; Machine Learning; Vehicle Finance Data mining; Decision trees; Intelligent computing; Investments; Learning systems; Machine learning; Predictive analytics; Vehicles; Credit portfolio; Credit risks; Decision tree classifiers; Machine learning approaches; Predictive modeling; Purchasing power; Service provider; Statistical techniques; Risk assessment",Strategic alignment
654,The cobas P 630 instrument: A dedicated pre-analytic solution to optimize COBAS® AmpliPrep/COBAS® TaqMan® system workflow and turn-around-time,"The cobas p 630, a fully automated pre-analytical instrument for primary tube handling recently introduced to complete the Cobas® TaqMan systems portfolio, was evaluated in conjunction with: the COBAS® AmpliPrep/COBAS® TaqMan HBV Test, v2.0, COBAS® AmpliPrep/COBAS® TaqMan HCV Test, v1.0 and COBAS® AmpliPrep/COBAS® TaqMan HIV Test, v2.0. The instrument performance in transferring samples from primary to secondary tubes, its impact in improving COBAS® AmpliPrep/COBAS® TaqMan workflow and hands-on reduction and the risk of possible cross-contamination were assessed. Samples from 42 HBsAg positive, 42 HCV and 42 HIV antibody (Ab) positive patients as well as 21 healthy blood donors were processed with or without automated primary tubes. HIV, HCV and HBsAg positive samples showed a correlation index of 0.999, 0.987 and of 0.994, respectively. To assess for cross-contamination, high titer HBV DNA positive samples, HCV RNA and HIV RNA positive samples were distributed in the cobas p 630 in alternate tube positions, adjacent to negative control samples within the same rack. None of the healthy donor samples showed any reactivity. Based on these results, the cobas p 630 can improve workflow and sample tracing in laboratories performing molecular tests, and reduce turnaround time, errors, and risks. © 2012 Elsevier B.V. The cobas P 630 instrument: A dedicated pre-analytic solution to optimize COBAS® AmpliPrep/COBAS® TaqMan® system workflow and turn-around-time Automation; HBV; HCV; HIV; Pre-analytics; Real-time PCR Automation, Laboratory; Hepatitis B; Hepatitis C; HIV Infections; Humans; Molecular Diagnostic Techniques; Specimen Handling; Time Factors; Workflow; Hepatitis B virus; Hepatitis C virus; hepatitis B surface antigen; hepatitis C antibody; Human immunodeficiency virus antibody; virus DNA; virus RNA; analytical equipment; analytical error; article; controlled study; hepatitis B; hepatitis C; human; Human immunodeficiency virus infection; laboratory automation; laboratory device; major clinical study; microbial contamination; molecular diagnosis; nonhuman; priority journal; risk assessment; risk reduction; tube; turnaround time; virus load; workflow",Financial management
655,Selecting project management methodologies for business intelligence projects - A value based approach,"While it is common practice to use value based decision models for decisions whether to invest in certain projects or not, there is scarce value based decision support for the selection of the most promising project management methodology to be applied in a specific Business Intelligence project. Addressing the lack of a formal yet practical decision model, this paper proposes a risk-adjusted net-present-value-based model to support decision makers in this particular decision situation. Apart from a project's estimated cash flows, we focus on two decisive risk parameters - the likelihood of environmental changes and the peril of improper system integration. Using an exemplary calculation, the trade-off between those risks and their impact is formalized and made transparent. Therefore, this paper suggests a decision model to improve the understanding of project management methodology types and hereupon the foundation for the selection of the appropriate one in a specific project setting. © (2013) by the AIS/ICIS Administrative Office All rights reserved. Selecting project management methodologies for business intelligence projects - A value based approach Business Intelligence; Decision Model; Project Management Methodology; Value-based Management Competitive intelligence; Decision support systems; Information systems; Management science; Models; Risk perception; Business Intelligence projects; Decision modeling; Decision situation; Environmental change; Project management methodology; System integration; Value-based approach; Value-based managements; Project management",Strategic alignment
656,Towards Software Performance Monitoring: An Approach for the Aerospace Industry,"Software applications are becoming one of the most valuable assets for companies, providing critical capabilities and functionalities to perform a wide range of operations in the industry. This paper aims to provide a view on software application portfolio monitoring and its integration into business intelligence systems for aerospace manufacturing companies. The key research question addressed is how critical software has become for aerospace industry and how software applications could be monitored. This question has been addressed by conducting an in depth review of current literature and by interviewing professionals from different aerospace companies. The results are a set of key findings regarding software impact in aerospace industry, and a monitoring proposal based in a traditional business intelligence architecture. By incorporating condition monitoring methodologies into the software application portfolio of the enterprise, benefits in maintenance budget allocation and risk avoidance are expected, thanks to a more precise and agile way of processing business data. Additional savings should be possible through further application portfolio optimisation. © 2015 The Authors. Published by Elsevier B.V. Towards Software Performance Monitoring: An Approach for the Aerospace Industry Aerospace; Business Intelligence; Condition Monitoring; Software Lifecycle; Software Maintenance Aerospace engineering; Aerospace industry; Budget control; Competitive intelligence; Computer software maintenance; Condition monitoring; Data handling; Information analysis; Aerospace; Aerospace manufacturing; Business intelligence systems; Monitoring methodologies; Portfolio optimisation; Software applications; Software life cycles; Software performance; Application programs",Financial management
657,Clustering techniques and their effect on portfolio formation and risk analysis,"This paper explores the application of three different port-folio formation rules using standard clustering techniques-K-means, K-mediods, and hierarchical-to a large financial data set (16 years of daily CRSP stock data) to determine how the choice of clustering technique may affect analysts' perceptions of the riskiness of different portfolios in the context of a prototype visual analytics system designed for financial stability monitoring. We use a two-phased experimental approach with visualizations to explore the effects of the different clustering techniques. The choice of clustering technique matters. There is significant variation among techniques, resulting in different ""pictures"" of the riskiness of the same underlying data when plotted to the visual analytics tool. This sensitivity to clustering methodolgy has the potential to mislead analysts about the riskiness of portfolios. We conclude that further research into the implications of portfolio formation rules is needed, and that visual analytics tools should not limit analysts to a single clustering technique, but instead should provide the facility to explore the data using different techniques. Clustering techniques and their effect on portfolio formation and risk analysis Clustering techniques; Financial stability monitoring; Visual analytics Data Science; Finance; Hierarchical clustering; Risk analysis; Risk assessment; Visualization; Clustering techniques; Experimental approaches; Financial data; Financial stability; K-means; Stock data; Visual analytics; Visual analytics systems; K-means clustering",Strategic alignment
658,"18th Americas Conference on Information Systems 2012, AMCIS 2012, Volume 3","The proceedings contain 572 papers. The special focus in this conference is on Information Systems. The topics include: Health diagnosis of communities of practices (CoPs); open source alternatives for business intelligence; identifying business process activity mappings by optimizing behavioral similarity; hanging with the right crowd; a taxonomy of web-based inbound open innovation initiatives; database intrusion detection: defending against the insider threat; an empirical study of the GIGO axiom in satisficing decisions; analysis of probabilistic news recommender systems; the influence of technology characteristics on privacy calculus; instant messaging privacy in the clouds; towards a component-based description of business models; two-sided cybermediary platforms; an integrative analysis of transactional e-government web usage; facebook usage in government-a case study of information content; deriving business value from asymmetric penalty-reward perspectives of IS users; create attention to attract attention-viral marketing of digital music in social networks; towards a framework for transforming business models into business processes; factors affecting perceived satisfaction with a BPM tool; hypercompetition in the Erp industry; structural flaws in the ethics of technology; ethical considerations for virtual worlds; personality, gender and careers in information technology; on IT control weaknesses in auditors' reports on internal control; effect of the SOX act on IT governance; the influence of general sustainability attitudes and value congruence on consumer behavior; user privacy in mobile advertising; system learning of user interactions; deploying mission critical learning management system using open source software; theorizing the dual role of information technology in technostress research; boundary spanning in business process management; ERP usability and the mangle of practice; direct manipulation tablet apps for education; comparing graphical and tangible user interfaces for a tower defense game; visual perception model for online target marketing; eye movements, perceptions, and performance; individual relationships with technology; internal audit function response to ERP systems implementation; auditing journal entries using self-organizing map; consumer perceptions of the adoption of electronic personal health records; investigating the reciprocal relationships within health virtual communities; designing and visualising healthcare delivery systems; how multinational firms use IT to manage their global operations; conflict, value diversity, and performance in virtual teams; mobile ICT and knowledge sharing in underserved communities; agile decision making framework to support mobile microloans for unbanked customers; the analysis of the telecommunications industry in Thailand; the business value of knowledge management; providing information feedback to bidders in online multi-unit combinatorial auctions; spatial modeling using agents; using probabilistic ontologies for video exploration; new directions, new challenges, and new understandings; function-based categorization of online product information types; exploring antecedents of habit on social network service; personality correlation analysis and applications in social networks; identifying experts in virtual forecasting communities; black males in IT higher education in the USA; technology features, empowering perceptions, and voicing behavior on microblog; information security policy compliance; a preliminary taxonomy for software failure impact; an examination of the success of post-merger IT integration; an analysis of and perspective on the information security maturity model; geographic information systems and the nonprofit sector; effectiveness of shallow hierarchies for document stores; a methodology for the development of web-based information systems; balanced resource allocation; demand response in smart grids; decision support for electric vehicle charging; the expectations for faculty in Latin America; mastering the social IT/Business Alignment Challenge; supply chain resource planning systems; towards a research framework for VLBA operation management; integrating enterprise system's 3rd wave into IS curriculum; a two-tier data-centric framework for flexible business process management; engagement in online communities; organisational semiotics methods to assess organisational readiness for internal use of social media; social media in the workplace; economics of pair programming revisited; social traps of agile methods; metadata exploitation in large-scale data migration projects; collaboratively assessing information quality on the web; reputation management in social commerce communities; E-Business adoption research; a preliminary information theory of difference; replacement of project manager during IT projects-a research agenda; a simulation study of project management and collaborative information technologies; the role of business information visualization in knowledge creation; effects of narrative structure and salient decision points in role playing games; adoption of pervasive e-health solutions; security practices and regulatory compliance in the healthcare industry; the role of demographic characteristics in health care strategic security planning; tailoring software process capability/maturity models for telemedicine systems; understanding dynamic collaboration in teleconsultation; the pathway to enterprise mobile readiness; investigating the role of social media and social capital; exploring 311-driven changes in city government; preventing the gradual decline of shared service centers; developing a conceptual framework for evaluating public sector transformation in the digital era; the impact of cultural differences on cloud computing ecosystems in YOU.S. and China; an examination of the impact of service climate on service productivity in the organizational context; information systems facilitating groundwater sustainability management; keeping electronic medical records secure and portable; the emerging role of robotics in home health care; information quality assessment technique to evaluate the information exchange; boundary dialogues in user-centric innovation; towards a meditation brain state model using electroencephalographic data; design method requirements for agile system of systems; design and evaluation of a socially enhanced classroom blog to pomote student learning in higher education; it is not all about the music: user preference for musicians on facebook; knowledge seeking and knowledge sharing in a nonprofit organizational partner network: a social network analysis; the mediating role of adaptive personalization in online shopping; exploring the temporal nature of sociomateriality from a work system perspective; sociomateriality as radical ontology; information security management; meeting global business information requirements with enterprise resource planning; knowledge sharing in social networking sites for e-collaboration; applying cognitive principles of similarity to data integration-the case of SIAM; reference model in design science research to gather and model information; impact of online content on attitudes and buying intentions; prospect theory and information security investment decisions; using domain knowledge to facilitate cyber security analysis; conceptualizing data security threats and countermeasures in the E-Discovery process with misuse cases; an empirical analysis of an individual's 360 degree protection from file and data loss; analysis of eBook lending: a game-theory approach; facilitating consumers' evaluation of experience goods and the benefits for vendors; three-factor Model vs. Two-Factor Model; automating enterprise architecture documentation using an enterprise service bus; the influence of role models on students' decisions to pursue the IS major; teaching ''people networking'' skills for CIS students; a case of bias in teaching, grading, and plagiarism; a relational view of accounting information sharing; reporting capabilities, financial closing time and effects on cost of equity capital; reflecting on the role of IT and IT research in healthcare; social media around the world; understanding the effects of freeriding in team dynamics; password policy effects on entropy and recall: research in progress; the role of individual characteristics on insider abuse intentions; building a methodology to assess the e-Government transformation success; optimizing freight delivery for less-than-truckload transportation; the influence of perceived information and network characteristics on the attitude towards information overload; information disclosure and generational differences in social network sites; trasactive memory systems virtual team training model; the case of open government and teaching and learning in a virtual world. 18th Americas Conference on Information Systems 2012, AMCIS 2012, Volume 3  ",Strategic alignment
659,A model-based approach to constructing corporate bond portfolios,"In this article, we develop a model-based approach to constructing investment grade and high yield corporate bond portfolios that outperform their respective prevalent benchmark indices and popular ETFs with better risk-return profiles, i.e., higher returns with lower or similar risk. We achieve outperformance after controlling for credit risk, duration risk, and downside risk. We also achieve outperformance using relatively smaller, more realistic portfolios. We conduct a number of variations of the approach to ensure results are robust. These model portfolios can be potentially converted into new fixed income indices or ETFs. Our model-based approach utilizes Moody's Analytics' EDF credit measures and FVS valuation framework as powerful tools to exploit relative value in the bond market. Our model-based approach suggests that EDF measures and FVS can add value to the process of investing in corporate bonds. Portfolio managers and analysts can use these tools, in addition to fundamental analysis, for early warning screening, identifying problematic names and sectors, and spotting relative value opportunities. A model-based approach to constructing corporate bond portfolios  ",Risk management
660,Learning/curriculum management systems (LCMS): Emergence of a new wave in medical education,"For millennia, classroom teaching prevailed as the paradigm of learning. Formal learning was scarce, and resources centered on the availability of the educator, rather than the learner. The preliminary applications of eLearning were therefore paralleled with the old classroom model and its associated characteristics. The old method of learning, while comprehensible, does not achieve the true potential of the new paradigm of the Internet as a learning medium. The first wave of e-learning was focused on solutions associated with administering classroom training, i.e., the Learning Management System (LMS). However, the discipline has developed and evolved into a second wave of more sophisticated e-learning, which requires an e-Learning Curriculum Management System (LCMS) to fulfil the needs of personalized and adaptive e-learning. Web-based LCMSs are increasingly utilized across medical universities on an international scale. The LCMS can aid in the delivery of medical education, broaden the capacity for tracking and reporting of teaching and learning across an institution, simplify and automate administrative and supervisory tasks, and facilitate institutional accreditation. These systems are equipped with business intelligence tools to analyze data and create reports to promote curriculum governance and education delivery. The most sophisticated systems additionally incorporate version control, multiple authors, and project management. LCMS comprises a multiuser environment where developers may create, store, reuse, manage, and deliver digital learning content from a central object repository. LCMS solutions are ideally suited to create content-centric learning strategies, supporting multiple methods for gathering and organizing content, leveraging content for multiple purposes, and achieving educational goals and objectives. Thus, LCMS in medical education, with focus on the learner, is a significant breakthrough in eLearning. This new wave of eLearning in medicine is anticipated to alter the landscape of education in favour of learners and their dynamically changing needs. Learning/curriculum management systems (LCMS): Emergence of a new wave in medical education CMS; Curriculum management system; ELearning; LCMS; Learning management system; Learning/curriculum management system; LMS Accreditation; Computer science; Curricula; Economics; Management science; Medical education; Project management; School buildings; Teaching; Technical presentations; CMS; LCMS; Learning management system; Learning/curriculum management system; LMS; E-learning",Monitoring and control
661,Efficient data structures for risk modelling in portfolios of catastrophic risk using mapreduce,"The QuPARA Risk Analysis Framework is an analytical framework implemented using MapReduce and designed to answer a wide variety of complex risk analysis queries on massive portfolios of catastrophic risk contracts. In this paper, we present data structure improvements that greatly accelerate QuPARA's computation of Exceedance Probability (EP) curves with secondary uncertainty. © The Authors. Published by Elsevier B.V. Efficient data structures for risk modelling in portfolios of catastrophic risk using mapreduce Catastrophic risk analysis; Data structures; Hadoop; Insurance; Mapreduce; Portfolio risk analytics; Reinsurance Data structures; Insurance; Query processing; Risk assessment; Risks; Catastrophic risks; Hadoop; Map-reduce; Portfolio risks; Reinsurance; Risk analysis",Risk management
662,Financial modeling with crystal ball and excel,"Updated look at financial modeling and Monte Carlo simulation with software by Oracle Crystal Ball This revised and updated edition of the bestselling book on financial modeling provides the tools and techniques needed to perform spreadsheet simulation. It answers the essential question of why risk analysis is vital to the decision-making process, for any problem posed in finance and investment. This reliable resource reviews the basics and covers how to define and refine probability distributions in financial modeling, and explores the concepts driving the simulation modeling process. It also discusses simulation controls and analysis of simulation results. The second edition of Financial Modeling with Crystal Ball and Excel contains instructions, theory, and practical example models to help apply risk analysis to such areas as derivative pricing, cost estimation, portfolio allocation and optimization, credit risk, and cash flow analysis. It includes the resources needed to develop essential skills in the areas of valuation, pricing, hedging, trading, risk management, project evaluation, credit risk, and portfolio management. • Offers an updated edition of the bestselling book covering the newest version of Oracle Crystal Ball • Contains valuable insights on Monte Carlo simulation-an essential skill applied by many corporate finance and investment professionals • Written by John Charnes, the former finance department chair at the University of Kansas and senior vice president of global portfolio strategies at Bank of America, who is currently President and Chief Data Scientist at Syntelli Solutions, Inc. Risk Analytics and Predictive Intelligence Division (Syntelli RAPID) Engaging and informative, this book is a vital resource designed to help you become more adept at financial modeling and simulation. © 2012 by John Charnes. Financial modeling with crystal ball and excel  ",Strategic alignment
663,"International Conference on Information Systems, ICIS 2012, Volume 4","The proceedings contain 308 papers. The special focus in this conference is on Information Systems. The topics include: Heterogeneity in IT landscapes and monopoly power of firms; community ecology for innovation concept; peer influence in a very large social network; assessing value in an online network of products; optimal pricing with positive network effects; research on viral marketing propagating oriented to marketing context; repurchase intentions of information technology; a multi-level analysis of the impact of health information technology on hospital performance; institutional logics of adoption and use of health information technology; post-acceptance of electronic medical records; socio-technical attachments and IT change; comparing the predictive ability of PLS and covariance models; a note of caution on covariance-equivalent models in information systems; differential effects of omitting formative indicators; being innovative about service innovation; estimating demand for applications in the new mobile economy; continuance of professional social networking sites; role of online social networks in job search by unemployed individuals; knowledge contribution in online network of practice; the dual role of IS specificity in governing software as a service; relational antecedents of information visibility in value networks; summarization of corporate risk factor disclosure through topic modeling; a hybrid method for cross-domain sentiment classification using multiple sources; designing intelligent expert systems to cope with liars; IS offshore project risk, contracts and team structure; vendor and client project managers; technology desirability; the role of green IS in developing eco-effectiveness; the impact of IT-enabled manufacturing capabilities on plant profitability; inter-industry IT spillovers after the dot-com bust; effects of undesired online video advertising choice on user behavior and attitude; unfulfilled obligations in recommendation agent use; understanding the formation of trust in IT artifacts; user decisions among digital piracy and legal alternatives for film and music; a probabilistic generative model for latent business networks mining; the effects of information disclosure policy on the diffusion of security attacks; interactivity of social media and online consumer behavior; strategies for establishing service oriented design in organizations; alternative genres of is research; ICT standardization strategies and service innovation in health care; technical support and IT capacity demand; online social networks as a source and symbol of stress; the effects of social network usage on organizational identification; using the Kano model to identify attractive user-interface software components; developing a theory of multitasking behavior; an empirical investigation of information systems departments' configurations; exploring the adaptation of enterprise systems implementation methodology; out of the box and onto the stage; an organizing vision perspective on green IS development; the emergence of sustainability as the new dominant logic; social influence and defaults in peer-to-peer lending networks; multi-screen strategy for selling mobile content to customers; the emergent role of IT capabilities; reducing price uncertainty through demand side management; factors affecting perceived persuasiveness of a behavior change support system; surfacing schemas from firms' informational engagements; optimal design of consumer review systems; measuring product type with dynamics of online product review variance; different effects on retailers of online product reviews; measurement of multitasking with focus shift analysis; an exploration of the impact of information privacy invasion; measuring mobile users' concerns for information privacy; examining the rationality of location data disclosure through mobile devices; towards a design theory for software project risk management systems; identifying optimal IT portfolios to promote healthcare quality; designing a web-based application to support peer instruction for very large groups; principles for knowledge creation in collaborative design science research; estimating optimal recommendation set sizes for individual consumers; predicting participation in social media sites by analyzing user participation patterns; a hidden Markov model for conversion rate dynamics in online retail; flow experience and continuance intention toward online learning; design and evaluation of a didactical service blueprinting method for large scale lectures; knowledge sharing and maturation in circles of trust; a model-driven method for the systematic literature review of qualitative empirical research; procedurally transparent design science research; advancing task elicitation systems - an experimental evaluation of design principles; the knowing-doing gap in research methods and what we should do about it; efficient and flexible management of enterprise information systems; granularity metrics for IT services; from observed outcomes to measurable performance; the effect of third party investigation on pay-per-click advertising; identifying factors of e-government acceptance - a literature review; effects of cultural cognitive styles on users' evaluation of website complexity; a sustainability model of green IT initiatives; towards a typology of green IS strategies; digital access, political networks and the diffusion of democracy; the effect of free access on the diffusion of scholarly ideas; an empirical examination of cultural biases in interpersonal economic exchange; effect of business intelligence and IT infrastructure flexibility on organizational agility; news recommender systems with feedback; a multi-theoretical framework for social network-based recommendation; improving coverage of design in information systems education; perceived IT security risks of cloud computing: conceptualization and scale development; capability leapfrogging in the Japanese IT services industry; predatory coercion in social media and protection of children online - a critical discourse analysis approach; encouraging collaborative idea-building in enterprise-wide innovation challenges; analyzing the impact of social media strategies on viewer engagement; supporters in deed - studying online support provision from the perspective of social capital; membership overlap and inter-community collaboration; developing customer agility through information management; keyword search patterns in sponsored link advertisements; the hidden effects of opening bids in online auctions; increasing dynamic capabilities through virtualized grid-in-cloud solutions; design principles for heterogeneity decisions in enterprise architecture management; IT stereotyping and the CEO-CIO headlock; exploring the role of un-enacted projects in IT project portfolio management; the difference of determinants of mobile data services' adoption and continuance - a longitudinal study; a study of effectiveness and satisfaction level of cloud CRM users in Taiwan's enterprises; investigating intelligent agents in a 3D virtual world; an empirical study of a two-sided model of fraudulent exchange; perceptual and conceptual effects of incidental exposure to web ads; self-disclosure on online social networks; critical success factors of location-based services; affect and online privacy concerns; the impact of training and social norms on information security compliance; security management in cross-organizational settings; the effect of customers' emotion on service recovery strategy in IT service failures; the influence of IS affordances on work practices in health care; multi-level knowledge transfer in software development outsourcing projects; organizational performance with environmental knowledge intensity; process visibility - towards a conceptualization and research themes; towards an oil crisis early warning system based on absolute news volume; constructing workflow models from agent profiles; formulating effective coordination strategies in agile global software development teams; the transmission of control in information systems projects; the structuration of task-oriented communication in innovative virtual teams and social media in the entrainment of contention to innovation. International Conference on Information Systems, ICIS 2012, Volume 4  ",Value management
664,Accounting for secondary uncertainty: Efficient computation of portfolio risk measures on multi and many core architectures,"Aggregate Risk Analysis is a computationally intensive and a data intensive problem, thereby making the application of high-performance computing techniques interesting. In this paper, the design and implementation of a parallel Aggregate Risk Analysis algorithm on multi-core CPU and many-core GPU platforms are explored. The efficient computation of key risk measures, including Probable Maximum Loss (PML) and the Tail Value-at-Risk (TVaR) in the presence of both primary and secondary uncertainty for a portfolio of property catastrophe insurance treaties is considered. Primary Uncertainty is the the uncertainty associated with whether a catastrophe event occurs or not in a simulated year, while Secondary Uncertainty is the uncertainty in the amount of loss when the event occurs. A number of statistical algorithms are investigated for computing secondary uncertainty. Numerous challenges such as loading large data onto hardware with limited memory and organising it are addressed. The results obtained from experimental studies are encouraging. Consider for example, an aggregate risk analysis involving 800,000 trials, with 1,000 catastrophic events per trial, a million locations, and a complex contract structure taking into account secondary uncertainty. The analysis can be performed in just 41 seconds on a GPU, that is 24x faster than the sequential counterpart on a fast multi-core CPU. The results indicate that GPUs can be used to efficiently accelerate aggregate risk analysis even in the presence of secondary uncertainty. © 2013 ACM. Accounting for secondary uncertainty: Efficient computation of portfolio risk measures on multi and many core architectures aggregate risk analysis; GPU computing; parallel computing; primary and secondary uncertainty; risk analytics; risk management Algorithms; Digital storage; Disasters; Loading; Parallel architectures; Parallel processing systems; Program processors; Risk analysis; Risk management; Aggregate risk; Catastrophe insurance; Design and implementations; Efficient computation; GPU computing; High-performance computing; Many-core architecture; primary and secondary uncertainty; Aggregates",Monitoring and control
665,Critical space prediction analysis in building construction-an application of dea-da and topsis,"Critical Space had been defined as its slack space is equal to zero. This Study offers the further and improved following definition of critical space: ""Critical Space is in whom conflicts are not allowed to occur due to significant productivity loss once occurred."" The definition is more accurate and helpful to construction management. The space with zero slack space does not necessarily because productivity loss. The purpose of this study is to calculate and find the critical space by using logics of space structures then to avoid conflicts occurred in critical space. The quantitative parameters including integration value and control value derived form Space Syntax and other three parameters including unit blocking rate, vertical direction flow and exit direction flow derived form identities form building construction can be used to the DEA-DA with TOPSIS model to predict critical space. DEA-DA can be modeled to predict financial crisis of construction companies and TOPSIS is a multi-criteria decision analysis method for finding ideal solutions. Among three prediction models featuring in this study, only DEA-EA model with TOPSIS method can be defined highly correlated as correlation coefficient of it is larger than 0.8. ©, 2014, Chinese Institute of Civil and Hydraulic Engineering. All right reserved. Critical space prediction analysis in building construction-an application of dea-da and topsis Critical space; DEA-DA; Space syntax; TOPSIS Construction industry; Forecasting; Predictive analytics; Productivity; Project management; Syntactics; Construction management; Correlation coefficient; Critical spaces; DEA-DA; Multi-criteria decision analysis; Quantitative parameters; Space syntax; TOPSIS; Construction",Strategic alignment
666,Portfolio mining,"Portfolio mining facilitates the creation of actionable knowledge, catalyzes innovations, and sustains research communities. © 2012 IEEE. Portfolio mining collaboration analysis; data mining; DIA2; discovery analytics; education; investment analysis; portfolio mining; search; temporal modeling Data mining; Education; Risk assessment; Collaboration analysis; DIA2; discovery analytics; Investment analysis; search; Temporal modeling; Investments",Strategic alignment
667,"Proceedings - 2014 2nd International Symposium on Computational and Business Intelligence, ISCBI 2014","The proceedings contain 25 papers. The topics discussed include: optimal control of time varying linear systems: neural networks; multi-orientation text detection by skeletonization (MOTDS); multiple-stage classification of human poses while watching television; multi-objective differential evolution based optimization of risk budgeted global asset allocation portfolios; hiding sound in image by k-LSB mutation using cuckoo search; solving the permutation flow shop problem with firefly algorithm; using multi-agent system to solve distributed constraint optimization problems; relation between foreign institutional investments and macro-economic factors - a case study; risk assessment for environmentally sustainable business planning and optimization based on the identification of regional meteorology pattern; and development and performance evaluation of support vector machine classifiers for Indian domestic tourists. Proceedings - 2014 2nd International Symposium on Computational and Business Intelligence, ISCBI 2014  ",Risk management
668,"International Conference on Information Systems, ICIS 2012, Volume 2","The proceedings contain 308 papers. The special focus in this conference is on Information Systems. The topics include: Heterogeneity in IT landscapes and monopoly power of firms; community ecology for innovation concept; peer influence in a very large social network; assessing value in an online network of products; optimal pricing with positive network effects; research on viral marketing propagating oriented to marketing context; repurchase intentions of information technology; a multi-level analysis of the impact of health information technology on hospital performance; institutional logics of adoption and use of health information technology; post-acceptance of electronic medical records; socio-technical attachments and IT change; comparing the predictive ability of PLS and covariance models; a note of caution on covariance-equivalent models in information systems; differential effects of omitting formative indicators; being innovative about service innovation; estimating demand for applications in the new mobile economy; continuance of professional social networking sites; role of online social networks in job search by unemployed individuals; knowledge contribution in online network of practice; the dual role of IS specificity in governing software as a service; relational antecedents of information visibility in value networks; summarization of corporate risk factor disclosure through topic modeling; a hybrid method for cross-domain sentiment classification using multiple sources; designing intelligent expert systems to cope with liars; IS offshore project risk, contracts and team structure; vendor and client project managers; technology desirability; the role of green IS in developing eco-effectiveness; the impact of IT-enabled manufacturing capabilities on plant profitability; inter-industry IT spillovers after the dot-com bust; effects of undesired online video advertising choice on user behavior and attitude; unfulfilled obligations in recommendation agent use; understanding the formation of trust in IT artifacts; user decisions among digital piracy and legal alternatives for film and music; a probabilistic generative model for latent business networks mining; the effects of information disclosure policy on the diffusion of security attacks; interactivity of social media and online consumer behavior; strategies for establishing service oriented design in organizations; alternative genres of is research; ICT standardization strategies and service innovation in health care; technical support and IT capacity demand; online social networks as a source and symbol of stress; the effects of social network usage on organizational identification; using the Kano model to identify attractive user-interface software components; developing a theory of multitasking behavior; an empirical investigation of information systems departments' configurations; exploring the adaptation of enterprise systems implementation methodology; out of the box and onto the stage; an organizing vision perspective on green IS development; the emergence of sustainability as the new dominant logic; social influence and defaults in peer-to-peer lending networks; multi-screen strategy for selling mobile content to customers; the emergent role of IT capabilities; reducing price uncertainty through demand side management; factors affecting perceived persuasiveness of a behavior change support system; surfacing schemas from firms' informational engagements; optimal design of consumer review systems; measuring product type with dynamics of online product review variance; different effects on retailers of online product reviews; measurement of multitasking with focus shift analysis; an exploration of the impact of information privacy invasion; measuring mobile users' concerns for information privacy; examining the rationality of location data disclosure through mobile devices; towards a design theory for software project risk management systems; identifying optimal IT portfolios to promote healthcare quality; designing a web-based application to support peer instruction for very large groups; principles for knowledge creation in collaborative design science research; estimating optimal recommendation set sizes for individual consumers; predicting participation in social media sites by analyzing user participation patterns; a hidden Markov model for conversion rate dynamics in online retail; flow experience and continuance intention toward online learning; design and evaluation of a didactical service blueprinting method for large scale lectures; knowledge sharing and maturation in circles of trust; a model-driven method for the systematic literature review of qualitative empirical research; procedurally transparent design science research; advancing task elicitation systems - an experimental evaluation of design principles; the knowing-doing gap in research methods and what we should do about it; efficient and flexible management of enterprise information systems; granularity metrics for IT services; from observed outcomes to measurable performance; the effect of third party investigation on pay-per-click advertising; identifying factors of e-government acceptance - a literature review; effects of cultural cognitive styles on users' evaluation of website complexity; a sustainability model of green IT initiatives; towards a typology of green IS strategies; digital access, political networks and the diffusion of democracy; the effect of free access on the diffusion of scholarly ideas; an empirical examination of cultural biases in interpersonal economic exchange; effect of business intelligence and IT infrastructure flexibility on organizational agility; news recommender systems with feedback; a multi-theoretical framework for social network-based recommendation; improving coverage of design in information systems education; perceived IT security risks of cloud computing: conceptualization and scale development; capability leapfrogging in the Japanese IT services industry; predatory coercion in social media and protection of children online - a critical discourse analysis approach; encouraging collaborative idea-building in enterprise-wide innovation challenges; analyzing the impact of social media strategies on viewer engagement; supporters in deed - studying online support provision from the perspective of social capital; membership overlap and inter-community collaboration; developing customer agility through information management; keyword search patterns in sponsored link advertisements; the hidden effects of opening bids in online auctions; increasing dynamic capabilities through virtualized grid-in-cloud solutions; design principles for heterogeneity decisions in enterprise architecture management; IT stereotyping and the CEO-CIO headlock; exploring the role of un-enacted projects in IT project portfolio management; the difference of determinants of mobile data services' adoption and continuance - a longitudinal study; a study of effectiveness and satisfaction level of cloud CRM users in Taiwan's enterprises; investigating intelligent agents in a 3D virtual world; an empirical study of a two-sided model of fraudulent exchange; perceptual and conceptual effects of incidental exposure to web ads; self-disclosure on online social networks; critical success factors of location-based services; affect and online privacy concerns; the impact of training and social norms on information security compliance; security management in cross-organizational settings; the effect of customers' emotion on service recovery strategy in IT service failures; the influence of IS affordances on work practices in health care; multi-level knowledge transfer in software development outsourcing projects; organizational performance with environmental knowledge intensity; process visibility - towards a conceptualization and research themes; towards an oil crisis early warning system based on absolute news volume; constructing workflow models from agent profiles; formulating effective coordination strategies in agile global software development teams; the transmission of control in information systems projects; the structuration of task-oriented communication in innovative virtual teams and social media in the entrainment of contention to innovation. International Conference on Information Systems, ICIS 2012, Volume 2  ",Value management
669,"International Conference on Information Systems, ICIS 2013, Volume 3","The proceedings contain 320 papers. The special focus in this conference is on Information Systems. The topics include: Computing requirements in open source software projects; a comparison of nonlinear pricing preference models for digital services; a computational approach to detecting and assessing sustainability-related communities in social media; a conceptualisation of management situations relevant for collaborative is research projects; a conceptualization of complexity in IS-driven organizational transformations; information security collective efficacy and vulnerability; a cross-sectional and temporal analysis of information consumption on twitter; a measurement model for investigating digital natives and their organisational behaviour; a model of product design and information disclosure investments; a problem-solving perspective on governance and product design in open source software projects; a qualitative approach to examine technology acceptance; a sensemaking approach to tablet users' accommodating practices; a systems approach to countermeasures in credibility assessment interviews; aligning business and it strategies in multi-business organizations; an empirical examination of users' information hiding in a crowdfunding context; an empirical investigation of a user-centric typology of innovation for evolving information systems products; an exploration of group information security compliance; an information quality evaluation framework of object tracking systems; an ontology artifact for information systems sentiment analysis; visualising perspectives of business and information systems; attention trade-off between two types of user contributions; the effects of user- and marketer-generated content on purchase decision processes; boundary spanning through enterprise social software; investment management systems and new institutional logics for regulatory compliance; bundling of software products and services to fight against piracy; capturing multi-stakeholder needs in customer-centric cloud service design; an empirical analysis of the impact of formal controls in strategic outsourcing contracts; cloud computing providers' unrealistic optimism regarding it security risks; cognitive biases in new technology appropriation; collective intelligence in market-based social decision making; comparing peer influences in large social networks - an empirical study on caller ringback tone; computer mediated social ties as predictors of SNS usage continuance; conceptualizing unexpected events in IT projects; coordinating one-to-many concurrent negotiation for service provision; counteracting the negative effect of form auto-completion on the privacy calculus; creating engaging student to student engagement in an online class; unpacking the building blocks of crowdsourcing collaboration processes; defining boundaries of web ads' perceptual fluency effect; designing business models for platform as a service; designing e-services for an ageing society; designing the modern ICT curriculum; developing a procedure model for business process standardization; developing and measuring IS scales using item response theory; digital traces of information systems; discursive meaning creation in crowdfunding; driving innovation and knowledge management using crowdsourcing; interplay of EA design factors, strategy types, and environments; effective sentiment analysis of corporate financial reports; eliminating failure by learning from it -systematic review of is project failure; navigating the ambiguity of materiality through collective design; enhancing shared understanding in collaborative online shopping; evaluating a new pattern development process for interface design; evaluating the quality of technology-mediated learning services; evolving the modular layered architecture in digital innovation; executive doctorate programs and the role of the information systems discipline; exploring foundations for using simulations in is research; exploring the customer perspective of agile development; exploring the effect of arousal and valence on mouse interaction; fit between knowledge transfer complexity and media capability; the impact of information pricing strategy on the pattern and effectiveness of word-of-mouth via social media; from open source to commercial software development - the community based software development model; some thoughts on technology implementation practice; a point of concern to IS use models' validity; how channel choice and service failure influence customer satisfaction; how customers trust mobile marketing applications; how E-consumers integrate diverse recommendations from multiple sources; how team cohesion leads to attitude change in the context of ERP learning; understanding reconfigurations of information systems and organizations in the Norwegian health sector; identifying patterns of idea diffusion in innovator networks; the missing link between it affordances and institutions for better health care in developing countries; impacts of creolization on trust and knowledge sharing in it-enabled global services sourcing; impacts of IT acceptance and resistance behaviors; implications of monitoring mechanisms on bring your own device (BYOD) adoption; improved medication compliance through health IT; improving medical decision-making using electronic health record systems; improving the semantics of conceptual-modeling grammars; inciting networks effects through platform authority; individual values for protecting identity in social networks; individualization of information systems - analyzing antecedents of IT consumerization behavior; information architecture for healthcare organizations; its antecedents and mediating effects on security compliant behavior; information system strategy for opportunity discovery and exploitation; information systems development outsourcing; institutionalization and the effectiveness of enterprise architecture management; internet use and well-being of young adults; investigating mobile messaging in healthcare organizations; different firm types, different alignment configurations; developing valid measures through CFA-based MTMM analysis; IT-enabled performative spaces in gender segregated work; knowledge contribution motivators - an expectation-confirmation approach; legitimating user participation in mature organisations- exploring social media adoption in a financial services organization; an analysis of the impact of mobile micro-blogging on communication and decision-making; mobile app portfolio management and developers' performance; mobile applications and access to personal information; mobile commerce in the new tablet economy; multi-agent based information systems for patient coordination in hospitals; network analysis for predicting academic impact; network diversity and social cohesion in creative performance; on the importance of organisational culture and structure in business process maturity; one-way mirrors and weak-signaling in online dating; online and offline sales channels for enterprise software; online health information use by disabled people; optimal information technology service pricing and capacity decision under service-level agreement; optimal location of charging stations in smart cities; organizational learning and the error of fixed strategies in IT innovation investment evaluation; pathways to value from business analytics; patients' adherence to health advice on virtual communities; peer-based quality assurance in information systems development; their effect on extracurricular work behaviors among IT professionals; success and reciprocity on crowdfunding platforms; predictive validity and formative measurement in structural equation modeling; preventive adoption of information security behaviors; understanding factors contributing to the escalation of software maintenance costs; privacy controls and content sharing patterns of online social network users; psychological empowerment of patients with chronic diseases; purposive selection and the quality of qualitative IS research; quality-adjusted consumer surplus for online labor markets with asymmetric information; quantifying the dynamic sales impact of location-based mobile promotion technologies; is-enabled political decision support with scenario analyses for the substitution of fossil fuels; information technologies and the possibility for imagination; roles of trust in privacy assurance and perceived disease severity on personal health information disclosure; selecting project management methodologies for business intelligence projects - a value based approach; service failure complaints identification in social media; role of social distance and social norms in online referral incentive systems; social media and citizen social movement process for political change; software developers' online chat as an intra-firm mechanism for sharing ephemeral knowledge; sources of power and CIO influence and their impact; statistical modeling of nanotechnology knowledge diffusion networks; systems of transfiguration and the adoption of IT under surveillance; taking a new-generation manager perspective to develop interface designs; task-technology fit for low-literate consumers; team adaptability in agile information systems development; technology, interoperability, and provision of public safety networks; the architecture of generativity in a digital ecosystem; the design of a network-based model for business performance prediction; the design of a tangible user interface for a real-time strategy game; the differences between recommender technologies in their impact on sales diversity; the dynamics of IS adaptation in multinational corporations; the emergence of social media as boundary objects in crisis response; the establishment of social IT sourcing organizations. International Conference on Information Systems, ICIS 2013, Volume 3  ",Strategic alignment
670,"18th Americas Conference on Information Systems 2012, AMCIS 2012, Volume 4","The proceedings contain 572 papers. The special focus in this conference is on Information Systems. The topics include: Health diagnosis of communities of practices (CoPs); open source alternatives for business intelligence; identifying business process activity mappings by optimizing behavioral similarity; hanging with the right crowd; a taxonomy of web-based inbound open innovation initiatives; database intrusion detection: defending against the insider threat; an empirical study of the GIGO axiom in satisficing decisions; analysis of probabilistic news recommender systems; the influence of technology characteristics on privacy calculus; instant messaging privacy in the clouds; towards a component-based description of business models; two-sided cybermediary platforms; an integrative analysis of transactional e-government web usage; facebook usage in government-a case study of information content; deriving business value from asymmetric penalty-reward perspectives of IS users; create attention to attract attention-viral marketing of digital music in social networks; towards a framework for transforming business models into business processes; factors affecting perceived satisfaction with a BPM tool; hypercompetition in the Erp industry; structural flaws in the ethics of technology; ethical considerations for virtual worlds; personality, gender and careers in information technology; on IT control weaknesses in auditors' reports on internal control; effect of the SOX act on IT governance; the influence of general sustainability attitudes and value congruence on consumer behavior; user privacy in mobile advertising; system learning of user interactions; deploying mission critical learning management system using open source software; theorizing the dual role of information technology in technostress research; boundary spanning in business process management; ERP usability and the mangle of practice; direct manipulation tablet apps for education; comparing graphical and tangible user interfaces for a tower defense game; visual perception model for online target marketing; eye movements, perceptions, and performance; individual relationships with technology; internal audit function response to ERP systems implementation; auditing journal entries using self-organizing map; consumer perceptions of the adoption of electronic personal health records; investigating the reciprocal relationships within health virtual communities; designing and visualising healthcare delivery systems; how multinational firms use IT to manage their global operations; conflict, value diversity, and performance in virtual teams; mobile ICT and knowledge sharing in underserved communities; agile decision making framework to support mobile microloans for unbanked customers; the analysis of the telecommunications industry in Thailand; the business value of knowledge management; providing information feedback to bidders in online multi-unit combinatorial auctions; spatial modeling using agents; using probabilistic ontologies for video exploration; new directions, new challenges, and new understandings; function-based categorization of online product information types; exploring antecedents of habit on social network service; personality correlation analysis and applications in social networks; identifying experts in virtual forecasting communities; black males in IT higher education in the USA; technology features, empowering perceptions, and voicing behavior on microblog; information security policy compliance; a preliminary taxonomy for software failure impact; an examination of the success of post-merger IT integration; an analysis of and perspective on the information security maturity model; geographic information systems and the nonprofit sector; effectiveness of shallow hierarchies for document stores; a methodology for the development of web-based information systems; balanced resource allocation; demand response in smart grids; decision support for electric vehicle charging; the expectations for faculty in Latin America; mastering the social IT/Business Alignment Challenge; supply chain resource planning systems; towards a research framework for VLBA operation management; integrating enterprise system's 3rd wave into IS curriculum; a two-tier data-centric framework for flexible business process management; engagement in online communities; organisational semiotics methods to assess organisational readiness for internal use of social media; social media in the workplace; economics of pair programming revisited; social traps of agile methods; metadata exploitation in large-scale data migration projects; collaboratively assessing information quality on the web; reputation management in social commerce communities; E-Business adoption research; a preliminary information theory of difference; replacement of project manager during IT projects-a research agenda; a simulation study of project management and collaborative information technologies; the role of business information visualization in knowledge creation; effects of narrative structure and salient decision points in role playing games; adoption of pervasive e-health solutions; security practices and regulatory compliance in the healthcare industry; the role of demographic characteristics in health care strategic security planning; tailoring software process capability/maturity models for telemedicine systems; understanding dynamic collaboration in teleconsultation; the pathway to enterprise mobile readiness; investigating the role of social media and social capital; exploring 311-driven changes in city government; preventing the gradual decline of shared service centers; developing a conceptual framework for evaluating public sector transformation in the digital era; the impact of cultural differences on cloud computing ecosystems in YOU.S. and China; an examination of the impact of service climate on service productivity in the organizational context; information systems facilitating groundwater sustainability management; keeping electronic medical records secure and portable; the emerging role of robotics in home health care; information quality assessment technique to evaluate the information exchange; boundary dialogues in user-centric innovation; towards a meditation brain state model using electroencephalographic data; design method requirements for agile system of systems; design and evaluation of a socially enhanced classroom blog to pomote student learning in higher education; it is not all about the music: user preference for musicians on facebook; knowledge seeking and knowledge sharing in a nonprofit organizational partner network: a social network analysis; the mediating role of adaptive personalization in online shopping; exploring the temporal nature of sociomateriality from a work system perspective; sociomateriality as radical ontology; information security management; meeting global business information requirements with enterprise resource planning; knowledge sharing in social networking sites for e-collaboration; applying cognitive principles of similarity to data integration-the case of SIAM; reference model in design science research to gather and model information; impact of online content on attitudes and buying intentions; prospect theory and information security investment decisions; using domain knowledge to facilitate cyber security analysis; conceptualizing data security threats and countermeasures in the E-Discovery process with misuse cases; an empirical analysis of an individual's 360 degree protection from file and data loss; analysis of eBook lending: a game-theory approach; facilitating consumers' evaluation of experience goods and the benefits for vendors; three-factor Model vs. Two-Factor Model; automating enterprise architecture documentation using an enterprise service bus; the influence of role models on students' decisions to pursue the IS major; teaching ""people networking"" skills for CIS students; a case of bias in teaching, grading, and plagiarism; a relational view of accounting information sharing; reporting capabilities, financial closing time and effects on cost of equity capital; reflecting on the role of IT and IT research in healthcare; social media around the world; understanding the effects of freeriding in team dynamics; password policy effects on entropy and recall: research in progress; the role of individual characteristics on insider abuse intentions; building a methodology to assess the e-Government transformation success; optimizing freight delivery for less-than-truckload transportation; the influence of perceived information and network characteristics on the attitude towards information overload; information disclosure and generational differences in social network sites; trasactive memory systems virtual team training model; the case of open government and teaching and learning in a virtual world. 18th Americas Conference on Information Systems 2012, AMCIS 2012, Volume 4  ",Strategic alignment
671,Including human factors in business ecosystem network to expose & predict budgetary risk,"During a project’s budget life cycle there is always the risk of expected income not materializing, or expected expenses differing. Most large organizations have a complexity of departments, where one department consumes another’s product, or multiple departments compete with one another by providing a similar product using different technologies. In such an interlocked environment, budgeting risks may propagate across related projects and impact many seemingly unrelated departments. Being able to predict the risks and expose them to all related parties is crucial. Some risks are truly unpredictable and therefore beyond the scope of our discussion, but some are due to deliberate, and predictable, decisions made by people. Organizations typically use ERP systems to manage their budgets, however these systems are not able to manage this kind of risks. This paper seeks to show an approach to manage such risks by creating a business ecosystem network model to present the complex relationships among an organization from a budget management perspective. By incorporating human factors the network will allow analytics on decision making patterns for key contributors and thus predict budgetary risk propagation on both the individual and aggregate level. © 2013 by Taylor & Francis Group, LLC. Including human factors in business ecosystem network to expose & predict budgetary risk Budget allocation optimization; Business ecosystem; Project management ",Financial management
672,On the basics for simulation of feedback-based stock trading strategies: An invited tutorial session,"This paper provides an overview of our CDC tutorial session covering the basics of simulation and performance evaluation associated with stock trading via feedback control methods. The specific trading algorithms which we describe fall under the umbrella of ""pure technical analysis"" in that they are model-free with no parametrization of the stock-price process assumed. True to technical analysis, we adopt the point of view that the stock price p(t) is an external input with no predictive model for its evolution. The feedback controller adapts the investment level I(t) based on the evolution of the trading gains or losses over time. In the introductory talk, it is explained how this point of view ""opens doors"" for new research contributions from the control community. The simulations which we consider are of two types: In some cases, the controller's performance is studied using synthetic classes of stock prices such as Geometric Brownian Motion. In other cases, real historical prices are brought into play. We refer to such a real-price simulation as a ""backtest."" Once we cover the trading mechanics, the notion of ""benchmark price classes,"" data acquisition and coding of algorithms, the focal point becomes performance evaluation. That is, with g(t) denoting cumulative gains or losses, we describe a number of metrics which are used to evaluate the performance associated with the trajectory pair (I(·), g(·)). Whereas the first half of the tutorial concentrates on trading a single stock, the last half addresses multi-asset portfolios, educational aspects and the notion of trading competitions. © 2013 IEEE. On the basics for simulation of feedback-based stock trading strategies: An invited tutorial session  Benchmarking; Commerce; Controllers; Costs; Data acquisition; Feedback control; Financial markets; Investments; Predictive analytics; Control community; Educational aspects; Feedback control methods; Feedback controller; Geometric Brownian motion; Parametrizations; Predictive modeling; Technical analysis; Electronic trading",Monitoring and control
673,Banking technology innovations in India: Enhancing customer value and satisfaction,"Background/Objectives: This paper studies the development in information and technology enable banks in value added services to be effective in satisfying customer needs by adopting new innovative solutions in banking services to meet perceived value and expectations. Methods/Statistical Analysis: To study the perceptions of bank customers, bank employees regarding the use of various products/services delivery channels and their acceptability in Andhra Pradesh. 200 sample respondents had interviewed in order to assess the number of existing bank customers availing e-banking services. Correlation analysis also applied in this study. Findings: Regardless of their size, profitability and growth demand that banks focus on serving customers at the right time, with the right level of service and at the right cost. Several factors are driving this customer focus. Number one, today's customers expect personalized pricing and portfolio mixes. Banks that cannot deliver will suffer reduced profitability. While banks, by default, sell every product to every customer, digital banking allows customization, providing the data and analytics capabilities needed to examine each customer's profitability and offer individualized or segmented products and pricing. Application/Improvements: To stay profitable and grow in the new digital economy, banks need to adopt a customer-centric business model, diversify online delivery of products and services channels, and begin making meaning from valuable trails of digital information. Banking technology innovations in India: Enhancing customer value and satisfaction Banking technology; Customer centric model; Digitalization; Technology led solutions ",Stakeholder management
674,Measuring and Querying Process Performance in Supply Chains: An Approach for Mining Big-Data Cloud Storages,"Survival in today's global environment means continuously improving processes, identifying and eliminating inefficiencies wherever they occur. With so many companies operating as part or all of complex distributed supply chain, gathering, collating and analyzing the necessary data to identify such improvement opportunities is extremely complex and costly. Although few solutions exist to correlate the data, it continues to be generated in vast quantities, rendering the use of highly scalable, cloud-based solutions for process analysis a necessity. In this paper we present an overview of an analytical framework for business activity monitoring and analysis, which has been realized using extremely scalable, cloud-based technologies. It provides a low-latency solution for entire supply chains or individual nodes in such chains to query process data stores in order to deliver business insight. A custom query language has been implemented which allows business analysts to design custom queries on processes and activities based on a standard set of process metrics. Ongoing developments are focused on testing and improving the scalability and latency of the system, as well as extending the query engine to increase its flexibility and performance. © 2015 The Authors. Published by Elsevier B.V. Measuring and Querying Process Performance in Supply Chains: An Approach for Mining Big-Data Cloud Storages Big Data; Business Performance Management; Business Process Analytics Data mining; Information management; Information systems; Management science; Project management; Query languages; Supply chains; Business Activity Monitoring; Business analysts; Business Performance Management; Business Process; Global environment; Process analysis; Process metrics; Querying process; Big data",Monitoring and control
675,"Proceedings of the 8th Iberian Conference on Information Systems and Technologies, CISTI 2013","The proceedings contain 196 papers. The topics discussed include: SimpleNFC: simplifying access to digital world services using NFC; context-aware platform for mobile device-based communication assistance; predicting human creativity perception; profiling web users preferences with text mining; context-aware user effectiveness assessment system for mobile applications; shape grammars for creative decisions in the architectural project; performance management in public Portuguese's hospitals; qualitative analysis of the database contents in university curriculum degree in computer science in the EHEA; software project management in small and very small entities; agile software project: proposal of a model to manage risks; applying recommendation techniques to support conceptual negotiation processes; improving a website with web analytics - a case study; and use of a virtual platform as a supporting element for the acquisition of basic mathematical skills in engineering students. Proceedings of the 8th Iberian Conference on Information Systems and Technologies, CISTI 2013  ",Risk management
676,The hardware accelerator debate: A financial risk case study using many-core computing,"The risk of reinsurance portfolios covering globally occurring natural catastrophes, such as earthquakes and hurricanes, is quantified by employing simulations. These simulations are computationally intensive and require large amounts of data to be processed. The use of many-core hardware accelerators, such as the Intel Xeon Phi and the NVIDIA Graphics Processing Unit (GPU), are desirable for achieving high-performance risk analytics. In this paper, we set out to investigate how accelerators can be employed in risk analytics, focusing on developing parallel algorithms for Aggregate Risk Analysis, a simulation which computes the Probable Maximum Loss of a portfolio taking both primary and secondary uncertainties into account. The key result is that both hardware accelerators are useful in different contexts; without taking data transfer times into account the Phi had lowest execution times when used independently and the GPU along with a host in a hybrid platform yielded best performance. © 2015 Elsevier Ltd. All rights reserved. The hardware accelerator debate: A financial risk case study using many-core computing Catastrophic risk; Financial risk; Hardware accelerators; Many-core computing; Risk analysis; Secondary uncertainty Acceleration; Computer graphics; Computer graphics equipment; Data transfer; Hardware; Program processors; Risk assessment; Risk management; Uncertainty analysis; Catastrophic risks; Financial risks; Hardware accelerators; Many-core computing; Secondary uncertainty; Risk analysis",Risk management
677,An analytics approach to designing patient centered medical homes,"Recently the patient centered medical home (PCMH) model has become a popular team based approach focused on delivering more streamlined care to patients. In current practices of medical homes, a clinical based prediction frame is recommended because it can help match the portfolio capacity of PCMH teams with the actual load generated by a set of patients. Without such balances in clinical supply and demand, issues such as excessive under and over utilization of physicians, long waiting time for receiving the appropriate treatment, and non-continuity of care will eliminate many advantages of the medical home strategy. In this paper, by using the hierarchical generalized linear model with multivariate responses, we develop a clinical workload prediction model for care portfolio demands in a Bayesian framework. The model allows for heterogeneous variances and unstructured covariance matrices for nested random effects that arise through complex hierarchical care systems. We show that using a multivariate approach substantially enhances the precision of workload predictions at both primary and non primary care levels. We also demonstrate that care demands depend not only on patient demographics but also on other utilization factors, such as length of stay. Our analyses of a recent data from Veteran Health Administration further indicate that risk adjustment for patient health conditions can considerably improve the prediction power of the model. © 2014, Springer Science+Business Media New York. An analytics approach to designing patient centered medical homes Bayesian; Generalized linear model; Healthcare analytics; Healthcare workforce; Multilevel; Multivariate; Patient centered medical home Age Factors; Aged; Aged, 80 and over; Bayes Theorem; Continuity of Patient Care; Data Interpretation, Statistical; Female; Health Services; Humans; Length of Stay; Male; Middle Aged; Patient Care Team; Patient-Centered Care; Primary Health Care; Quality of Health Care; Retrospective Studies; Sex Factors; Socioeconomic Factors; United States; United States Department of Veterans Affairs; Workload; age; aged; Bayes theorem; female; government; health care quality; health service; human; length of stay; male; middle aged; organization and management; patient care; primary health care; retrospective study; sex difference; socioeconomics; statistical analysis; statistics and numerical data; United States; utilization; very elderly; workload",Strategic alignment
678,"International Conference on Information Systems, ICIS 2012, Volume 1","The proceedings contain 308 papers. The special focus in this conference is on Information Systems. The topics include: Heterogeneity in IT landscapes and monopoly power of firms; community ecology for innovation concept; peer influence in a very large social network; assessing value in an online network of products; optimal pricing with positive network effects; research on viral marketing propagating oriented to marketing context; repurchase intentions of information technology; a multi-level analysis of the impact of health information technology on hospital performance; institutional logics of adoption and use of health information technology; post-acceptance of electronic medical records; socio-technical attachments and IT change; comparing the predictive ability of PLS and covariance models; a note of caution on covariance-equivalent models in information systems; differential effects of omitting formative indicators; being innovative about service innovation; estimating demand for applications in the new mobile economy; continuance of professional social networking sites; role of online social networks in job search by unemployed individuals; knowledge contribution in online network of practice; the dual role of IS specificity in governing software as a service; relational antecedents of information visibility in value networks; summarization of corporate risk factor disclosure through topic modeling; a hybrid method for cross-domain sentiment classification using multiple sources; designing intelligent expert systems to cope with liars; IS offshore project risk, contracts and team structure; vendor and client project managers; technology desirability; the role of green IS in developing eco-effectiveness; the impact of IT-enabled manufacturing capabilities on plant profitability; inter-industry IT spillovers after the dot-com bust; effects of undesired online video advertising choice on user behavior and attitude; unfulfilled obligations in recommendation agent use; understanding the formation of trust in IT artifacts; user decisions among digital piracy and legal alternatives for film and music; a probabilistic generative model for latent business networks mining; the effects of information disclosure policy on the diffusion of security attacks; interactivity of social media and online consumer behavior; strategies for establishing service oriented design in organizations; alternative genres of is research; ICT standardization strategies and service innovation in health care; technical support and IT capacity demand; online social networks as a source and symbol of stress; the effects of social network usage on organizational identification; using the Kano model to identify attractive user-interface software components; developing a theory of multitasking behavior; an empirical investigation of information systems departments' configurations; exploring the adaptation of enterprise systems implementation methodology; out of the box and onto the stage; an organizing vision perspective on green IS development; the emergence of sustainability as the new dominant logic; social influence and defaults in peer-to-peer lending networks; multi-screen strategy for selling mobile content to customers; the emergent role of IT capabilities; reducing price uncertainty through demand side management; factors affecting perceived persuasiveness of a behavior change support system; surfacing schemas from firms' informational engagements; optimal design of consumer review systems; measuring product type with dynamics of online product review variance; different effects on retailers of online product reviews; measurement of multitasking with focus shift analysis; an exploration of the impact of information privacy invasion; measuring mobile users' concerns for information privacy; examining the rationality of location data disclosure through mobile devices; towards a design theory for software project risk management systems; identifying optimal IT portfolios to promote healthcare quality; designing a web-based application to support peer instruction for very large groups; principles for knowledge creation in collaborative design science research; estimating optimal recommendation set sizes for individual consumers; predicting participation in social media sites by analyzing user participation patterns; a hidden Markov model for conversion rate dynamics in online retail; flow experience and continuance intention toward online learning; design and evaluation of a didactical service blueprinting method for large scale lectures; knowledge sharing and maturation in circles of trust; a model-driven method for the systematic literature review of qualitative empirical research; procedurally transparent design science research; advancing task elicitation systems - an experimental evaluation of design principles; the knowing-doing gap in research methods and what we should do about it; efficient and flexible management of enterprise information systems; granularity metrics for IT services; from observed outcomes to measurable performance; the effect of third party investigation on pay-per-click advertising; identifying factors of e-government acceptance - a literature review; effects of cultural cognitive styles on users' evaluation of website complexity; a sustainability model of green IT initiatives; towards a typology of green IS strategies; digital access, political networks and the diffusion of democracy; the effect of free access on the diffusion of scholarly ideas; an empirical examination of cultural biases in interpersonal economic exchange; effect of business intelligence and IT infrastructure flexibility on organizational agility; news recommender systems with feedback; a multi-theoretical framework for social network-based recommendation; improving coverage of design in information systems education; perceived IT security risks of cloud computing: conceptualization and scale development; capability leapfrogging in the Japanese IT services industry; predatory coercion in social media and protection of children online - a critical discourse analysis approach; encouraging collaborative idea-building in enterprise-wide innovation challenges; analyzing the impact of social media strategies on viewer engagement; supporters in deed - studying online support provision from the perspective of social capital; membership overlap and inter-community collaboration; developing customer agility through information management; keyword search patterns in sponsored link advertisements; the hidden effects of opening bids in online auctions; increasing dynamic capabilities through virtualized grid-in-cloud solutions; design principles for heterogeneity decisions in enterprise architecture management; IT stereotyping and the CEO-CIO headlock; exploring the role of un-enacted projects in IT project portfolio management; the difference of determinants of mobile data services' adoption and continuance - a longitudinal study; a study of effectiveness and satisfaction level of cloud CRM users in Taiwan's enterprises; investigating intelligent agents in a 3D virtual world; an empirical study of a two-sided model of fraudulent exchange; perceptual and conceptual effects of incidental exposure to web ads; self-disclosure on online social networks; critical success factors of location-based services; affect and online privacy concerns; the impact of training and social norms on information security compliance; security management in cross-organizational settings; the effect of customers' emotion on service recovery strategy in IT service failures; the influence of IS affordances on work practices in health care; multi-level knowledge transfer in software development outsourcing projects; organizational performance with environmental knowledge intensity; process visibility - towards a conceptualization and research themes; towards an oil crisis early warning system based on absolute news volume; constructing workflow models from agent profiles; formulating effective coordination strategies in agile global software development teams; the transmission of control in information systems projects; the structuration of task-oriented communication in innovative virtual teams and social media in the entrainment of contention to innovation. International Conference on Information Systems, ICIS 2012, Volume 1  ",Value management
679,"International Conference on Information Systems, ICIS 2013, Volume 2","The proceedings contain 320 papers. The special focus in this conference is on Information Systems. The topics include: Computing requirements in open source software projects; a comparison of nonlinear pricing preference models for digital services; a computational approach to detecting and assessing sustainability-related communities in social media; a conceptualisation of management situations relevant for collaborative is research projects; a conceptualization of complexity in IS-driven organizational transformations; information security collective efficacy and vulnerability; a cross-sectional and temporal analysis of information consumption on twitter; a measurement model for investigating digital natives and their organisational behaviour; a model of product design and information disclosure investments; a problem-solving perspective on governance and product design in open source software projects; a qualitative approach to examine technology acceptance; a sensemaking approach to tablet users accommodating practices; a systems approach to countermeasures in credibility assessment interviews; aligning business and it strategies in multi-business organizations; an empirical examination of users information hiding in a crowdfunding context; an empirical investigation of a user-centric typology of innovation for evolving information systems products; an exploration of group information security compliance; an information quality evaluation framework of object tracking systems; an ontology artifact for information systems sentiment analysis; visualising perspectives of business and information systems; attention trade-off between two types of user contributions; the effects of user- and marketer-generated content on purchase decision processes; boundary spanning through enterprise social software; investment management systems and new institutional logics for regulatory compliance; bundling of software products and services to fight against piracy; capturing multi-stakeholder needs in customer-centric cloud service design; an empirical analysis of the impact of formal controls in strategic outsourcing contracts; cloud computing providers unrealistic optimism regarding it security risks; cognitive biases in new technology appropriation; collective intelligence in market-based social decision making; comparing peer influences in large social networks an empirical study on caller ringback tone; computer mediated social ties as predictors of SNS usage continuance; conceptualizing unexpected events in IT projects; coordinating one-to-many concurrent negotiation for service provision; counteracting the negative effect of form auto-completion on the privacy calculus; creating engaging student to student engagement in an online class; unpacking the building blocks of crowdsourcing collaboration processes; defining boundaries of web ads perceptual fluency effect; designing business models for platform as a service; designing e-services for an ageing society; designing the modern ICT curriculum; developing a procedure model for business process standardization; developing and measuring IS scales using item response theory; digital traces of information systems; discursive meaning creation in crowdfunding; driving innovation and knowledge management using crowdsourcing; interplay of EA design factors, strategy types, and environments; effective sentiment analysis of corporate financial reports; eliminating failure by learning from it systematic review of is project failure; navigating the ambiguity of materiality through collective design; enhancing shared understanding in collaborative online shopping; evaluating a new pattern development process for interface design; evaluating the quality of technology-mediated learning services; evolving the modular layered architecture in digital innovation; executive doctorate programs and the role of the information systems discipline; exploring foundations for using simulations in is research; exploring the customer perspective of agile development; exploring the effect of arousal and valence on mouse interaction; fit between knowledge transfer complexity and media capability; the impact of information pricing strategy on the pattern and effectiveness of word-of-mouth via social media; from open source to commercial software development - the community based software development model; some thoughts on technology implementation practice; a point of concern to IS use models' validity; how channel choice and service failure influence customer satisfaction; how customers trust mobile marketing applications; how E-consumers integrate diverse recommendations from multiple sources; how team cohesion leads to attitude change in the context of ERP learning; understanding reconfigurations of information systems and organizations in the Norwegian health sector; identifying patterns of idea diffusion in innovator networks; the missing link between it affordances and institutions for better health care in developing countries; impacts of creolization on trust and knowledge sharing in it-enabled global services sourcing; impacts of IT acceptance and resistance behaviors; implications of monitoring mechanisms on bring your own device (BYOD) adoption; improved medication compliance through health IT; improving medical decision-making using electronic health record systems; improving the semantics of conceptual-modeling grammars; inciting networks effects through platform authority; individual values for protecting identity in social networks; individualization of information systems - analyzing antecedents of IT consumerization behavior; information architecture for healthcare organizations; its antecedents and mediating effects on security compliant behavior; information system strategy for opportunity discovery and exploitation; information systems development outsourcing; institutionalization and the effectiveness of enterprise architecture management; internet use and well-being of young adults; investigating mobile messaging in healthcare organizations; different firm types, different alignment configurations; developing valid measures through CFA-based MTMM analysis; IT-enabled performative spaces in gender segregated work; knowledge contribution motivators an expectation-confirmation approach; legitimating user participation in mature organisations- exploring social media adoption in a financial services organization; an analysis of the impact of mobile micro-blogging on communication and decision-making; mobile app portfolio management and developers performance; mobile applications and access to personal information; mobile commerce in the new tablet economy; multi-agent based information systems for patient coordination in hospitals; network analysis for predicting academic impact; network diversity and social cohesion in creative performance; on the importance of organisational culture and structure in business process maturity; one-way mirrors and weak-signaling in online dating; online and offline sales channels for enterprise software; online health information use by disabled people; optimal information technology service pricing and capacity decision under service-level agreement; optimal location of charging stations in smart cities; organizational learning and the error of fixed strategies in IT innovation investment evaluation; pathways to value from business analytics; patients adherence to health advice on virtual communities; peer-based quality assurance in information systems development; their effect on extracurricular work behaviors among IT professionals; success and reciprocity on crowdfunding platforms; predictive validity and formative measurement in structural equation modeling; preventive adoption of information security behaviors; understanding factors contributing to the escalation of software maintenance costs; privacy controls and content sharing patterns of online social network users; psychological empowerment of patients with chronic diseases; purposive selection and the quality of qualitative IS research; quality-adjusted consumer surplus for online labor markets with asymmetric information; quantifying the dynamic sales impact of location-based mobile promotion technologies; is-enabled political decision support with scenario analyses for the substitution of fossil fuels; information technologies and the possibility for imagination; roles of trust in privacy assurance and perceived disease severity on personal health information disclosure; selecting project management methodologies for business intelligence projects a value based approach; service failure complaints identification in social media; role of social distance and social norms in online referral incentive systems; social media and citizen social movement process for political change; software developers' online chat as an intra-firm mechanism for sharing ephemeral knowledge; sources of power and CIO influence and their impact; statistical modeling of nanotechnology knowledge diffusion networks; systems of transfiguration and the adoption of IT under surveillance; taking a new-generation manager perspective to develop interface designs; task-technology fit for low-literate consumers; team adaptability in agile information systems development; technology, interoperability, and provision of public safety networks; the architecture of generativity in a digital ecosystem; the design of a network-based model for business performance prediction; the design of a tangible user interface for a real-time strategy game. International Conference on Information Systems, ICIS 2013, Volume 2  ",Strategic alignment
680,Squaring the circle: A new alternative to alternative-assessment,"Many quality assurance systems rely on high-stakes assessment for course certification. Such methods are not as objective as they might appear; they can have detrimental effects on student motivation and may lack relevance to the needs of degree courses increasingly oriented to vocational utility. Alternative assessment methods can show greater formative and motivational value for students but are not well suited to the demands of course certification. The widespread use of virtual learning environments and electronic portfolios generates substantial learner activity data to enable new ways of monitoring and assessing students through Learning Analytics. These emerging practices have the potential to square the circle by generating objective, summative reports for course certification while at the same time providing formative assessment to personalise the student experience. This paper introduces conceptual models of assessment to explore how traditional reliance on numbers and grades might be displaced by new forms of evidence-intensive student profiling and engagement. © 2014 © 2014 Taylor & Francis. Squaring the circle: A new alternative to alternative-assessment alternative assessment; e-assessment; graduate profiles; learning analytics ",Monitoring and control
681,EXsight: An analytical framework for quantifying financial loss in the aftermath of catastrophic events,"In this paper we explore the design of an analytical framework for quantifying financial loss in the aftermath of catastrophic events. The idea is to aggregate the thousands of exposure databases received by a single reinsurer into a giant loosely structured exposure portfolio and then use Big Data analysis technology, originally developed in the context of web-scale analytics, to rapidly perform natural but ad-hoc loss analysis immediately after an event. As in many situational analysis problems, the challenge here is to work with both categorical and geospatial data, deal with partial data often at varying levels of aggregation, integrate data from many sources, and provide an analysis framework in which analyses can be rapidly performed in the hours, days, and weeks immediately after an event. © 2014 IEEE. EXsight: An analytical framework for quantifying financial loss in the aftermath of catastrophic events catastrophe; exposure data; framework; MongoDB; post-event; reinsurance; Risk analytics Expert systems; Losses; catastrophe; Exposure data; framework; MongoDB; post-event; reinsurance; Database systems",Strategic alignment
682,Shine a light on big data,"The potential of big data is very beneficial for organizations looking for insights from fresh sources of information through social media, automated meter readings, and other channels. Information integration and governance (IIG) should therefore become a natural part of big data analytics projects. It should provide automated discovery, profiling, and understanding of diverse data sets to offer a comprehensive context for making informed decisions. In addition, IIG should provide the agility to accommodate a wide variety of data and seamlessly integrate data with diverse technologies. Advanced IBM InfoSphere IIG portfolio capabilities have been developed to address the uncertainty that can challenge organizations and to help increase end-user confidence in big data by bringing data context out of the shadows and making it clearly visible. Taking advantage of InfoSphere capabilities to build customized dashboards and views, business partners as well as individual user organizations are designing dashboards to address specific needs. Shine a light on big data  Automated discovery; Automated meter readings; Business partners; Data contexts; Diverse data sets; Information integration; Informed decision; Sources of informations; Societies and institutions",Governance
683,Towards an economic foundation for the decision between agile and plan-driven project management in a business intelligence context,"Lacking a formal yet practical decision model, nowadays decision makers mostly follow corporate guidelines or their intuition when it comes to the decision between agile and plan-driven project management in Business Intelligence projects. As one size does not fit all, using management methods hyped by temporary fashion or other management methods not adapted to the situation bears the risk of project failure. Thus, this paper proposes a risk-adjusted net present value-based model to support decision makers in the selection of the appropriate management method for Business Intelligence projects. We focus on two decisive risk parameters - the likelihood of environmental changes and the peril of improper system integration - and a project's estimated cash flows. As a result, the tradeoff between different characteristics of risks and cash flows in a specific project is formalized and made transparent. In summary, this research-in-progress paper sketches the idea of a practical decision model that improves the foundation for the selection of the appropriate management method. Towards an economic foundation for the decision between agile and plan-driven project management in a business intelligence context  Information analysis; Management science; Project management; Risk perception; Business Intelligence projects; Decision makers; Decision modeling; Environmental change; Management method; Net present value; Risk of projects; System integration; Decision making",Strategic alignment
684,"18th Americas Conference on Information Systems 2012, AMCIS 2012, Volume 2","The proceedings contain 572 papers. The special focus in this conference is on Information Systems. The topics include: Health diagnosis of communities of practices (CoPs); open source alternatives for business intelligence; identifying business process activity mappings by optimizing behavioral similarity; hanging with the right crowd; a taxonomy of web-based inbound open innovation initiatives; database intrusion detection: defending against the insider threat; an empirical study of the GIGO axiom in satisficing decisions; analysis of probabilistic news recommender systems; the influence of technology characteristics on privacy calculus; instant messaging privacy in the clouds; towards a component-based description of business models; two-sided cybermediary platforms; an integrative analysis of transactional e-government web usage; facebook usage in government-a case study of information content; deriving business value from asymmetric penalty-reward perspectives of IS users; create attention to attract attention-viral marketing of digital music in social networks; towards a framework for transforming business models into business processes; factors affecting perceived satisfaction with a BPM tool; hypercompetition in the Erp industry; structural flaws in the ethics of technology; ethical considerations for virtual worlds; personality, gender and careers in information technology; on IT control weaknesses in auditors' reports on internal control; effect of the SOX act on IT governance; the influence of general sustainability attitudes and value congruence on consumer behavior; user privacy in mobile advertising; system learning of user interactions; deploying mission critical learning management system using open source software; theorizing the dual role of information technology in technostress research; boundary spanning in business process management; ERP usability and the mangle of practice; direct manipulation tablet apps for education; comparing graphical and tangible user interfaces for a tower defense game; visual perception model for online target marketing; eye movements, perceptions, and performance; individual relationships with technology; internal audit function response to ERP systems implementation; auditing journal entries using self-organizing map; consumer perceptions of the adoption of electronic personal health records; investigating the reciprocal relationships within health virtual communities; designing and visualising healthcare delivery systems; how multinational firms use IT to manage their global operations; conflict, value diversity, and performance in virtual teams; mobile ICT and knowledge sharing in underserved communities; agile decision making framework to support mobile microloans for unbanked customers; the analysis of the telecommunications industry in Thailand; the business value of knowledge management; providing information feedback to bidders in online multi-unit combinatorial auctions; spatial modeling using agents; using probabilistic ontologies for video exploration; new directions, new challenges, and new understandings; function-based categorization of online product information types; exploring antecedents of habit on social network service; personality correlation analysis and applications in social networks; identifying experts in virtual forecasting communities; black males in IT higher education in the USA; technology features, empowering perceptions, and voicing behavior on microblog; information security policy compliance; a preliminary taxonomy for software failure impact; an examination of the success of post-merger IT integration; an analysis of and perspective on the information security maturity model; geographic information systems and the nonprofit sector; effectiveness of shallow hierarchies for document stores; a methodology for the development of web-based information systems; balanced resource allocation; demand response in smart grids; decision support for electric vehicle charging; the expectations for faculty in Latin America; mastering the social IT/Business Alignment Challenge; supply chain resource planning systems; towards a research framework for VLBA operation management; integrating enterprise system's 3rd wave into IS curriculum; a two-tier data-centric framework for flexible business process management; engagement in online communities; organisational semiotics methods to assess organisational readiness for internal use of social media; social media in the workplace; economics of pair programming revisited; social traps of agile methods; metadata exploitation in large-scale data migration projects; collaboratively assessing information quality on the web; reputation management in social commerce communities; E-Business adoption research; a preliminary information theory of difference; replacement of project manager during IT projects-a research agenda; a simulation study of project management and collaborative information technologies; the role of business information visualization in knowledge creation; effects of narrative structure and salient decision points in role playing games; adoption of pervasive e-health solutions; security practices and regulatory compliance in the healthcare industry; the role of demographic characteristics in health care strategic security planning; tailoring software process capability/maturity models for telemedicine systems; understanding dynamic collaboration in teleconsultation; the pathway to enterprise mobile readiness; investigating the role of social media and social capital; exploring 311-driven changes in city government; preventing the gradual decline of shared service centers; developing a conceptual framework for evaluating public sector transformation in the digital era; the impact of cultural differences on cloud computing ecosystems in YOU.S. and China; an examination of the impact of service climate on service productivity in the organizational context; information systems facilitating groundwater sustainability management; keeping electronic medical records secure and portable; the emerging role of robotics in home health care; information quality assessment technique to evaluate the information exchange; boundary dialogues in user-centric innovation; towards a meditation brain state model using electroencephalographic data; design method requirements for agile system of systems; design and evaluation of a socially enhanced classroom blog to pomote student learning in higher education; it is not all about the music: user preference for musicians on facebook; knowledge seeking and knowledge sharing in a nonprofit organizational partner network: a social network analysis; the mediating role of adaptive personalization in online shopping; exploring the temporal nature of sociomateriality from a work system perspective; sociomateriality as radical ontology; information security management; meeting global business information requirements with enterprise resource planning; knowledge sharing in social networking sites for e-collaboration; applying cognitive principles of similarity to data integration-the case of SIAM; reference model in design science research to gather and model information; impact of online content on attitudes and buying intentions; prospect theory and information security investment decisions; using domain knowledge to facilitate cyber security analysis; conceptualizing data security threats and countermeasures in the E-Discovery process with misuse cases; an empirical analysis of an individual's 360 degree protection from file and data loss; analysis of eBook lending: a game-theory approach; facilitating consumers' evaluation of experience goods and the benefits for vendors; three-factor Model vs. Two-Factor Model; automating enterprise architecture documentation using an enterprise service bus; the influence of role models on students' decisions to pursue the IS major; teaching ''people networking'' skills for CIS students; a case of bias in teaching, grading, and plagiarism; a relational view of accounting information sharing; reporting capabilities, financial closing time and effects on cost of equity capital; reflecting on the role of IT and IT research in healthcare; social media around the world; understanding the effects of freeriding in team dynamics; password policy effects on entropy and recall: research in progress; the role of individual characteristics on insider abuse intentions; building a methodology to assess the e-Government transformation success; optimizing freight delivery for less-than-truckload transportation; the influence of perceived information and network characteristics on the attitude towards information overload; information disclosure and generational differences in social network sites; trasactive memory systems virtual team training model; the case of open government and teaching and learning in a virtual world. 18th Americas Conference on Information Systems 2012, AMCIS 2012, Volume 2  ",Strategic alignment
685,"18th Americas Conference on Information Systems 2012, AMCIS 2012, Volume 1","The proceedings contain 572 papers. The special focus in this conference is on Information Systems. The topics include: Health diagnosis of communities of practices (CoPs); open source alternatives for business intelligence; identifying business process activity mappings by optimizing behavioral similarity; hanging with the right crowd; a taxonomy of web-based inbound open innovation initiatives; database intrusion detection: defending against the insider threat; an empirical study of the GIGO axiom in satisficing decisions; analysis of probabilistic news recommender systems; the influence of technology characteristics on privacy calculus; instant messaging privacy in the clouds; towards a component-based description of business models; two-sided cybermediary platforms; an integrative analysis of transactional e-government web usage; facebook usage in government-a case study of information content; deriving business value from asymmetric penalty-reward perspectives of IS users; create attention to attract attention-viral marketing of digital music in social networks; towards a framework for transforming business models into business processes; factors affecting perceived satisfaction with a BPM tool; hypercompetition in the Erp industry; structural flaws in the ethics of technology; ethical considerations for virtual worlds; personality, gender and careers in information technology; on IT control weaknesses in auditors' reports on internal control; effect of the SOX act on IT governance; the influence of general sustainability attitudes and value congruence on consumer behavior; user privacy in mobile advertising; system learning of user interactions; deploying mission critical learning management system using open source software; theorizing the dual role of information technology in technostress research; boundary spanning in business process management; ERP usability and the mangle of practice; direct manipulation tablet apps for education; comparing graphical and tangible user interfaces for a tower defense game; visual perception model for online target marketing; eye movements, perceptions, and performance; individual relationships with technology; internal audit function response to ERP systems implementation; auditing journal entries using self-organizing map; consumer perceptions of the adoption of electronic personal health records; investigating the reciprocal relationships within health virtual communities; designing and visualising healthcare delivery systems; how multinational firms use IT to manage their global operations; conflict, value diversity, and performance in virtual teams; mobile ICT and knowledge sharing in underserved communities; agile decision making framework to support mobile microloans for unbanked customers; the analysis of the telecommunications industry in Thailand; the business value of knowledge management; providing information feedback to bidders in online multi-unit combinatorial auctions; spatial modeling using agents; using probabilistic ontologies for video exploration; new directions, new challenges, and new understandings; function-based categorization of online product information types; exploring antecedents of habit on social network service; personality correlation analysis and applications in social networks; identifying experts in virtual forecasting communities; black males in IT higher education in the USA; technology features, empowering perceptions, and voicing behavior on microblog; information security policy compliance; a preliminary taxonomy for software failure impact; an examination of the success of post-merger IT integration; an analysis of and perspective on the information security maturity model; geographic information systems and the nonprofit sector; effectiveness of shallow hierarchies for document stores; a methodology for the development of web-based information systems; balanced resource allocation; demand response in smart grids; decision support for electric vehicle charging; the expectations for faculty in Latin America; mastering the social IT/Business Alignment Challenge; supply chain resource planning systems; towards a research framework for VLBA operation management; integrating enterprise system's 3rd wave into IS curriculum; a two-tier data-centric framework for flexible business process management; engagement in online communities; organisational semiotics methods to assess organisational readiness for internal use of social media; social media in the workplace; economics of pair programming revisited; social traps of agile methods; metadata exploitation in large-scale data migration projects; collaboratively assessing information quality on the web; reputation management in social commerce communities; E-Business adoption research; a preliminary information theory of difference; replacement of project manager during IT projects-a research agenda; a simulation study of project management and collaborative information technologies; the role of business information visualization in knowledge creation; effects of narrative structure and salient decision points in role playing games; adoption of pervasive e-health solutions; security practices and regulatory compliance in the healthcare industry; the role of demographic characteristics in health care strategic security planning; tailoring software process capability/maturity models for telemedicine systems; understanding dynamic collaboration in teleconsultation; the pathway to enterprise mobile readiness; investigating the role of social media and social capital; exploring 311-driven changes in city government; preventing the gradual decline of shared service centers; developing a conceptual framework for evaluating public sector transformation in the digital era; the impact of cultural differences on cloud computing ecosystems in YOU.S. and China; an examination of the impact of service climate on service productivity in the organizational context; information systems facilitating groundwater sustainability management; keeping electronic medical records secure and portable; the emerging role of robotics in home health care; information quality assessment technique to evaluate the information exchange; boundary dialogues in user-centric innovation; towards a meditation brain state model using electroencephalographic data; design method requirements for agile system of systems; design and evaluation of a socially enhanced classroom blog to pomote student learning in higher education; it is not all about the music: user preference for musicians on facebook; knowledge seeking and knowledge sharing in a nonprofit organizational partner network: a social network analysis; the mediating role of adaptive personalization in online shopping; exploring the temporal nature of sociomateriality from a work system perspective; sociomateriality as radical ontology; information security management; meeting global business information requirements with enterprise resource planning; knowledge sharing in social networking sites for e-collaboration; applying cognitive principles of similarity to data integration-the case of SIAM; reference model in design science research to gather and model information; impact of online content on attitudes and buying intentions; prospect theory and information security investment decisions; using domain knowledge to facilitate cyber security analysis; conceptualizing data security threats and countermeasures in the E-Discovery process with misuse cases; an empirical analysis of an individual's 360 degree protection from file and data loss; analysis of eBook lending: a game-theory approach; facilitating consumers' evaluation of experience goods and the benefits for vendors; three-factor Model vs. Two-Factor Model; automating enterprise architecture documentation using an enterprise service bus; the influence of role models on students' decisions to pursue the IS major; teaching ""people networking"" skills for CIS students; a case of bias in teaching, grading, and plagiarism; a relational view of accounting information sharing; reporting capabilities, financial closing time and effects on cost of equity capital; reflecting on the role of IT and IT research in healthcare; social media around the world; understanding the effects of freeriding in team dynamics; password policy effects on entropy and recall: research in progress; the role of individual characteristics on insider abuse intentions; building a methodology to assess the e-Government transformation success; optimizing freight delivery for less-than-truckload transportation; the influence of perceived information and network characteristics on the attitude towards information overload; information disclosure and generational differences in social network sites; trasactive memory systems virtual team training model; the case of open government and teaching and learning in a virtual world. 18th Americas Conference on Information Systems 2012, AMCIS 2012, Volume 1  ",Strategic alignment
686,Driving continuous improvement by developing and leveraging lean key performance indicators,"Lean advocates defining value from the perspective of the customer, striving for perfection, continuous improvement, and reducing waste. However, unlike formal lean programs in the manufacturing sector, the Architecture-Engineering- Construction (AEC) industry often uses the Last Planner System®(LPS) and forms ad hoc project teams to manage their lean programs. To advance to the next stage of improving project performance, we propose that the AEC industry begin adopting an available set of lean metrics and analytics that are more effective in evaluating system performance. These metrics and analytics can help project teams aggregate and filter project and enterprise information. They can then determine lean key performance indicators that reveal new opportunities for continuous improvement of the production system. Ensuring that a holistic objective as well as a good governance structure is in place is important to leverage the metrics and analytics as enablers for global optimization. Otherwise, misuse may lead to measurement drift and local optimization from misguided attempts to improve one metric in isolation. By aligning lean metrics and analytics to delivery, stakeholder management, and risk mitigation strategies, owners of capital programs and their service providers can attain better project outcomes and accelerate continuous improvement objectives. Driving continuous improvement by developing and leveraging lean key performance indicators Analytics; KPIs; Lean governance; Measurement drift; Metrics; System performance Global optimization; Project management; Analytics; KPIs; Lean governance; Metrics; System performance; Benchmarking",Governance
687,Real options analysis on strategic partnership dealing of biotech start-ups,"Biotech start-ups have the competitive advantages of quicker and lower-cost at combing the innovative technologies and the niche markets, in comparing with big pharmaceutical companies. However they are facing with the short-term financing fluctuations from venture capital and initial public offering markets since this time's financial crisis, while universities' long-term basic research activities are continually producing life-science findings as their innovative resources. Then strategic partnerships with big pharmaceutical companies are becoming more important fundraising sources for biotech start-ups. Thus, how can such innovative but small start-ups equally negotiate their deal-structures with the resource-rich but innovation starving big pharmaceutical companies? For the research question above, a promising methodology seems the more objective metrics or analytics on the value of decision-making rather than traditional means like the comparison with similar cases or the subjective judgment based on past experience suitable for incremental improvement. Therefore this paper discusses on the opportunities and challenges of each application of the Monte Carlo simulation to cash flow forecasting of research and development investment, the rainbow sequential compound option to risk-hedge, and the stochastic optimization to negotiating the portfolio of license-fee elements. © 2013 Global Institute of Flexible Systems Management. Real options analysis on strategic partnership dealing of biotech start-ups Biotech start-up; Real options; Strategic partnership ",Strategic alignment
688,"Evolutionary multiobjective optimization in water resources: The past, present, and future","This study contributes a rigorous diagnostic assessment of state-of-the-art multiobjective evolutionary algorithms (MOEAs) and highlights key advances that the water resources field can exploit to better discover the critical tradeoffs constraining our systems. This study provides the most comprehensive diagnostic assessment of MOEAs for water resources to date, exploiting more than 100,000 MOEA runs and trillions of design evaluations. The diagnostic assessment measures the effectiveness, efficiency, reliability, and controllability of ten benchmark MOEAs for a representative suite of water resources applications addressing rainfall-runoff calibration, long-term groundwater monitoring (LTM), and risk-based water supply portfolio planning. The suite of problems encompasses a range of challenging problem properties including (1) many-objective formulations with four or more objectives, (2) multi-modality (or false optima), (3) nonlinearity, (4) discreteness, (5) severe constraints, (6) stochastic objectives, and (7) non-separability (also called epistasis). The applications are representative of the dominant problem classes that have shaped the history of MOEAs in water resources and that will be dominant foci in the future. Recommendations are provided for which modern MOEAs should serve as tools and benchmarks in the future water resources literature. Evolutionary multiobjective optimization in water resources: The past, present, and future Evolutionary algorithms; Interactive visual analytics; Long-term groundwater monitoring; Many-objective optimization; Model calibration; Water supply ",Monitoring and control
689,Predictive network analytics for national research investment,"Research is a risky business. The starting point of research is ignorance: if we already have answers to our questions or simply undertaking routine works to get answers, we would not be undertaking research in the first instance. Australia spends approximately 2.2% GDP (or $27.7B AUD) in research and development. So we are taking considerable risks as a country. Fortunately, some research areas are less risky than others. They have well-established theoretical foundations and experimental methodologies, proper access to infrastructures and equipment, and above all a critical mass of researchers to advance the state of knowledge. In ""emerging"" areas of research however the risks are considerably higher there may not be an established theory, methodology, or even a critical mass of researchers available. Finding the right approach to fund emerging research is a serious policy challenge. The European Research Council and the National Science Foundation (in the YOU.S.) have both independently initiated works in developing approaches to identify and fund emerging research in recent years. If we accept the suggestion that research investment is akin to portfolio investment (to maximize the expected return while minimize risk over an entire investment portfolio), then investing in emerging research amounts to investing in high risk options with high expected return. But how do we pick ""winners"" from ""imposers""? How can we tell we are not picking ""one hit wonders""? How can we spot ""sleeping beauties"" which may take years to mature? The availability of large scale global bibliographic (and other relevant) data from both open and commercial sources presents an intriguing opportunity for data miners and machine learners to contribute to these debates. In this presentation, we shall examine the general shape of the problem definition to look at the ""why"" and ""what"" instead of the ""how"". Our aim is to present and engage the Australasian data mining and machine learning communities in a conversation about an intellectually challenging and exciting problem that can have wide spread impact on how governments, funding agencies and industries make strategic decisions in R&D investment. © 2013, Australian Computer Society, Inc. Predictive network analytics for national research investment  Artificial intelligence; Data mining; Learning systems; European Research Council; Experimental methodology; Investment portfolio; Machine learning communities; National Science Foundations; Portfolio investment; Research and development; Theoretical foundations; Investments",Strategic alignment
690,QuPARA: Query-driven large-scale portfolio aggregate risk analysis on MapReduce,"Modern insurance and reinsurance companies use stochastic simulation techniques for portfolio risk analysis. Their risk portfolios may consist of thousands of reinsurance contracts covering millions of individually insured locations. To quantify risk and to help ensure capital adequacy, each portfolio must be evaluated in up to a million simulation trials, each capturing a different possible sequence of catastrophic events (e.g., earthquakes, hurricanes, etc.) over the course of a contractual year. We present a flexible framework for portfolio risk analysis that can answer a rich variety of catastrophic risk queries. Rather than aggregating simulation data in order to produce a small set of high-level risk metrics efficiently (as done in production risk management systems), our focus is on queries on unaggregated or partially aggregated data. The goal is to allow analysts to obtain answers to a wide variety of unanticipated but natural ad hoc queries, which can help actuaries or underwriters to better understand the multiple dimensions (e.g., spatial correlation, seasonality, peril features, construction features, financial terms, etc.) that can impact portfolio risk and thus company solvency. We implemented a prototype system, called QuPARA, using Apache's Hadoop implementation of the MapReduce paradigm. This allows the user to utilize large parallel compute servers in order to answer ad hoc queries efficiently even on very large data sets typically encountered in practice. We describe the design and implementation of QuPARA and present experimental results that demonstrate its feasibility. © 2013 IEEE. QuPARA: Query-driven large-scale portfolio aggregate risk analysis on MapReduce ad hoc risk analytics; aggregate risk analytics; Hadoop; MapReduce; portfolio risk Aggregates; Big data; Information management; Query processing; Risk assessment; Risk management; Stochastic models; Stochastic systems; ad hoc risk analytics; Aggregate risk; Hadoop; Map-reduce; Portfolio risks; Risk analysis",Risk management
691,ICEIS 2014 - Proceedings of the 16th International Conference on Enterprise Information Systems,The proceedings contain 210 papers. The topics discussed include: possibilistic interorganizational workflow net for the recovery problem concerning communication failures; distributed knowledge management architecture and rule based reasoning for mobile machine operator performance assessment; machine learning techniques for topic spotting; fuzzy DEMATEL model for evaluation criteria of business intelligence; an evolutionary algorithm for graph planarization by vertex deletion; evaluating artificial neural networks and traditional approaches for risk analysis in software project management - a case study with PERIL dataset; using visualization and text mining to improve qualitative analysis; DC2DP: a dublin core application profile to design patterns; domain ontology for time series provenance; video stream transmodality; assisting speech therapy for autism spectrum disorders with an augmented reality application; and adding semantic relations among design patterns. ICEIS 2014 - Proceedings of the 16th International Conference on Enterprise Information Systems  ,Risk management
692,High-frequency financial statistics with parallel R and Intel Xeon Phi coprocessor,"Financial statistics covers a wide array of applications in the financial world, such as (high frequency) trading, risk management, pricing and valuation of securities and derivatives, and various business and economic analytics. Portfolio allocation is one of the most important problems in financial risk management. One most challenging part in portfolio allocation is the tremendous amount of data and the optimization procedures that require computing power beyond the currently available desktop systems. In this article, we focus on the portfolio allocation problem using high-frequency financial data, and propose a hybrid parallelization solution to carry out efficient asset allocations in a large portfolio via intra-day high-frequency data. We exploit a variety of HPC techniques, including parallel R, Intel Math Kernel Library, and automatic offloading to Intel Xeon Phi coprocessor in particular to speed up the simulation and optimization procedures in our statistical investigations. Our numerical studies are based on high-frequency price data on stocks traded in New York Stock Exchange in 2011. The analysis results show that portfolios constructed using high-frequency approach generally perform well by pooling together the strengths of regularization and estimation from a risk management perspective. We also investigate the computation aspects of large-scale multiple hypothesis testing for time series data. Using a combination of software and hardware parallelism, we demonstrate a high level of performance on high-frequency financial statistics. © 2014 IEEE. High-frequency financial statistics with parallel R and Intel Xeon Phi coprocessor high-frequency financial analysis; Intel Xeon Phi Coprocessor; massive parallelism; parallel R Commerce; Computing power; Coprocessor; Electronic trading; Financial markets; Frequency estimation; Risk perception; Time series; Time series analysis; Co-processors; Financial analysis; High frequency HF; High-frequency financial analyse; High-frequency trading; Intel xeon phi coprocessor; Massive parallelism; Optimization procedures; Parallel R; Risks management; Risk management",Risk management
693,"19th Americas Conference on Information Systems, AMCIS 2013, Volume 2","The proceedings contain 438 papers. The special focus in this conference is on Information Systems. The topics include: A cloud-based service for affordable cost analysis; a conceptual examination of distrusting beliefs in older adults about the internet; a consulting model of global service learning; a contingent model of project organization and management; a decision making model for the adoption of cloud computing in Jamaican organizations; a foundation of a first-person perspective systems analysis; a framework for collaborative augmented reality applications; a framework for enterprise social media guidelines; a framework to analyze E-government OSS adoption benefits; a framework to support practitioners in evaluating business-IT alignment models; a measure for assessing the adequacy of DDOS defenses; a systematic classification and analysis of NFRs; about the need for semantically enriched reference models; adopting agile methods for follow-the-sun software development; an analysis of product uncertainty and seller uncertainty; aggregating, analyzing, and diffusing natural disaster information; ameliorating ERP workflow using a sociomaterial lens; an active learning approach to teaching undergraduate introduction to MIS course; an agile approach to systems analysis and design teaching and learning; an empirical study of designing simplicity for mobile application interaction; an examination of IT initiative portfolio characteristics and investment allocation; an exploration of organizational capabilities for emergency response; an icon taxonomy for semi-literate communities; an information security model and its validation; an inquiry into mental models of web interface design; an investigation of the effect of IT occupational subculture on the relationship between knowledge sharing and IT diffusion in organizations; an organizing framework for literacy; an understanding of the impact of gamification on purchase intentions; antecedents and effects of computer self-efficacy on social networking adoption among Asian online users; architecture and implementation of a decision support system for software industry business models; assessing internet source credibility; attitude change process toward ERP systems using the elaboration likelihood model; barriers to mission-critical open source software adoption by organizations; behaviour analysis of distributed systems under time change constraints; blogging as a liminal space; high-value impact through multidisciplinary design science programs of research; breaking the norm - on the determinants of informational nonconformity in online social networks; bringing together BPM and social software; building context-aware access control in enterprise ontologies; capitalizing on social media analysis - insights from an online review on business models; carbon footprint of IT-services - a comparative study of energy consumption for offline and online storage usage; career paths training for the first year students in information systems science-motivational view; chains of control in agile software development; challenges in offshore outsourcing relationship management - a Peruvian perspective; challenges of blind students and IT-based mitigation strategies; cloudifying desktops - a taxonomy for desktop virtualization; CMC influence on voluntarily collaborating knowledge workers' perception of equivocal tasks; collective learning paradigm for rapidly evolving curriculum; a hybrid personalized movie recommender based on perceived similarity; common patterns of cloud business models; comparing ABET-accredited IS undergraduate programs and the ACM 2010IS model curriculum; comprehensive access control for data warehouses; conceptualizing the impact of social capital on knowledge creation; configuring value creation processes for global service; control-related motivations and information security policy compliance; corporate enactments of social control across social media affordances; creative innovativeness with information systems (IS) and its role in quality IS usage; moving from the philosophical to the empirical in the search for causal explanations; critical success factors for ERP system upgrades - the case of a German large-scale enterprise; cultural impact on E-service use in Saudi Arabia; current state of the digital deception studies in IS; customer involvement in organizational innovation - toward an integration concept; data modeling in the cloud; decision support using linked, social, and sensor data; design, evaluation and impact of educational olfactory interfaces; designing decision support systems at the interface between complex and complicated domains; determinants and consequences of herding in P2P lending markets; determining microblogging effectiveness for capturing quality knowledge; developing a governance model for successful business process standardization; developing targeted text messages for enhancing medication adherence; development of a change readiness scale for electronic medical record systems implementation at hospitals; differences between FCM and fuzzy ANP techniques in the process of organizational change readiness assessment; digital service flexibility and performance of credit unions; drive a website performance using web analytics; drivers of cultural differences in information system adoption - a case study; dynamic model to assess organisational readiness during information system implementation; effects of persuasive claims on desirability and impulse purchase behavior; effects of RFID technology on profitability and efficiency in retail supply chains; efficacy of communication support in collaborative online shopping; efficiency and device versatility of graphical and textual passwords; emotion and memory in technology adoption and diffusion; empowering organizations through customer knowledge acquisition; enabling collaboration in virtual manufacturing enterprises with cloud computing; engagement of information technology professionals with their work; enhancing privacy using community driven recommendations; enhancing service lifecycle management - costing as part of service descriptions; enterprise app stores for mobile applications - development of a benefits framework; enterprise architecture software tool support for small and medium-sized enterprises; enterprise systems implementation success in the shakedown phase; environmental pressure on software as a service adoption; estimating the quality of data using provenance; evaluating advanced forms of social media use in government; evaluating cooperation in IT teams using a fuzzy multicriteria sorting method; evaluating the information systems women network (ISWN) mentoring program; evaluating the performance of government IT projects in the Caribbean; examining high performance teams in information systems projects; examining personal information privacy-protective responses (IPPR) with the use of smart devices; examining the use of social media in customer co-creation; exploration of risk management process usage levels and their relationship to project outcomes; exploring subscription renewal intention of operational cloud enterprise systems - a stakeholder perspective; exploring the aesthetic effects of the golden ratio in the design of interactive products; exploring the factors influencing the usage intention of facebook fan page - a preliminary study; exploring the factors that influence social computing intentions; exploring the impact of online reviews with brand equity for online software purchasing behavior; extending successful ebusiness models to the mobile internet; extracting product features from online consumer reviews; eye gazing behaviors in online deception; facilitating collaboration and peer learning through anchored asynchronous online discussions; facilitating conflict resolution of models for automated enterprise architecture documentation; facilitating the adoption of public services using high definition video; factor analysis of critical success factors for data quality; factors of password-based authentication; factors that affect information and communication technology adoption by small businesses in China; feasibility analysis of an assessment model of knowledge acquisition in virtual environments; fostering efficiency in information systems support for product-service systems in the manufacturing industry; framing group norms in virtual communities; global ERP implementations and harmonization of practices in multinational corporations; potentials and challenges to the public health sector of of developing countries; growth of an organizational field for infrastructure; harnessing anomalous preferences of anonymous users for lean information systems development; hidden or implicit contextual factors influencing user participation in online production communities; identification of driving forces in service innovations; illuminating organizing vision careers through case studies; impact of geospatial reasoning ability and perceived task-technology fit on decision-performance; impact of strategic alignment on IT outsourcing success in a complex service setting; impact of unified communications on communication, relationship building and performance; impact of users' cognitive responses on user satisfaction in online community; impediments to enterprise system implementation across the system lifecycle; in search of insights for institutionalization of telemedicine in the health care system in Ethiopia. 19th Americas Conference on Information Systems, AMCIS 2013, Volume 2  ",Strategic alignment
694,Data intensive enterprise applications,"Today almost all big enterprises act globally, which results in a growing need for a new kind of data analytics. Imagine a company where data from distribution and sales needs to be combined with increasing online sales on multiple platforms and marketing across new social media channels. Here, new real-time analytics using Cloud Computing concepts can open new perspectives. SAP has had a strong presence in the Business Intelligence (BI) market. The company pioneered concepts to collect, combine, and analyze company wide information. As a result, SAP customers enjoy BI capabilities that are strongly integrated with their SAP operational systems (e.g., ERP, CRM). In recent years, companies have leveraged Cloud Computing as a means for lowering the Total Cost of Ownership (TCO) of various types of business applications that are provided On-Demand. SAP already offers products such as SAP Business ByDesign, which is offered as a Software-as-a-Service (SaaS) On-Demand product. Feature-rich Cloud storage solution such as VISION Cloud enables SAP to integrate new innovations to its On-Demand software portfolio. This chapter describes how VISION Cloud enriches SAP's Instant Business Intelligence analytical On-Demand service. © 2013, IGI Global. Data intensive enterprise applications  ",Governance
695,Parallel simulations for analysing portfolios of catastrophic event risk,"At the heart of the analytical pipeline of a modern quantitative insurance/reinsurance company is a stochastic simulation technique for portfolio risk analysis and pricing process referred to as Aggregate Analysis. Support for the computation of risk measures including Probable Maximum Loss (PML) and the Tail Value at Risk (TVAR) for a variety of types of complex property catastrophe insurance contracts including Cat eXcess of Loss (XL), or Per-Occurrence XL, and Aggregate XL, and contracts that combine these measures is obtained in Aggregate Analysis. In this paper, we explore parallel methods for aggregate risk analysis. A parallel aggregate risk analysis algorithm and an engine based on the algorithm is proposed. This engine is implemented in C and OpenMP for multi-core CPUs and in C and CUDA for many-core GPUs. Performance analysis of the algorithm indicates that GPUs offer an alternative HPC solution for aggregate risk analysis that is cost effective. The optimised algorithm on the GPU performs a 1 million trial aggregate simulation with 1000 catastrophic events per trial on a typical exposure set and contract structure in just over 20 seconds which is approximately 15x times faster than the sequential counterpart. This can sufficiently support the real-time pricing scenario in which an underwriter analyses different contractual terms and pricing while discussing a deal with a client over the phone. © 2012 IEEE. Parallel simulations for analysing portfolios of catastrophic event risk aggregate risk analysis; GPU computing; Monte Carlo simulation; parallel risk engine; risk management; risk analytics Algorithms; Application programming interfaces (API); Computer simulation; Costs; Engines; Monte Carlo methods; Program processors; Risk management; Stochastic models; Aggregate risk; Analysis algorithms; Catastrophe insurance; Catastrophic event; GPU computing; Parallel simulations; Performance analysis; Stochastic simulation technique; Aggregates",Risk management
696,Evaluation of enterprise training programs using business process management,"The investment in human capital, by means of training delivered in enterprise, became an important constituent of enterprise competitiveness strategy. Consequently business managers require from their human resources managers, training departments, or even of consultants working in the field of training, the proofs of training investment yield in terms of tangible and intangible profits. To evaluate training in enterprise, two models predominate, namely the model of Kirkpatrick and that of Phillips. In this paper, we propose an approach of training project evaluation, based on business process management. It is an approach which fills the gaps raised in the literature and ensures an alignment between training activities and business needs. Evaluation of enterprise training programs using business process management Business activity monitoring; Business intelligence; Business process management; E-Learning; Evaluation; Machine learning algorithms; Optimization; Return on investment Administrative data processing; Competition; E-learning; Enterprise resource management; Investments; Learning algorithms; Learning systems; Managers; Profitability; Project management; Business Activity Monitoring; Business Process; Business process management; Business-intelligence; E - learning; Evaluation; Machine learning algorithms; Optimisations; Process management; Returns on investment; Personnel training",Strategic alignment
697,E-portfolios in support of informal learning,"Nowadays, informal learning is very much part of everyone is life, even when individuals are not aware that they engage in informal learning. Therefore it is vital that individuals and organisations become aware of the value of informal learning. Not only that, but individuals need to take control of their informal learning and make it known to others. This article illustrates how e-portfolios, as a store of learning activities and resulting products, can support reflection on the learning process by allowing learners to monitor their learning behaviour. Findings indicate that ease of use is crucial. User interface design should accommodate the needs of the learner to promote uptake of the tool. The e-portfolio has to be an integral part of the learner's working and learning processes, and assist the learner by tracking and presenting his learning activities for easy inclusion into the e-portfolio. Copyright © 2014, IGI Global. E-portfolios in support of informal learning Decision support; E-portfolio; Informal learning; Knowledge management; Learning analytics; Lifelong learning; Showcase ",Strategic alignment
698,Finding problem specific shannon information in high dimensional input space for artificial neural networks,"Artificial neural networks, due to their ability to find the underlying model even in complex highly nonlinear and highly coupled problems, have found significant use as prediction engines in many domains. However, in problems where the input space is of high dimensionality, there is the unsolved problem of reducing dimensionality in some optimal way such that Shannon information important to the prediction is preserved. The important Shannon information may be a subset of total information with an unknown partition, unknown coupling and linear or nonlinear in nature. Solving this problem is an important step in classes of machine learning problems and many data mining applications. This paper describes a semi-automatic algorithm that was developed over a 5-year period while solving problems with increasing dimensionality and difficulty in (a) flow prediction for a magnetically levitated artificial heart (13 dimensions), (b) simultaneous chemical identification/concentration in gas chromatography (22 detection dimensions with wavelet compressed time series of 180,000 points), and finally in (c) financial analytics portfolio prediction in credit card and sub-prime debt problems (80 to 300 dimensions of sparse data with a portfolio value of approximately US$300,000,000.00). The algorithm develops a map of input space combinations and their importance to the prediction. This information is used directly to construct the optimal neural network topology for a given error performance. Importantly, the algorithm also produces information that shows whether the space between input nodes is linear or nonlinear; an important parameter in determining the number of training points required in the reduced dimensionality of the training set. Software was developed in the MatLAB environment using the Artificial Neural Network Toolbox, Parallel and Distributed Computing toolboxes, and runs on Windows or Linux based supercomputers. Trained neural networks can be compiled and linked to server applications and run on normal servers or clusters for transaction or web based processing. In this paper, application of the algorithm to two separate financial analytics prediction problems with large dimensionality and sparse data sets are shown. The algorithm is an important development in machine learning for an important class of problems in prediction, clustering, image analysis, and data mining. In the first example application for subprime debt portfolio analysis, performance of the neural network provided a 98.4% prediction rate, compared to 33% rate using traditional linear methods. In the second example application regarding credit card debt, performance of the algorithm provided a 95% accurate prediction (in terms of match rate), and is 10% better than other methods we have compared against, primarily logistic regression. © 2013 World Scientific Publishing Company. Finding problem specific shannon information in high dimensional input space for artificial neural networks Artificial neural network; Credit card debt; Dimensionality reduction; Financial prediction; High dimension space; Information theory; Shannon information; Sub-prime debt Artificial heart; Chemical detection; Computation theory; Computer operating systems; Couplings; Data mining; Distributed computer systems; Finance; Forecasting; Gas chromatography; Information theory; Machine learning; MATLAB; Neural networks; Supercomputers; Credit cards; Dimensionality reduction; Financial prediction; High dimensions; Shannon information; Sub-prime debt; Clustering algorithms",Strategic alignment
699,"Games, risks, and analytics: Several illustrative cases involving national security and management situations","This paper presents and compares four models of games and risk analyses designed to support strategic and policy decisions, three focusing on national security issues and one on project management. They share a common core of probability, linked decisions among the parties involved, and risks to a principal decision maker. Their structure is based on systems and decision analysis. Their level of complexity depends on the strategies, the environment, and assumptions of variation over time of probabilities, preferences, and options. They are part of the field of analytics and some of its real-life applications. The first model is based on a one-move game, in which the United States faces risks of terrorist attacks by several possible groups using various types of weapons. The result is a probabilistic ranking, at a given time, of the threat posed by these weapons. The second model is a dynamic simulation of counterterrorism scenarios in an alternate game between a government and a terrorist group. The objective is to compare the stabilizing effects of different short- and long-term government strategies. The third model is a dynamic evaluation of nuclear counterproliferation strategies involving an analysis of the weapon development program of a particular country with evolving intent and capabilities and of the effectiveness of different YOU.S. strategies to prevent or delay its success. The fourth model is a principal- agent representation of the development of an engineered system, in which an agent in charge of part of the project may consider meeting a deadline by cutting corners if he falls behind schedule, generally increasing the system failure probability. The goal is to support the decisions of the manager in setting constraints and incentives. This paper shows how a set of similar game and risk analysis models at different levels of complexity can provide valuable insights to decision makers, both in national security and management situations, and help them avoid mistakes such as excessive focus on the short term and underestimation of dependencies. It compares their capabilities, including the number of moves, dynamics of the underlying situation, possible changes of context, actors' preferences and strategic options, and risk characterization. ©2012 INFORMS. Games, risks, and analytics: Several illustrative cases involving national security and management situations Analytics; Counterterrorism; Failure probability; Game analysis; National security; Nuclear proliferation; Practice; Principal-agent model; Risk analysis ",Risk management
700,"15th Americas Conference on Information Systems, AMCIS 2009, Volume 4","The proceedings contain 80 papers. The special focus in this conference is on information systems, technologies, the developments of technologies and engineering in America. The topics include: utilitarian and hedonic values of social network services; opportunities and challenges in analysis of social networks; an international model for IS PhD program in low-income countries; the impact of information security rating on vendor competition; tradeoffs in managing the quality of marketing data; limitations of weighted sum measures for information quality; developing an IT project management course to meet changing industry needs; a fuzzy-logical approach for integrating multi-agent estimations; a multi-agent simulation framework for automated negotiation in order promising; incorporating choice into models of technology adoption; factors contributing to an effective business intelligence product; an analytic framework for design-oriented research concepts; the role of gender in the hedonic and utilitarian value of digital games; applying Kolb's theory to distance learning; the effectiveness of deceptive tactics in phishing; modelling online passwords protection intention; criteria for evaluating authentication systems; the role of the family in the IT career goals of middle school Latinas; support structures for women in Information Technology careers; cooperation issues in developing the BOP market; effects of website interactivity on online retail shopping behaviour; generation gap and the impact of the web on goods quality perceptions; an investigation of power in Knowledge Management Systems and leveraging ICT for collaborative decision-making in global customer support. 15th Americas Conference on Information Systems, AMCIS 2009, Volume 4  ",Value management
701,Analytics underlying the metallgesellschaft hedge: Short term futures in a multi-period environment,"In a highly publicized example, a marketing and refining subsidiary of Metallgesellschaft controlled short term derivative positions reportedly equivalent to 160 million barrels of oil, 80 times the daily output of Kuwait. Presumably, the short term positions were taken to hedge oil contracts to customers over an extended period. This paper develops the analytics underlying the hedging of long term flow commitments with short term futures contracts. Its contributions includes the determination of minimum variance hedging paths for multiperiod flow portfolios and the evaluation of both period-by-period and end-of-horizon volatilities under various hedging schemes. The analysis allows for basis risk, non-zero cost-of-carry and spot diffusion processes that have a non-zero market price of risk. The results are developed in the context of an efficient market and standard equilibrium pricing models. © 1999 Kluwer Academic Publishers. Analytics underlying the metallgesellschaft hedge: Short term futures in a multi-period environment  ",Monitoring and control
702,"32nd International Conference on Information System 2011, ICIS 2011, Volume 1","The proceedings contain 299 papers. The special focus in this conference is on Information System. The topics include: Situation awareness through social collaboration platforms in distributed work environments; attaining and enacting green leadership; culture and organizational computer-mediated communication; inter-organizational effects on sociomaterial imbrications and change; intrusiveness of online video advertising and its effects on marketing outcomes; dynamic personal feedback in acquiring information to manage your health; emotions as predictors of performance in virtual worlds; a profiling model for readmission of patients with congestive heart failure; information sharing in NHS polyclinics; integrating self-service kiosks into healthcare delivery organizations; an empirical study with strategic alignment in the healthcare industry; improving knowledge-intensive health care processes beyond efficiency; understanding the drivers and outcomes of healthcare organizational privacy responses; IT-based capabilities, service innovation, and quality in health care; microprocesses of healthcare technology implementation under competing institutional logics; understanding the impact of internet media on patient-clinician trust; measuring emotions in electronic markets; social networking and extending social capacity; an entropy index for multitasking behavior; an instrument for measuring SOA maturity; computer-mediated social networks and environmental behavior; an effective and efficient subpopulation extraction method in very large social networks; reputation and preemption strategies in competing technology networks; software adoption under network effects; consumer product consideration and choice at purchase time at online retailers; business values of community source; learning from peers on social media platforms; code architecture and open source software development; the price and quantity of IT-related intangible capital; the impact of third-party information on the dynamics of online word-of-mouth and retail sales; the roles of agency and artifacts in assembling open data complementarities; the role of culture and personality in the leadership process in virtual teams; user satisfaction of E-government procurement systems in developing countries; differences in knowledge seeking ties between the US and Singapore students; modeling quality dynamics in IT services management; herding behavior as a network externality; exploring information systems control alignment in organizations; the user-centered nature of awareness creation in computer-mediated communication; critical factors affecting compliance to campus alerts; substance and influence in brand communities; the attitude construct in IT adoption research - a scientometric analysis; examining trends of technology diffusion theories in information systems; management of change to ensure IS success; information systems development as a social process; testing tournament selection in creative problem solving using crowds; understanding online payment method choice; the influence of demands and resources on emotional exhaustion with the information systems profession; CIO survival and the composition of the top management team; social capital in the ICT sector - a network perspective on executive turnover and startup performance; activity awareness as a means to promote connectedness, willingness to do additional work, and congeniality; effects of media synchronicity on communication performance; comprehension of online consumer-generated product review; the role of product recommendation agents in collaborative online shopping; three classes of attitude and their implications for IS research; gender differences in virtual collaboration on a creative design task; toward deep understanding of persuasive product recommendation agents; understanding is education quality in developing countries; SYSCO's best business practices (BBP); public expenditure management through khajane - an integrated financial MIS; arguments for the adoption of a heuristic approach to IS research; information systems collaborations as boundary spanning; towards an evidence-based research approach in information systems; grounding theory from Delphi studies; PLS marker variable approach to diagnosing and controlling for method variance; benefits from using continuous rating scales in online survey research; evaluating two automatic methods for classifying information technology concepts; exploring interpersonal relationships in security information sharing; explaining the difference between how security cues and security arguments improve secure behavior; the impact of security practices on regulatory compliance and security performance; personal health records in cloud computing environments; empirical analysis of data breach litigation; extending UTAUT to predict the use of location-based services; project and organizational antecedents of effort withholding in IT project teams; modeling and checking business process compliance rules in the financial sector; participation in open source communities and its implications for organizational commitment; an empirical test of the theory of relationship constraints; an approach for portfolio selection in multi-vendor IT outsourcing; the essential dynamics of information infrastructures; a framework for investigating open innovation processes in ISD; four facets of a process modeling facilitator; a conceptual life event framework for government-to-citizen electronic services provision; perceptual congruence between IS users and professionals on IS service quality - insights from response surface analysis; dynamic service level agreement management for efficient operation of elastic information systems; impact of business intelligence and IT infrastructure flexibility on competitive performance; knowledge refinement effectiveness; towards a framework for measuring knowledge management service productivity; measuring the business value of online social media content for marketers; a data-centric perspective for workflow model management; protecting privacy against regression attacks in predictive data mining; the effects of job design on employees' knowledge contribution to electronic repositories; BI and CRM for customer involvement in product and service development; an effective method of discovering target groups on social networking sites; environmental scanning for customer complaint identification in social media; the effects of user identity and sanctions in online communities on real-world behavior; F-commerce and the crucial role of trust; when artificial feedback hurts - empirical evidence from community-based configuration systems; knowledge collaboration in distributed practice communities; a social identity perspective on participation in virtual healthcare communities; boundary-spanning documents in online communities; network stability and social contagion on the mobile internet; digital complementary assets; trusting social location technologies and interactions; telepresence in business meetings; exploring the role of online social network dependency in habit formation; a sociomateriality practice perspective of online social networking; automatic reputation assessment in Wikipedia; strategic decision support for smart-leasing infrastructure-as-a-service; the impact of different types of satisfaction on C2C platform loyalty; automated negotiations under uncertain preferences; nurturing sales entrepreneurship in consumer-to-consumer marketplaces; competing across different channels for personalized service; the influences of negativity and review quality on the helpfulness of online reviews; winner determination of open innovation contests in online markets; a dual view on IT challenges in corporate divestments and acquisitions; managing the IT integration of acquisitions by multi-business organizations; management commitments that maximize business impact from IT; institutional work and artifact evolution; four perspectives on architectural strategy; a coevolutionary journey of strategic knowledge management alignment; use of social media in disaster management; unpacking the duality of design science; acquiring IT competencies through focused technology acquisitions; IT artifacts and the state of IS research; senior scholars' forum and online social networking and citizen engagement. 32nd International Conference on Information System 2011, ICIS 2011, Volume 1  ",Strategic alignment
703,An experimental study of financial portfolio selection with visual analytics for decision support,"We investigate the decision process as applied to the practical task of choosing a financial portfolio. We developed PortfolioCompare, an interactive visual analytic decision support tool that helps the consumer quickly create, compare and choose among several portfolios consisting of different financial instruments. PortfolioCompare facilitates the analysis of risk and return aspects of each portfolio considered. We investigate behavior in this task using an economic experiment in which the user actively creates and compares portfolios from a set of funds. We elicit risk preferences using a separate task and find that subjects using PortfolioCompare make decisions that are closer to their risk tolerance as compared to subjects presented with similar information in textual form. This finding suggests that PortfolioCompare helps understand risk aspects of portfolios. Portfolio selections are improved during the course of the decision process, suggesting that this tool is valuable for decision support. © 2011 IEEE. An experimental study of financial portfolio selection with visual analytics for decision support Economic decision-making; Human-computer interaction; Knowledge discovery; Laboratory studies; Visualization for the masses Decision making; Decision support systems; Knowledge management; Risk analysis; Risk assessment; Systems science; Visualization; Decision process; Decision support tools; Decision supports; Economic decision-making; Economic experiments; Experimental studies; Financial portfolio; Human-computer; Knowledge discovery; Laboratory studies; Portfolio selection; Risk aspects; Risk preference; Risk tolerance; Visual analytics; Human computer interaction",Risk management
704,Probabilistic safety risk analysis in complex domains: Application to unmanned aircraft systems,"Aviation is a complex domain characterized by low probability, high consequence events with scarce data; hence, safety risk modeling is particularly challenging. There continues to be a persistent need to develop the analytics to capture both the explicit and implicit risks inherent in such domains. A Value-based Time-phased Bayesian Network (VTBN) extends a conventional Bayesian Network (BN) by including features from a Dynamic Bayesian Network such as temporal risk factors that are enhanced by Multi-Attribute Value Theory (MAVT). The proposed VTBN integrates the quantitative analytic constructs of BNs and MAVT with the qualitative formalism of a structured hazard taxonomy. The enhanced methodology provides a framework for the systematic inclusion of the explicit risk inherent from the BN and the non-apparent implicit risk in the BN that exists in large complex systems. Preliminary modeling results suggest that VTBNs offer promise for advanced risk assessment, particularly for Unmanned Aircraft Systems (UAS) where civilian data are especially sparse. In this paper, the analytic constructs of a VTBN are demonstrated with an application of safety risk modeling to aid in the prioritization of a portfolio of mitigations for a futuristic UAS scenario. © 2011 by the American Institute of Aeronautics and Astronautics, Inc. All rights reserved. Probabilistic safety risk analysis in complex domains: Application to unmanned aircraft systems  Aircraft; Aviation; Risk assessment; Technology; Unmanned aerial vehicles (UAV); Complex domains; Dynamic Bayesian networks; Large complex systems; Low probability; Multi-attribute value theories; Preliminary modeling; Probabilistic safety; Unmanned aircraft system; Bayesian networks",Strategic alignment
705,A survey of open source tools for business intelligence,"The industrial use of open source Business Intelligence (BI) tools is not yet common. It is therefore of interest to explore which possibilities are available for open source BI and compare the tools. In this survey paper, we consider the capabilities of a number of open source tools for BI. In the paper, we consider three Extract-Transform-Load (ETL) tools, three On-Line Analytical Processing (OLAP) servers, two OLAP clients, and four database management systems (DBMSs). Further, we describe the licenses that the products are released under. It is argued that the ETL tools are still not very mature for use in industry while the DBMSs are mature and applicable to real-world projects. The OLAP servers and clients are not as powerful as commercial solutions but may be useful in less demanding projects. © Springer-Verlag Berlin Heidelberg 2005. A survey of open source tools for business intelligence  Competitive intelligence; Database systems; Online systems; Project management; Real time systems; Servers; Extract-Transform-Load (ETL) tools; On-Line Analytical Processing (OLAP) servers; Open source tools; Real-world projects; Open systems",Risk management
706,"32nd International Conference on Information System 2011, ICIS 2011, Volume 4","The proceedings contain 299 papers. The special focus in this conference is on Information System. The topics include: Situation awareness through social collaboration platforms in distributed work environments; attaining and enacting green leadership; culture and organizational computer-mediated communication; inter-organizational effects on sociomaterial imbrications and change; intrusiveness of online video advertising and its effects on marketing outcomes; dynamic personal feedback in acquiring information to manage your health; emotions as predictors of performance in virtual worlds; a profiling model for readmission of patients with congestive heart failure; information sharing in NHS polyclinics; integrating self-service kiosks into healthcare delivery organizations; an empirical study with strategic alignment in the healthcare industry; improving knowledge-intensive health care processes beyond efficiency; understanding the drivers and outcomes of healthcare organizational privacy responses; IT-based capabilities, service innovation, and quality in health care; microprocesses of healthcare technology implementation under competing institutional logics; understanding the impact of internet media on patient-clinician trust; measuring emotions in electronic markets; social networking and extending social capacity; an entropy index for multitasking behavior; an instrument for measuring SOA maturity; computer-mediated social networks and environmental behavior; an effective and efficient subpopulation extraction method in very large social networks; reputation and preemption strategies in competing technology networks; software adoption under network effects; consumer product consideration and choice at purchase time at online retailers; business values of community source; learning from peers on social media platforms; code architecture and open source software development; the price and quantity of IT-related intangible capital; the impact of third-party information on the dynamics of online word-of-mouth and retail sales; the roles of agency and artifacts in assembling open data complementarities; the role of culture and personality in the leadership process in virtual teams; user satisfaction of E-government procurement systems in developing countries; differences in knowledge seeking ties between the US and Singapore students; modeling quality dynamics in IT services management; herding behavior as a network externality; exploring information systems control alignment in organizations; the user-centered nature of awareness creation in computer-mediated communication; critical factors affecting compliance to campus alerts; substance and influence in brand communities; the attitude construct in IT adoption research - a scientometric analysis; examining trends of technology diffusion theories in information systems; management of change to ensure IS success; information systems development as a social process; testing tournament selection in creative problem solving using crowds; understanding online payment method choice; the influence of demands and resources on emotional exhaustion with the information systems profession; CIO survival and the composition of the top management team; social capital in the ICT sector - a network perspective on executive turnover and startup performance; activity awareness as a means to promote connectedness, willingness to do additional work, and congeniality; effects of media synchronicity on communication performance; comprehension of online consumer-generated product review; the role of product recommendation agents in collaborative online shopping; three classes of attitude and their implications for IS research; gender differences in virtual collaboration on a creative design task; toward deep understanding of persuasive product recommendation agents; understanding is education quality in developing countries; SYSCO's best business practices (BBP); public expenditure management through khajane - an integrated financial MIS; arguments for the adoption of a heuristic approach to IS research; information systems collaborations as boundary spanning; towards an evidence-based research approach in information systems; grounding theory from Delphi studies; PLS marker variable approach to diagnosing and controlling for method variance; benefits from using continuous rating scales in online survey research; evaluating two automatic methods for classifying information technology concepts; exploring interpersonal relationships in security information sharing; explaining the difference between how security cues and security arguments improve secure behavior; the impact of security practices on regulatory compliance and security performance; personal health records in cloud computing environments; empirical analysis of data breach litigation; extending UTAUT to predict the use of location-based services; project and organizational antecedents of effort withholding in IT project teams; modeling and checking business process compliance rules in the financial sector; participation in open source communities and its implications for organizational commitment; an empirical test of the theory of relationship constraints; an approach for portfolio selection in multi-vendor IT outsourcing; the essential dynamics of information infrastructures; a framework for investigating open innovation processes in ISD; four facets of a process modeling facilitator; a conceptual life event framework for government-to-citizen electronic services provision; perceptual congruence between IS users and professionals on IS service quality - insights from response surface analysis; dynamic service level agreement management for efficient operation of elastic information systems; impact of business intelligence and IT infrastructure flexibility on competitive performance; knowledge refinement effectiveness; towards a framework for measuring knowledge management service productivity; measuring the business value of online social media content for marketers; a data-centric perspective for workflow model management; protecting privacy against regression attacks in predictive data mining; the effects of job design on employees' knowledge contribution to electronic repositories; BI and CRM for customer involvement in product and service development; an effective method of discovering target groups on social networking sites; environmental scanning for customer complaint identification in social media; the effects of user identity and sanctions in online communities on real-world behavior; F-commerce and the crucial role of trust; when artificial feedback hurts - empirical evidence from community-based configuration systems; knowledge collaboration in distributed practice communities; a social identity perspective on participation in virtual healthcare communities; boundary-spanning documents in online communities; network stability and social contagion on the mobile internet; digital complementary assets; trusting social location technologies and interactions; telepresence in business meetings; exploring the role of online social network dependency in habit formation; a sociomateriality practice perspective of online social networking; automatic reputation assessment in Wikipedia; strategic decision support for smart-leasing infrastructure-as-a-service; the impact of different types of satisfaction on C2C platform loyalty; automated negotiations under uncertain preferences; nurturing sales entrepreneurship in consumer-to-consumer marketplaces; competing across different channels for personalized service; the influences of negativity and review quality on the helpfulness of online reviews; winner determination of open innovation contests in online markets; a dual view on IT challenges in corporate divestments and acquisitions; managing the IT integration of acquisitions by multi-business organizations; management commitments that maximize business impact from IT; institutional work and artifact evolution; four perspectives on architectural strategy; a coevolutionary journey of strategic knowledge management alignment; use of social media in disaster management; unpacking the duality of design science; acquiring IT competencies through focused technology acquisitions; IT artifacts and the state of IS research; senior scholars' forum and online social networking and citizen engagement. 32nd International Conference on Information System 2011, ICIS 2011, Volume 4  ",Strategic alignment
707,Calculating quantile-based risk analytics with L-estimators,"Quantile-based measures of risk, e.g., value at risk (VaR), are widely used in portfolio risk applications. Increasing attention is being directed toward managing risk, which involves identifying sources of risk and assessing the economic impact of potential trades. This article compares the performance of two quantile-based VaR estimators commonly applied to assess the market risk of option portfolios and the credit risk of bond portfolios. © Emerald Backfiles 2007. Calculating quantile-based risk analytics with L-estimators  ",Risk management
708,Too close to the hedge: The case of long term capital management LP Part one: Hedge fund analytics,"In September 1998, the well-known US hedge fund, Long Term Capital Management (LTCM) announced it had lost 44 per cent ($2.1 billion) of its investors' money in August alone, and more than 52 per cent from the beginning of the year. This shock caused the Federal Reserve System to organise a bail-out by banks and investment houses to save the fund from liquidation and to prevent follow-on damage to US financial markets. Part One of this Case Study briefly reviews the loss of value by LTCM and the damage caused to investors and creditors. LTCM is placed in context as the Case looks at the nature of hedge funds in the investment business in general - their structure, regulatory position, trading strategies and risk/return profiles. Hedge fund investment (sometimes called 'skill-based investment strategies') is examined to see how compatible it is with traditional theoretical models of investment - notably modern portfolio theory and the capital asset pricing model. As far as hedge funds are concerned, traditional theory is deficient in several respects. Skill-strategy investment managers claim to be able to produce superior absolute returns, in most cases at lower risk, to traditional long-only mutual funds. Hedge funds, although varying widely in their trading styles, are usually characterised by short selling and leverage. LTCM shares these characteristics which provide some of the background against which the imminent collapse of the fund can be assessed. Too close to the hedge: The case of long term capital management LP Part one: Hedge fund analytics  ",Strategic alignment
709,"16th Americas Conference on Information Systems, AMCIS 2010, Volume 5","The proceedings contain 75 papers. The special focus in this conference is on business intelligence, business process modelling, project management, IT organizational learning and healthcare information technology. The topics include: selecting and ranking IT governance practices for electric utilities; examining virtual tourism community in China; a social network perspective of information systems project management; a theoretical model of the enterprise system agility life-cycle; an inductive classification scheme for green IT initiatives; forensic data mining; an exploratory study about microblogging acceptance at work; conceptualizing public service value in e-government services; e-learning to improve intercultural communication; assessment of ubiquitous healthcare information systems benefits; a typology of design knowledge; IT enabled in-home direct selling presentations; a methodology for profiling literature using co-citation analysis; building traceability systems; text classification with imperfect hierarchical structure knowledge; taxonomy development in information systems; from cross-organizational business process to public services; process complexity impact on IS audit service quality; a meta-model based approach to the description of resources and skills; warehousing and analyzing streaming data quality information; from green IT to sustainable innovation; a cybernetic view on data quality management; the impact of instant messaging in the workplace and a theory-based approach for a modular system of interactive decision aids. 16th Americas Conference on Information Systems, AMCIS 2010, Volume 5  ",Governance
710,"Introduction to Fixed Income Analytics: Relative Value Analysis, Risk Measures, and Valuation","A comprehensive introduction to the key concepts of fixed income analytics. The First Edition of Introduction to Fixed Income Analytics skillfully covered the fundamentals of this discipline and was the first book to feature Bloomberg screens in examples and illustrations. Since publication over eight years ago, the markets have experienced cathartic change. That is why authors Frank Fabozzi and Steven Mann have returned with a fully updated Second Edition. This reliable resource reflects current economic conditions, and offers additional chapters on relative value analysis, value-at-risk measures and information on instruments like TIPS (treasury inflation protected securities). Offers insights into value-at-risk, relative value measures, convertible bond analysis, and much more. Includes updated charts and descriptions using Bloomberg screens. Covers important analytical concepts used by portfolio managers Understanding fixed-income analytics is essential in today's dynamic financial environment. The Second Edition of Introduction to Fixed Income Analytics will help you build a solid foundation in this field. © 2010 John Wiley & Sons, Inc. All rights reserved. Introduction to Fixed Income Analytics: Relative Value Analysis, Risk Measures, and Valuation  ",Risk management
711,A single solution to intelligent lifecycle management,"Forest City Ratner Companies (FCRC), New York City's largest real-estate developer has implemented Proliance from Meridian, an integrated lifecycle tool to manage their real-estate operations. Proliance is designed to optimize the plan, build and operate phases of capital construction and facility projects. It is a tool that captures data about the company's assets throughout their development lifecycle and gives the information needed to achieve business intelligence. The tool replaces manual paper shuffling with electronic workflow, creates an audit trial for changes and approvals, and provides transparency into its project portfolio. The company will benefit better visibility, increased productivity, standardized workflow, improved reporting, decreased risk and solid ROI with the implementation of this tool. A single solution to intelligent lifecycle management  Competitive intelligence; Optimization; Productivity; Project management; Strategic planning; Sustainable development; audit trial; Forest City Ratner Companies (FCRC) (CO); Integrated lifecycle tool; Construction industry",Capacity management
712,"16th Americas Conference on Information Systems, AMCIS 2010, Volume 3","The proceedings contain 81 papers. The special focus in this conference is on business intelligence, business process modelling, project management, IT organizational learning and healthcare information technology. The topics include: conflict, conflict management and performance in virtual teams; challenges of teaching information quality; the influence of human factors on vulnerability to information security breaches; clinicians' emotions and telestroke use; information technology adoption in Latin American microenterprises; antecedents of sustainable management support for IT-related initiatives; collaboration and end-user information management tools; iterative development of professional knowledge intensive business processes; tutorial on latent growth models for longitudinal data analysis; net neutrality and its implications; information security practices in Latin America; risks and hidden costs; satisfaction with social networking sites; the dilemma of addressing SAP skills shortages in developing countries; decision support variables for reverse logistics; decision support variables for reverse logistics in econometric model; building community sustainability with geographic information systems; the impact of individual centrality and helping on knowledge sharing; the extended advertising network model; human interaction with structure in the computing environment; a cultural theory analysis of information systems adoption; integration for innovation; critical competencies for the Brazilian CIO; tacit knowledge transfer within organisations; a method for business sequential data prediction and Re-conceptualizing IS strategic alignment. 16th Americas Conference on Information Systems, AMCIS 2010, Volume 3  ",Strategic alignment
713,"16th Americas Conference on Information Systems, AMCIS 2010, Volume 6","The proceedings contain 80 papers. The special focus in this conference is on business intelligence, business process modelling, project management, IT organizational learning and healthcare information technology. The topics include: a reference model catalog of models for business process analysis; web standards in the IS curriculum; coping with nuisances on the web; mitigating the effects of partial resource failures for cloud providers; awaiting explanation in the field of enterprise architecture management; business process maturity's effect on performance; information systems maintenance; cultural differences in implementing business process management systems; collaborative business process modeling using 3D virtual environments; ethical dimensions of the information society; a structural model of information system quality; challenges in knowledge management; a structuration approach to online communities of practice; making a case for scenario-based learning in IS and executive education; agent-based simulation for evaluation of a mobile emergency management system; organic evolution and the capability maturity of business intelligence; hybrid approach for the maintenance of materialized webviews; the impact of information technology on european post-trading; metaphysics of change and identity; management of service innovation projects; integration process factors and effects; IT shared service center and external market activities; OpenID as an approach for user-centric identity management; Wikipedia as an academic reference; core asset repository methodology (CARM) for software reuse and antecedents of organizational competency development. 16th Americas Conference on Information Systems, AMCIS 2010, Volume 6  ",Value management
714,Real time modeling for financial and performance management,"As cement companies look to improve their operations in the face of ever increasing global pressure and competition, continuous improvement programs whether formal or informal have been undertaken by companies in an effort to remain competitive and improve their competitive position. To ensure that continuous improvement projects and initiatives are adding value to the company, a real-time performance measurement system must be coupled with continuous improvement programs. A proper Real-Time Performance Measurement System providing business intelligence is critical to showing the value of projects to the company, recovering investment as soon as possible and ensuring the strategic nature of initiatives and projects in a timeframe, resolution and reach that make sense. It brings together the proper elements of finance, strategy, and operations to effectively measure performance, provide timely feedback to the right people, and enable performance improvement. In this paper, we will present a methodology and the basic components necessary to develop a real-time performance measurement system that can be used in continuous manufacturing industries to measure and enable performance improvement. To the extent possible, representative plant data, processes, etc. will be used to demonstrate this. © 2005 IEEE. Real time modeling for financial and performance management  Cement industry; Competitive intelligence; Data acquisition; Data processing; Mathematical models; Project management; Continuous manufacturing industries; Performance management; Real time modeling; Real time systems",Strategic alignment
715,Risk analytics for hedge funds,"The rapid growth of the hedge fund industry presents significant business opportunity for the institutional investors particularly in the form of portfolio diversification. To facilitate this, there is a need to develop a new set of risk analytics for investments consisting of hedge funds, with the ultimate aim to create transparency in risk measurement without compromising the proprietary investment strategies of hedge funds. As well documented in the literature, use of dynamic options like strategies by most of the hedge funds make their returns highly non-normal with fat tails and high kurtosis, thus rendering Value at Risk (VaR) and other mean-variance analysis methods unsuitable for hedge fund risk quantification. This paper looks at some unique concerns for hedge fund risk management and will particularly concentrate on two approaches from physical world to model the non-linearities and dynamic correlations in hedge fund portfolio returns: Self Organizing Criticality (SOC) and Random Matrix Theory (RMT).Random Matrix Theory analyzes correlation matrix between different hedge fund styles and filters random noise from genuine correlations arising from interactions within the system. As seen in the results of portfolio risk analysis, it leads to a better portfolio risk forecastability and thus to optimum allocation of resources to different hedge fund styles. The results also prove the efficacy of self-organized criticality and implied portfolio correlation as a tool for risk management and style selection for portfolios of hedge funds, being particularly effective during non-linear market crashes. Risk analytics for hedge funds Dynamic Conditional Correlation (DCC) multivariate GARCH; Hedge Fund Risk Analysis; Implied Portfolio Correlation; Random Matrix Theory; Self Organized Criticality Commerce; Costs; Investments; Matrix algebra; Strategic planning; Dynamic condition correlation (DCC) multivariate GARCH; Hedge fund risk analysis; Implied portfolio correlation; Random matrix theory; Self organized criticality; Risk assessment",Strategic alignment
716,Wealth forever: The analytics of stock markets,"This book is the first of its kind in providing, simultaneously and comprehensively, historical, institutional and theoretical foundations for developments in the stock market. It debunks many a myth about stock price behavior and the valuation of stocks. The traditional valuation models are tested and shown to be often weak and unreliable, especially when applied to the valuation of technology stocks. New paradigms are suggested. The authors seek to answer many questions about the stock market: Why invest in stocks, how to invest in stocks, how to value stocks, how to change the risk profile of portfolios, how to analyze the results of stock investing, and how to minimize estate taxes and maximize control, even after death. All aspects of the stock market are covered, including the basic tools that will enable the reader to understand the stock market basics, the history of stock market performance in the US and overseas, the various ways to value stocks and to assess their risk, and the various methods that have been proposed to capitalize on the inefficiencies of the stock market, be they temporary or permanent. The book also deals with the derivative markets for stocks. © 2003 by World Scientific Publishing Co. Pte. Ltd. All rights reserved. Wealth forever: The analytics of stock markets  ",Risk management
717,Agent-network: a fusion method for heterogeneous business intelligence technologies,"In order to fuse heterogeneous business intelligence (BI) technologies such as rough set, fuzzy logics, decision tree, group decision, swarm algorithm, data mining, genetic algorithm, artificial neural networks, a complex network and multi-Agent based method called as Agent-network was put forward. The method includes three basic mechanisms: intelligence resource (IR) aggregating mechanism based on autonomous Agent and complex network, IR using mechanism based on ""competing for post"" and ""selecting the best for cooperation"", IR optimizing mechanism based on performance evaluation and ""survival of the fittest"". A BI software system based on the method is a complex network composed of many Agents as nodes. Each Agent is a unit of BI resource which represents a computing model or an algorithm. By means of the three mechanisms of Agent-network, a BI system can realize two levels of BI technology fusion. One is called as fusion on system structure: new Agents are added into the network continuously and less value existing Agents are deleted from it, all valued Agents (on behalf of all excellent BI technologies) are aggregated in the system. Another is called as fusion on system application: by using some marketing mechanism such as negotiation, bidding and auction, the system always can find an optimal Agent portfolio to respond to a specific service request of user. Agent-network: a fusion method for heterogeneous business intelligence technologies Business intelligence; Complex network; Intelligence fusion; Multi-Agent Autonomous agents; Decision trees; Fuzzy logic; Fuzzy neural networks; Fuzzy sets; Heterogeneous networks; Mining; Multi agent systems; Technology; Artificial Neural Network; Basic mechanism; Business intelligence; Complex network; Complex networks; Computing model; Fusion methods; Group decision; Intelligence fusion; Multi-Agent; Multi-agent based methods; Performance evaluation; Rough set; Service requests; Software systems; Swarm algorithms; System applications; System structures; Technology fusion; Data mining",Financial management
718,"10th Americas Conference on Information Systems, AMCIS 2004","The proceedings contain 577 papers. The special focus in this conference is on Information Systems. The topics include: Integrating Human-Computer Interaction Development into SDLC: A Methodology; integrating Tomorrow’s Government: An Exploratory Case Study of Resistance to Data Consolidation; intelligent Agents and Information Resource Management; intelligent Agents; interpreting Scenario-Based Design from an Information Systems Perspective; Introducing New IT Project Management Practices – a Case Study; investigating Deception in Cyberspace; investigating Factors Influencing Telemedicine Usage in Developing Countries; Investigating the Socio-economic Characteristics of Residential Consumers of Broadband in the UK; investments in Web Technologies and Firm Value Effects; job Satisfaction of Information Technology Professionals; KBSVM: KMeans-based SVM for Business Intelligence; KMS-Enabled Individual Learning in the Workplace; KeyTEx – An Integrated Prototype for Semi-automatic Metadata Assignment and Network-based Content Retrieval; knowledge Management Capability and Firm Performance: An Empirical Investigation; knowledge Management as an antecedent of Enterprise System Success; knowledge Management at General Electric: A Technology Transfer Case Study; knowledge Management in Developing Countries: understanding an interpretivist approach; knowledge Worker Adoption of Time Management Tools: Satisfaction and Perceived Effectiveness; knowledge as Process, not Data: the role of Process Based Systems in developing organisational knowledge and behaviour.; LIMEFRAME: LearnIng content Management systEm in the FRAmework of MEtacognition; lagged Impact of Information Technology on Organizational Productivity; learning Agents for Dynamic Supply Network Management; learning Effectiveness of Cross-Cultural Virtual Teams; learning through Telemedicine Networks: The Case Study of a Wound Care Network; lessons Learned in Global Videoconference Training: Action Research at a Community-based Organization; visualizing Cyber Personality. 10th Americas Conference on Information Systems, AMCIS 2004  ",Financial management
719,A review of portfolio planning. Models and systems.,"This chapter provides an overview of a number of portfolio planning models which have been proposed and investigated over the past 50 years. The mean-variance (MV) model of Markowitz is a single period static portfolio planning model, and, in recent times, it has become the core decision engine of many portfolio analytics and planning systems in the construction of the risk-return efficient frontier. The estimation of the underlying parameters which are required as the input to MV analysis is an important modeling step. Small changes in the input can have a large impact on the optimal asset weights. Diagonal models are of interest as the corresponding quadratic forms can then be expressed as variable separable functions, which in turn are approximated as piecewise linear functions. The use of factor models to describe asset returns can also lead to a diagonal form provided the composition of the covariance matrix is appropriately exploited. © 2003 Elsevier Ltd All rights reserved. A review of portfolio planning. Models and systems.  ",Capacity management
720,Change management: From knowledge about innovative se to capabilities for industrial se projects,"sd&m AG, software design & management, has the development and integration of custom built information systems for business critical processes as its area of business. IT consulting with engineering and implementation competence makes it complete. Our clients are major companies and organizations. They want to achieve a competitive edge by implementing custom built solutions. The core competence of the 1.600 person company is the design of IT architectures and the realization of complex projects in a cooperative way with clients. The overall key success factor is the qualification of the team. Continuously to manage is the growth including small acquisitions, the dynamic of technology and the increasing expectations of our clients. For our clients we are innovation partner. Our understanding is not to be the earliest possible adapter for the last hype but being able to differentiate between hype and future value. We use two sources for improvement: project experience and innovation management. Project experience: We have implemented continuous cycles of learning and improvement, project experiences and best practices are collected and evaluated by communities and sd&m Research, enriched by scientific methods together with universities. Best practices certified by our best software architects will be finished to excellent solutions and become part of our organized knowledge, can be distributed by our development platform as a solution pattern or component, influence our training schools, flow into publications and community work and sometimes result in a book. Announcement and distribution is one of the key success factors. Innovation management: Results of our innovation management are in a first phase knowledge with small experience about a new programming language or environment, a new integration product suite or a new way to design application systems and their successful integration into complex application landscapes. In the following phase we go to our market and challenge our clients view on concrete innovation examples. If we find their interest we identify together real small pilot projects. After project end together with the client we evaluate the results compared to what we expected. If it was valuable for one client we go back with our first capabilities and make to steps: (a) building a new community and (b) designing qualification methods. The community works on the topic of interest with more prototypes, cookbooks and infrastructure - we invest. The qualification methods have at least to outputs: how to integrate the content into the standard qualification activities and how to ramp up the experienced part of the team. For really important content in the domain of our core competence we implement so called schools. In every school we educate 20 - 30 software engineers in 4 to 5 days distributed over the whole company. The trainers come from the community. The life cycle of this school type is 1 - 2 years, then the content is established in the standard education. Parallel with the first projects starts the above describe improvement process by systematic evaluation of project experiences in the new arenas. On the other side we work on the design of the new service offering and bring it to the market. This works only with qualified colleagues. It is always the same: fighting for the first reference clients and projects. Effects where we were successful are for example .net, SAP NetWeaver Technology and business intelligence solutions. First class qualified software engineers and architects are necessary to keep this continuous change management alive with the objective to transfer knowledge into capabilities. Change management: From knowledge about innovative se to capabilities for industrial se projects Software Engineering Education Architectural design; Design; Education; Engineering; Engineering education; Integration; Life cycle; Management; Programming theory; Societies and institutions; Software design; Software engineering; Computer software; Information technology; Project management; Spontaneous emission; Business Process; Change management; Competitive edges; Core competence; Innovation management; IS design; IT architecture; Key success factors; Two sources; Best practices; Business intelligences; Change managements; Community works; Competitive edges; Complex applications; Complex projects; Core competences; Design applications; Development platforms; Innovation managements; IT architectures; Key success factors; NetWeaver; New services; Pilot projects; Product suites; Programming languages; Project experiences; Scientific methods; Software architects; Software Engineering Education; Software engineers; Systematic evaluations; Two sources; Project management; Software design",Governance
721,ICx acquires Griffen in global marketability drive,"Washington DC-based homeland-security ICx Technologies has acquired portable chemical-detection group Griffin Analytics Technologies of the US in a bid to increase its global marketability. Griffin becomes a part of ICx's chemical, biological, radiological, nuclear, and explosive (CBRNE) detection division where its chemical detection technology will complement ICx's capabilities, which range from bio-threat to radiation and explosives. Griffin's leading-edge mass-spectrometry technology has significant potential and will open up new applications. ICx will immediately provide a larger footprint and a platform for Griffin to further penetrate the global defense and security markets. The company will provide numerous opportunities to participate in high-profile projects that involve the installation of state-of-the-art, complete security solutions. ICx acquires Griffen in global marketability drive  Chemical sensors; Explosives; Installation; Marketing; Project management; Radiation effects; Security systems; Global marketability; Griffin (CO); Security markets; Security solutions; Mergers and acquisitions",Risk management
722,"IEEE/IAFE Conference on Computational Intelligence for Financial Engineering, Proceedings (CIFEr)","The proceedings contain 58 papers. The topics discussed include: estimation of default probability by three-factor structural model; bankruptcy prediction for credit risk using an auto-associative neural network in Korean firms; the predictive power of dividend yields analyzed by methods preserving time- dependent structures; analytics and algorithms for geometric average trigger reset options; a multiobjective genetic programming approach for pricing and hedging derivative securities; fast Monte Carlo valuation of barrier options for jump diffusion processes; on generalized arbitrage pricing theory analysis: empirical investigation of the macroeconomics modulated independent state-space model; attrition and preemption in credit/debit cards incentives: models and experiments; microeconomic modeling of financial time series with long term memory; risk related non linearities in exchange rates: a comparison of parametric and semiparametric estimates; an artificial neural network framework for dual interest rate parity; and a Monte-Carlo method for portfolio optimization under partially observed stochastic volatility. IEEE/IAFE Conference on Computational Intelligence for Financial Engineering, Proceedings (CIFEr)  ",Monitoring and control
723,An identification and classification of enterprise portal functions and features,"Enterprise portal is a type of new information system that can help companies and their employees to manage, share, and use previously disparate information. There are more than 60 vendors that are offering corporate portal solutions. With so many vendors, selecting the right one can be a difficult task. The primary objective of this research is to identify and evaluate the functions and features in enterprise portal products. In particular, this study develops a simplified model that can be used for identifying and classifying the functions and features in corporate portal software. The results of this study may be useful to information technology managers, educators, and students involved in knowledge management, business intelligence, information systems resources management, and data management. System developers, software engineers, project managers, financial managers, and data architects can use the functions and features identified in this study as benchmarking tools for evaluating portals capabilities. An identification and classification of enterprise portal functions and features Information management; Information systems; Internet; Knowledge workers Benchmarking; Database systems; Enterprise resource planning; Information dissemination; Information management; Information technology; Knowledge based systems; Personnel; Portals; Project management; Societies and institutions; Software engineering; Enterprise portals; Information systems; Knowledge management; Knowledge workers; Electronic commerce",Financial management
724,Healthcare informatics: Improving efficiency and productivity,"Healthcare Informatics: Improving Efficiency and Productivity examines the complexities involved in managing resources in our healthcare system and explains how management theory and informatics applications can increase efficiencies in various functional areas of healthcare services. Delving into data and project management and advanced analytics, this book details and provides supporting evidence for the strategic concepts that are critical to achieving successful healthcare information technology (HIT), information management, and electronic health record (EHR) applications. This includes the vital importance of involving nursing staff in rollouts, engaging physicians early in any process, and developing a more receptive organizational culture to digital information and systems adoption. We owe it to ourselves and future generations to do all we can to make our healthcare systems work smarter, be more effective, and reach more people. The power to know is at our fingertips; we need only embrace it. -From the foreword by James H. Goodnight, PhD, CEO, SAS Bridging the gap from theory to practice, it discusses actual informatics applications that have been incorporated by various healthcare organizations and the corresponding management strategies that led to their successful employment. Offering a wealth of detail, it details several working projects, including: A computer physician order entry (CPOE) system project at a North Carolina hospital E-commerce self-service patient check-in at a New Jersey hospital The informatics project that turned a healthcare system's paper-based resources into digital assets Projects at one hospital that helped reduce excesses in length of stay, improved patient safety; and improved efficiency with an ADE alert system A healthcare system's use of algorithms to identify patients at risk for hepatitis Offering the guidance that healthcare specialists need to make use of various informatics platforms, this book provides the motivation and the proven methods that can be adapted and applied to any number of staff, patient, or regulatory concerns. © 2010 by Taylor & Francis Group, LLC. Healthcare informatics: Improving efficiency and productivity  ",Strategic alignment
725,Beyond VaR: parametric and simulation-based risk management tools,"An extended simulation-based risk management toolkit developed on top of the analytical tools presented by Litterman is introduced. These tools provide additional insights when the portfolio contains non-linearities, when the market distributions are not normal or when there are multiple horizons. It is demonstrated that efficient computational methods are available that require no additional simulation to obtain risk management analytics. The limitations of these tools are also discussed as well as the steps for dealing with them. Beyond VaR: parametric and simulation-based risk management tools  Computer aided software engineering; Computer simulation; Laws and legislation; Marketing; Optimization; Risk management; Societies and institutions; Financial institutions; Portfolio risks; Risk measurement; Value at risk; Financial data processing",Risk management
726,Adjusted case-based software effort estimation using bees optimization algorithm,"Case-Based Reasoning (CBR) has achieved a considerable interest from researchers for solving non-trivial or ill-defined problems such as those encountered by project managers including support for software project management in predictions and lesson learned. Software effort estimation is the key factor for successful software project management. In particular, the use of CBR for effort estimation was favored over regression and other machine learning techniques due to its performance in generating reliable estimates. However, this method was subject to variety of design options which therefore has strong impact on the prediction accuracy. Selection of CBR adjustment method and deciding on the number of analogies are such two important decisions for generating accurate and reliable estimates. This paper proposed a new method to adjust the retrieved project efforts and find optimal number of analogies by using Bees optimization algorithm. The Bees algorithm will be used to search for the best number of analogies and features coefficient values that will be used to reduce estimates errors. Results obtained are promising and the proposed method could form a useful extension for Case-based effort prediction model. © 2011 Springer-Verlag. Adjusted case-based software effort estimation using bees optimization algorithm Bees Algorithm; Case-Based Reasoning; Software Effort Estimation Algorithms; Estimation; Information management; Knowledge based systems; Mathematical models; Object oriented programming; Optimization; Project management; Forecasting; Genetic algorithms; Knowledge based systems; Learning systems; Object oriented programming; Predictive analytics; Project management; Adjustment method; Bees algorithms; CBr; Coefficient values; Design option; Effort Estimation; Effort prediction model; Key factors; Machine learning techniques; Non-trivial; Optimal number; Optimization algorithms; Prediction accuracy; Project managers; Reliable estimates; Software Effort Estimation; Software project management; Bees algorithms; Casebased reasonings (CBR); Effort prediction model; Machine learning techniques; Optimization algorithms; Prediction accuracy; Software effort estimation; Software project management; Case based reasoning; Case based reasoning",Strategic alignment
727,Performance management goes inside,"The significant aspects of enterprise performance management (EPM) are discussed. EPM is about knowledge and improvement, that defines the metric, sets the goal, monitors the process and measures performance. EPM systems have appeared as data warehouse systems, and other add on products that range from executive dashboards, balanced scorecard applications and activity based management systems. Many users already understand that it is virtually impossible to separate analytics and reporting from budgeting and planning, or incentive management from key performance indicators (KPI) on which they are based. Performance management goes inside  Automation; Budget control; Customer satisfaction; Data warehouses; Decision making; Decision support systems; Performance; Purchasing; Customer relationship management (CRM); Enterprise performance management (EPM); Information exchange; Project management",Monitoring and control
728,Notice of Retraction: Fuzzy multi-objective decision making method with incomplete weighting factors information in the solar investment project evaluation,"In the fuzzy multi-objective decision-making process of solar investment project, the weight factors information is sometimes difficult or impossible to ascertain in advance. With the membership degree linear weighted programming method, the solar investment fuzzy multi-objective decision-making is researched in the incomplete weight factors information case. First, the unknown weight factors multi-objective decision making problems are converted to equivalent non-linear programming problem, and the corresponding Lagrange function is constructed, and the optimal solution meeting the non-negative constraint conditions is obtained, and then objective comparative membership degree linear weighted average of integrated values of all projects are obtained and they are sorted with the value. Three different solar investment projects are proposed according to monocrystalline silicon, polycrystalline silicon and amorphous silicon solar cells. Six factor indicators are selected, including investment costs, operating costs, economic benefits, state taxation, environmental benefits, system reliability, in which economic benefits is calculated on the basis of annual global solar radiation forecast value based on GM (1,1) gray prediction model. The two cases, completeness unknown of weight factors and smattering information of weight factors, are discussed separately. The final evaluation results are consistent with the actual situation and human judgment. © 2010 IEEE. Notice of Retraction: Fuzzy multi-objective decision making method with incomplete weighting factors information in the solar investment project evaluation Fuzzy multiobjective decision making; Incomplete weighting factors; Solar investment project Amorphous silicon; Economic and social effects; Economics; Investments; Monocrystalline silicon; Nonlinear programming; Operating costs; Polycrystalline materials; Predictive analytics; Project management; Solar cells; System theory; Taxation; Environmental benefits; Global solar radiation; Gray prediction models; Multi objective decision making; Multi-objective decision-making problems; Nonlinear programming problem; Solar investments; Weighting factors; Decision making",Capacity management
729,Environmental scanning systems: State of the art and first instantiation,"The 2008/2009 economic crisis provided a sustainable impulse for improving environmental scanning systems (ESS). Although a rich body of knowledge exists, concepts are not often used in practice. This article contributes a literature review addressing six findings for ESS design to become more applicable than the state of the art. They are structured by the elements of information systems (IS) design theories. Addressing the lack of a sound requirements analysis, our first finding proposes a 360- degree ESS for executives' ""managing a company"" task and presents how to select just the most important scanning areas to keep focus. Three other findings cover the IS model perspective focusing on a better ""grasp"" of weak signals: define concrete indicators and use IT to identify relevant because-effective-chains, leverage IT to automate day-to-day routines and monitor the variety of indicators' movements, and leverage expert experience and translate indicators' impact into a balanced opportunity-and-threat portfolio. From the methods perspective on ESS, we fifth propose to incorporate scanning results into executives' decision-making process more closely by generating scenarios from a set of assumptions and the development of indicators. Retrospective controls to update the ESS continuously and collaboration to share the scanning findings in day-to-day operation is our sixth finding. Finally, an instantiation at a large international company helped us validate our findings and to highlight how current developments in IS contribute to successful design, implementation, and day-to-day operation of new-generation ESS. Environmental scanning systems: State of the art and first instantiation Balanced chance and risk management; Business intelligence information systems (IS) design; Corporate management Design; Imaging systems; Industry; Information systems; Risk management; Sustainable development; Body of knowledge; Corporate management; Day-to-day operations; Decision making process; Design theory; Economic crisis; Environmental scanning; Expert experience; International company; Literature reviews; Requirements analysis; Scanning area; State of the art; Weak signals; Scanning",Governance
730,Notice of Retraction: Risk prediction model for construction projects based on rough sets and artificial neural networks,"Combined with the research status of construction safety management and considered of the human, machine and environment, the security control indexes of ""wounded rate per thousand"" and ""ecnomic loss per million cost of project"" are used to analysis the influencing factors of the construction project and the indexes system was set up. With introducing information entropy to rough set reduction algorithm, the index system was reduced as the input of the artificial neural networks (ANN). Then, the safety prediction model of construction project was established based on rough sets(RS) and ANN. The testing results show that the proposed system provides not only an effective prediction of the main influencing risk factors for construction projects, but also a reliable tool for safety management. © 2009 IEEE. Notice of Retraction: Risk prediction model for construction projects based on rough sets and artificial neural networks Artificial neural networks; Construction project; Information entropy; Risk factors prediction; Rough sets; Safety management Forecasting; Neural networks; Predictive analytics; Rough set theory; Safety engineering; Safety testing; Construction projects; Construction safety; Information entropy; Risk factors; Risk prediction models; Rough set reduction; Safety management; Security controls; Project management",Risk management
731,"MIPRO 2006 - 29th International Convention: Digital Economy - 3rd ALADIN, Information Systems Security and Business Intelligence Systems","The proceedings contain 250 papers. The special focus in this conference is on Information Systems Security and Business Intelligence Systems. The topics include: Optimizing Er3+- luminescent properties in photonic glasses; low pressure chemical vapor deposition of different silicon nanostructures; comparison of dynamic properties of interface traps in silicon and silicon carbide; characterization of InGaAs/InAIAs separate absorption and multiplication layers (SAM) avalanche photodiodes (APD) for Geiger mode operation; fiber optic sensor with dielectric annular core for chemical trace analysis; microsensors based on tensometric layers sputtered on silicon substrates; retraction force measurement with silicon piezoresistor; properties of lateral bipolar transistors in SiGe technology; comparison of semiconductor simulation models on the basis of well-tempered nMOSFETs; a new FGMOS active resistor with improved linearity and lncreased frequency response; bulk driven cascode current mirror; a low-vottage track and hold circuit using CMOS current mirror with pseudo-resistive pseudo-resistive feedback; digitally-selected optimal curvature-corrected voltage reference using FGMOS devices; low voltage class AB opamp; current mode analog multiplier; high accuracy function generator for analog signal processing; the challenges in low noise CMOS analogue design; interconnect delay time estimation; design and optimisation of local interconnected network interface; offset cancellation of CMOS inverter comparators; l bit CMOS current mode digital full adder; two-to-one multiplexer using pseudoresistive load; design of two-channel FIR filter banks with equiripple reconstruction error; dynamic range compressor/expander for digital audio based on LMS algorithm; ultrasonic level measuring; RF front-Bnd of DRM receiver; realization of the lower cut off frequency in the intraoperative evoked potential monitoring amplifier; laboratory model of a rectifier with power factor correction; automated feeding system; cooperative learning of electronics; web service based knowledge grid for biomedicine; a performance analysis of J2ME web services extension; quality of service oriented active routers design; hidden access mechanism for demonstrating and teaching the grid; intelligent sensor-grid computing the nearest future; virtual grid execution environment; frontiers of computational grids; grid-based solution for financial modeling; cluster image processing in ceramic tiles quality evaluation; cost optimization of WDM networks with circuit switching; dimensioning methods in transport UTRAN network; resilience mechanisms in optical transmission networks; network routing function in VoIP and IMS networks; using network simulation software in teaching routing concepts; analysis of connection establishment process of Bluetooth system V1.1 and V1.2; WLAN and Bluetooth coexistence methods and interference reduction; convergence of mobile broadband wireless technologies; prospects of MIMO techniques for broadband wireless systems; flash-OFDM in next-generation mobile broadband networks; mobiles video stream using edge detection; implementation of the diameter-based Cx interface in the IP multimedia subsystem; change request management process evolution; six sigma methodology for software development process improvement; student project management; effectiveness assessment of using project management software in ICT projects in Croatia; empirical research on use of the decision support systems (DSS) on a sample of Croatian managers; ad manager - system for managing value added service content; mobile value added service platform with adaptable business logic data flows; -agent-based creation and provisioning of telecom service; introducing QoS parameters into design and development of real-time systems; an approach to modelling and implementation of the IP network management and control node; seamless mSCTP handover analysis in IEEE 802.11 networks; problems and proposed solution in testing public e-services development project; load test methodology for primary health care information system; connection tracer based method in 2Gl3G PLMN call-processing error detection; LFazzy control scheme for the gantry crane position and load swing control; determination of dynamic model of natural gas cooler in CPS molve III using computer; one method for maintaining accuracy in implementation of fast Fourier transform on fixed point digital signal processors; predicate abstraction based verification framework; interactive simulation and test environment for real-time scheduling ' I-SITER; design and implementation of a wireless accessible network of industrial robot; a contribution to the investigation of algebraic model structures in qualitative space; computerized control techniques on blectromechanical systems; analysis of H.264 decoder's interpolation; personal identification number complexity evaluation; an approach to embedded internet system development; web based document distribution system; using web services to transfer meteorological measurement data; collection and presentation of measurement data acquired via web services; a proposal of telelaboratory structure based on free tools; optimal VR stimulation in functional assessment of whiplash injuries; fluid flow animation; priority scheduling with genetic programming; automatizacija provjere ispravnosti paketa mjerenjem mase; napredno upravljanje you realnom wemenu procesom transporta plina; cellular automata method for removing of date stamps from digital photos; registration techniques for high-resolution visual and low-resolution infrared images; genetic algorithm with adaptive mutation probability in nonstationary environments; H.264 deblocking filter algorithm for parallel processing of pixel locations; object detection based on gray-scale morphology; FPGA verification and emulation of the analog TV-IF demodulator SoC; Slovenian-English speech-to-speech translation; Croatian telephone speech recognition; aspects of a theory and the present state of speech synthesis; automated extraction of fuzzy decision rules from databases using rough set model; news article recommendation using artificial immune system; machine learning techniques for enhancing decisions support systems within organizations; design of substation wire busbars using genetic algorithms; the role of knowledgeo science and technology in education; beer game in the function to illustrate the bullwhip effect in a supply chain; data base web applications for distance learning; user interface model of interactive education software; using XML in education; information and media literacy in kindergarten; students' attitude towards virtual laboratory classes; assistive technologies for hearing impaired persons; the pilot project of distant learning in elementary schools located far from zadar; using elearning methods in teaching mathematics; creating and using information systems in bducation of ''non-professionals'' in informatics; teaching process development according to RUP methodoiogy; virtual learning environments - astronomy school; machine learning algorithms used for validation of the student knowledge; distance education - instructional video; educational microcontroller based system for construction of photoplethysmographs; implementing data acquisition system at laboratory exercises on introduction to electronics; singte chip data acquisition system with USB connectivity; use of e-learning technologies to develop competence in online communication; peer assessment in learning introductory programming course; computer and information literacy; the impact of ICT on economic growth of transition countries; the need for introduction of computer studies department in gymnasiums in republic of Serbia; 3D modeling, vizualization and animation at power electrical engineering study programme; appliance of information and communication technology in Slovenian schools; methodological framework of learning financial modeling within excel spreadsheet interface; instructional objectives and implementation of the course ''game development and virtual environments''; adjustment of the informatics curriculum at the faculty of civil engineering to new trends; real-time learner modeling using gaze-tracking in distributed adaptive E-learning environments; system theories in educations; about IT curricula models - the topology and the applicatons; using educational portals and integration with LMS; new development of teaching concepts in multimedia learning for electrical power systems introducing sonification; computer methods and network support in machine mechanisms design; bridging the gap between school science and everyday experiences through comluterised experiments; teaching programming to secondary school students. MIPRO 2006 - 29th International Convention: Digital Economy - 3rd ALADIN, Information Systems Security and Business Intelligence Systems  ",Financial management
732,"13th Americas Conference on Information Systems, AMCIS 2007, Volume 3","The proceedings contain 66 papers. The special focus in this conference is on information systems, business improvements and mobile technologies in business in America. The topics include: roles of Information Technology in distributed and open innovation process; a service model for the development of management systems for IT-enabled services; design and implementation of semantic decision support system for supplier performance contract monitoring and execution; flexibility of multi-agent problem-solving based on mutual understanding; medical material management support using data mining and analytics; using neural networks to forecast box office success; exploring the relationship between IS performance and organizational culture in the government and public sector organizations; a value oriented conceptual model for innovation in local government; a theoretical framework for assessing the role of authenticity in visitor interactions with museum technologies; improving the knowledge base by integrating vital sign data into pre-hospital patient care records; identification of customer value of healthcare services in Taiwan; the effect of simplicity and perceived control on perceived ease of use; examining the relationships among personality traits, IT-specific traits, and perceived ease of use; reducing requirement perception gaps through coordination mechanisms in software development team; an evaluation of a workshop with a focus on fostering teaching excellence through research; toward recruitment and retention strategies based on the early exposure to the IT occupational culture and analyzing Scrum agile software development with development process, social factor, and project management lenses. 13th Americas Conference on Information Systems, AMCIS 2007, Volume 3  ",Strategic alignment
733,Efficient project management by quality improvement of project processes,"Current practice in infrastructure projects shows that projects take on characteristics of the process. Synergy of project and process methodologies enabled the documentation of project-process risks and issues, establishment of a know-how base and metrics and analytics being a prerequisite for business process redesign and change management. Introduction of quality management processes within the project results in a reduction of required commitment and cost, increased quality of project delivery, faster establishment of the project-process organization and change management through the integration of project and processes management. Mentioned management approach reduces the amount of uncertainty in projects, variations in project processes and increases the repeatability and automation. In conclusion, there is a need for further development of organizational maturity through the establishment of the entity that would manage organizational changes. © 2011 MIPRO. Efficient project management by quality improvement of project processes Change management; Metric management; Process; Project; Quality Engineering exhibitions; Information technology; Management science; Microelectronics; Project management; Technology transfer; Business process redesign; Change management; Further development; Infrastructure project; Know-how; Management process; Metric management; Organizational change; Organizational maturity; Process; Project; Project delivery; Project process; Quality improvement; Quality management",Value management
734,GIS grids and the business use of GIS data,"Grid computing is becoming as essential part of different business analysis. In traditional business computing infrastructures data transfer occurs to and from computing resources at the network edges. In the other hand, most business activities are bound to space and location. The aim of this chapter is to describe the business use of geographic data (business intelligence) and Geographic Information System (GIS) grids. As conclusion business intelligence helps to improve productivity by giving users information they need when they need it most at the point of decision. Organizations that effectively use geographic information elements analyzing their risk portfolio and compliance activities can reduce costs and increase the clarity of their operations. Grid computing is an answer to the needs of efficient GIS aided analysis. When geographic data, grid computing and business information are combined, they create new possibilities to enhance and broaden the standpoints of already existing data within organizations. © 2009, IGI Global. GIS grids and the business use of GIS data  ",Risk management
735,Managing procurement spend using advanced compliance analytics,"Often the processes for purchasing commodities and services within a business enterprise are centralized into a procurement organization. These purchases are often sourced from one or more suppliers, or vendors, based on contract terms and conditions (such as price, payment terms etc.), availability, and quality or legacy habit of purchasing service with known vendors. We have found that many organizations lack appropriate processes and disciplines to drive demand to preferred suppliers. Thus these enterprises are unable to leverage the value of the pre-negotiated contracts due to lack of process education, approval process steps or appropriate purchasing tools that could result in significant amounts of spending that would be considered not compliant (not being sourced through preferred suppliers). Depending upon the size of the organization, such transactions range from several million dollars to billions of dollars. Manually sifting or employing typical query tools to review large amounts of spend transaction data with multiple attributes to identify the level of non compliant spend and identify areas to take action is a daunting task. In this paper, we discuss a software solution for spend compliance analytics that includes measurements of cost savings due to increased compliance and identification of areas where spend tends to be non compliant. We have developed a web enabled advanced analytical solution called Compliance Analytics Tool (CAT) that embeds a two phase methodology for compliance management. In the first phase, we use advanced data mining techniques to segment a large amount of historical spend transactions to quickly identify promising areas of improvement, exploiting a multitude of purchasing attributes such as business unit, procurement category, suppliers, etc. The second phase employs portfolio optimization techniques to further focus on specific segments that provide maximum benefit based on desired compliance targets or available budget. We also discuss the solution architecture that integrates business analytics along with business intelligence tools, dashboards, and data warehousing. © 2011 IEEE. Managing procurement spend using advanced compliance analytics Compliance Analytics; CPLEX; Data Mining; Optimization; Procurement; Spend Analysis; SPSS Modeler Data mining; Electronic commerce; Financial data processing; Industry; Optimization; Sales; Compliance Analytics; CPLEX; Procurement; Spend Analysis; SPSS Modeler; Data warehouses",Governance
736,Managing the implementation of business intelligence systems: A critical success factors framework,"The implementation of a BI system is a complex undertaking requiring considerable resources. Yet there is a limited authoritative set of CSFs for management reference. This article represents a first step of filling in the research gap. The authors utilized the Delphi method to conduct three rounds of studies with 15 BI system experts in the domain of engineering asset management organizations. The study develops a CSFs framework that consists of seven factors and associated contextual elements crucial for BI systems implementation. The CSFs are committed management support and sponsorship, business user-oriented change management, clear business vision and well-established case, business-driven methodology and project management, business-centric championship and balanced project team composition, strategic and extensible technical framework, and sustainable data quality and governance framework. This CSFs framework allows BI stakeholders to holistically understand the critical factors that influence implementation success of BI systems. Copyright © 2008, IGI Global. Managing the implementation of business intelligence systems: A critical success factors framework Business intelligence (BI) system; Critical success factors (CSFs); Delphi method; Framework Decision making; Human resource management; Information analysis; Management science; Project management; Business intelligence systems; Considerable resources; Critical success factor; Delphi method; Engineering asset managements; Framework; Implementation success; Systems implementation; Network function virtualization",Capacity management
737,Supporting decision makers with fuzzy cognitive maps: These extensions of cognitive maps can process uncertainty and hence improve decision making in R&D applications.,"Fuzzy Cognitive Maps are fuzzy graph structures that represent causal reasoning. This article proposes a means of building and using such maps to support the managers as well as analysts of R&D portfolios by making prediction comparisons between the research projects under evaluation. The proposal considers the factors within the decision environment and their relationships. With this methodology it becomes possible for analysts to automate (or support, at least) their decision processes. Supporting decision makers with fuzzy cognitive maps: These extensions of cognitive maps can process uncertainty and hence improve decision making in R&D applications. Business intelligence; Cognitive mapping; Decision patterns; Project selection Cognitive systems; Competitive intelligence; Fuzzy rules; Fuzzy sets; Fuzzy systems; Causal reasoning; Cognitive mapping; Decision environment; Decision patterns; Decision process; Fuzzy cognitive map; Process uncertainties; Project selection; Decision making",Value management
738,Process improvement projects shortcomings and resolution,"Purpose – The purpose of this paper is to describe a business management system that addresses the following issue: Lean Six Sigma, total quality management, and other process improvement efforts center on the execution of process improvement projects; however, often these projects (e.g. Lean Six Sigma Black Belt projects) are identified in silos and do not benefit the business as a whole, e.g. $125 million is reported saved, but nobody can find the money. Design/methodology/approach – The paper “Where processimprovement projects go wrong,” Wall Street Journal, January 25, 2010 (www.smartersolutions.com/blog/forrestbreyfogle/?p=2726) elaborates on the shortcomings of typical process improvement efforts by building an analog between process improvement programs and a spring's stressstrain curve – stretching, yielding, and failing. To address the described issues, process improvement efforts need to be part of an overall enhanced business management system in order to have longlasting success. This structured organizational framework should integrate predictive scorecards with targeted strategies creation that blends analytics with innovation, which lead to the establishment of functional performance goals that pull for the creation of enterpriseasawholebeneficial improvement projects, which positively impacts these target objectives. Findings – The described ninestep Integrated Enterprise Excellence (IEE) business management system provides the framework for achievement of these abovedescribed needs. The IEE system, for example, blends analytics with theory of constraints, competitive assessments, and economic environment so that created project work efforts have a wholesystemperformance measurement benefit. Research limitations/implications – In Lean Six Sigma and Lean kaizen event programs, improvement projects are often selected from a brainstorminglist of potential opportunities. Initial gains when starting such a deployment can be achieved; however, this effort typically stalls out and the process improvement teams are laidoff when times get tough. The reason for this roughtime downsizing is that the previous team process improvement project efforts were not, in the eyes of executives, expended in areas so that a significant overall enterprise benefit was achieved. IEE provides a business management system for addresses these issues so that business improvement efforts have a wholeorganization benefit. Practical implications – The IEE system can be used by management to address the business management problems of the day, e.g. management issues that led to the financial crisis and the problems that Toyota is now experiencing. Originality/value – Many who have studied the IEE system have said that this system provides a framework for how business should be run and should be taught in business schools. © 2010, Emerald Group Publishing Limited Process improvement projects shortcomings and resolution Process management; Project management; Six sigma ",Strategic alignment
739,Advances in business analytics at HP laboratories,"HP Labs’ Business Optimization Lab is a group of researchers focused on developing innovations in business analytics that deliver value to HP. This chapter describes several activities of the Business Optimization Lab, including work in product portfolio management, prediction markets, modeling of rare events in marketing, and supply chain network design. © Springer Science+Business Media, LLC 2010. Advances in business analytics at HP laboratories  ",Strategic alignment
740,"Innovative web portal reservoir knowledge base integrates engineering, production, geosciences and economic data sets","This paper describes a novel web portal, which has been created for the North Sea area to meet the need for a knowledge base of reservoir and engineering data targeted towards exploration and production professionals to assist them in evaluating prospects and projects, assessing development risks, and measuring prospects against other opportunities in their company portfolios. Operators in the deepwater Gulf of Mexico have found a similar service an invaluable resource when researching analog fields, as reservoir assessment and benchmarking are at the heart of good appraisal, sanction and development decisions. Through utilizing analogous data, decision makers can evaluate assumed data based on its statistical place in similar fields and geological regions. When analytics are performed in this manner, uncertainty risk can be reduced. The North Sea knowledge base features innovative applications, including a GIS interface and a set of analytical tools, and is maintained and supported by a team of engineers and geologists who provide analysis, case studies, and statistics on key data and fields. Directed towards exploration, appraisal and development teams in the North Sea area, the knowledge base contains coverage of all productive sands discovered, reservoirs either currently in production or sanctioned for development. The service provides regularly updated reservoir metrics and analogs, and offers a unique combination of reservoir engineering, geological, production, facilities and economic data analysis, together with regional knowledge. Users may also leverage the public domain information and analysis in the knowledge base by deploying secure servers in-house to store, manage, and compare proprietary data with the data and data analysis stream provided. This model leverages internal security systems to keep client data safe, while providing their engineering, geologic, and other subsurface technical staff with a single stop for current asset status, up-to date analogs, play and regional data, and both internal and external best practices, case studies, and literature, etc. With an extendable data model to manage structured and unstructured data, this solution provides a framework to ensure that exploration and asset teams can access all data of immediate interest, and also highlights other key knowledge related to specific prospect, appraisal and or development evaluation needs. Copyright 2011, Society of Petroleum Engineers. Innovative web portal reservoir knowledge base integrates engineering, production, geosciences and economic data sets  Engineers; Innovation; Knowledge based systems; Knowledge management; Natural resources exploration; Offshore oil well production; Offshore petroleum prospecting; Petroleum reservoirs; Portals; Research; Analytical tool; Asset teams; Company portfolio; Current assets; Decision makers; Deepwater; Development risk; Development teams; Economic data; Engineering data; Exploration and productions; Geosciences; Gulf of Mexico; Internal security; Knowledge base; North Sea; Public domain information; Reservoir assessment; Reservoir engineering; Technical staff; Unstructured data; Web portal; Petroleum reservoir evaluation",Risk management
741,A mean-variance analysis of self-financing portfolios,"This paper develops the analytics and geometry of the investment opportunity set (IOS) and the test statistics for self-financing portfolios. A self-financing portfolio is a set of long and short investments such that the sum of their investment weights, or net investment, is zero. This contrasts with a standard portfolio that has investment weights summing to one. Examples of self-financing portfolios are hedges, overlays, arbitrage portfolios, swaps, and long/short portfolios. A standard portfolio plus the IOS of self-financing portfolios form a restricted IOS hyperbola with restricted efficient set constants that differ from the usual constants. The restrictions affect statistical tests of portfolio efficiency, which are developed for the self-financing restrictions. As an application, we consider the self-financing portfolios formed by Fama and French (1992, 1993, 1995), based on market capitalization and value. In contrast to Fama and French (1992, 1993, 1995), we find that their restricted IOS is significantly different from the unrestricted IOS with the implication that the Fama-French tests are misspecified. A mean-variance analysis of self-financing portfolios Intersection; Investment opportunity set; Long/short portfolios; Spanning Benchmarking; Investments; Marketing; Statistical methods; Fama-French tests; Investment opportunity set; Market capitalization; Mean variance analysis; Management science",Strategic alignment
742,Simulation of adaptive project management analytics,"Typically, IT projects are delivered over-budget and behind schedule. In this paper, we explore the effects of common project management practices that contribute to these problems and suggest a better alternative that can utilize resources more effectively. Our alternative approach uses (a) a thorough analysis of risks affecting activities in a project plan (i.e., the root factors leading to cost and time overruns), and (b) an optimization of the resources allocated to each activity in the project plan to maximize the probability of on time and within budget project completion. One key feature of our method is its capability to adapt and learn the risk factors affecting activities during the course of the project, enabling project managers to reallocate resources dynamically to ensure a better outcome given the updated risk profile. We use simulations to test the performance of our optimization algorithm and to gain insights into the benefits of adaptive re-planning. © 2007 IEEE. Simulation of adaptive project management analytics  Adaptive algorithms; Budget control; Optimization; Risk assessment; Risks; Alternative approach; IT projects; Key feature; On time; Optimization algorithms; Over-budget; Project completion; Project management practices; Project managers; Re-planning; Reallocate resources; Risk factors; Risk profiling; Project management",Risk management
743,Challenging the power of ERP packages,"Benefits gained by the information technology (IT) firm Mitsui Babcock after choosing the right single enterprise resource planning (ERP) system are discussed. The firm recognized the need for a development environment to give them the capabilities to develop some applications. They are using IFS Applications running on AIX from a central server and database. Mitsui-Babcock has implemented Actuate web-delivered business analytics to supplement IFS' reporting, enquiry and screen-based facilities. Challenging the power of ERP packages  Contracts; Costs; Database systems; Industrial management; Information technology; Internet; Logistics; Portals; Project management; Servers; UNIX; Bills of materials (BOM); Payroll systems; Virtual private network (VPN); Enterprise resource planning",Financial management
744,"2009 International Conference on Business Intelligence and Financial Engineering, BIFE 2009","The proceedings contain 197 papers. The topics discussed include: an algorithm for determining neural network architecture using differential evolution; predicting China's energy consumption using artificial neural networks and genetic algorithms; the application of artificial neural networks in risk assessment on high-tech project investment; the finger movement identification based on fuzzy clustering and BP neural network; an application of Hopfield neural network in target selection of mergers and acquisitions; modelling and prediction of the CNY exchange rate using RBF neural network; personal credit rating assessment for the national student loans based on artificial neural network; multi-colony ant algorithm using both repulsive operator and pheromone crossover based on multi-optimum for TSP; chaotic search-based adaptive immune genetic algorithm; an improved genetic algorithm based on hK1 subdivision and fixed point; and a risk measure with conditional expectation and portfolio optimization with fuzzy uncertainty. 2009 International Conference on Business Intelligence and Financial Engineering, BIFE 2009  ",Risk management
745,"16th Americas Conference on Information Systems, AMCIS 2010, Volume 1","The proceedings contain 75 papers. The special focus in this conference is on business intelligence, business process modelling, project management, IT organizational learning and healthcare information technology. The topics include: influence of ICT on masculinities and time management; computer-assisted qualitative data analysis software; including work system co-existence, alignment, and coordination in systems analysis and design; an ontological foundation for agile modeling with UML; identifying the IT readiness of small and medium sized enterprises; the process of habit formation in IS post-adoption; customers as service innovators within business networks; signaling mechanisms and survival of service providers in an electronic market; proximal business intelligence on the semantic web; identifying the core topics and themes of data and information quality research; attributes of information; scalability and performance of a virtualized SAP system; a business intelligence perspective on the future internet; analysis of county level e-government implementations; from project management to project leadership; a comparative analysis of persona clustering methods; insights from the use of business intelligence in the Norwegian music industry; reuse-mechanisms for mass customizing IT-service agreements; IS project escalation in developing countries; customer satisfaction with cell phone banking in South Africa; unpacking green it and effects of semantic quality in business process modeling. 16th Americas Conference on Information Systems, AMCIS 2010, Volume 1  ",Governance
746,"Post-Web era could bring off-the-shelf connectivity, interoperability","The transitions, which are based on changes in technology, are discussed. As technology platforms become more powerful, applications likewise grow in scope and sophistication, ushering in advances like optimization engines, workflow tools and online analytical processing (OLAP) used in business intelligence and enterprise performance management applications. The most exciting technology of the moment is Web services as embodied in Microsoft .NET and IBM WebSphere. An increasing emphasis on the benefits of connectivity, flexibility and collaboration capabilities by the likes of IBM and SAP is also discussed. Post-Web era could bring off-the-shelf connectivity, interoperability  Client server computer systems; Interoperability; Minicomputers; Open systems; Optimization; Problem solving; Project management; Off-the-shelf connectivity; Post-web era; Technology platforms; Web-based architectures; World Wide Web",Governance
747,Environmental Performance in the Petroleum Industry: Hidden Risks and Value Potential for Strategic Investors,"Innovest Strategic Value Advisors Inc has created the EcoValue'21 analytics platform to assess the strategic risks and profit opportunities of major industrial companies, translating them into financial terms, and thereby unlocking hidden value potential for investors. Over the past year, despite the stock market's considerable volatility, an enhanced S&P 500 portfolio index based upon EcoValue '21 has consistently outperformed it S&P 500 benchmark by 150-200 basis points. Similar results were found in the petroleum industry, indicating that investor returns can be substantially improved by investing in companies with superior eco-efficiency. Among petroleum companies, wide variations exist in environmental risk exposure and management capability to manage risk and capitalize on environmentally-driven business opportunities. Growing environmental restrictions and costs further drive the industry focus on cost reduction and consolidation. Environmental Performance in the Petroleum Industry: Hidden Risks and Value Potential for Strategic Investors  ",Risk management
748,"Convertible bonds: Model, value attribution, and analytics","Convertible bonds provide investors an option to convert the bond into the underlying equity. For this reason, a convertible bond is exposed to both equity and interest rate risk. Incorporating these two sources of risk into the model is particularly important for callable issues. In this study, a two-factor model is used to analyze a sample of bonds. The model shows that the correlation of stock risk and interest rate risk may affect convertible bond prices significantly. The bond pricing model also provides portfolio analytics and can decompose a convertible bond into its basic components - the stock and bonds with different maturities. This approach enables an investor to implement a more precise hedging strategy than is possible using only delta. Convertible bonds: Model, value attribution, and analytics  ",Risk management
749,Risk assessment modelling for the South African construction industry,"The construction project environment is fraught with risks of every conceivable nature and high levels of uncertainty. A research project was therefore initiated to define the most important construction risks from the insurance stakeholders' point of view. The risk management functionality in terms of the formal risk culture, risk framework and risk practices within the participating construction organisations were tested through a comprehensive questionnaire. Analysis of the completed questionnaires indicated that the results were consistent and repeatable. It was found that construction companies generally have weak risk management cultures, frameworks and practices, even though risk management awareness was relatively high at the construction project level This seemed to emanate from the separation of project- and enterprise-related risks. The most important risks in the new proposed model are (1) the loss of key employees and business intelligence, (2) contractual related failure, (3) unfavourable financial market conditions and (4) failures of key contractors and clients. © 2008 PICMET. Risk assessment modelling for the South African construction industry  Construction industry; Finance; Industrial management; Insurance; Management; Project management; Risk analysis; Risk management; Safety engineering; Separation; Technology; Business intelligence; Construction companies; Construction projects; Construction risks; Financial markets; Management functionality; Portland; Research projects; Sustainable economy; Technology managements; Risk assessment",Risk management
750,"32nd International Conference on Information System 2011, ICIS 2011, Volume 3","The proceedings contain 299 papers. The special focus in this conference is on Information System. The topics include: Situation awareness through social collaboration platforms in distributed work environments; attaining and enacting green leadership; culture and organizational computer-mediated communication; inter-organizational effects on sociomaterial imbrications and change; intrusiveness of online video advertising and its effects on marketing outcomes; dynamic personal feedback in acquiring information to manage your health; emotions as predictors of performance in virtual worlds; a profiling model for readmission of patients with congestive heart failure; information sharing in NHS polyclinics; integrating self-service kiosks into healthcare delivery organizations; an empirical study with strategic alignment in the healthcare industry; improving knowledge-intensive health care processes beyond efficiency; understanding the drivers and outcomes of healthcare organizational privacy responses; IT-based capabilities, service innovation, and quality in health care; microprocesses of healthcare technology implementation under competing institutional logics; understanding the impact of internet media on patient-clinician trust; measuring emotions in electronic markets; social networking and extending social capacity; an entropy index for multitasking behavior; an instrument for measuring SOA maturity; computer-mediated social networks and environmental behavior; an effective and efficient subpopulation extraction method in very large social networks; reputation and preemption strategies in competing technology networks; software adoption under network effects; consumer product consideration and choice at purchase time at online retailers; business values of community source; learning from peers on social media platforms; code architecture and open source software development; the price and quantity of IT-related intangible capital; the impact of third-party information on the dynamics of online word-of-mouth and retail sales; the roles of agency and artifacts in assembling open data complementarities; the role of culture and personality in the leadership process in virtual teams; user satisfaction of E-government procurement systems in developing countries; differences in knowledge seeking ties between the US and Singapore students; modeling quality dynamics in IT services management; herding behavior as a network externality; exploring information systems control alignment in organizations; the user-centered nature of awareness creation in computer-mediated communication; critical factors affecting compliance to campus alerts; substance and influence in brand communities; the attitude construct in IT adoption research - a scientometric analysis; examining trends of technology diffusion theories in information systems; management of change to ensure IS success; information systems development as a social process; testing tournament selection in creative problem solving using crowds; understanding online payment method choice; the influence of demands and resources on emotional exhaustion with the information systems profession; CIO survival and the composition of the top management team; social capital in the ICT sector - a network perspective on executive turnover and startup performance; activity awareness as a means to promote connectedness, willingness to do additional work, and congeniality; effects of media synchronicity on communication performance; comprehension of online consumer-generated product review; the role of product recommendation agents in collaborative online shopping; three classes of attitude and their implications for IS research; gender differences in virtual collaboration on a creative design task; toward deep understanding of persuasive product recommendation agents; understanding is education quality in developing countries; SYSCO's best business practices (BBP); public expenditure management through khajane - an integrated financial MIS; arguments for the adoption of a heuristic approach to IS research; information systems collaborations as boundary spanning; towards an evidence-based research approach in information systems; grounding theory from Delphi studies; PLS marker variable approach to diagnosing and controlling for method variance; benefits from using continuous rating scales in online survey research; evaluating two automatic methods for classifying information technology concepts; exploring interpersonal relationships in security information sharing; explaining the difference between how security cues and security arguments improve secure behavior; the impact of security practices on regulatory compliance and security performance; personal health records in cloud computing environments; empirical analysis of data breach litigation; extending UTAUT to predict the use of location-based services; project and organizational antecedents of effort withholding in IT project teams; modeling and checking business process compliance rules in the financial sector; participation in open source communities and its implications for organizational commitment; an empirical test of the theory of relationship constraints; an approach for portfolio selection in multi-vendor IT outsourcing; the essential dynamics of information infrastructures; a framework for investigating open innovation processes in ISD; four facets of a process modeling facilitator; a conceptual life event framework for government-to-citizen electronic services provision; perceptual congruence between IS users and professionals on IS service quality - insights from response surface analysis; dynamic service level agreement management for efficient operation of elastic information systems; impact of business intelligence and IT infrastructure flexibility on competitive performance; knowledge refinement effectiveness; towards a framework for measuring knowledge management service productivity; measuring the business value of online social media content for marketers; a data-centric perspective for workflow model management; protecting privacy against regression attacks in predictive data mining; the effects of job design on employees' knowledge contribution to electronic repositories; BI and CRM for customer involvement in product and service development; an effective method of discovering target groups on social networking sites; environmental scanning for customer complaint identification in social media; the effects of user identity and sanctions in online communities on real-world behavior; F-commerce and the crucial role of trust; when artificial feedback hurts - empirical evidence from community-based configuration systems; knowledge collaboration in distributed practice communities; a social identity perspective on participation in virtual healthcare communities; boundary-spanning documents in online communities; network stability and social contagion on the mobile internet; digital complementary assets; trusting social location technologies and interactions; telepresence in business meetings; exploring the role of online social network dependency in habit formation; a sociomateriality practice perspective of online social networking; automatic reputation assessment in Wikipedia; strategic decision support for smart-leasing infrastructure-as-a-service; the impact of different types of satisfaction on C2C platform loyalty; automated negotiations under uncertain preferences; nurturing sales entrepreneurship in consumer-to-consumer marketplaces; competing across different channels for personalized service; the influences of negativity and review quality on the helpfulness of online reviews; winner determination of open innovation contests in online markets; a dual view on IT challenges in corporate divestments and acquisitions; managing the IT integration of acquisitions by multi-business organizations; management commitments that maximize business impact from IT; institutional work and artifact evolution; four perspectives on architectural strategy; a coevolutionary journey of strategic knowledge management alignment; use of social media in disaster management; unpacking the duality of design science; acquiring IT competencies through focused technology acquisitions; IT artifacts and the state of IS research; senior scholars' forum and online social networking and citizen engagement. 32nd International Conference on Information System 2011, ICIS 2011, Volume 3  ",Strategic alignment
751,Designing an e-portfolio for assurance of learning focusing on adoptability and learning analytics,"The Assurance of Learning for Graduate Employability framework is a quality assurance model for curriculum enhancement for graduate employability, enabling graduates to achieve ""the skills, understandings and personal attributes that make [them] more likely to secure employment and be successful in their chosen occupations to the benefit of themselves, the workforce, the community and the economy"" (Yorke, 2006). Of particular note is the framework's dependence on three foundations, including easy access to integrated and accessible tools for staff and student self-management. In other words, this approach to curriculum quality depends on staff and student access to tools that enable them to self-manage their learning. This paper examines two aspects which informed the design of a student e-portfolio system, iPortfolio, intended for students' self-management of their learning, particularly recording evidence of their achievement of capabilities. The paper focuses on two particular considerations in the design of the iPortfolio: adoptability and learning analytics. Adoptability means the phase preceding adoption, whether students have the devices, platforms and technology skills to be able to use such an innovation. The iPortfolio also facilitates learning analytics: it has the capability to gather data related to learning indicators for course quality assurance purposes. Both adoptability and analytics are very dynamic fields: new devices, platforms and applications constantly spark changes in user habits, and policy changes mean institutions need to be able to provide new data, often at short notice. In the conclusion, the paper suggests how tools such as the iPortfolio can be designed for 'future proofing' and sustainability. Designing an e-portfolio for assurance of learning focusing on adoptability and learning analytics  ",Strategic alignment
752,CIT 2007: 7th IEEE International Conference on Computer and Information Technology,The proceedings contain 190 papers. The topics discussed include: business intelligence-an industrial perspective; the IUPS physiome project; large-scale evaluation infrastructure for information access technologies enhancement and creativity; active XML schema containment checking based on tree automata theory; an efficient detection of conflicting updates in valid XML; a journey towards more efficient processing of XML data in (O)RDBMS; schema-aware keyword search over XML streams; SemFilter: a simple and efficient semantic filtering of XML messages; ensembles of region based classifiers; neural network for text classification based on singular value decomposition; an efficient method of genetic algorithm for text clustering based on singular value decomposition; predicting coronary artery disease from heart rate variability using classification and statistical analysis; and deputy mechanism for OLAP over imprecise data and composite measure. CIT 2007: 7th IEEE International Conference on Computer and Information Technology  Competitive intelligence; Computer applications; Genetic algorithms; Neural networks; Project management; XML; Semantic filtering; Text classification; Information technology,Value management
753,Analyzing the impact of credit migration in a portfolio setting,"Credit migration is an essential component of credit portfolio modeling. In this paper, we outline a framework for gauging the effects of credit migration on portfolio risk measurements. For a typical loan portfolio, we find credit migration can explain as much as 51% of volatility and 35% of economic capital. We compare through-the-cycle migration effects, implied by agency rating transitions, with point-in-time migration, implied by EDF™ (Expected Default Frequency) transitions, and find that migration of point-in-time credit quality accounts for a greater fraction of total portfolio risk when compared with through-the-cycle dynamics. In a stylized analytic setting, we show that, when controlling for PD term structure effects, higher likelihood of moving away from the current credit state does not necessarily imply greater risk. Finally, we review methods for generating high-frequency transition matrices, needed for analyzing instruments with cash flows or contingencies whose frequencies are asynchronous to an available transition matrix. We further demonstrate that the naïve application of such methods can result in material deviations to portfolio analytics. © 2011 Moody's Investors Service Inc. and/or its affiliates. Analyzing the impact of credit migration in a portfolio setting Credit migration; Credit portfolio management; Credit risk; Markov model; Transition matrix ",Risk management
754,On optimal risk/return-efficient arbitrage portfolio,"Arbitrage portfolios arise extensively in the theory and practice of finance. However, compared to the standard portfolios, there are still somewhat little publications focusing on the analytics and empirical tests of the optimal arbitrage portfolios. Based on the comparison of the standard portfolio and the arbitrage portfolio, Fang (2006) introduces the strict definition of the arbitrage size, arbitrage portfolio and its return and then Fang (2007) and Fang (2008) respectively present mean-variance analyses for portfolios and arbitrage opportunities. However, it is very complicate to obtain the frontier arbitrage portfolio and there is no analytical formula for general frontier arbitrage portfolios since Fang's definition of the arbitrage portfolio includes a non-smooth condition. In this paper, compared with the Sharp ratio, the optimal risk/return-efficient arbitrage portfolio is introduced, which will be shown to be related to Korkie and Turtle's frontier arbitrage portfolio (Korkie and Turtle, 2002), and hence to be expressed by an analytical formula. © 2009 IEEE. On optimal risk/return-efficient arbitrage portfolio Analytical formula; Arbitrage portfolio; KT-frontier arbitrage portfolio; Optimal risk/return- efficient arbitrage portfolio Analytical formula; Analytical formulas; Arbitrage opportunity; Arbitrage portfolio; Empirical test; KT-frontier arbitrage portfolio; Mean-variance analysis; Non-smooth; Optimal risk/return- efficient arbitrage portfolio; Theory and practice; Optimization",Risk management
755,"International Conference on Information Systems, ICIS 2012, Volume 2","The proceedings contain 308 papers. The special focus in this conference is on Information Systems. The topics include: Heterogeneity in IT landscapes and monopoly power of firms; community ecology for innovation concept; peer influence in a very large social network; assessing value in an online network of products; optimal pricing with positive network effects; research on viral marketing propagating oriented to marketing context; repurchase intentions of information technology; a multi-level analysis of the impact of health information technology on hospital performance; institutional logics of adoption and use of health information technology; post-acceptance of electronic medical records; socio-technical attachments and IT change; comparing the predictive ability of PLS and covariance models; a note of caution on covariance-equivalent models in information systems; differential effects of omitting formative indicators; being innovative about service innovation; estimating demand for applications in the new mobile economy; continuance of professional social networking sites; role of online social networks in job search by unemployed individuals; knowledge contribution in online network of practice; the dual role of IS specificity in governing software as a service; relational antecedents of information visibility in value networks; summarization of corporate risk factor disclosure through topic modeling; a hybrid method for cross-domain sentiment classification using multiple sources; designing intelligent expert systems to cope with liars; IS offshore project risk, contracts and team structure; vendor and client project managers; technology desirability; the role of green IS in developing eco-effectiveness; the impact of IT-enabled manufacturing capabilities on plant profitability; inter-industry IT spillovers after the dot-com bust; effects of undesired online video advertising choice on user behavior and attitude; unfulfilled obligations in recommendation agent use; understanding the formation of trust in IT artifacts; user decisions among digital piracy and legal alternatives for film and music; a probabilistic generative model for latent business networks mining; the effects of information disclosure policy on the diffusion of security attacks; interactivity of social media and online consumer behavior; strategies for establishing service oriented design in organizations; alternative genres of is research; ICT standardization strategies and service innovation in health care; technical support and IT capacity demand; online social networks as a source and symbol of stress; the effects of social network usage on organizational identification; using the Kano model to identify attractive user-interface software components; developing a theory of multitasking behavior; an empirical investigation of information systems departments' configurations; exploring the adaptation of enterprise systems implementation methodology; out of the box and onto the stage; an organizing vision perspective on green IS development; the emergence of sustainability as the new dominant logic; social influence and defaults in peer-to-peer lending networks; multi-screen strategy for selling mobile content to customers; the emergent role of IT capabilities; reducing price uncertainty through demand side management; factors affecting perceived persuasiveness of a behavior change support system; surfacing schemas from firms' informational engagements; optimal design of consumer review systems; measuring product type with dynamics of online product review variance; different effects on retailers of online product reviews; measurement of multitasking with focus shift analysis; an exploration of the impact of information privacy invasion; measuring mobile users' concerns for information privacy; examining the rationality of location data disclosure through mobile devices; towards a design theory for software project risk management systems; identifying optimal IT portfolios to promote healthcare quality; designing a web-based application to support peer instruction for very large groups; principles for knowledge creation in collaborative design science research; estimating optimal recommendation set sizes for individual consumers; predicting participation in social media sites by analyzing user participation patterns; a hidden Markov model for conversion rate dynamics in online retail; flow experience and continuance intention toward online learning; design and evaluation of a didactical service blueprinting method for large scale lectures; knowledge sharing and maturation in circles of trust; a model-driven method for the systematic literature review of qualitative empirical research; procedurally transparent design science research; advancing task elicitation systems - an experimental evaluation of design principles; the knowing-doing gap in research methods and what we should do about it; efficient and flexible management of enterprise information systems; granularity metrics for IT services; from observed outcomes to measurable performance; the effect of third party investigation on pay-per-click advertising; identifying factors of e-government acceptance - a literature review; effects of cultural cognitive styles on users' evaluation of website complexity; a sustainability model of green IT initiatives; towards a typology of green IS strategies; digital access, political networks and the diffusion of democracy; the effect of free access on the diffusion of scholarly ideas; an empirical examination of cultural biases in interpersonal economic exchange; effect of business intelligence and IT infrastructure flexibility on organizational agility; news recommender systems with feedback; a multi-theoretical framework for social network-based recommendation; improving coverage of design in information systems education; perceived IT security risks of cloud computing: conceptualization and scale development; capability leapfrogging in the Japanese IT services industry; predatory coercion in social media and protection of children online - a critical discourse analysis approach; encouraging collaborative idea-building in enterprise-wide innovation challenges; analyzing the impact of social media strategies on viewer engagement; supporters in deed - studying online support provision from the perspective of social capital; membership overlap and inter-community collaboration; developing customer agility through information management; keyword search patterns in sponsored link advertisements; the hidden effects of opening bids in online auctions; increasing dynamic capabilities through virtualized grid-in-cloud solutions; design principles for heterogeneity decisions in enterprise architecture management; IT stereotyping and the CEO-CIO headlock; exploring the role of un-enacted projects in IT project portfolio management; the difference of determinants of mobile data services' adoption and continuance - a longitudinal study; a study of effectiveness and satisfaction level of cloud CRM users in Taiwan's enterprises; investigating intelligent agents in a 3D virtual world; an empirical study of a two-sided model of fraudulent exchange; perceptual and conceptual effects of incidental exposure to web ads; self-disclosure on online social networks; critical success factors of location-based services; affect and online privacy concerns; the impact of training and social norms on information security compliance; security management in cross-organizational settings; the effect of customers' emotion on service recovery strategy in IT service failures; the influence of IS affordances on work practices in health care; multi-level knowledge transfer in software development outsourcing projects; organizational performance with environmental knowledge intensity; process visibility - towards a conceptualization and research themes; towards an oil crisis early warning system based on absolute news volume; constructing workflow models from agent profiles; formulating effective coordination strategies in agile global software development teams; the transmission of control in information systems projects; the structuration of task-oriented communication in innovative virtual teams and social media in the entrainment of contention to innovation. International Conference on Information Systems, ICIS 2012, Volume 2  ",Value management
756,Economics of development strategies utilising option and portfolio analytics,"Purpose: The purpose of this paper is to integrate land use and option pricing theories using case study analyses to compare a portfolio of uses comprising single and mixed-use development on the same site and assess the effects on the risk-return profile of potential development schemes. The integration of land use development based on highest and best use (HBU) is tested against a combination of uses on the selected sites at a point in time in the downswing of the real estate cycle. Design/methodology/approach: The proposed methodology integrates the development valuation approach with option theory in which both consider the relationships of cost and value associated with alternative development options. The approach used in this paper addresses the broader consideration of project coordination inclusive of land use flexibility and opportunity costs endogenously associated with development strategies. By investigating the uncertainty of economic options specific to the development process, the methodology considers the significance of complementary components of strategic decisions and entrepreneurial effort within a return/risk management strategy. Findings: The stochastic model when compared to the real option model enhances strategic decisions and development project management by allowing the consideration of single/mixed-use alternatives. The development process is facilitated by the research findings whereby alternative uses are tested to maximise the potential use of the site. The analyses consider optimal funding strategies in developing and investing for a range of use options on regeneration sites. Practical implications: The significant insights apparent from the research is the quantification of the strategic specification of development as a productive process and an investment endeavour. The proposed model enables a comparison of a HBU based on a single development, a mixed-use development or a combination of uses as the difference between the scenarios impacts on land value and profit measures, especially where these measures are calculated as distributive residuals. Originality/value: The stochastic model developed in this paper provides a value-added contribution to real estate literature by considering the complexity of the interrelationships between urban land economics, land use theory, valuation appraisal methodologies, portfolio analysis and option pricing as applied in the development of regeneration schemes. © Emerald Group Publishing Limited. Economics of development strategies utilising option and portfolio analytics Design and development; Portfolio investment; Real estate; Regeneration ",Strategic alignment
757,A new project control model as underlying support for the balanced scorecard formalization,"Different areas of business administration are influenced by a great variety of that allow the structured decision process to produce information in order to optimize it, becoming this production a fundamental element of the business intelligence. Business intelligence is part of the software engineering which is closer to Strategic Business Administration. These tools, like the information systems allow the strategy design to be improved by the manager, following the execution of the strategy. Since the capacity of executing a strategy is more important than the strategy itself, the analysis of actual and accurate information is essential to take decisions, but in most of cases the decisions and activities are done in a ""craft"" way because there are not steps to be followed, so the best performance of the strategy planned to reach the objectives and goals established can not be assured. The purpose of this work is to introduce formal and rigorous methods in the activities of study and design of the business intelligence environment. A new project control model as underlying support for the balanced scorecard formalization  Information systems; Mathematical models; Optimization; Process control; Project management; Software engineering; Business Administration; Business intelligence environments; Decision processes; Competitive intelligence",Monitoring and control
758,Cross-validation and ensemble analyses on multiple-criteria linear programming classification for credit cardholder behavior,"In credit card portfolio management, predicting the cardholders' behavior is a key to reduce the charge off risk of credit card issuers. As a promising data mining approach, multiple criteria linear programming (MCLP) has been successfully applied to classify credit cardholders' behavior into two or multiple-groups for business intelligence. The objective of this paper is to study the stability of MCLP in classifying credit cardholders' behavior by using cross-validation and ensemble techniques. An overview of the two-group MCLP model formulation and a description of the dataset used in this paper are introduced first. Then cross-validation and ensemble methods are tested respectively. As the results demonstrated, the classification rates of cross-validation and ensemble methods are close to the rates of using MCLP alone. In other words, MCLP is a relatively stable method in classifying credit cardholders' behavior. © Springer-Verlag Berlin Heidelberg 2004. Cross-validation and ensemble analyses on multiple-criteria linear programming classification for credit cardholder behavior Classification; Credit Card Portfolio Management; Cross-Validation; Data Mining; Ensemble; Multi-criteria Linear Programming Classification (of information); Financial data processing; Investments; Linear programming; Classification rates; Credit cardholders; Cross validation; Ensemble; Ensemble techniques; Multi-criteria; Multiple criteria; Portfolio managements; Data mining",Value management
759,"16th Americas Conference on Information Systems, AMCIS 2010, Volume 2","The proceedings contain 84 papers. The special focus in this conference is on business intelligence, business process modelling, project management, IT organizational learning and healthcare information technology. The topics include: emergent leadership, gender, and culture; the empirical investigation of a wiki based group system in organizations; identifying areas for risk sharing in IT outsourcing; improving the trust of users on social networking sites via self-construal traits; data analysis for healthcare; general knowledge supported news analysis for portfolio risk prediction; secure web development teaching modules; an integrated model of individual web security behavior; toward optimal churn management; CIO turnover, IS alignment and revolutionary change; knowledge creation and firm performance; conceptualizing Green IT Organizational Learning (GITOL); virtual worlds as adjustable environments for immersion in business meetings; the evolving influence of diversity and media in virtual organizations; adoption of sustainability in IT Services; social technologies; knowledge sharing initiatives in a Chinese professional services firm; developing an assessment process for a master of information systems program; outsourcing contact centres to a developing country; convolution of complex process models and ratios; antecedents to supply chain integration; specifying the software project risk construct; collective sensemaking in virtual teams; benchmarking web-based image retrieval; analyzing enterprise systems delivery modes for small and medium enterprises; the fair factories clearinghouse and publishing IS research in and about Latin America. 16th Americas Conference on Information Systems, AMCIS 2010, Volume 2  ",Risk management
760,"Analysis of correlation between CROBEX index value change, investment fund type, investment portfolio and investment funds share value change using data mining techniques","This paper examines a correlation between Croatian equity index CROBEX's value change and Croatian open investment funds share price change by applying data mining techniques. Apart from the type of the fund, in consideration is taken the fact how much funds invest in Croatian equities that is the equities which rate on Zagreb Stock Exchange. While working, the problem was observed in two aspects, the analytical models were made on continual and discrete values. The problem was elaborated with few data mining methods and in few software toolkits, and in the end the synthesis of the obtained results was made. Analysis of correlation between CROBEX index value change, investment fund type, investment portfolio and investment funds share value change using data mining techniques Bayesian networks; Business intelligence; CROBEX; Decision trees; Investment funds; Neural networks; Temporal data mining Bayesian networks; Competitive intelligence; Data mining; Decision trees; Economics; Information technology; Neural networks; Croatians; CROBEX; Data mining methods; Data mining techniques; Decision trees (DTs); Discrete values; Index CROBEX; Index values; Investment funds; Investment portfolio; Share price; Software toolkits; Temporal data mining; Zagreb Stock Exchange; Investments",Strategic alignment
761,Measurement of performance in the support of variant management; [Performance Measurement zur Untersẗtzung des Variantenmanagements],"The manufacturing of individual customized product solutions leads to a varied portfolio of products and therefore requires a holistic variant management. In this case extended data bases supporting the planning, management and control process are indispensable. This article discusses the design of a Performance Measurement resp. a KPI's focussed information system based on the SAP BI landscape at KSB AG. In particular, the synergistic potential of Performance Measurement and Business Intelligence applied to variant management at KSB AG will be shown. The combination of these two domains provides the basis for variance- oriented KPI's. © Carl Hanser Verlag, M̈nchen. Measurement of performance in the support of variant management; [Performance Measurement zur Untersẗtzung des Variantenmanagements]  Engineering; Management; Customized products; Data basis; Management and controls; Performance measurements; Two domains; Variant managements; Benchmarking",Monitoring and control
762,Information logistics strategy - Analysis of current practices and proposal of a framework,"Although managerial issues of data warehousing and business intelligence received considerable attention in recent years, the strategy process has not often been investigated. This is particularly true for the recently proposed, more holistic concept of information logistics. This paper discusses the state of the art of information logistics (IL) strategy and proposes a framework of analysis which combines supply chain oriented decomposition with functional decomposition. As a foundation for proposed strategy components, current IL strategy practices are analyzed by means of a survey. The findings show that IL strategy seems to be linked to company size, governance type (business line vs. business process), and IL organization. Supply chain oriented IL strategy components (sourcing, delivery, portfolio) gain less attention than IL solution development and maintenance strategy components. © 2009 IEEE. Information logistics strategy - Analysis of current practices and proposal of a framework  Logistics; Management science; Supply chain management; Supply chains; Business intelligences; Business process; Company sizes; Current practices; Data warehousing; Functional decompositions; Information logistics; Maintenance strategies; State-of-the arts; Strategy process; Strategic planning",Governance
763,Ontology based metrics - Applying business intelligence on PLM,"In this paper, we are proposing an ontology-based approach to use the information base of PLM systems to support project analysis and management of development processes. At first, we will give a quick overview of generic engineering processes and involved IT systems followed by an analysis of identified pitfalls and shortcomings. This paper however is focussed on presenting meaningful metrics for the evaluation of PLM data in a project management context based on a semantic integration model. After giving a short oversight of semantic web technologies, we will introduce our approach based on these techniques. We use semantic maps to translated technical information supplied by a PLM system into meaningful key figures to bridge the gap between technical and organisational data. Then we introduce PERMETER, a 'semantic radar system' to pierce the thick mist of product complexity using ontologies and metrics. We will conclude with a summary and an outlook on further activities. Copyright © 2008, Inderscience Publishers. Ontology based metrics - Applying business intelligence on PLM Metrics; Ontology; PLM; Project management; Semantic web ",Financial management
764,"16th Americas Conference on Information Systems, AMCIS 2010, Volume 7","The proceedings contain 87 papers. The special focus in this conference is on business intelligence, business process modelling, project management, IT organizational learning and healthcare information technology. The topics include: towards a human processual approach of business-IT alignment; the role of an effective IT intervention for micro-enterprises; prediction of RFID performance in supply chains; incentive and control mechanisms for mitigating relational risk in IT outsourcing relationships; information seeking in organisations; service identification through value chain analysis and prioritization; a method to assess value of integrated operations; an approach for trust and IS engagement in professional-orientated work; developing an information infrastructure for international trade; a theoretical approach to netsourcing research; a systems view of software requirement volatility; fad-like technology adoption as a social action; knowledge transfer challenges for universities and SMEs in the USA; the credibility crisis in IS; the gateways of information on the cloud era; peer-to-peer service quality and its consequences in virtual communities; common data exchange standards; teaching with free software, open formats, and collaborative culture; a process-oriented tool development in the open innovation paradigm; influence of brokering in data warehouse projects; teaching business intelligent with an executive dashboard; toward a user commitment continuum; considering visual artifacts in IT cultures; measuring flow experience of computer game players; quality of XBRL US GAAP taxonomy; projects as social movements and understanding awareness diffusion in microblogging. 16th Americas Conference on Information Systems, AMCIS 2010, Volume 7  ",Governance
765,Analytics for software development,"Despite large volumes of data and many types of metrics, software projects continue to be difficult to predict and risky to conduct. In this paper we propose software analytics which holds out the promise of helping the managers of software projects turn their plentiful information resources, produced readily by current tools, into insights they can act on. We discuss how analytics works, why it is a good fit for software engineering, and the research problems that must be overcome in order to realize its promise. Copyright 2010 ACM. Analytics for software development Analytics; Project management Engineering research; Project management; Analytics; Information resource; Research problems; Software development; Software project; Software design",Risk management
766,BI for IT,"The metadata repository (MDR) requirements needed to enable successful Enterprise Architecture (EA) initiatives are discussed. The MDR encompasses three important dimensions of the enterprise architecture problem such as corporate strategies, goals, servers, and data element, attributes and relationships characterizing the business measurements, life cycles, configuration, and providing Business Intelligence (BI) to support strategic planning and portfolio management. The MDR responsible for providing BI for Information technology (IT) provides IT Information Management, IT Information Delivery. The platform should be capable of applying rules to enforce structure of the data and to validate the allowed values, identify conflicting updates, and manage out-of-date stale information. The platform supports the security requirements of corporate and government IT organizations by providing certain control features. BI for IT  ",Governance
767,E-learning seen as an enterprise business process,"The investment in the human capital, by means of training delivered in enterprise, became an important constituent of enterprise competitiveness strategy. Consequently business managers require the proofs of training investment yield, in terms of tangible and intangible profits, from their human resources managers, training departments, or even from training consultants. To evaluate enterprise training, two models predominate, namely the model of Kirkpatrick and that of Phillips. In this paper, we propose an approach of training project evaluation, based on business process management. It is an approach which fills the gaps raised in the literature and ensures an alignment between training activities and business needs. E-learning seen as an enterprise business process Business activity monitoring; Business intelligence; Business process management; E-learning; Evaluation; Optimization Competitive intelligence; Computer science; E-learning; Electronic commerce; Enterprise resource management; Industry; Information systems; Management science; Managers; Optimization; Personnel training; Profitability; Project management; Business Activity Monitoring; Business managers; Business needs; Business process management; Enterprise business process; Evaluation; Human capitals; Phillips; Training projects; Competition",Financial management
768,"Artificial intelligence, information system delivery and analytical capabilities in customer retention and relationship management","In this paper we review fundamental issues and analytical capabilities in customer retention and relationship management (CRRM). Emphasis is placed on the mesh among analytical capabilities, tactical and strategic CRRM, the role of next generation information systems delivery in leveraging new knowledge in CRRM, and the use of artificial intelligence to achieve autonomic management of the customer portfolio, We look particularly at the financial services industry as a general applications domain, and take illustrative examples in insurance and retail banking. Even so, these examples are not numerical but are intimately verbalised to highlight the scope of artificial intelligence and information systems work in CRRM. The use of information matrices in approaching tactical and strategic CRRM work is mentioned in the paper. Artificial intelligence, information system delivery and analytical capabilities in customer retention and relationship management Artificial intelligence; CRRM analytics; Decision support systems; Information systems delivery; Tactical and strategic CRRM Customer satisfaction; Data communication systems; Database systems; Decision support systems; Information dissemination; Information management; Insurance; Knowledge based systems; Customer relationship and retention management; Information system delivery; Knowledge management; Artificial intelligence",Strategic alignment
769,Public sector business intelligence -an open source approach,"Approaches to strategic Information Systems (IS) are standard in private organisations. In Public Administrations (PA), however, only first steps towards strategic managing support are undertaken. Deficits become apparent especially regarding new public management (NPM) approaches, such as new accounting systems or an output-oriented management. While PA are often obliged to implement such NPM approaches in order to collect management-relevant data, there is little support regarding how to employ these new structures in terms of strategic management. Here, Balanced Scorecards (BSC) can provide a valuable strategic management instrument. Core problems that such an approach faces in the domain of PA are domain specific requirements, data collection problems in a heterogeneous IS environment, and financial limitations due to budgetary restrictions. This paper presents a design science-oriented case study on BSC design and organisational implementation in a medium-sized local PA. It introduces a solution based on an Open Source Software (OSS) approach which addresses the problem of data collection by BSC and project management software integration. A case study analysis identifies generalisable issues which can potentially be applied to other situational and organisational contexts. Public sector business intelligence -an open source approach Design science case study; New public management; Open source software; Public administration; Strategic information systems Computer software; Data acquisition; Design; Information systems; Management science; Project management; Research; Strategic planning; Design science; New public management; Open Source Software; Public administration; Strategic information systems; Information management",Strategic alignment
770,The dynamics of relationship marketing in international sponsorship networks,"Purpose: While various scholars have identified relationship marketing objectives as a rationale for sports sponsorship engagement, analytic investigations of the implications of a relational approach to the corporate sponsorship network have been slow to materialize. The purpose of this paper is to advance the discussion of sponsorship as a means of industrial sports marketing towards a network conceptualization, which can be dissected from both the perspective of the sponsoring firms and that of the sponsored enterprise. Design/methodology/approach: This paper employs an illustrative case-based approach to the application of network analysis tools as a means of exploring the relationship marketing dynamics of corporate sponsorship portfolios. Findings: Several research propositions and applicable network analytics are presented within the context of Formula One racing team sponsorship portfolios. The concepts of network range, density, power, growth, and social capital are explored in regards to their influence on network actors and prospective actors. Practical implications: Though often neglected in sponsorship research, B2B relational objectives are the focus of this paper, where various evaluative methods are suggested and their dynamic implications illustrated. Originality/value: By utilizing an international contextual case and explicating several analytic network measures, this research extends the investigation of sports sponsorship beyond the image and awareness-based objectives that have dominated this area of research. This application of social network analysis to the study of inter-organizational networks in sport builds on the discussion of sponsorship as a bilateral relationship and advances the dialog towards a broader exploration of corporate sponsors and sport enterprises as network partners. © Emerald Group Publishing Limited. The dynamics of relationship marketing in international sponsorship networks Corporate hospitality; Formula One; Networks; Relationship marketing; Sponsorship ",Strategic alignment
771,Targeted optimization of efficiency,"Process instrumentation and analytics are essential for monitoring the plant, controlling the flow of substrates, and feeding the biogas. For these tasks, Siemens offers a broad and versatile product portfolio. In biogas plants, continuous level monitoring can be applied using ultrasonic or radar devices such as the HydroRanger 200 with a remote ultrasonic transducer. The transducer can be installed anywhere in the process or plant, while the controller unit allows monitoring at a location that can be accessed easily. For metering and for monitoring and balancing volumetric flows, flow meters using various measuring techniques are required. The SITRANS F device family covers all measuring applications in a biogas plant. The heat quantities produced after the combined heat and power plant are calculated, if required, by a heat metering system with calibration capability. The SITRANS FUE950 heat meter can be used in conjunction with the cost-efficient SITRANS F M electromagnetic flow meter or the ultrasonic flow meters of the SITRANS F US series. Targeted optimization of efficiency  ",Monitoring and control
772,A Note on the Analytics and Geometry of Limiting Mean - Variance Investment Opportunity Sets,"This paper extends the mathematics developed by Merton (1972) to the limiting investment opportunity set as smaller risk assets are added. Investment opportunity sets of risky assets are well-known to be described by hyperbolae in mean-standard deviation space. In practice, the asset classes in portfolios may vary from high risk common stocks to near cash assets. Low variability assets change the appearance of the investment opportunity set to the extent that a unique optimum risky asset portfolio disappears. The limiting result is similar to the investment opportunity set that arises when two assets are perfectly correlated. The location of the IOS is shown to mathematically depend upon the level of the riskless interest rate and one slope parameter. The slope parameter is estimable, using a finite number of assets, and represents a bound on market Sharpe ratios. © 1997 Kluwer Academic Publishers. A Note on the Analytics and Geometry of Limiting Mean - Variance Investment Opportunity Sets Convergence; Equity premium; Estimation; Investment opportunity sets; Markowitz ",Risk management
773,Is real time a waste sometimes?,"The concept of real-time enterprise performance management (rtEPM) is discussed. The rtEPM involves a range of enabling technologies and applications, including supply chain event management, web-based performance portals, enterprise application integration (EAI), analytics, data historians, and business intelligence solutions. The business benefit from rtEPM is similar to real-time feedback in manufacturing process control applications, where feedback reduces variability. In order to add value, rtEPM must be based on critical, time sensitive components, linked to performance management matrics, supported by change management, and business improvement processes. Is real time a waste sometimes?  Competitive intelligence; Feedback; Information technology; Product development; Project management; Real time systems; World Wide Web; Change management; Enterprise application integration (EAI); Real-time enterprise performance management (rtEPM); Supply chain event management; Enterprise resource planning",Risk management
774,"ICEB 2008 - Proceedings of the 8th International Conference on Electronic Business: ""Enriching Global Business Practices""","The proceedings contain 38 papers. The topics discussed include: identifying success factors for developing web applications - a research report; users and usage of community websites: the myhamilton.ca experience; web 2.0 and commercial disputes: a case study of information sharing in e-arbitrations and e-mediations; a multi-agent business intelligence framework for the travel sector; a visual map to identify high risk banks - a data mining application; decision supporting methodology and system based on theory of constraints for making an optimal product portfolio strategy in shipbuilding industry; a case study in e-government solutions; e-governance in Japan: analysis of the current status of e-government and local e-services; the valence of online consumer reviews and purchase decision: examining the moderating effects of product type and consumer expertise; and an inventory model with two classes of customers in on-line rental service: consumer model approach. ICEB 2008 - Proceedings of the 8th International Conference on Electronic Business: ""Enriching Global Business Practices""  ",Strategic alignment
775,Data mining with SQL server 2008 business intelligence development studio a hands-on approach,"Data mining is an emerging field of study in Information Systems programs. The purpose of this tutorial is to provide a hands-on approach on how to utilize SQL Server 2008 Business Intelligence platforms to teach a senior level data mining methods class in an Information Systems program. Our intention is to highlight the capabilities of the platform to faculty currently teaching a business intelligence course and are interested in expanding their teaching portfolio to the data mining area with hands-on exercises and projects. The platform allows faculty to focus on teaching the analytical aspects of data mining and the usage of algorithms through practical hands-on demonstrations, homework assignments and projects. As a result, students are expected to gain a conceptual understanding of data mining and the application of data mining algorithms for the purpose of decision support. Such a platform allows faculty to provide a comprehensive coverage of the topic with practical hands-on experience. The availability of this set of tools transforms the role of a student from a programmer of data mining algorithms (doing low level IT) to a business intelligence analyst. Data mining with SQL server 2008 business intelligence development studio a hands-on approach  Algorithms; Decision support systems; Information systems; Teaching; Business Intelligence platform; Conceptual understanding; Data mining algorithm; Data mining methods; Decision supports; Hands-on exercise; Hands-on experiences; Homework assignments; Information systems programs; Low level; SQL servers; Teaching portfolio; Data mining",Strategic alignment
776,FinVis: Applied visual analytics for personal financial planning,"FinVis is a visual analytics tool that allows the non-expert casual user to interpret the return, risk and correlation aspects of financial data and make personal finance decisions. This interactive exploratory tool helps the casual decision-maker quickly choose between various financial portfolio options and view possible outcomes. FinVis allows for exploration of inter-temporal data to analyze outcomes of short-term or long-term investment decisions. FinVis helps the user overcome cognitive limitations and understand the impact of correlation between financial instruments in order to reap the benefits of portfolio diversification. Because this software is accessible by non-expert users, decision-makers from the general population can benefit greatly from using FinVis in practical applications. We quantify the value of FinVis using experimental economics methods and find that subjects using the FinVis software make better financial portfolio decisions as compared to subjects using a tabular version with the same information. We also find that FinVis engages the user, which results in greater exploration of the dataset and increased learning as compared to a tabular display. Further, participants using FinVis reported increased confidence in financial decision-making and noted that they were likely to use this tool in practical application. ©2009 IEEE. FinVis: Applied visual analytics for personal financial planning Casual information visualization; Economic decision-making; Personal finance; Visual analytics; Visualization of risk Computer software; Economics; Graphical user interfaces; Information analysis; Information systems; Investments; Visualization; Cognitive limitations; Data sets; Decision makers; Economic decision-making; Experimental economics; Expert users; Financial data; Financial decisions; Financial planning; Financial portfolio; General population; Information visualization; Long-term investment; Portfolio diversification; Temporal Data; Visual analytics; Decision making",Risk management
777,Teaching business process improvements - Making the right choice,"A significant question in today's competitive market that has direct business ramifications is how to assess the impact of business improvement projects and information technology changes before a commitment is made since an impact assessment can affect expectations of yield, return and quality improvements. Projects are often started without the baseline measures needed to asses the degree of improvement. More importantly, the ranking and impact assessment is often completely subjective. While subjective impact is important, it needs tempering with simple but more objective methods. Current approaches used to measure the performance of the business relate to processes, such as dashboards of metrics, value chain performance, and value-management that all focus on the operational results rather than the processes are at the heart of the question. None of them provide preliminary insight or an assessment of which processes that should be improved, where the largest return is related to the larger risk or what will provide the greatest corporate impact for the least amount of investment or risk. There are few known business techniques or analytics that provide the insight required to assess the most significant business improvements prior to making that choice. Most analysis includes both qualitative and quantitative elements. Together, they provide the foundation for reasonable decision-making with regard to the situation being examined. Business processes are a series of actions or operations of producing something. The attributes of a business process can be either descriptive or quantitative. Businesses usually use one or more of these attribute(s) or metrics to represent the performance of the process (i.e. cycle time, inventory turns, and ratios of various sorts). Requirements for process improvement are statements supporting the need to change the actions within the process to improve its overall performance. This is a form of general requirements analysis. While some believe there is no way to generally analyze an enterprise others have examines some techniques to do so (Kowalkowski and McElyea). This paper will provide a step-by-step approach to examine and assess current business processes using a context based assessment method that allows you to understand the implications of changing the processes from various business perspectives. These context based methods also require some descriptive analytics to manipulate the working models. They provide a focused way to improve business processes based on requirements while examined the impact of the proposed changes to the business processes. Coupled with some quantitative measures, they can provide a means of assessing both return and risk. © American Society for Engineering Education, 2006. Teaching business process improvements - Making the right choice  Competition; Marketing; Project management; Quality assurance; Teaching; Value engineering; Business improvement projects; Business process improvements; Business ramifications; Impact assessment; Competitive intelligence",Strategic alignment
782,"Process Analytical Technology, continuous manufacturing, and the development of a surrogate model for dissolution: a pharmaceutical manufacturing statistical engineering case study","Statistical engineering is a recent movement that seeks to promote a systematic application of statistical tools combined with a comprehensive view of project management to solve important commercial problems. We discuss how statistical engineering principles were applied in the development of a surrogate model for dissolution (also referred to as in vitro release), an important physical property of solid dose pharmaceutical products required for marketing. The approach relies on spectroscopic analytical methods, referred to more generally as Process Analytic Technology, to enable monitoring of a batch of pharmaceutical product in real time. Real time release (RTR) is the release to the market of a batch of final product, nearly immediately after its production, rather than the weeks to months currently needed using conventional wet chemistry analytical methods. The development of a surrogate model advances the goal of RTR of a batch of pharmaceutical product, and the application of statistical engineering tools enhances a coherent development process as illustrated in a case study. © 2023 Taylor & Francis Group, LLC. Process Analytical Technology, continuous manufacturing, and the development of a surrogate model for dissolution: a pharmaceutical manufacturing statistical engineering case study continuous manufacture; process analytic technology; statistical engineering; surrogate model Commerce; Dissolution; Statistical mechanics; Statistics; Analytical method; Case-studies; Continuous manufacture; Pharmaceutical products; Process analytic technology; Process analytical technology; Process analytics; Real- time; Statistical engineering; Surrogate modeling; Project management",Capacity management
783,Extracting Insights from Big Source Code Repositories with Automatic Clustering of Projects by File Names and Types,"Software project delivery requires a set of related activities to be conducted. The output of these activities form a collection of unstructured data such as specifications, requirements, manuals, source code and packaging files which is stored in configuration management systems. Software repositories are infrastructures to support project management activities and can be composed with several systems that include code change management, bug tracking, code review, build system, release binaries, wikis, forums, etc. This large and variable data collection provides opportunities for text mining tasks which further can be utilized for software delivery or business intelligence related goals. The proposed approach uses machine learning methods to inspect large software repositories and classifies the results to propose insights to project managers as an aide to make strategic business decisions. In the present work every software project is described by a certain representation (vector or set of vectors), which is constructed from names and types of the files from project content. These representations are used to find clusters of similar projects in big code repositories and highlight specific properties of the found groups: most used names and types of the file. © 2023 IEEE. Extracting Insights from Big Source Code Repositories with Automatic Clustering of Projects by File Names and Types business intelligence; machine learning; natural language processing; software repository mining; source code mining; unsupervised learning Codes (symbols); Computer programming languages; Data acquisition; Information analysis; Information management; Learning algorithms; Natural language processing systems; Project management; Business-intelligence; Language processing; Machine-learning; Natural language processing; Natural languages; Software project; Software repositories; Software repository mining; Source code repositories; Source-code mining; Machine learning",Strategic alignment
784,Strategic Management Accounting on Competitive Advantage,"This study investigates the effect of business strategy on strategic management accounting (SMA) and its indirect effect on the competitive advantage of the rice export business in Thailand. A total of 215 major rice export businesses in Thailand in 2022 are chosen as the sample population, and data are collected. The results of the structural equation model (SEM) of the analysis model are found using an index that examines the absolute quality-of-fit measure. Additionally, the results of ordinary least squares (OLS) regression analysis, path coefficients, and hypothesis testing show that business strategy positively affects SMA. In addition, SMA plays a positive and significant role in competitive advantage. As for the theoretical contribution, conceptual SMA is explained by a resource-based view that focuses on resources influence on sustainable performance through competitive advantage. It is hoped that the present study will contribute significantly to the managerial contribution of rice export businesses, as SMA could support organizations in creating a competitive advantage. © 2023 IGI Global. All rights reserved. Strategic Management Accounting on Competitive Advantage Big Data Analytics; Business Strategy; Competitive Advantage; Strategic Management Accounting ",Strategic alignment
785,CARBON IMPACT ASSESSMENT OF BRIDGE CONSTRUCTION BASED ON RESILIENCE THEORY,"The construction and management of large-scale projects have the characteristics of complexity, dynamic and of-fline, and how to evaluate it is a research problem accurately. This study addresses this question through multidisciplinary cross-applied research. The research analyses and optimizes the environmental impact of the construction stage of super-large bridges by establishing a theoretical model system of environmental impact resilience. The analysis shows that industrialized construction can save 56.31% of materials compared with traditional construction but increase the consumption of machinery and personnel by 11.18%. Ultimately, environmental pollution can be significantly reduced. This study breaks through the difficulty of accurately evaluating discrete dynamic factors. It has realized the application of multidisciplinary research to solve management optimization and design problems in the elastic and dynamic changes of super-large bridges during construction. This research provides rich theoretical models and advanced analytics experience data for environmental resilience impacts and project resilience management models, laying a solid scientific foundation for dynamic control and sustainable development assessment of statically indeterminate structures in the future. © 2023 The Author(s). Published by Vilnius Gediminas Technical University. CARBON IMPACT ASSESSMENT OF BRIDGE CONSTRUCTION BASED ON RESILIENCE THEORY energy; environment; industrialized; material; project management; response Bridges; Carbon; Environmental management; Information management; Project management; Sustainable development; Bridge constructions; Characteristics of complexity; Energy; Environment; Impact assessments; Industrialized; Large Bridges; Large-scale projects; Response; Theoretical modeling; Environmental impact",Value management
786,Towards a semantic enriching framework for construction site images,"Recent applications of deep learning (DL)-based methods in construction have achieved notable momentum in making the construction management process smarter. The deployment of DL-based visual analytics systems needs to semantically enrich the original visual resources to improve situation awareness and scene understanding of the jobsite. This paper proposes a semantic enriching framework that integrates consolidated DL technologies with prior domain knowledge to prompt a traceable, explainable high-level information inferring process to explore semantic information behind visual resources collected from construction sites. The introduced framework starts from feature extraction from multiple aspects; then transforms the acquired facts into ontological assertions to iteratively reach higher-level interpretations achieved by semantic reasoning and querying. A case study, which manually grounds the primitive facts of a sample image and simulates the reasoning process based on the collected facts, is carried out to demonstrate the feasibility of the proposed framework. © 2023 the Author(s). Towards a semantic enriching framework for construction site images  Case based reasoning; Concentration (process); Deep learning; Domain Knowledge; Image processing; Iterative methods; Project management; Construction management process; Construction sites; Domain knowledge; High-level information; Job sites; Learning technology; Learning-based methods; Scene understanding; Situation awareness; Visual analytics systems; Semantics",Strategic alignment
787,Retail Managers’ Preparedness to Capture Customers’ Emotions: A New Synergistic Framework to Exploit Unstructured Data with New Analytics,"Although emotions have been investigated within strategic management literature from an internal perspective, managers’ ability and willingness to understand consumers’ emotions, with emphasis on the retail sector, is still a scarcely explored theme in management research. The aim of this paper is to explore the match between the supply of new analytical tools and retail managers’ attitudes towards new tools to capture customers’ emotions. To this end, Study 1 uses machine learning algorithms to develop a new system to analytically detect emotional responses from customers’ static images (considering the exemplar emotions of happiness and sadness), whilst Study 2 consults management decision-makers to explore the practical utility of such emotion recognition systems, finding a likely demand for a number of applications, albeit tempered by concern for ethical issues. While contributing to the retail management literature with regard to customers’ emotions and big data analytics, the findings also provide a new framework to support retail managers in using new analytics to survive and thrive in difficult times. © 2021 British Academy of Management and Wiley Periodicals LLC. Retail Managers’ Preparedness to Capture Customers’ Emotions: A New Synergistic Framework to Exploit Unstructured Data with New Analytics  ",Strategic alignment
789,Multi-Criteria decision analysis approach for selecting feasible data analytics platforms for precision farming,"Cloud computing has become a crucial part of smart farming systems. It offers various services, from data storage to data analytics and visualization. However, selecting a feasible platform is challenging since many factors and criteria need to be considered by decision-makers based on the organization's requirements to select the most optimal cloud solution. This study aimed to provide a systematic approach to selecting cloud computing-based data analytics platforms for precision farming. There are three important stages within the proposed approach: the preparation stage, the integrated model using Analytic Hierarchy Process (AHP) and Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS) as an evaluation framework, and model evaluation. Three cloud computing platforms were evaluated using the proposed model for a novel smart farming project: Amazon Web Services, Google Cloud Platform, and Microsoft Azure. The results show that Google Cloud Platform (S2) is best optimal platform for the smart farming project called smart-in-ag based on the criteria and requirements defined by stakeholders. To validate the consistency and robustness of our proposed model, the sensitivity analysis method was applied to 13 cases. It was demonstrated that the proposed approach is consistent and robust for helping the experts who choose a cloud computing-based data analytics platform in a smart farming project. To the best of our knowledge, this study is the first application for selection the cloud computing platform for a real smart farming project. © 2023 The Author(s) Multi-Criteria decision analysis approach for selecting feasible data analytics platforms for precision farming AHP; Cloud computing; Data Analytics Platform; Multi-Criteria Decision-Analysis (MCDA); Smart Farming; TOPSIS Analytic hierarchy process; Cloud analytics; Data visualization; Decision making; Digital storage; Hierarchical systems; Sensitivity analysis; Web services; Cloud computing platforms; Cloud-computing; Data analytic platform; Data analytics; Ideal solutions; Multi-criteria decision analysis; Multi-criteria decision-analyse; Precision-farming; Smart farming; Technique for order of preference by similarity to ideal solution; analytical hierarchy process; feasibility study; multicriteria analysis; precision agriculture; project management; stakeholder; Data Analytics",Strategic alignment
792,Integrated and data Science-informed seabed characterisation for optimised foundation design,"Unlike traditional offshore oil and gas projects which generally involve a limited number of structures and are confined to a relatively small seabed footprint, renewable energy projects such as offshore wind farms can comprise many tens or even hundreds of structures dispersed across an extensive area. Driven by the need for offshore wind to reduce costs while also minimising project risk, there is currently a strong focus within the offshore industry on identifying new ways to extract the full benefit from all available site investigation data and carry this benefit through to engineering design. This paper provides a demonstration of two such approaches that utilise geo-data from offshore Western Australia, potentially representative of conditions of future offshore wind farms; the first is an integrated approach combining geophysical and geotechnical data through a seismic inversion process, while the second encompasses statistical analysis of geotechnical cone penetrometer and soil strength test data combined with a Bayesian compressive sampling-based spatial interpolation method. The demonstrations yielded useful findings about the methodologies and associated input requirements. It is envisaged this work will lead to the development of an efficient integrated framework for interpreting geo-data that will inform future offshore site investigation and geotechnical design practice. © 2023 The Authors Integrated and data Science-informed seabed characterisation for optimised foundation design  Australia; Western Australia; Cost engineering; Electric utilities; Offshore oil well production; Offshore wind turbines; Sampling; Soil testing; Statistical tests; Foundation design; Geo-data; Offshore oil; Offshore winds; Offshores; Oil and gas projects; Project risk; Reduce costs; Renewable energy projects; Site investigations; alternative energy; data inversion; design; foundation; future prospect; geotechnical engineering; integrated approach; offshore engineering; optimization; seafloor; seismic survey; site investigation; soil mechanics; soil strength; soil-structure interaction; structural analysis; structural response; wind farm; wind field; Offshore wind farms",Risk management
794,Framework for a Predictive Progress Model–case of infrastructure projects,"Project progress is an apprehension for every project, as it indicates how the project is likely to meet the associated milestones. Utilizing historical data from archived projects can assist managers in predicting project progress. By leveraging the power of data analytics, this research attempts to highlight data trends based on data collected from 279 infrastructure projects in the UAE. Specifically, this research rigorously analyses the relationships between project budget, duration, and progress using K-means clustering techniques and hypothesis testing. We then provide predictive models using Autoregressive Integrated Moving Average–ARIMùA and Multivariate regression models that allow managers to predict with a 99.15% accuracy the monthly progress of an infrastructure project over the next three months. This research provides project managers with a comprehensive framework that combines data analytics techniques with agility practices to predict short-term project progress to take proactive measures on different influencing factors. © 2022 International Society of Management Science and Engineering Management. Framework for a Predictive Progress Model–case of infrastructure projects clustering; clusters; Data analytics; framework; hypothesis testing; predictive models; project management; project progress; regression; time series; trend, ARIMA; trend, infrastructure ",Monitoring and control
796,Sustainable Tourism X,"The proceedings contain 11 papers. The topics discussed include: impacts of climate change on tourism areas in Germany: an overview of recommended adaptation measures and their communication; sustainable hotel design strategies: tourism as a tool for circular bioeconomy in fragile ecosystems; cold-water recreational diving experiences: the case of kelp forests; fishing tourism and sustainability in the Canary Islands, Spain; rural tourism under the new normal: new potentials from a Japanese perspective; new paradigm of spiritual tourism: adding an important layer to sustainable tourism; strategic management of tourism sustainability through the Greek stakeholders’ perspective on the impacts of events: the case of Patras’ Carnival, Greece; role of artificial intelligence and big data analytics in smart tourism: a resource-based view approach; in search of the desired sustainable tourism: a review of life cycle assessment (LCA) tourism studies; and sustainable practices in hotels in Sri Lanka: analysis of environmentally sustainable aspects. Sustainable Tourism X  ",Governance
799,Construction Management Supported by BIM and a Business Intelligence Tool,"The construction sector generates large amounts of heterogeneous and dynamic data characterized by their fragmentation throughout the life cycle of a project. Immediate and accurate access to that data is fundamental to the management, decision-making and analysis by construction owners, supervisors, managers, and technicians involved in the different phases of the project life cycle. However, since construction project data are diverse, dispersed, uncorrelated, and difficult to visualize, a reliable basis for decision-making can rarely be established by the management team. Aiming to bridge this gap, a methodology for data management during building construction by means of Data with BIM and Business Intelligence (BI) analysis tools was developed in this study. This methodology works by extracting data from 3D parametric model and integrating it with a BI tool, through which data are visualized and interrelated with the same database, the BIM model. To demonstrate the applicability of the methodology, a study case was carried out. It was shown that this methodology provides a collaborative platform for accurate data analysis to the construction management and supervision teams, allowing project stakeholders to access and update data in real-time, in permanent linkage with the BIM model. Additionally, improving the reliability of the decision-making process and ensuring project deliverability, the developed methodology contributes to a more sustainable management process by decreasing errors and resource consumption, including energy. Therefore, the main goal of this study is to present a methodology for data analysis with BIM models integrated with BI for sustainable construction management. © 2022 by the authors. Licensee MDPI, Basel, Switzerland. Construction Management Supported by BIM and a Business Intelligence Tool data analysis; parametric modelling; Power BI; sustainable management 3D modeling; Architectural design; Bridges; Construction; Construction industry; Data handling; Decision making; Human resource management; Information management; Life cycle; Project management; Sustainable development; Business-intelligence; Construction management; Construction sectors; Heterogeneous data; Intelligence tool; Large amounts; Parametric models; Power; Power business intelligence; Sustainable management; Information analysis",Financial management
802,Data governance and the secondary use of data: The board influence,"The business analytics and strategic management literatures suggest that organizations should seek to exploit data as a key mechanism for competitive advantage. However, the rules of engagement are evolving, the regulatory landscape is becoming increasingly complex, and examples of poor outcomes are increasingly common. The board – in its role of setting and monitoring risk appetite – needs to be able to govern the risk/reward trade-off of the data asset. Contemporary data governance approaches are inadequate: they are overly rigid and risk oriented, limited in scope to an organization's self-interest rather than considering the broad set of stakeholders, and do not provide a platform for the board to manage this critical risk. This paper uses a unique set of informants – 41 board directors – to demonstrate that differences in board perspectives influence how organizations explore the secondary use of data. Furthermore, this paper identifies a set of relevant individual, organizational and environmental factors and presents empirically based configurations of these factors that lead organizations to consider (or neglect) the secondary use of data as a critical enabler of competitive advantage. © 2023 Elsevier Ltd Data governance and the secondary use of data: The board influence Board oversight of IT; Business analytics; Data governance; Fuzzy-set Qualitative Comparative Analysis (fsQCA); Secondary use of data ",Strategic alignment
803,Digital Transformation for Business and Society: Contemporary Issues and Applications in Asia,"The advancement of technology, such as data analytics and AI, has led to the birth of Industry 4.0, in which technology seems to be at the centre of development. However, as the COVID-19 pandemic created havoc, the entire world production chain was seriously affected, highlighting that machines alone, although fully connected and automated, cannot function without people. This book addresses contemporary issues and the impact of digital transformation on individuals, businesses and governments in Asia. As Asian nations are moving fast towards the digital economy, this edited collection offers new perspectives on understanding emerging business opportunities as well as the challenges faced. Chapters explore the variables that accelerate digital transformation, notably the COVID-19 pandemic, the rapid development of information and communication technology, AI, big data and affordable internet access, and their impact on business and society. With rich, empirical studies from leading researchers, this book will be a reference for academics and scholars across business disciplines, including information, technology and innovation management, organisational and strategic management, as well as those interested in industrial development. © 2024 selection and editorial matter, Mohammad Nabil Almunawar, Patricia Ordóñez de Pablos and Muhammad Anshari. Digital Transformation for Business and Society: Contemporary Issues and Applications in Asia  ",Strategic alignment
804,"Apply Data Science: Introduction, Applications and Projects","This book offers an introduction to the topic of data science based on the visual processing of data. It deals with ethical considerations in the digital transformation and presents a process framework for the evaluation of technologies. It also explains special features and findings on the failure of data science projects and presents recommendation systems in consideration of current developments. Machine learning functionality in business analytics tools is compared and the use of a process model for data science is shown. The integration of renewable energies using the example of photovoltaic systems, more efficient use of thermal energy, scientific literature evaluation, customer satisfaction in the automotive industry and a framework for the analysis of vehicle data serve as application examples for the concrete use of data science. The book offers important information that is just as relevant for practitioners as for students and teachers. © The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Fachmedien Wiesbaden GmbH, part of Springer Nature 2023. Apply Data Science: Introduction, Applications and Projects Big Data; business applications; CRM; Customer Relationship Management; Data Science Book; Digitisation; Project management ",Strategic alignment
805,An Integration of Big Data and Blockchain for Strategic Analysis of Schools in Thailand,"In the era of digital transformation, the information used for strategic management in schools needs to be utilized for maximum benefit, efficiency, and effectiveness. Further, it requires storage in a secure, transparent, and verifiable manner worldwide. Recognizing the importance of this research contribution, the researchers designed a strategic data storage and analysis system to be used in larger secondary schools by integrating big data, artificial intelligence, and blockchain technologies. The integrated system provides a platform to collect and analyze annual operational plans for schools by facilitating compatibility with text analytics and machine learning technologies of large, high-security schools in Thailand. The results showed that the plans and projects for each department in the school were encrypted at the time of login, and the system generated a starting block. Through each step of the corresponding analysis process, new blocks are created continuously, meaning each plan and project will generate a chain of blocks encrypted with the hash function until the plan and project are approved. Furthermore, the system has a high level of security and user satisfaction (X-= 4.00, SD. 0.76)). Thus, the system could be implemented and adopted in schools to support educational management in the 21st century. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG. An Integration of Big Data and Blockchain for Strategic Analysis of Schools in Thailand Artificial intelligence; Big data; Blockchain; Strategic analysis; Text Analytics Artificial intelligence; Big data; Data integration; Digital storage; Hash functions; Block-chain; Data analysis system; Data storage systems; Digital transformation; Secondary schools; Strategic analysis; Strategic data; Strategic management; Text analytics; Thailand; Blockchain",Strategic alignment
806,"Exploring the Applications, Data and Services Needed for a Cloud-Based Workplace for the Public Sector","To ensure digital independency, the European program Gaia-X pursues the goal to provide the public sector a secure cloud with open source applications, data and services for a digital workplace [1].Which applications, data and services are needed and useful for a digital sovereign workplace for public employees? This paper presents the preliminary results of the Gaia-X POSSIBLE project and its contextual interviews with public administrative employees conducted in fall 2022, investigating their needs in regard to the Phoenix office suite by Dataport and the digital workplace beyond. Within the Gaia-X POSSIBLE project, according to our knowledge, for the first time public administration employees were involved in user research. To facilitate implementation of the study and within the given scope, in this first round, public administrative employees at state and municipality level in Schleswig-Holstein (Northern Germany) were targeted. These two target groups were supplemented by a third target group, namely students of Public Administration at the University of Applied Sciences for Public Administration and Services. We aimed at 5–20 participants per target group. In total, 32 contextual interviews were conducted. The contextual interviews were evaluated on the basis of a qualitative content analysis according to Mayring [11]. The 5 most essential office applications were e-mail, calendar, word processing, video conferencing and a shared document management system. In addition, an e-file application, telephony and PDF processing were rated as most useful, supplemented by notes, project management and other creativity and collaborative applications. For further studies, we suggest to extend the target group to other federal states, the federal government and other European countries. Quantitative analytics and/or surveys could provide reliable information on user numbers of office, specialist applications and data services. Also, usability studies focused on each single application would deliver more details. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG. Exploring the Applications, Data and Services Needed for a Cloud-Based Workplace for the Public Sector Public Administration; Service Design Research; User Studies Application programs; Information services; Personnel; Project management; Public administration; Word processing; Application data; Application services; Cloud-based; Design research; European program; Public sector; Service design research; Services designs; Target group; User study; Video conferencing",Risk management
810,A Business Analytics Approach to Strategic Management using Uncovering Corporate Challenges through Topic Modeling,"Business analytics is a robust strategic management tool, and topic modeling is a technique that can be leveraged to derive insights from vast collections of unstructured data. Topic modeling is an automated method that identifies abstract concepts, or ""topics,"" present in various data sources, such as customer feedback, social media posts, and news articles. Through topic modeling, organizations can gain a better understanding of their customers, competitors, and market trends, which can be used to make informed strategic decisions, such as identifying new business opportunities, enhancing product or service offerings, and recognizing potential risks. Moreover, by integrating topic modeling with other business analytics approaches, such as predictive modeling, organizations can gain a more comprehensive perspective of their performance and make data-driven decisions. In essence, topic modeling is a valuable tool for strategic management that provides organizations with the insights they need to stay ahead of the competition and make informed decisions. To make effective strategic decisions, it is crucial to comprehend an organization's internal and external environments fully. The proposed approach utilizes text-mining techniques to augment traditional management tools, such as SWOT analysis or growth-share matrix. By examining narrative materials, such as financial disclosures, we apply topic modeling to identify critical challenges faced by an organization. We then quantify the language used in these materials in terms of risk and optimism, which provides a detailed understanding of a company's strengths and weaknesses and helps identify business units, activities, and processes that may be at risk. Additionally, this approach can be used to compare a company with its competitors or the broader market. © 2023 NSP Natural Sciences Publishing Cor. A Business Analytics Approach to Strategic Management using Uncovering Corporate Challenges through Topic Modeling Business Approach; Performance Analysis; Strategic management; Text mining; Topic modeling ",Strategic alignment
811,What Drives Success in Data Science Projects: A Taxonomy of Antecedents,"Organizations have been trying to reshape their business processes and transform them into a smart environment to attain sustainable competitive advantage in their markets. Data science enables organizations to define interconnected and self-controlled business processes by analyzing the massive amount of unstandardized and unstructured high-speed data produced by heterogeneous Internet of Things devices. However, according to the latest research, the success rate of data science projects is lower than other software projects, and the literature review conducted reveals a fundamental need for determining success drivers for data science projects. To address these research gaps, this study investigates the determinants of success and the taxonomy of antecedents of success in data science projects. We reviewed the literature systematically and conducted an expert panel by following a Delphi method to explore the main success drivers of data science projects. The main contributions of the study are twofold: (1) establishing a common base for determinants of success in data science projects (2) guiding organizations to increase the success of their data science projects. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG. What Drives Success in Data Science Projects: A Taxonomy of Antecedents Critical success factors; Data science; Project management; Project success Competition; Data Science; Digital storage; Sustainable development; Taxonomies; Business Process; Critical success factor; High-speed data; Literature reviews; Project success; Science projects; Smart environment; Software project; Success factors; Sustainable competitive advantages; Project management",Value management
812,Connecting the Dots: A Programmatic Approach to Data Science within Engineering,"The importance of “data acumen” for STEM students has been well-articulated by scholars and industry professionals-in part because data science infiltrates many areas of engineering and science. Yet within engineering programs, students often have few opportunities to develop expertise in data science or even to explore how data science is relevant to their degree specializations. This paper reports on an NSF-funded study of a program that prepares STEM students to engage with data science in coursework and then mentors them as they secure internships and complete a capstone that demonstrates their application of data science expertise. Drawing on a mixed-methods study, including student reflections, capstone project assessment, and survey reporting, this paper suggests not only that students make deep connections between their existing majors and data science but also that students trained in our data science micro-credential have unique opportunities to improve critical super-skills, including written communication, project management, iterative thinking, and real-world problem-solving. © American Society for Engineering Education, 2023. Connecting the Dots: A Programmatic Approach to Data Science within Engineering  Application programs; Data Science; Engineering education; Iterative methods; Project management; Capstone projects; Courseworks; Engineering program; Industry professionals; Mixed method; Programmatics; Project assessment; Project surveys; Specialisation; Written communications; Students",Capacity management
814,Machine learning in project analytics: a data-driven framework and case study,"The analytic procedures incorporated to facilitate the delivery of projects are often referred to as project analytics. Existing techniques focus on retrospective reporting and understanding the underlying relationships to make informed decisions. Although machine learning algorithms have been widely used in addressing problems within various contexts (e.g., streamlining the design of construction projects), limited studies have evaluated pre-existing machine learning methods within the delivery of construction projects. Due to this, the current research aims to contribute further to this convergence between artificial intelligence and the execution construction project through the evaluation of a specific set of machine learning algorithms. This study proposes a machine learning-based data-driven research framework for addressing problems related to project analytics. It then illustrates an example of the application of this framework. In this illustration, existing data from an open-source data repository on construction projects and cost overrun frequencies was studied in which several machine learning models (Python’s Scikit-learn package) were tested and evaluated. The data consisted of 44 independent variables (from materials to labour and contracting) and one dependent variable (project cost overrun frequency), which has been categorised for processing under several machine learning models. These models include support vector machine, logistic regression, k-nearest neighbour, random forest, stacking (ensemble) model and artificial neural network. Feature selection and evaluation methods, including the Univariate feature selection, Recursive feature elimination, SelectFromModel and confusion matrix, were applied to determine the most accurate prediction model. This study also discusses the generalisability of using the proposed research framework in other research contexts within the field of project management. The proposed framework, its illustration in the context of construction projects and its potential to be adopted in different contexts will significantly contribute to project practitioners, stakeholders and academics in addressing many project-related issues. © 2022, The Author(s). Machine learning in project analytics: a data-driven framework and case study  Artificial Intelligence; Logistic Models; Machine Learning; Retrospective Studies; Support Vector Machine; article; artificial intelligence; artificial neural network; dependent variable; feature selection; human; independent variable; k nearest neighbor; machine learning; physician; prediction; random forest; recursive feature elimination; support vector machine; artificial intelligence; retrospective study; statistical model",Strategic alignment
817,Qoruz: riding the influencer marketing wave,"Learning outcomes: After working through the case and the assignment questions, students will be able to understand the current practices and importance of influencer marketing strategies within overall marketing strategies; appreciate both the debate and dissonance that surround influencer performance measures; outline the key elements of Qoruz.com’s investments and efforts that brought them success; understand the strategic intent and justify the logic of operationalisation of Qoruz.com by creating two different SBUs after they launched a vastly improved tech platform; and evaluate potential strategies that Qoruz.com could use to move ahead and cement its supremacy in the influencer marketing space. Case overview / synopsis: Interest in influencer marketing which found many takers during the pandemic was expected to intensify and form the core of many brand strategies. Coupled with this heightened interest and increased budget overlay, demands from brands and agencies alike for clearer ROI linkages and KPIs that have better correlation with business goals, have gained momentum. Qoruz, an early entrant in the influencer marketing space in India, attributed their success to their focus on product innovation and service quality. From a predominantly narrow service offering providing analytics that facilitated decision-making for influencer marketing campaigns, their recently launched multi-feature platform enabled them to expand their services and consolidate their position. However, today, in an increasing volatile market, drawn by the high growth trajectory of the influencer marketing space, many players had jumped in and tried to introduce technology-based platforms with almost similar features while aggressively playing the price card. With the monetary and economic conditions under pressure and constantly changing demands of clients, Qoruz.com found itself faced with a dilemma to protect their first mover advantage. The co-founders of Qoruz realised that to give confidence to their loyal client base, and really cement their leadership, they would need to urgently take stock and relook at their strategy afresh relying on their deep experience of the industry, loyalty of their customers and their tech-centric DNA to build a holistic and ambitious strategy. Complexity academic level: This case is designed for use by graduate and under-graduate level students in marketing management and strategic management courses. Supplementary materials: Teaching notes are available for educators only. Subject code: CSS 11: Strategy. © 2023, Emerald Publishing Limited. Qoruz: riding the influencer marketing wave B2B marketing; Business strategy; Customer relationship management; Influencer marketing; Marketing strategy; Product development; Social media strategy; Solutions selling; Technology start-up ",Strategic alignment
818,The Role and Impact of Big Data in Organizational Risk Management,"Technological advancement has exposed companies to various risks. With the adoption of technological infrastructures in many companies, various processes have been rendered vulnerable to different forms of threats. Evidence from the current empirical studies on organizational management indicates effective risk assessment is a crucial aspect in any organization. Global technological advancement has equally redefined risk assessment and management strategies. Global leading technology companies such as Apple Inc., Amazon Inc., and Google are leveraging modern technology to unlock hidden data to aid in risk assessment and management. As evidenced in this research report, the use of big data has revolutionized risk assessment in many companies across the world. Big data technology has enabled organizations to collect, store, and assess huge information to aid in risk assessment and management. The processes of risk identification, assessment, mitigation, monitoring, and reporting have been redefined due to the adoption of big data analytical technology. As a result, this research reviews the specific roles played by big data in organizational risks management. The research carries out a comparative case study analysis among three companies that utilize big data in risk management. Specifically, the roles of big data technology in risk management at Apple Inc., Amazon Inc., and at Google are identified. Results from this comparative analysis are used in formulating recommendations for various organizations that desire to adopt big data analytical technology in risk management. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG. The Role and Impact of Big Data in Organizational Risk Management Big data; Big data analytics; Organizational risk management; Project risk management ",Risk management
820,Combining Integrated Project Management with Agile Hydraulic Fracturing Workflow to Successfully Deliver Infill Well Campaign in Heterogeneous Tight Volcanic Reservoir,"Vedanta Ltd, Cairn Oil & Gas operates a tight gas field which produces majorly from the volcanic reservoir. Commercial production is possible post hydraulic fracturing which has been established through successful fracturing campaigns in the past. Hydraulic fracturing in volcanics is a complex operation due to the inherent heterogeneity and associated uncertainty of reservoir properties in these tight formations (~0.1 md). With added challenge of stimulating infill wells landed in partially depleted zones, fracturing becomes the key operation governing the ability to meet production target, project timelines and associated costs. Based on new seismic data obtained during field production and previous campaigns, Cairn executed a development campaign and is now following up with an infill campaign to improve and sustain the production plateau in the Raageshwari Gas reservoir. With consideration of economics of the field development, a strong focus on continuous improvement, optimization and developing the subsurface understanding along with lean and integrated project and contract management strategies to improve operational performance were key to achieving project objectives. A continuous improvement strategy through production technology initiatives was applied to stimulation campaign by re-designing completion, exhaustive data gathering/analysis and optimizing stage count, proppant volume through machine aided data analytics, testing and clean-up time, minimize hookup time to achieve execution targets and accelerate and maximize production time. Proppant onset calculations and well operating envelop were re-defined to ensure longevity to wells. Apart from summarizing the key learnings of the re-development campaign from a petroleum & completion engineering standpoint, the paper would emphasize on the challenges expected & observed and mitigation methods in the infill campaign - the most noticeable of the challenges being depletion. The project resulted in successfully completing 42 wells with ~ 250 frac stages and strategically placing ~ 63 million lb of proppants in gas producing zones. This lead to the production increment of ~ 40% to the overall production which has proven critical to plateau extension. The learnings also form the basis of the future infill campaign. Apart from the technical considerations, the paper would expand on the integrated project management strategies which resulted into achieving volume targets within the assigned budget while managing the local considerations and field specific challenges during the execution phase. This paper lays out a framework on optimized data collection, evaluation and integration for continuous improvement. It maps the uncertainties associated with highly laminated reservoirs and challenges to look at in an infill campaign. Most importantly the Integrated Contract & Project Management framework would provide operators with valuable insights to execute a frac intensive development/infill campaign within the planned budget/resources. Copyright © 2023, Society of Petroleum Engineers. Combining Integrated Project Management with Agile Hydraulic Fracturing Workflow to Successfully Deliver Infill Well Campaign in Heterogeneous Tight Volcanic Reservoir  Budget control; Data Analytics; Fracture; Gases; Infill drilling; Petroleum reservoir evaluation; Project management; Proppants; Reservoir management; Seismology; Tight gas; Commercial productions; Continuous improvements; Gas fields; Infill wells; Integrated programs; Oil gas; Uncertainty; Volcanic reservoirs; Volcanics; Work-flows; Gas industry",Capacity management
821,Oil and Gas Skills for Low-Carbon Energy Technologies,"This paper aims to demonstrate the transferable skills of oil and gas professionals relevant to the energy transition and low-carbon technologies. We examined both core technical and non-core technical skills. The approach to this study involved using the SPE competency matrix for oil and gas professionals and mapping it against the different concepts related to decarbonization and renewable energy sources. We covered core technical and non-core technical oil and gas skills, and how they related to concepts such as carbon capture and sequestration, underground energy storage, rare earth elements, and renewable energy. We graded each item in the oil and gas competency matrix to demonstrate how relevant they were to the low-carbon technologies. We referenced case studies in research where oil and gas skills are being used in several aspects of the energy mix. The results showed that the technical competencies of oil and gas professionals were most useful in areas such as carbon storage, underground energy storage, and geothermal energy. The study also showed that geoscience skills cut across almost every aspect of low-carbon technologies and other concepts related to the energy transition. We also observed that non-technical competencies such as project management, HSE (health, safety, and environment), and business development skills cut across all low-carbon technologies and were very relevant for renewable energy resources like solar, wind, and hydro. Data science and digital skills were seen to be applicable to all low-carbon energy technologies. Using the case studies, we discuss the required upskilling, reskilling, and cross-skilling. The novel deliverable of this study is a comprehensive skillset map that shows where the oil and gas skills fit within low-carbon energy technologies. Professionals with skills in geosciences, reservoir engineering, production engineering, drilling, and wells engineering can see what areas they can easily transfer their skills, where they require reskilling, and where upskilling will be required. This could help organizations design a reskilling and upskilling strategy to help oil and gas professionals remain relevant in the energy transition. Copyright © 2023, Society of Petroleum Engineers. Oil and Gas Skills for Low-Carbon Energy Technologies  Carbon capture; Energy storage; Geothermal energy; Project management; Rare earths; Underground gas storage; Case-studies; Competency matrices; Energy transitions; Geosciences; Low-carbon energy technology; Low-carbon technologies; Oil and gas; Technical competencies; Transferable skills; Underground energy storages; Digital storage",Financial management
822,Digital Administration of the Project Based on the Concept of Smart Construction,"This study is devoted to the problem of updating the administration system of a construction project based on the integration of the concepts of process-oriented and project-oriented management in the context of the implementation of the latest technologies and digital models of smart construction, building information modeling (BIM) and the Internet of Things (IoT). In accordance with this significant transformations are subject not only to the operating system of enterprise management, but also to the course of business processes in the management environment of an investment and construction project. The result of the study is a model of a digital design company for construction lifecycle management. Such a model for managing a virtual design enterprise will optimize and reduce the costs of existing operating systems for managing a construction enterprise. The concept of introducing a life cycle management system for construction objects using BIM in the areas of reconstruction projects is proposed. With further development and implementation, this industry-specific BIM platform management model can form an information technology integration framework for reorganizing the business processes of a construction enterprise. The basic structure of the BIM platform is described, consisting of four components. There are cloud computing, big data analytics, Internet of Things and Blockchain information technologies. The need for the evolution of IoT information technologies from intelligent things to an intelligent planet is shown. In the future, it is planned to develop an integrated innovative structure of a holding type that supports the digital transformation of the operating system of enterprise management in the construction industry. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG. Digital Administration of the Project Based on the Concept of Smart Construction Construction company; Digital transformation of administration systems; Information modeling in construction; Operational management system; Project management ",Financial management
826,Using Learning Analytics and Student Perceptions to Explore Student Interactions in an Online Construction Management Course,"The expansion of online learning in higher education has both contributed to researchers exploring innovative ways to develop learning environments and created challenges in identifying student interactions with course material. Learning analytics is an emerging field that can identify student interactions and help make data-informed course design decisions. In this case study, learning analytics were collected from 113 students in three course sections of an online construction management course in the Canvas learning management system (LMS). Surveys were used to collect students' perceptions of the course design and materials to correlate with the students' interactions with the course materials. The survey findings showed the students found watching the lecture videos and reading the lecture slides to be the most helpful aspects of the course materials in their learning. Findings from the learning analytics showed that students' interactions with the course decreased after the midterm exam. Based on the results, online course instructors can leverage their learning analytics to understand student interactions and make data-informed course design changes to improve their online learning environments.  © 2022 This work is made available under the terms of the Creative Commons Attribution 4.0 International license,. Using Learning Analytics and Student Perceptions to Explore Student Interactions in an Online Construction Management Course  Computer aided instruction; Curricula; Design; E-learning; Project management; Surveys; Case-studies; Construction Management Courses; Course design; Course material; Design decisions; High educations; Learning environments; Online learning; Student interactions; Student perceptions; Students",Strategic alignment
827,Business Intelligence Adoption for Small and Medium Enterprises: Conceptual Framework,"All businesses have many issues, especially small and medium enterprises trying to survive with traditional technology. Therefore, enterprises need to adopt business intelligence by using the management of information technology systems to overcome the issues. This study proposes a conceptual framework that identifies the potential factors that influence the adoption of business intelligence systems in the SME industry in Libya. Therefore, this study was established based on two main theories: the technology acceptance model (TAM) and the unified theory of adopting and using technology (UTAUT). In line with the previous studies that investigated this type of influence, this study recommended a conceptual framework containing several factors: change management, knowledge sharing, information quality, IT project management, the perceived usefulness of a BIS, and the perceived ease of adoption of a BIS. This study did not consider the environmental factors’ effect on adopting a BIS (business intelligence system); this is due to the different characteristics of each small and medium enterprise in terms of the sector or industry type. © 2023 by the authors. Business Intelligence Adoption for Small and Medium Enterprises: Conceptual Framework business analytics; business intelligence; framework; information technology performance management; Libya case study ",Value management
828,The Adoption of a Big Data Approach Using Machine Learning to Predict Bidding Behavior in Procurement Management for a Construction Project,"Big data technologies are disruptive technologies that affect every business, including those in the construction industry. The Thai government has also been affected and attempted to use machine learning techniques with the analytics of big data technologies to predict which construction projects have a winning price over the project budget. However, this technology was never developed, and the government did not implement it because they had data obtained via a traditional data collection process. In this study, traditional data were processed to predict the behavior in Thai government construction projects using a machine learning model. The data were collected from the government procurement system in 2019. There were seven input data, including the project owner department, type of construction project, bidding method, project duration, project scale, winning price overestimated price, and winning price over budget. A range of classification techniques, including an artificial neural network (ANN), a decision tree (DC), and a K-nearest neighbor (KNN), were used in this study. According to the results, after hyperparameter tuning, the ANN had the greatest prediction accuracy of 78.9 percent. This study confirms that the data from the Thai government procurement system can be investigated using machine learning techniques from big data technologies. © 2023 by the authors. The Adoption of a Big Data Approach Using Machine Learning to Predict Bidding Behavior in Procurement Management for a Construction Project artificial neural network; big data; construction budgeting; machine learning; procurement management artificial neural network; data set; machine learning; project management; spatiotemporal analysis",Strategic alignment
829,Experiential learning of management principles via online student-manager discussions,"This teaching brief describes an experiential project used in a graduate Principles of Management course for nonbusiness undergraduate students. Groups of four to six first-year MBA students interviewed a seasoned manager online twice over the 8-week course and discussed the applications of course material. Project subtopics included an introduction to management, strategic management, ethics and social responsibility, innovation and change management, international business, organizational structure, authority and job design, human resource management, leadership, and communication, operations management, and business analytics. Students completed a group report and an individual reflection on their experience. Over 92% of graduate students in the class indicated that the project was a positive learning activity. © 2023 Decision Sciences Institute. Experiential learning of management principles via online student-manager discussions experiential; graduate; management ",Risk management
830,Generalized linear model-based data analytic approach for construction equipment management,"The utilisation of equipment on construction sites can be challenging as it is expensive and bulky with difficulties on flexible dispatching. It is therefore essential that equipment be managed efficiently and effectively. Equipment idleness can reflect the utilization efficiency, but there is limited research on quantitative analysis. The aim of this paper is to determine and examine the influencing factors of equipment idleness and predict the possible idleness considering the construction schedule accordingly. Both data preprocessing and data analysis are considered. Site diary data from a real-life project in Hong Kong was used in this research. Firstly, the semi-structured data is processed into a structured schema for analysis. Secondly, a model is proposed to find potential correlations and internal insights from the processed data. The number of idle equipment is used as the response variable to establish a multiple generalized linear model (GLM) with negative binomial regression. Location, project stage, activity, and labour are considered as possible independent variables. Three hypotheses are proposed and the model that considers the interaction between the number of labour and the number of activities has the highest fitting accuracy. Model validation shows that the project manager can make timely scheduling adjustments with the corresponding predictive results. © 2023 Elsevier Ltd Generalized linear model-based data analytic approach for construction equipment management Construction management; Equipment management; Generalized linear model (GML); Site diary Project management; Regression analysis; Analytic approach; Construction management; Construction sites; Data analytics; Equipment management; Generalized linear model; Model-based OPC; Site diary; Utilization efficiency; Construction equipment",Risk management
831,An extension of the diffusion of innovation theory for business intelligence adoption: A maturity perspective on project management,"This study's objective is to analyze the factors that influence whether or not small and medium-sized enterprises (SMEs) use business intelligence. Based on an exhaustive assessment of the literature, the study offers a model dependent on the diffusion of innovation and augmented with factors expressing the idea of project management maturity (PMM). The research applied structural equation modelling (SEM) to examine data obtained from 112 Jordanian company workers. The findings showed that the adoption of business intelligence has a positive and significant relationship to the complexity, compatibility, and relative advantage of business intelligence; the level of project management maturity has a significant effect on the level of relative advantage, compatibility, and complexity; and the level of project management maturity is significantly associated with the change management and knowledge sharing practices in SMEs. However, we contend that further study has to be carried out, particularly in the context of developing nations, in order to get a comprehensive understanding of how different SMEs may effectively deploy and make use of business intelligence. © 2023 Growing Science Ltd. All rights reserved. An extension of the diffusion of innovation theory for business intelligence adoption: A maturity perspective on project management Business intelligence; Developing country; Diffusion of innovation; Project management; SMEs ",Value management
832,How Learning Analytics Can Help Orchestration of Formative Assessment? Data-driven recommendations for Technology-Enhanced Learning,"Formative assessment provides teachers with feedback to help them adapt their behavior. To manage the increasing number of students in higher education, technology-enhanced formative assessment tools can be used to maintain and hopefully improve teaching and learning quality thanks to the high amount of data that are generated by their usage. Based on literature and on authentic usages of the formative assessment system called Elaastic, we use learning analytics to provide evidence-based knowledge about formative assessment practices. An Elaastic sequence consists in asking learners to answer a choice question with a vote, a confidence degree and a rationale, then to confront their viewpoints through a peer grading activity, and finally to answer the same question a second time. Benefits of such sequences are measured through the increase of correct answers between the two votes. Our results suggest that: (A)&#x00A0;Benefits of sequences increase when close to 50&#x0025; of learners&#x0027; first votes are correct; (B)&#x00A0;Benefits of sequences increase when peers provide better grades to rationales related to correct answers than others; (C)&#x00A0;Sequences benefits do not significantly increase when learners who provided correct answers are more confident than learners who did not; (D)&#x00A0;Grades attributed by peers depend on such peer&#x0027;s confidence degree; (E) Self-grading is inaccurate in peer grading context; (F)&#x00A0;The amount of evaluations each learner performs makes no significant difference in terms of sequences benefits. These results lead to recommendations regarding formative assessment and to a new data-informed formative assessment process which is discussed at the end of the paper. Crown How Learning Analytics Can Help Orchestration of Formative Assessment? Data-driven recommendations for Technology-Enhanced Learning Behavioral sciences; Data mining; decision-making; Education; Electronic mail; formative assessment; learning analytics; peer assessment; peer instruction; Physics; Project management; Task analysis; technology-enhanced formative assessment Behavioral research; Data mining; E-learning; Electronic mail; Grading; Project management; Teaching; Behavioral science; Confidence degree; Decisions makings; Formative assessment; Learning analytic; Peer assessment; Peer instruction; Task analysis; Technology-enhanced formative assessment; Decision making",Risk management
834,"Business intelligence and analytic (BIA) stage-of-practice in micro-, small- and medium-sized enterprises (MSMEs)","Purpose: While business intelligence and analytic (BIA) systems have been developed by large corporations around the world, micro-, small- and medium-sized enterprises (MSMEs) have recently paid attention and deployed BIA adoption, particularly during the Covid-19 pandemic disruption. This study sheds light on how MSMEs adopt the BIA systems and then proposes a framework for the BIA adoption process in the context of MSMEs. Design/methodology/approach: The multiple case research design and interpretivism approach are employed for expanding the theoretical boundary of the strategic management fields in BIA adoption by MSMEs. In total, 35 semi-structured interviews were conducted with senior managers and owners involved in BIA adoption from 17 participating MSMEs. Findings: The research study identifies three BIA adoption stages with specific technical and managerial features in the path of BIA adoption in each stage, corresponding to the level of BIA maturity of MSMEs. The authors also highlight other factors that directly influence the successful adoption and transformation from each stage to another. Research limitations/implications: The research study identifies three BIA adoption stages with specific technical and managerial features in the path of BIA adoption at each stage that corresponds to the level of BIA maturity of MSMEs. Besides, this study also extends the current literature on BIA adoption in an organisation during the Covid-19 pandemic by identifying several contextual barriers that directly influence the BIA adoption. Practical implications: Research findings can help business leaders and owners of MSMEs to determine the BIA maturity of their organisation. Furthermore, the authors’ framework can also be used by consultancies and standard setters to develop detailed BIA adoption strategies and tactics that support MSMEs' digitalisation towards BIA adoption. Originality/value: The research study’s results highlight that contextual factors, leadership competencies, motivations and barriers for BIA adoption can also be used to help MSMEs' leaders and owners to trigger, advance or eliminate challenges for the adoption of BIA initiatives in MSMEs. © 2023, Emerald Publishing Limited. Business intelligence and analytic (BIA) stage-of-practice in micro-, small- and medium-sized enterprises (MSMEs) Adoption; Business analytics; Business intelligence; Micro; Small and medium enterprises (MSMEs) ",Value management
835,Evaluating Data Science Project Agility by Exploring Process Frameworks Used by Data Science Teams,"The lack of effective team process is often noted as one of the key drivers of data science project inefficiencies and failures. To help address this challenge, this research reports on semi-structured interviews, across 16 organizations, which explored data science agile framework usage. While 62% of the organizations reported using an agile framework, none actually followed the Scrum Guide (or any other published framework), but rather, each organization had defined their own process that incorporated one or more aspects of Scrum. The other organizations used a proprietary/ad-hoc approach, often based on a proprietary data science life cycle. In short, while many data science teams are trying to be agile, they are adapting existing frameworks to work within a data science context. Future research could explore how data science teams can best achieve agility, perhaps via new agile frameworks that address the unique data science project management challenges. © 2023 IEEE Computer Society. All rights reserved. Evaluating Data Science Project Agility by Exploring Process Frameworks Used by Data Science Teams Agile; Data Science; Team Process Data Science; Life cycle; Project management; Ad hoc approach; Agile; Management challenges; Process framework; Research reports; Science projects; Semi structured interviews; Team process; Human resource management",Value management
838,"Integrating machine learning and network analytics to model project cost, time and quality performance","This study aims to connect project management, network science and machine learning in an accessible overview applied to a real original dataset. Based on an initial literature review of applicable project performance measures and attributes, relevant project data were collected through an online survey. The information was split into three categories, including the basic project measures (five attributes), project stakeholder network measures (seven attributes), and project complexity measures (seven attributes). In total, 70 responses were collected, and five machine learning approaches (i.e. support vector machine, logistic regression, k-nearest neighbour, random forest and extreme gradient boosting) were applied to model the relationships between project attributes, networks and the Iron Triangle of project cost, time and quality. The results confirm the expected trends affecting project performance and provide an example for the discussion of the applicability of integrated machine learning and network analytics approaches to modelling project performance. The article demonstrates in an accessible way a real case of integration of machine learning, network science and project management and suggests avenues for further research and applications in practice. © 2023 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group. Integrating machine learning and network analytics to model project cost, time and quality performance cost performance; Machine learning; project complexity; stakeholder networks; time performance ",Monitoring and control
839,Big-Data Driven Framework to Estimate Vehicle Volume Based on Mobile Device Location Data,"Vehicle volume serves as a critical metric and the fundamental basis for traffic signal control, transportation project prioritization, road maintenance planning, and more. Traditional methods of quantifying vehicle volume rely on manual counting, video cameras, and loop detectors at a limited number of locations. These efforts require significant labor and cost for expansions. Researchers and private sector companies have also explored alternative solutions, such as probe vehicle data, although this still suffers from a low penetration rate. In recent years, along with the technological advancement in mobile sensors and mobile networks, the quantity of mobile device location data (MDLD) has been growing dramatically in spatiotemporal coverage of the population and its mobility. This paper presents a big-data driven framework that can ingest terabytes of MDLD and estimate vehicle volume over a larger geographical area with a larger sample size. The proposed framework first employs a series of cloud-based computational algorithms to extract multimodal trajectories and trip rosters. A scalable map matching and routing algorithm is then applied to snap and route vehicle trajectories to the roadway network. The observed vehicle counts on each roadway segment are weighted and calibrated against ground truth control totals, that is, annual vehicle-miles traveled and annual average daily traffic. The proposed framework is implemented on the all-street network in the State of Maryland using MDLD for the entire year of 2019. The results demonstrate that our proposed framework produces reliable vehicle volume and also its transferability and generalization ability. © National Academy of Sciences: Transportation Research Board 2023. Big-Data Driven Framework to Estimate Vehicle Volume Based on Mobile Device Location Data big data analytics; cloud computing; location-based services; origin and destination data; vehicle volume Big data; Cloud analytics; Data Analytics; Location; Population statistics; Street traffic control; Telecommunication services; Traffic signals; Vehicles; Video cameras; Big data analytic; Cloud-computing; Critical metrics; Data analytics; Data driven; Location data; Location-based services; Origin and destination data; Origin and destinations; Vehicle volume; Location based services",Strategic alignment
840,A Smart Approach to In-House Analytics and Business Management 4.0,"This research discusses the use of a smart in-house analytics approach as part of accelerating digital transformation and achieving strategic business management goals. The paper aims to determine the main directions of strategic management in the digital economy and the content of a contemporary business process management system. The authors investigate business intelligence systems as tools for implementing smart business management 4.0 approaches to analyze and make real-time management decisions in an uncertain environment. The authors highlight the reasons for using business intelligence (BI) systems and the challenges that these systems address in terms of implementing a smart approach. The functionality of BI systems and the cycle of data analysis and control decisions in providing a complete and reliable analysis of a company’s business processes are explored. Considering the emergence of new areas of technology, the authors discuss the business and technology fundamentals of Business 4.0. The analysis shows the need for management transformation using the agile methodology within the framework of digitalization of business processes and innovation. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG. A Smart Approach to In-House Analytics and Business Management 4.0 Artificial Intelligence; Business 4.0; Business analytics; Business intelligence; Digitalization; Management; Smart approach ",Strategic alignment
841,HOW CAN MANAGEMENT ACCOUNTING CONTRIBUTE TO RISK MANAGEMENT STRATEGIES WITHIN ORGANIZATION CASE STUDY OF SMALL AND MEDIUM ENTERPRISES: EVIDENCE FROM EMERGING COUNTRIES; [COMO A CONTABILIDADE DE GESTÃO PODE CONTRIBUIR PARA AS ESTRATÉGIAS DE GESTÃO DE RISCO DENTRO DA ORGANIZAÇÃO ESTUDO DE CASO DE PEQUENAS E MÉDIAS EMPRESAS: EVIDÊNCIAS DE PAÍSES EMERGENTES]; [CÓMO PUEDE CONTRIBUIR LA CONTABILIDAD DE GESTIÓN A LAS ESTRATEGIAS DE GESTIÓN DE RIESGOS DENTRO DE LA ORGANIZACIÓN ESTUDIO DE CASO DE PEQUEÑAS Y MEDIANAS EMPRESAS: EVIDENCE FROM EMERGING COUNTRIES],"Purpose: Current study aimed at exploring the impact of management accounting in supporting risks management strategies within Small and Medium Enterprises in Jordan and GCC countries during the fiscal year 2021-2022. Theoretical framework: Competency of management accounting was chosen as the independent variable which included (Strategic Management, Professional Ethics and Values, Reporting and Control, Leadership, Technology and Analytics, Business Acumen and Operations), while the variable of risk management strategies was chosen to be the dependent variable. Design/methodology/approach: Through quantitative approach and distributing a questionnaire on (257) financial and accounting managers within Small and Medium Enterprises in Jordan and GCC countries. Findings: Based on screening the data through SPSS; results of study was able to identify and accept the main hypothesis which argued that ""Management accounting competency has a statistically positive influence on risk management strategies within Small and Medium Enterprises in Jordan and GCC countries "" with R= 0.877 and a variance of influence that reached 76.9%. As for sub-variables, it was seen through statistical processing that business acumen and operations and leadership were the most influential variables on risk management strategies with a high variance that indicated a statistical influence of 72% and 42.3% respectively. Research, Practical & Social implications: Study recommended developing and increasing awareness of the importance of management accounting in organizational risk management by promoting the comprehensive and integrated perspective and abandoning the individual perspective of management accounting in the organization; further recommendations were presented in the study. Originality/Value: These finding could be used as a reference for Small and Medium Enterprises in Jordan and GCC countries. © 2023 AOS-Estratagia and Inovacao. All Rights Reserved. HOW CAN MANAGEMENT ACCOUNTING CONTRIBUTE TO RISK MANAGEMENT STRATEGIES WITHIN ORGANIZATION CASE STUDY OF SMALL AND MEDIUM ENTERPRISES: EVIDENCE FROM EMERGING COUNTRIES; [COMO A CONTABILIDADE DE GESTÃO PODE CONTRIBUIR PARA AS ESTRATÉGIAS DE GESTÃO DE RISCO DENTRO DA ORGANIZAÇÃO ESTUDO DE CASO DE PEQUENAS E MÉDIAS EMPRESAS: EVIDÊNCIAS DE PAÍSES EMERGENTES]; [CÓMO PUEDE CONTRIBUIR LA CONTABILIDAD DE GESTIÓN A LAS ESTRATEGIAS DE GESTIÓN DE RIESGOS DENTRO DE LA ORGANIZACIÓN ESTUDIO DE CASO DE PEQUEÑAS Y MEDIANAS EMPRESAS: EVIDENCE FROM EMERGING COUNTRIES] Accounting; Emerging Countries; Management Accounting; Risk Management; SME ",Strategic alignment
842,Automated vision-based construction progress monitoring in built environment through digital twin,"Effective progress monitoring is ineviTable for completing the construction of building and infrastructure projects successfully. In this digital transformation era, with the data-centric management and control approach, the effectiveness of monitoring methods is expected to improve dramatically. ”Digital Twin,” which creates a bidirectional communication flow between a physical entity and its digital counterpart, is found to be a crucial enabling technology for information-aware decision-making systems in manufacturing and other automotive industries. Recognizing the benefits of this technology in production management in construction, researchers have proposed Digital Twin Construction (DTC). DTC leverages building information modeling technology and processes, lean construction practices, on-site digital data collection mechanisms, and Artificial Intelligence (AI) based data analytics for improving construction production planning and control processes. Progress monitoring, a key component in construction production planning and control, can significantly benefit from DTC. However, some knowledge gaps still need to be filled for the practical implementation of DTC for progress monitoring in the built environment domain. This research reviews the existing vision-based progress monitoring methods, studies the evolution of automated vision-based construction progress monitoring research, and highlights the methodological and technological knowledge gaps that must be addressed for DTC-based predictive progress monitoring. Subsequently, it proposes a framework for closed-loop construction control through DTC. Finally, the way forward for fully automated, real-time construction progress monitoring built upon the DTC concept is proposed. © 2023 Automated vision-based construction progress monitoring in built environment through digital twin  Automation; Automotive industry; Construction; Data Analytics; Decision making; Metadata; Process control; Project management; Building projects; Built environment; Construction of buildings; Construction progress; Data centric; Digital transformation; Infrastructure project; Knowledge gaps; Production planning and control; Vision based; Production control",Capacity management
843,Preparing Students for Construction Management Technology Curriculum,"Teaching technology in construction engineering and management curriculum has been a topic of education research for some time. The influx of Building Information Modeling has dominated the literature in recent years, while data analytics and visualization have increased importance in the field. Many programs continue to grapple with teaching technology alongside the fundamental concepts of the discipline. To address these challenges, the Construction Management Department at the University of Washington has conducted a holistic review of technology across the curriculum for our undergraduate program. In recent years, we developed a new prerequisite class to better prepare students for learning the technologies across estimating, scheduling, and building information modeling. In this paper, we present this holistic curriculum philosophy and design for technology in our construction management undergraduate program including the preparation class, and three subsequent lab classes in estimating, scheduling, and building information modeling. This paper includes a teaching and learning evaluation to assess the success of this curriculum design, the transfer of learning across the curriculum, and the gaps we need to address in terms of emerging technology trends in data analytics and project management. We will discuss the strategies of preparing students to engage with technology across an undergraduate curriculum and define technology readiness for CM programs. © American Society for Engineering Education, 2022. Preparing Students for Construction Management Technology Curriculum  Curricula; Data Analytics; Data visualization; Engineering education; Information theory; Learning systems; Scheduling; Building Information Modelling; Construction engineering; Construction management; Data analytics; Education research; Fundamental concepts; Management technologies; Teaching technologies; Technology curricula; Undergraduate projects; Students",Capacity management
844,Mining Health Informatics Job Advertisements: Insights for Higher Education Programs and Job Seekers,"This paper used web scraping and data mining to analyze 831 health informatics job advertisements on indeed.com. Results showed that 87% of jobs explicitly required a college degree in a related field, 41% of jobs preferred a graduate degree, while 29% preferred or required professional certification. The analysis showed that preferred skills were analytics problem solving, communication skills, oral communication, interpersonal skills, project management, statistics, and critical thinking. The analysis also showed that college degrees, certifications, and the above-mentioned skill set are in high demand for working in the field of health informatics, especially in states with large populations and strong economies. Our results inform curriculum development of health informatics programs in higher education, which helps map knowledge units across the curricula to bridge the skills gap and meet employers' expectations. At the same time, the results help job seekers familiarize themselves with what employers seek in a successful candidate. © 2023 IEEE Computer Society. All rights reserved. Mining Health Informatics Job Advertisements: Insights for Higher Education Programs and Job Seekers Employers; Health Informatics; Job Posts; Job Seekers; Text Mining Data mining; Medical informatics; Project management; Employer; Graduate degrees; Health informatics; Higher education programmes; Job post; Job seekers; Professional certifications; Text-mining; Web data; Web scrapings; Curricula",Stakeholder management
845,Introduction and Practice of Total System Engineering to University and College Education,"In recent years, with advancements in information science and technology such as AI and data science, there has been a severe shortage of IT engineers in the industries. In response, an increasing number of universities are establishing new courses in data and information science. Traditionally, engineering students at universities and colleges study basic, specialized, and experimental subjects from admission to graduation, following the curriculum map. From the curriculum maps of various universities, it can be seen that specialized subject credits have been increased gradually, while balancing the speed of credit acquisition in general subjects. However, newly established information science universities and faculties are designing curriculum maps that allow students to evenly study specialized subjects over four years, rather than following the annual step-by-step acquisition of specialized subjects. These curriculum maps also include subjects related to Total System Engineering and management, which are essential to information and data science. However, few universities teach the basics of project management and system engineering. In 2022, the author moved to the International College of the Thai-Nichi Institute of Technology and is working towards creating systematic content for project management and system engineering. This paper describes the processes and results of these efforts.  © 2023 IEEE. Introduction and Practice of Total System Engineering to University and College Education Curriculum Mapping; KOSEN; Project Management; System Engineering; TNIC Balancing; Curricula; Data Science; Engineering education; Information management; Students; College education; Curriculum mapping; Curriculum maps; Data and information; IT engineers; KOSEN; Science and Technology; TNIC; Universities and colleges; University education; Project management",Monitoring and control
847,Automation in Project Management 4.0 with Artificial Intelligence,"In today’s world, digital transformation is common and innovative in each factor of life. Now, most of the human works are being done by robots and machines in project management. Artificial intelligence has the potential to change the way of project management (PM) tasks which are currently performed and can be changed and controlled in the recent future. Companies are searching out diverse approaches to beautify the performance with taking care of several options like reliability, accountability, etc., all simultaneously. Industries have initialized construction of AI embedded tools to assist the team to control their challenge(s). This study explores how our existing PM profession can lead to change the strategy by using AI inputs and how AI helps in enhancing project management and transforming it as Project Management 4.0. It enables to automate various time-taking venture of control that is crucial for success. AI also helps in evolving task automation to efficient analytics of the project, searching for resources to help, and building efficient strategies. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. Automation in Project Management 4.0 with Artificial Intelligence Artificial intelligence; Business intelligence; Product management; Project management Artificial intelligence; Business-intelligence; Digital transformation; Efficient strategy; Embedded tools; Management tasks; Performance; Product management; Task automation; Project management",Monitoring and control
848,"7th International Conference on Information Technology in Disaster Risk Reduction, ITDRR 2022","The proceedings contain 23 papers. The special focus in this conference is on Information Technology in Disaster Risk Reduction. The topics include: InCReASE: A Dynamic Framework Towards Enhancing Situational Awareness in Cyber Incident Response; Privacy by Design in CBRN Technologies Targeted to Vulnerable Groups: The Case of PROACTIVE; Transformation of an Esvecees (SVCS) Value to Spherical Coordinates as the Result of the Earthquake Forecasting Using SLHGN; Data Analytics of Climate Using the PCA-VARI Model Case Study in West Java, Indonesia; location Mention Recognition from Japanese Disaster-Related Tweets; creation and Use of Virtual Simulations for Measuring Situation Awareness of Incident Commanders; learning Early Detection of Emergencies from Word Usage Patterns on Social Media; development and Evaluation of a Shelter Simulator Using Gamification; strategic Approach to Food System Resiliency from Community-Based Initiatives During the Covid-19 Pandemic; web-Based Tool to Facilitate Resilience-Related Information Management; enhancing Interoperability and Inferring Evacuation Priorities in Flood Disaster Response; situational Disabilities in Information Systems for Situational Awareness in Flood Situations in Nigeria; providing Situational Awareness to Emergency Responders Using Drones; Application of the Fuel Cell Vehicle to Support ICT in Emergency Response; COVID-19 Sāvdhān: Harnessing the Telecom Infrastructure for COVID-19 Management; collapsed Building Detection Using Multiple Object Tracking from Aerial Videos and Analysis of Effective Filming Techniques of Drones; Challenges and Implementation of CBRN Sensor Networks in Urban Areas; preface; the Strategic Management of Disaster Risk Mitigation; developing Information Systems for Collaborative Emergency Management: Requirements Analysis and Prototyping; Shortcomings of Netcentric Operations During the COVID-19 Pandemic. 7th International Conference on Information Technology in Disaster Risk Reduction, ITDRR 2022  ",Risk management
849,RESEARCH ETHICS IN APPLIED ECONOMICS,"Emphasizing the new challenges posed by the data science revolution, digital media, and changing norms, Research Ethics in Applied Economics examines the ethical issues faced by quantitative social scientists at each stage of the research process. The first section of the book considers project development, including issues of project management, selection bias in asking research questions, and political incentives in the development and funding of research ideas. The second section addresses data collection and analysis, discussing concerns about participant rights, data falsification, data management, specification search, p-hacking, and replicability. The final section focuses on sharing results with academic audiences and beyond, with an emphasis on self-plagiarism, social media, and the importance of achieving policy impact. The discussion and related recommendations highlight emergent issues in research ethics. Featuring perspectives from experienced researchers on how they address ethical issues, this book provides practical guidance to both students and experienced practitioners seeking to navigate ethical issues in their applied economics research. © 2024 Anna Josephson and Jeffrey D. Michler. RESEARCH ETHICS IN APPLIED ECONOMICS  ",Risk management
850,MANAGERIAL OPPORTUNITIES IN APPLICATION OF BUSINESS INTELLIGENCE IN CONSTRUCTION COMPANIES,"In construction projects, managers make multiple decisions every day. Most of these decisions are relatively unimportant; some of them are critical and could lead to the success or failure of a construction project. To ensure construction companies make effective managerial decisions, decision making requires performing an initial technical and economic analysis, comparing different decision-making solutions, using a planning system, and ensuring project implementation based on the provided plans. Therefore, the use of powerful systems such as business intelligence (BI), which play a central role in management and decision-making, is essential in project-based companies. The current study aims to determine and evaluate the main managerial opportunities in the application of BI in project-based construction companies using a descriptive survey approach. An empirical research questionnaire consisting of 60 factors and 7 categories was adopted. The questionnaire, after confirming its validity and reliability, was distributed to 100 experts engaged in 5 active project-based construction companies who were familiar with BI topics. To analyse the data, a one-sample t-test and the Friedman test were performed using the SPSS software. The findings indicated that the importance of the identified opportunities for the use of BI in project-based construction companies is above average and that, in the case of using BI in such companies, these opportunities can be used to improve project performance. The results of the current study can help managers and other stakeholders as an effective decision-making tool to better implement BI in project-based companies. © 2023 The Author(s). MANAGERIAL OPPORTUNITIES IN APPLICATION OF BUSINESS INTELLIGENCE IN CONSTRUCTION COMPANIES business intelligence; construction companies; construction project; project-based companies Construction industry; Economic analysis; Information analysis; Project management; Software testing; 'current; Business-intelligence; Construction companies; Construction project managers; Construction projects; Decisions makings; Managerial decision; Project-based; Project-based company; Technical analysis; Decision making",Value management
852,"Sustainable Development and the Digital Economy: Human-centricity, Sustainability and Resilience in Asia","The advancement of technology, such as data analytics and artificial intelligence (AI), has led to the birth of Industry 4.0, in which technology seems to be the centre of development. However, as the Covid-19 pandemic created havoc, the entire world production chain has beenseriously affected, highlighting that machines alone, although fully connected and automated, cannot function without people. This book addresses the pillars of moving towards Industry 5.0 for sustainable development, drawing on examples from Asia. As Asian nations are moving fast toward the digital economy, this edited collection offers new perspectives on understanding emerging business opportunities as well as the challenges faced. Chapters span the three pillars of Industry 5.0, human centricity, sustainability and resilience, and includes topics related to people management for creating wealth, technology advancements in supporting creativity, resilience and agility of organisations, as well as the important issue of sustainability in future industrial development. With rich, empirical studies from leading researchers, this book will be a reference for academics and scholars across business disciplines, including information, technology and innovation management, organisational and strategic management, as well as those interested in industrial development and sustainability. © 2024 selection and editorial matter, Mohammad Nabil Almunawar, Patricia Ordóñez de Pablos and Muhammad Anshari; individual chapters, the contributors. Sustainable Development and the Digital Economy: Human-centricity, Sustainability and Resilience in Asia  ",Strategic alignment
854,"""To thrive at an undergraduate institution, one must love teaching and advising""","In her interview, Jessica Karanian tells us about her goal of staying in academia and finding a balance between teaching and research. After holding a visiting assistant professor position, she now works at an undergraduate-focused university. This role requires an interest in teaching and advising. Research in this setting is primarily done with undergraduate research assistants as volunteers or when enrolled in independent research project courses. Jessica provides some advice on how to prepare for this career path, particularly in engaging in teaching and advising when possible and in looking for professional development opportunities. She also reminds us that there are many jobs that involve stimulating, rewarding, and meaningful work-we just need to keep our minds open. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2022. ""To thrive at an undergraduate institution, one must love teaching and advising"" Academia; Administrative duties; Advising; Cognitive neuroscience; Data science; Liberal arts; Mentorship; Programming; Project management; Skill development; Teaching; Teaching intensive; Technical skills; Undergraduate research; Undergraduates; Visiting assistant professor ",Strategic alignment
855,Teaching Simulations Supported by Artificial Intelligence in the Real World,"Video conferencing has enabled synchronous communication in a classroom and created multi-sensory content to stimulate learners. Artificial intelligence involves complex equations that are better taught using a constructive pedagogy where students experiment with alternative ways of solving the same problem. Multiple-choice questions have high reliability and can easily reveal student skill levels in a quick way. The Australian Computer Society accreditation exercise ensures that the content for each subject serves as a flexible template for teaching. The geographical extent of the country requires the presence of multiple subordinate campuses affiliated to a main campus. Following the concept of strands, it was also necessary to show continuity in learning and assessments between the first- and second-year subjects. Student feedback for subjects with artificial intelligence-based simulations showed that several students found it difficult to understand lectures and assignments. Hence, to measure student learning, we introduced a Kahoot quiz during the recess of each lecture that students could join through their mobile phones from different campuses. Software project management is challenging for students with vision or attention-related disorders. We taught them how to use charts to visually observe variables and narrow down possible relationships before performing in-depth analysis. One of the main purposes of education is employability. Hence, greater context to real world industry examples was introduced into lectures. © 2023 by the authors. Teaching Simulations Supported by Artificial Intelligence in the Real World data science; online learning; simulation ",Risk management
856,Use Cases for Software Development Analytics: A Case Study,"Context Software engineering activities provide practitioners with large volumes of data that software analytics tools can use for many purposes, including defect prediction and effort estimation. However, the adoption of such tools depends on the information they provide and the real needs of practitioners. While existing research has focused on what developers need, the needs of managers are not well understood. Aims This study provides an in-depth analysis of the information needs of software practitioners from one organization that performs research, development, and innovation projects with industry partners. Understanding these practitioners' needs enables the development of better analytics solutions to support managerial decision-making. Method We interviewed practitioners in leadership positions and analyzed the collected data using Grounded Theory coding techniques, i.e., open and selective coding. Results We identified 19 software analytics use cases and classified them into four dimensions: quality, people, project management, and knowledge management. We also elicited several indicators to meet the identified use cases and captured key aspects concerning the organization's analytics scenario. Conclusions Although our results are particularly relevant to organizations similar to the one in which we conducted the study, they aim to serve as input for implementing new analytics solutions by practitioners and researchers in general.  © 2022 ACM. Use Cases for Software Development Analytics: A Case Study indicators; information needs; software analytics; use cases Behavioral research; Decision making; Managers; Project management; Software design; Analytic solution; Analytic tools; Case-studies; Defect prediction; Effort Estimation; Engineering activities; Information need; Large volumes; Software analytic; Use case; Knowledge management",Strategic alignment
857,"12th International Symposium on Intelligent Manufacturing and Service Systems, IMSS 2023","The proceedings contain 76 papers. The special focus in this conference is on International Symposium on Intelligent Manufacturing and Service Systems. The topics include: Prediction of Employee Turnover in Organizations Using Machine Learning Algorithms: A Decision Making Perspective; Remaining Useful Life Prediction of Machinery Equipment via Deep Learning Approach Based on Separable CNN and Bi-LSTM; Internet of Medical Things (IoMT): An Overview and Applications; using Social Media Analytics for Extracting Fashion Trends of Preowned Fashion Clothes; an Ordered Flow Shop Scheduling Problem; fuzzy Logic Based Heating and Cooling Control in Buildings Using Intermittent Energy; generating Linguistic Advice for the Carbon Limit Adjustment Mechanism; Autonomous Mobile Robot Navigation Using Lower Resolution Grids and PID-Based Pure Pursuit Controller; societies Becoming the Same: Visual Representation of the Individual via the Faceapp: Application; a Digital Twin-Based Decision Support System for Dynamic Labor Planning; an Active Learning Approach Using Clustering-Based Initialization for Time Series Classification; Finger Movement Classification from EMG Signals Using Gaussian Mixture Model; calculation of Efficiency Rate of Lean Manufacturing Techniques in a Casting Factory with Fuzzy Logic Approach; simulated Annealing for the Traveling Purchaser Problem in Cold Chain Logistics; a Machine Vision Algorithm Approach for Angle Detection in Industrial Applications; integrated Infrastructure Investment Project Management System Development for Mega Projects Case Study of Türkiye; Municipal Solid Waste Management: A Case Study Utilizing DES and GIS; a Development of Imaging System for Thermal Isolation in the Electric Vehicle Battery Systems; resolving the Ergonomics Problem of the Tailgate Fixture on the Robotic Production Line. 12th International Symposium on Intelligent Manufacturing and Service Systems, IMSS 2023  ",Strategic alignment
858,Deep learning-based data analytics for safety in construction,"Deep learning has been acknowledged as being robust in managing and controlling the performance of construction safety. However, there is an absence of state-of-the-art review that examines its developments and applications from the perspective of data utilization. Our review aims to fill this void and addresses the following research question: what developments in deep learning for data mining have been made to manage safety in construction? We systematically review the extant literature of deep-learning-based data analytics for construction safety management, including: (1) image/video-based; (2) text-based; (3) non-visual sensor-based; and (4) multi-modal-based. The review revealed three challenges of existing research in the construction industry: (1) lack of high-quality database; (2) inadequate ability of deep learning models; and (3) limited application scenarios. Based on our observations for the prevailing literature and practice, we identify that future research on safety management is needed and focused on the: (1) development of dynamic multi-modal knowledge graph; and (2) knowledge graph-based decision-making for safety. The application of deep learning is an emerging line of inquiry in construction, and this study not only identifies new research opportunities to support safety management, but also facilitates practicing deep learning for construction projects. © 2021 Deep learning-based data analytics for safety in construction Computer vision; Deep learning; Knowledge graph; Natural language processing; Safety Accident prevention; Computer vision; Construction industry; Data Analytics; Data mining; Decision making; Deep learning; Graphic methods; Natural language processing systems; Project management; Construction safety; Data analytics; Data utilization; Deep learning; Development and applications; Knowledge graphs; Multi-modal; Performance; Safety management; State-of-the art reviews; Knowledge graph",Strategic alignment
859,Seven ways to make a data science project fail,"The rapid emergence of data science as a field has made it a rival or replacement for information science from an industry perspective. In particular, the “big data” meme in data science and a heavy reliance on “black box” technology emphasize the quantity of data used in a project and asks, “what data do we have” rather than “what data do we need to solve our business problems.” This perspective also undermines the perceived importance of domain expertise, user research, data semantics and provenance, and other considerations valued in information science. This article uses a composite (and somewhat caricatured) case study of a data science project and discusses seven ways in which it is destined to fail, and then explains how “good information science” would have prevented or ameliorated them. Data science and information science need to recognize that together they can accomplish more than they can accomplish separately. © 2023 The Author Seven ways to make a data science project fail Case study; Collaboration; Data science; ISchools; Multidisciplinarity; Project management Data Science; Semantics; Black-box technology; Business problems; Case-studies; Collaboration; Domain expertise; Ischool; Multidisciplinarity; Projects fail; Science projects; User research; Project management",Value management
860,Blockchain Based Software Engineering Requirements Analysis and Management,"Requirements analysis and engineering is a vital phase in any software project’s lifecycle, and the success or failure during this phase mainly determines the entire project’s outcome. Recently the challenges incredibly increased in the software industry either in the technology, project management, or requirements engineering, drive-by leveraging the diversity of the tremendous tools and techniques that aim to avoid failure in requirements engineering and analysis. However, the tools and techniques will not always tackle the most significant challenge: validate, align, and confirm the needs and outcomes accordingly to both parties, the customer and the vendor, ensuring the confirmation and validation process is trustworthy. A unique process that ensures the requirements reliability is by introducing a framework that will authenticate the requirements analytics and engineering by Ethereum blockchain technology smart contract for the customers and vendor to guarantee substantial agreement on all requirements aspects through the project’s lifecycle. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. Blockchain Based Software Engineering Requirements Analysis and Management Blockchain; Decentralized application; Ethereum; Smart contract ",Risk management
862,The 4th International Workshop on Talent and Management Computing (TMC'2023),"In today's competitive and fast-evolving business environment, it is a critical time for organizations to rethink how to deal with the talent and management related tasks in a quantitative manner. Indeed, thanks to the era of big data, the availability of large-scale talent data provides unparalleled opportunities for business leaders to understand the rules of talent and management, which in turn deliver intelligence for effective decision making and management for their organizations. In the past few years, talent and management computing have increasingly attracted attentions from KDD communities, and a number of research/applied data science efforts have been devoted. To this end, the purpose of this workshop, i.e., the 4th International Workshop on Talent and Management Computing (TMC'2023), is to bring together researchers and practitioners to discuss both the critical problems faced by talent and management related domains, and potential data-driven solutions by leveraging state-of-the-art data mining technologies.  © 2023 Owner/Author. The 4th International Workshop on Talent and Management Computing (TMC'2023) group based decision making; professional social networks; strategic management; talent behavior modeling Data mining; Information management; Social networking (online); Behaviour models; Business environments; Critical time; Decisions makings; Group based decision making; Group-based decision; International workshops; Professional social networks; Strategic management; Talent behavior modeling; Decision making",Value management
863,Industry Perception of the Knowledge and Skills Required to Implement Sensor Data Analytics in Construction,"Construction, one of the largest industries in the world, consistently underperforms and faces barriers in leveraging the full potential of applying analytics to sensor data due to a lack of a skilled workforce. The prospects for data-driven solutions to address emerging construction challenges and enhance performance across project life cycles are therefore constrained. Through mixed-method research utilizing a survey and focus group, this study investigates the knowledge and skills required for graduating construction engineering and management students to implement sensor data analytics in the construction sector. The findings revealed that sensor data analytics knowledge and skills are required to systemically process and analyze data from sensing technologies and present them in formats for effective decision-making. The presented key knowledge areas, specific skills, and their significance can aid the construction industry and academics to streamline professional development efforts to match the actual demands, allowing for more efficacy in workforce training. The future construction workforce is expected to gain a competitive edge with sensor data analytics knowledge and skills as the ubiquitous integration of sensing technologies continues to drive the tremendous growth of sensor data.  © 2023 American Society of Civil Engineers. Industry Perception of the Knowledge and Skills Required to Implement Sensor Data Analytics in Construction Construction education; Knowledge and skills; Sensing technologies; Sensor data analytics Construction industry; Decision making; Digital storage; Engineering education; Life cycle; Office buildings; Project management; Students; Construction education; Data analytics; Data driven; Industry Perceptions; Knowledge and skill; Large industries; Sensing technology; Sensor data analytic; Sensors data; Skilled workforces; Data Analytics",Strategic alignment
866,Standard terms as analytical variables for collective data sharing in construction management,"In order to facilitate data analysis techniques for solving construction management (CM) problems, especially in a collaborative manner, it is essential to define the data schemas of various construction information as the common variables. However, one of the critical barriers is that there is no standard set of variables addressed for collaborative research and practice. Therefore, defining standard variable terms is essential to share, utilize, and analyze construction data systematically and efficiently. In this context, the purpose of this paper is to propose a structured set of standard CM variable terms to enable the global collective analyses of CM data, not only for human managers but also for automated machine inferences. To address this issue, firstly, bibliographic data was extracted from Scopus, focusing on CM research, and the VOSviewer was used to analyze the bibliographic information. Proposed standard variable terms were then organized into a hierarchical structure with two levels, including eight variables in the first level and fifty-seven variables in the second level. Additionally, examples of utilizing standard variable terms were presented to the traditional analytical techniques, ontology, and artificial intelligence (AI) techniques to illustrate the viability of the proposed variable terms in CM. The proposed standard variable terms can be used as the basis for further detailed taxonomy toward collective advanced analytics implementation in CM. This approach will significantly enhance automated knowledge sharing between different applications and machines, resulting in global collective intelligence among unspecified CM professionals. © 2023 Elsevier B.V. Standard terms as analytical variables for collective data sharing in construction management Bibliometric analysis; Construction management; Data analysis; Data sharing; Standard variable terms Bibliographies; Data handling; Information analysis; Information management; Bibliometrics analysis; Collaborative practices; Collaborative research; Construction data; Construction information; Construction management; Data analysis techniques; Data Sharing; Management problems; Standard variable term; Project management",Strategic alignment
868,A Predictive Analytics Framework for Mobile Crane Configuration Selection in Heavy Industrial Construction Projects,"Predictive analytics have been used to improve efficiency and productivity in the construction industry by leveraging the insights from historical data with a variety of applications in project management. In the planning process of heavy industrial construction projects, mobile crane selection plays a critical role in the project’s success, and poor choice of mobile crane configurations can lead to unnecessary cost-overrun and delayed schedules. In this research, the authors propose a predictive analytics framework for crane configuration selection using combined heuristic search and artificial neural network (ANN) approaches for heavy industrial construction projects. The heuristic search allows the practitioners to select the crane configurations based on engineering rules, while the ANN model utilizes the historical project data to help select crane configurations. The K-fold cross-validation is conducted to validate the designed ANN model and improve the accuracy of predictions. The results from the cross-validation test set have shown 70% accuracy. © 2022 by the authors. Licensee MDPI, Basel, Switzerland. A Predictive Analytics Framework for Mobile Crane Configuration Selection in Heavy Industrial Construction Projects artificial neural net-work; heavy construction; heuristic search; mobile crane selection; predictive analytics ",Strategic alignment
870,Use of big data in strategic management as a new perspective,"In recent years, interest in big data has increased in the field of management. With digitalization, companies have started to receive data flow from many channels at the same time. Many sources such as the internet of things, connected sensors, cloud systems, social media, and daily operational records of companies have become an important source of big data for companies. In addition to encouraging companies to strengthen their technological infrastructure, the use of big data has also revealed important strategic and cultural adaptation problems. In this scope, the aim of the study is to examine the effect of big data analytics on the managerial capabilities and performance of companies. In this context, the studies in the literature on the subject have been examined. As a result of the literature review, issues related to big data that are thought to affect the managerial ability of the company have been discussed. © 2023, IGI Global. All rights reserved. Use of big data in strategic management as a new perspective  ",Strategic alignment
871,Implementing Remote-Sensing Methodologies for Construction Research: An Unoccupied Airborne System Perspective,"In the past four decades, remote-sensing data and methodologies have been increasingly used in many fields to improve the efficiency, accuracy, or safety of data collection, and construction management is no exception. In recent years, unoccupied airborne systems (UAS), commonly referred to as drones, have emerged as an important platform for the collection of remote-sensing data. When coupled with increasingly affordable sensors and automated image processing techniques, UAS have made the application of remote-sensing approaches to myriad construction and civil engineering problems practical and attractive to the point that UAS-based airborne imaging is becoming routine for many construction management tasks. Based on our review of literature, a comprehensive discussion on the use of UAS methodologies for collecting and analyzing data to assist with construction research is much needed. This paper was developed to discuss current best practices in UAS aerial imagery collection and processing for construction research, including defining key terms. In addition, this paper discusses a variety of methods for the analysis of UAS collected aerial imagery for the following tasks: preconstruction planning, material tracking, project progress tracking, safety, as-built documentation, and building/structure inspection. This paper also includes a perspective on the future of UAS for a variety of construction management tasks. This work contributes to the Data Management and Data Science Body of Knowledge by informing construction researchers of the state of the art of UAS data collection and analysis methodologies. This study also provides practitioners with a comprehensive guide to the use of UAS for onsite construction management tasks, which is an essential component of e-Construction.  © 2022 American Society of Civil Engineers. Implementing Remote-Sensing Methodologies for Construction Research: An Unoccupied Airborne System Perspective Construction research; Remote sensing; Unoccupied aircraft systems (UAS) Aerial photography; Antennas; Construction; Data acquisition; Information management; Project management; Safety engineering; Unmanned aerial vehicles (UAV); Aerial imagery; Airborne systems; Aircraft systems; Construction management; Construction research; Data collection; Management tasks; Remote sensing data; Remote-sensing; Unoccupied aircraft system; Remote sensing",Strategic alignment
872,Agile software development in construction projects - A report summarizing the DigitalTWIN research project; [Agile Softwareentwicklung in Bauprojekten – Ein Bericht aus dem Forschungsprojekt DigitalTWIN],"Agile software development in construction projects – a report summarizing the DigitalTWIN research project. Project management in the construction sector is characterized by manifold and numerous interdependencies, high project volumes with variable constraints and long project durations. It requires structured and defined planning principles as well as flexibility, as criteria, assessments, and opinions change or only become clear during the course of a project. Digital tools for project management are consequently an important focus in the DigitalTWIN research project. The implementation of agile software development in the project course of grid she will structures and cable net facades show the advantages of a close collaboration between project stakeholders and specialists in data science and software engineering. Real project progress can be evaluated faster and more precisely, process designs can be developed based on data, and proven workflows can be adjusted to specific situations without losing the relation to a growing database. The paper gives an introduction to management methods and shows how knowledge and recommended actions for construction projects can be obtained faster from relevant data using individual software development. The required cloud infrastructures, platform technologies and methods are developed step by step in demonstrators and provide an outlook how collaboration can be structured, fast, comprehensive, digital and trustworthy using distributed IT systems. © 2022, Ernst und Sohn. All rights reserved. Agile software development in construction projects - A report summarizing the DigitalTWIN research project; [Agile Softwareentwicklung in Bauprojekten – Ein Bericht aus dem Forschungsprojekt DigitalTWIN] application-oriented use of methods; Building Information Modeling (BIM); cable net facade; digitalisation; general; grid she will; procedures; processes; special structures Agile manufacturing systems; Architectural design; Cables; Construction industry; Curricula; Digital devices; Software design; Application-oriented; Application-oriented use of method; Building information modeling; Building Information Modelling; Cable-net facades; Digitalization; General; Grid shells; Procedure; Process; Special structure; Project management",Risk management
873,Construction Project Management with Digital Twin Information System,"The current stage in the development of the construction industry is its digitalization, which requires the transformation of processes and models based on the use of digital platforms and twins. Datasets in a construction project accumulates quickly and becomes difficult to store and process due to the large file sizes of CAD,GIS and BIM technologies. Managing such datasets is a complex problem, since the usefulness of such data lies in ensuring that it is available and used as needed by all participants in a construction project. The article proposes to develop an information system for creating digital twins of construction project management as an integrator of digital tools based on Big Data Analytics and BIM technology, which is already the standard for the digitalization of the construction industry. The study describes four types of digital twins and ten properties of Big Data required for construction project. Conceptual model of Big Data domains for construction project and BIM-model of Big Data exchange and information transfer for construction project digital twin are considered. The authors propose a framework for creating digital twins of construction project management that uses three main components: BIM-models, Big Data Analytics and Knowledge Base. The results of the study are structure of information system of creating digital twin of territory urban planning project for Design Company and example using a BIM-oriented software product. © 2022 IJETAE Publication House. All right reserved. Construction Project Management with Digital Twin Information System Big Data; BIM-model; construction projectmanagement; digital construction; digital twin ",Strategic alignment
877,A literature review on applications of Industry 4.0 in Project Management,"Industry 4.0 is a growing trend in the field of project management in the modern era. As a result, this study identifies emerging and growing trends in project management as they relate to Industry 4.0. Before now, no research has been conducted to examine existing themes and research trends in this expanding field. To fill these gaps, this paper seeks to comprehend current academic research relating to Industry 4.0 and smart manufacturing in project management. We conducted a literature search, content review, and exploratory study on the subject area, and we used Scopus as the main database to identify related articles. We used cluster analysis, network analysis, citation analysis, and co-citation analysis to identify the primary vehicle of publications, research centres, and researchers. The key finding of our research is that Industry 4.0 is attracting international researchers' attention; thus, we aim to identify the new and emerging research area in Industry 4.0 and improve its productivity. The term Industry 4.0 first appeared in the market in 2012. Following that, researchers have thoroughly investigated the implications of this newly emerging field and concluded that various technologies, such as the Internet of Things, big data analytics, and artificial intelligence, play an important role in transforming organisations by improving their productivity. Countries such as Australia, Brazil, Canada, China, and other European countries expressed interest in this field and quickly joined the collaboration. Industry 4.0 is now regarded as a popular research topic in the field of project management. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature. A literature review on applications of Industry 4.0 in Project Management Industry 4.0; Project Management; Project Management tools; Smart Factory; Smart Manufacturing ",Value management
879,Demand-driven business strategy: Digital transformation and business model innovation,"Demand-Driven Business Strategy explains the ways of transforming business models from supply driven to demand driven through digital technologies and big data analytics. The book covers important topics such as digital leadership, the role of artificial intelligence, and platform firms and their role in business model transformation. Students are walked through the nature of supply- and demand-driven models and how organizations transform from one to the other. Theoretical insights are combined with real-world application through global case studies and examples from Amazon, Google, Uber, Volvo and Picnic. Chapter objectives and summaries provide consistent structure and aid learning, whilst reflective questions encourage further thought and discussion. Comprehensive and practical, this is an essential text for advanced undergraduate and postgraduate students studying strategic management, marketing, business innovation, consumer behavior, digital transformation and entrepreneurship. © 2022 Cor Molenaar. All rights reserved. Demand-driven business strategy: Digital transformation and business model innovation  ",Strategic alignment
880,Successful Project Completion During the COVID-19 Pandemic - A Lesson Learnt,"COVID-19 pandemic has taught us how to continue with the day-to-day activities interacting and working from remote locations. In this paper, we have highlighted the positive approach necessary to complete a project with success under this constraint by interacting regularly with the relevant stakeholders keeping focus on the final project deliverables. The salient points with supporting references are chalked out which might be helpful for others to follow if faced with stressful situations that COVID-19 pandemic taught us. © 2022, Springer Nature Switzerland AG. Successful Project Completion During the COVID-19 Pandemic - A Lesson Learnt Business intelligence; Information technology; Project management; Self-reflection; Small and medium size enterprises (SMEs) Lesson learnt; Project completion; Remote location; Salient points; Self reflection; Small and medium size enterprise; Small and medium-size enterprise; Project management",Governance
881,Bridging the divide between data scientists and clinicians,"Collaboration between data scientists and domain experts is necessary for the success of healthcare machine learning projects. Our present concern is the relationship between data scientists and clinicians, which often faces tensions commonly encountered by multidisciplinary teams. It is important to be able to prevent these differences from creating divisions that can derail a project. In this paper, we focus on understanding the interplay between these roles and where conflict can arise due to communication issues, varying incentives, and differing perspectives. © 2022 The Authors Bridging the divide between data scientists and clinicians Artificial intelligence (AI); Data science; Healthcare; Machine learning (ML); Project management ",Strategic alignment
882,How do business students self-regulate their project management learning? A sequence mining study,"The relation between learning strategies and academic achievement has been proven to be strong in multiple studies. Still, the connection between micro-level SRL processes and the academic achievement of business students in learning project management remains unstudied. The current study aims to find how sequence mining can identify students using different learning tactics and strategies in terms of micro-level SRL processes. Our findings show that there are differences in the use of tactics and strategies between low and high performing students. Understanding the differences in how low and high performing students apply different micro-level SRL processes can help practitioners identify students in need of support for SRL. © 2022 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0). CEUR Workshop Proceedings (CEUR-WS.org) How do business students self-regulate their project management learning? A sequence mining study academic achievement; learning analytics; learning tactics; micro-level SRL processes; project management; Sequence mining Project management; 'current; Academic achievements; Learning analytic; Learning projects; Learning strategy; Learning tactic; Micro level; Micro-level SRL process; Sequence mining; Students",Strategic alignment
883,Project Artifacts for the Data Science Lifecycle: A Comprehensive Overview,"Through knowledge extraction from data with various methods, Data Science (DS) allows organizations to achieve improvements in performance. The execution of these projects is mainly supported by DS process models such as CRISP-DM. As a high percentage of DS undertakings are failing, revisions to current DS project management practices become necessary. Amongst others, ensuring traceability, reproducibility, and knowledge retention across the project present important success factors in DS projects. Some of the DS process models feature documentation artifacts for this purpose but not comprehensively for the complete DS lifecycle. Accordingly, in this research, existing documentation deliverables for DS are identified and examined by means of a literature review. Based on the established best practices from the process models, the contents of 18 derived project artifacts for DS documentation are synthesized for the DS lifecycle to improve DS project management. © 2022 IEEE. Project Artifacts for the Data Science Lifecycle: A Comprehensive Overview Data Science; Documentation; Process Models; Project Artifacts; Project Management Data Science; Life cycle; CRISP-DM; Current data; Documentation; Knowledge extraction; Performance; Process-models; Project artifacts; Project management practices; Reproducibilities; Science projects; Project management",Value management
884,On the Application of SCRUM in Data Science Projects,"The emerging discipline of Data Science poses several challenges for teams conducting projects in the field as notably, the majority of Data Science teams fail to deliver the expected outcomes. To improve the results, researchers tried to adapt agile project methodologies like Scrum for Data Science projects. Scrum in particular is often implemented due its success in software engineering. However, the basic Scrum framework has proven itself to be too strict for Data Science, due to frequent unpredictabilities of Data Science tasks. Consequently, adaptions were made to traditional Scrum to make it more suitable for the new challenges. This article discusses further adaptations and suggests that Scrum in itself is usable in Data Science, however, additional adaptations of the core concepts need to be envisioned. © 2022 IEEE. On the Application of SCRUM in Data Science Projects data science; project management; SCRUM Data Science; Software engineering; Science projects; SCRUM; Project management",Capacity management
888,Use of Artificial Intelligence Smart Tools in Projects,"In recent years, artificial intelligence (AI) has gotten a lot of press and attention. Learning and adapting through constant algorithmic interventions aid in the development of a firm grasp of the situation at hand, as well as the handling of more difficult challenges as they arise. Cognitive psychology and knowledge-based applications engineering are at the heart of AI. AI-assisted quicker computing approaches are being used to address complex difficulties. Simultaneously, new, and more complicated issues emerge, which are difficult for the human mind to understand and envision, and which are far from being solved. Dynamic developments in global technical, economic, political, and environmental settings provide insurmountable challenges to corporate executives and strategists. Projects and project management are used to carry out business plans and government welfare goals. Throughout the project's timeframe, project managers must cope with restrictions. Regardless of methodology, predictive or iterative, the existence of various stakeholders and managing their requirements creates a slew of issues for the project manager and team. This paper aims to explain the existing industrial procedures employed by project managers that utilize AI principles. The author's aim is to set out obstacles and realistic alternatives for next generation project managers to apply AI in managing stakeholder expectations, resolving dissent and conflicts, and assuring flawless project delivery. © 2022 IEEE. Use of Artificial Intelligence Smart Tools in Projects Artificial intelligence; Project management; Project management analytics; Stakeholder engagement; Stakeholder management Human resource management; Iterative methods; Knowledge based systems; Managers; Algorithmics; Application engineering; Cognitive psychology; Dynamic development; Human mind; Knowledge-based applications; Project management analytic; Project managers; Stakeholder engagement; Stakeholder management; Project management",Risk management
890,Strategic Management for HEIs based on Data Analytics: literature review Case study UISRAEL; [Gestión Estratégica para IES basado en Analítica de Datos: revisión de literatura Caso de estudio UISRAEL],"The purpose of this research is to know the current situation of strategic management in Higher Education Institutions, based on Data Analytics - Business Analytics, as a tool for new strategies in predictive models, since, just as companies now store massive data at Big Data level, where they focus on capturing and processing them, Higher Education Institutions do it in the same way, to make decisions. The strategic management in the Higher Education Institutions to be analyzed is based on the deployment model of the competitive strategy to be projected in the operation of the institutions, taking into account the axiomatic framework of strategic planning, a quality work culture and evidencing the results in key indicators. Therefore, a systematic review of the literature is carried out, called a secondary study and consists of five key steps: identification of questions for the review, identification of relevant works, evaluation of the quality of the studies, summary of the evidence, and finally, interpretation of the results. Additionally, one of the Ecuadorian Higher Education Institutions is shown as a case study. © 2022 IEEE Computer Society. All rights reserved. Strategic Management for HEIs based on Data Analytics: literature review Case study UISRAEL; [Gestión Estratégica para IES basado en Analítica de Datos: revisión de literatura Caso de estudio UISRAEL] Data analytics; Decision making; Hei; strategic management Data Analytics; Predictive analytics; Strategic planning; Business analytics; Case-studies; Current situation; Data analytics; Decisions makings; E-studio; Hei; Higher education institutions; Literature reviews; Strategic management; Decision making",Strategic alignment
891,"Computer vision applications in construction: Current state, opportunities & challenges","Thousands of images and videos are collected from construction projects during construction. These contain valuable data that, if harnessed efficiently, can help automate or at least reduce human effort in diverse construction management activities such as progress monitoring, safety management, quality control and productivity tracking. Extracting meaningful information from images requires the development of technology and algorithms that enable computers to understand digital images or videos, replicating the functionality of human visual systems. This is the goal of computer vision. This review aims at providing an updated and categorized overview of computer vision applications in construction by examining the recent developments in the field and identifying the opportunities and challenges that future research needs to address to fully leverage the potential benefits of Computer Vision. We restrict the focus to four areas that can benefit the most from computer vision - Safety Management, Progress Monitoring, Productivity Tracking and Quality Control. © 2021 Elsevier B.V. Computer vision applications in construction: Current state, opportunities & challenges Computer vision; Monitoring; Technology in construction; Visual data analytics Data Analytics; Productivity; Project management; Quality control; 'current; Computer vision applications; Construction management; Construction projects; Data analytics; Management activities; Safety management; Technology in construction; Visual data; Visual data analytic; Computer vision",Change Management
892,Developing Transversal Skills and Strengthening Collaborative Blended Learning Activities in Engineering Education: A Pilot Study,"There is an ongoing debate about the fact that educational models of the industrial age are no longer adequate. Future educational models need to provide students with the abilities to solve complex problems as well as to collaborate and interact to generate new meaning and knowledge. In order to do so, teamwork activities should be pedagogically strengthened and technologically augmented in science and engineering education. This paper presents a pilot study implementing a blended learning scenario in a bachelor course at Ecole polytechnique federale de Lausanne (EPFL). In this framework, digital tools are proposed to support collaboration. The tools considered are a learning experience platform, an integrated Tasks Management Application, as well as a collaborative reporting editor. This article also presents our findings on the usability and adoption of those tools, as well as preliminary results on their impact on engagement and teamwork. We also mention the effect of collaborative writing on contributions. Finally, we draw insightful lessons on engagement, the use of learning analytics, and the peer evaluation of teamwork.This paper contributes to science and engineering education by providing new insights on teamwork activities supported by digital tools in blended learning scenarios. © 2022 IEEE. Developing Transversal Skills and Strengthening Collaborative Blended Learning Activities in Engineering Education: A Pilot Study collaborative learning; learning analytics; learning experience platform; project management; science and engineering education; transversal skills Digital devices; Education computing; Engineering education; Blended learning; Collaborative learning; Educational modelling; Learning analytic; Learning experience platform; Learning experiences; Pilot studies; Science and engineering; Science and engineering education; Transversal skills; Project management",Strategic alignment
893,BUILDING A CONCEPTUAL FRAMEWORK FOR USING BIG DATA ANALYTICS IN THE BANKING SECTOR,"Big Data and Big Data Analytics (BDA) are becoming trending technologies of the future. This topic has garnered considerable interest from researchers and businesses. However, BDA research in the banking sector has proven to be extremely limited and mixed. Addressing the challenges of BDA application and laying the foundation for BDA to improve banking efficiency raises significant questions about strategic management in the banking sector. Through a systematic review of the literature and a case study in Hungarian banks, this study intends to address the major inconsistencies in existing ideas about BDA applications. This study also proposes a conceptual model to evaluate the impact of factors influencing the use of BDA in the banking sector and investigates whether BDA affects the performance of banks. Our study finds that the use of BDA in the banking sector has to be aligned with the creation of dynamic capabilities that positively and directly affect banking in terms of the market and operational performance. Meanwhile, the dynamic capabilities created by BDA usage have a moderating impact on bank performance through improved risk management performance. Furthermore, this research helps managers focus on key factors, namely technological infrastructures, Big Data skills, data quality, and top management support, to boost the efficiency of using BDA. © 2022 Mykolo Romerio Universitetas. All rights reserved. BUILDING A CONCEPTUAL FRAMEWORK FOR USING BIG DATA ANALYTICS IN THE BANKING SECTOR banking efficiency; Big Data analytics; risk management ",Strategic alignment
894,Issues and Challenges of the Data Analytics Development Project in the Center of Information System and Financial Technology,"The development of information technology encourages the government to digitize business processes. The digitization generates a large and varied volume from various data sources so that data analytics development projects are required to overcome this to support the organization's data-driven decision making. This study aims to identify problems and challenges in the data analytics development project at the Center for Information Systems and Financial Technology (Pusintek) then the data analytics can prepare a strategic plan to minimize the issues and challenges so that the data analytic is optimally successful. The identification of issues and challenges in the development of data analytics projects is carried out through interviews with employees who carry out data analytics development projects at Pusintek in 2021 and then analyzed using qualitative analysis methods. Result of this study, the most issues and challenges in the data analytics development project at Pusintek are the data information management and data analytics teams, followed by management, project management, and process. These issues and challenges should get more attention from the organization to be followed up immediately so that data analytics projects can be successful.  © 2022 IEEE. Issues and Challenges of the Data Analytics Development Project in the Center of Information System and Financial Technology challenge; data analytics; data analytics project; government; issue Decision making; Human resource management; Information management; Information systems; Information use; Project management; Business Process; Challenge; Data analytic project; Data analytics; Development programmes; Digitisation; Government; Issue; Issues and challenges; Data Analytics",Strategic alignment
897,Considerations in the Selection of a Forensic LIMS,"Laboratory Information Management System (LIMS) is the commercial name for software designed to manage laboratory operations, i.e., tracking laboratory samples and related data throughout their laboratory life cycle. This includes login (aka accessioning); assignment of analysis; analytical raw data generation and final data calculations; quality control sample analyses; sample storage; and eventual warehousing of all data for future reporting and data analytics. Nearly every forensic laboratory in operation today has some automated means for managing these data and reports produced by the laboratory. Some organizations use Excel® spreadsheets, relational database systems, systems developed in-house, customized solutions, and commercial off-the-shelf products, all of which can be sold by the vendor as an out-of-the-box product or can be highly customized to meet the needs of that specific client. Many organizations find selecting the right LIMS provider to be an arduous task. Further, the implementation and validation of the system beyond that selection process is even more complicated and time-consuming. It is not as simple as installing a new piece of equipment. Understanding the related processes and LIMS functions can alleviate much of the angst, uncertainty, and inefficiency that can otherwise accompany these implementations. This article will present many of the topics an organization should take into consideration as they begin a new LIMS project. © 2023 Elsevier Ltd. All rights reserved. Considerations in the Selection of a Forensic LIMS Analytics; Automation; Crime Scene; Data; Forensic; Forensics; Hosted; Integration; Interfaces; ISO Accreditation Standards 9001, 27001, 17025; Laboratory; Laboratory Information Management System; Licensing; LIMS; Migration; On-Premise; Project Management; Property; Software; Validation; Verification; Web-Based ",Strategic alignment
899,Public relations in the age of data: corporate perspectives on social media analytics (SMA),"Purpose: The aim of this study was to understand how public relations leaders view and use social media analytics (SMA) and the impact of SMA on the public relations function. Design/methodology/approach: The research involved in-depth interviews with chief communication officers (CCOs) from leading multinational corporate brands. Findings: The findings revealed that although CCOs perceive social media analytics as strategically important to the advancement of public relations, the use of social media data is slowed by challenges associated with building SMA capacity. Theoretical and practical implications: – The research extends public relations theory on public relations as a strategic management function and provides practical insights for building SMA capabilities. Originality/value: The study is among the first to provide empirical evidence of how companies are using social media analytics to enhance public relations efforts. © 2021, Emerald Publishing Limited. Public relations in the age of data: corporate perspectives on social media analytics (SMA) Big data; Corporate communication; Public relations; Social media analytics; Social media listening; Social media monitoring; Strategic communication management ",Strategic alignment
900,Critical Success Factors of Business Intelligence Systems Implementation,"Business intelligence (BI) systems enable rapid decision-making in an era of big data and constantly evolving market dynamics. Yet, the majority of BI implementation efforts fail to deliver productive outcomes. We use a technical, organizational, and process perspective to identify significant criteria that contribute to successful implementation. A taxonomy of perspectives, criteria, and factors is introduced. This work provides insights into developing a decision model to support successful BI implementation.  © 1973-2011 IEEE. Critical Success Factors of Business Intelligence Systems Implementation Business intelligence; decision making; innovation management; project management; user experience Decision making; Information analysis; Information management; Business intelligence systems; Business-intelligence; Data integrity; Decision modeling; Decisions makings; Market dynamics; Organisational; Stakeholder; Success factors; Systems implementation; Project management",Value management
904,"DTESI 2022 - Proceedings of the 7th International Conference on Digital Technologies in Education, Science and Industry","The proceedings contain 41 papers. The topics discussed include: virtual reality in digital health: a literature review; development of computer models of measuring devices for the study of radio engineering disciplines; the use of a digital double for effective control of an object with many destabilizing nonlinear feedbacks; face mask detector for Raspberry Pi based on computer vision and edge computing; movement coordination of swarm robotic systems; using project management tools in the process of modernizing the healthcare system of the Republic of Kazakhstan; adopting data mining and social media analytics to achieve customer satisfaction; adopting data mining and social media analytics to achieve customer satisfaction; and application of the algorithm for analyzing financial instruments based on correlation coefficient. DTESI 2022 - Proceedings of the 7th International Conference on Digital Technologies in Education, Science and Industry  ",Value management
906,Amalgamation of Business Intelligence with Corporate Strategic Management,"Strategic management is one of the most important aspects that lead to companies’ success. The process of building a complex strategy needs a lot of time and effort, especially with the increasing speed of changes in the markets and the speed of obtaining information. It was necessary to use new tools that help decision makers. Here the role of business intelligence has emerged, which provides all that is necessary for the decision makers to be in a state of readiness to build strategies or modify them based on real-time data. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG. Amalgamation of Business Intelligence with Corporate Strategic Management Business intelligence; Competitive advantage; Data integration; Data quality; ETL process; On-line analytical processing; Reporting system; Strategic management; SWOT analysis Competition; Data integration; Decision making; Information analysis; Information management; Metals; Business-intelligence; Competitive advantage; Data quality; Decision makers; ETL process; Management IS; On-line analytical processing; Reporting systems; Strategic management; SWOT analysis; Strategic planning",Strategic alignment
907,The Progress of Business Analytics and Knowledge Management for Enterprise Performance Using Artificial Intelligence and Man-Machine Coordination,"This study aims to explore the integration of human-computer interaction (HCI) technology and platform ecosystem in artificial intelligence (AI) environment, thus providing a practical basis for the intelligent development of strategic management of platform ecosystem. With clothing e-commerce as an example, first, the business model of brand clothing is simply analyzed. Then, the fashion knowledge management method is adopted to build the fashion data warehouse. The platform intelligent clothing ecosystem is innovatively put forward through the research of business analytics and management mode of clothing e-commerce industry. The optimized genetic algorithm is used to solve the objective function of the model, and a flexible production scheduling model with multiple constraints and maximum cost-saving is established. Finally, the questionnaire results of voice interaction users are analyzed by HCI customer trust model. © 2022 IGI Global. All rights reserved. The Progress of Business Analytics and Knowledge Management for Enterprise Performance Using Artificial Intelligence and Man-Machine Coordination Artificial Intelligence; Benefit; Business Analysis; Enterprise Performance; Human-Computer Interaction; Knowledge Management; Platform Ecosystem Artificial intelligence; Data warehouses; Ecosystems; Electronic commerce; Genetic algorithms; Knowledge management; Production control; Benefit; Business analysis; Business analytics; Business knowledge; E- commerces; Enterprise performance; Interaction platform; Interaction technology; Man machines; Platform ecosystems; Human computer interaction",Financial management
908,The Risk Management Process for Data Science: Gaps in Current Practices,"Data science projects have unique risks, such as potential bias in predictive models, that can negatively impact the organization deploying the models as well as the people using the deployed models. With the increasing use of data science across a range of domains, the need to understand and manage data science project risk is increasing. Hence, this research leverages qualitative research to help understand the current practices concerning the risk management processes organizations currently use to identify and mitigate data science project risk. Specifically, this research reports on 16 semi-structured interviews, which were conducted across a diverse set of public and private organizations. The interviews identified a gap in current risk management processes, in that most organizations do not fully understand, nor manage, data science project risk. Furthermore, this research notes the need for a risk management framework that specifically addresses data science project risks. © 2022 IEEE Computer Society. All rights reserved. The Risk Management Process for Data Science: Gaps in Current Practices  Data Science; Current practices; Predictive models; Process organization; Project risk; Public organizations; Qualitative research; Research reports; Risk management process; Science projects; Semi structured interviews; Risk management",Risk management
909,"A critical review of text-based research in construction: Data source, analysis method, and implications","The advancement of natural language processing and text mining techniques facilitate automatic non-trivial pattern extraction and knowledge discovery from text data. However, text-based research has received less attention compared to image- and sensor-based research in the construction industry. Hence, this paper performs a comprehensive review to understand the current state and future insights of text analytics focusing on the data source and analysis method. This study identifies various kinds of text data sources from project documents as well as open data in the websites. In addition, the review finds that the ontology- and rule-based approach has been dominant, at the same time, recent research has attempted to apply the state-of-the-art machine learning methods. It is envisioned that there are potential advancements in construction engineering and management based on the latest text analysis methods along with the enriched data by the digital transformation. © 2021 Elsevier B.V. A critical review of text-based research in construction: Data source, analysis method, and implications Construction; Data source; Natural language processing; Review; Text analysis method; Text mining; Text-based research; Unstructured text data Construction; Construction industry; Data mining; Learning algorithms; Learning systems; Metadata; Open Data; Project management; Reviews; Construction data; Critical review; Data-source; Language processing; Natural languages; Text data; Text-analysis methods; Text-based research; Text-mining; Unstructured text data; Natural language processing systems",Strategic alignment
911,A Natural Language Processing-Based Approach for Clustering Construction Projects,"Many construction project owners group their projects into different work types to facilitate effective project management decisions. This categorization significantly helps owner agencies narrow down and analyze the historical projects of a similar type to extract meaningful patterns that can support various project management decisions. However, many owners, particularly state highway agencies, typically rely on project engineers' subjective judgments to classify a new project or do not even have systematic project classification criteria. In addition, many projects are a mixture of work types with varying proportions. A systematic and objective process of classifying projects is desirable to generate a more accurate and less disputable categorization of projects, thereby improving project management decision-making. This study proposes a natural language processing-based model for grouping projects with similar work components and portions into the same group. For a specific project, work items' descriptions and cost composition are the key input variables of the model. Bid tabulation data from a highway agency were collected and used for model development and evaluation. © 2022 Construction Research Congress 2022: Computer Applications, Automation, and Data Analytics - Selected Papers from Construction Research Congress 2022. All rights reserved. A Natural Language Processing-Based Approach for Clustering Construction Projects  Data Analytics; Decision making; Natural language processing systems; Clusterings; Construction projects; Management decisions; New projects; Project classification; Project engineers; Project owners; State highway agencies; Subjective judgement; Work type; Project management",Strategic alignment
912,HRSE: Reframing the Human Resource Management with MBSE,"According to the ISO/IEC/IEEE 15288 standard, human resource management process is an essential part of organizational project-enabling processes, becoming more and more important during the process of science, technological and industrial innovation. However, many enterprises get lost when they confront overlapped complexity such as confusion of terminology systems, adaption of interwoven development pipeline system, and optimization of workforce topology. Therefore, this article presents a model-based approach for human resource management in enterprises, providing improved cognition, analytics and implementation. In particular, a comprehensive MBSE toolbox and a set of models based on KARMA language are constructed for effective human resource management. Moreover, within the specific domain of human resource management, a system of systems is built up through a strategic initiative, guiding tens of thousands of employees and dozens of organizations towards long-term goals of high-quality development. The case study and measurement results demonstrate that human resource systems engineering (HRSE) conforms to the equivalent applicability of MBSE methods. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. HRSE: Reframing the Human Resource Management with MBSE High-quality development; Human resource management; KARMA language; MBSE ISO Standards; Natural resources management; Project management; Resource allocation; High quality; High-quality development; Human resources management; ISO/IEC; KARMA language; MBSE; Organisational; Reframing; Resource management process; Resource system; Human resource management",Financial management
917,Driving Project Efficiency and Cost Control in Nigeria's Oil and Gas Industry,"Oil and gas companies continue to contend with numerous existential challenges that are exerting pressure on their activities. Some of the challenges includes the rise in alternative and renewable energy, persistent talent challenge, increase in supply chain complexity, price volatility and growing competitive pressure (EIA, 2020). The need to respond effectively to these challenges has made cost control a strategic imperative and competitive necessity. Cost control should be applied as a strategic lever for improving project efficiency, influencing growth and boosting profitability through conscious and proactive approaches, rather than using cost management as a defensive tool (Aguilar and Ittner, 2018). New innovations like digitization, advance analytics and artificial intelligence provide potential solutions capable of addressing these problems (Bohari, 2020). These innovations can easily be implemented and enable rapid transformation of the industry. Cost control and project management initiatives have been implemented across most of the oil companies to weather the impact of rising cost and the low oil price. It was realized that having the right system in place, deploying modern technology, effective data collection and analysis, process reengineering, and automation are imperatives for driving cost reduction, capital efficiency and delivering competitive projects (Aguilar and Ittner, 2018). For instance, the leadership of one of the oil companies mandated the application of cost control and effective project management to well projects cost (circa $300-500mln annually). A combination of programs, innovations and practices were implemented between 2019 and 2021 and the control process was strengthened which resulted in savings of $20mln+ per annum year-on-year on drilling and wells related cost. Additionally, cost overruns were reduced and cost recovery from the JV partners improved due to better cost transparency and efficient cost management. This paper aims at contributing to the quality of cost management decisions, and improved planning and performance by exploring the impacts of technological disruption on cost control and assessing how it can drive efficiency and improve performance in oil and gas companies in Nigeria through a review of existing literature on the subject. © 2022, Society of Petroleum Engineers. Driving Project Efficiency and Cost Control in Nigeria's Oil and Gas Industry  Cost benefit analysis; Cost effectiveness; Digital storage; Gas industry; Gasoline; Petroleum industry; Process control; Project management; Public utilities; Reengineering; Supply chains; Alternative energy; Cost controls; Cost management; Efficiency control; Nigeria; Oil and gas companies; Oil and Gas Industry; Oil companies; Project cost; Project efficiency; Cost reduction",Monitoring and control
918,Investigating the Role of Critical Success Factors in Achieving the Success of Agile Projects in the Gaza Strip,"Achieving project success is a critical element of project management, and identifying the critical success factors (CSFs) that contribute to it is imperative. Agile project management has gained significant attention due to its flexibility, adaptability, and iterative ap-proach, but achieving project success in agile projects remains a challenge. In order to identify the CSFs that have a significant impact on project outcomes, this study examines the role of CSFs in achieving project success in agile projects. A structured online ques-tionnaire was used to collect data from 109 project managers and team members working on agile proj-ects, which was then analyzed using various statisti-cal methods. According to the study's findings, seven factors significantly influence the success of projects: scope and cost management, leadership, agile analytics techniques, customer involvement, teamwork, planning and scheduling, and effective communica-tion, which account for 71.9% of the total variance explained by the CSFs components. The analysis of Pearson correlation coefficients between CSFs and agile project outcomes indicates a positive correlation between each CSF and the four project outcomes (timeliness, cost, quality, and customer satisfaction). The regression analysis includes two significant pre-dictors: scope and cost management, and planning and scheduling, explaining 67.7% of the variation in project outcomes. The findings provide valuable insights for project managers in the Gaza Strip to enhance project success with agile methods by focusing on CSFs. ACM CCS (2012) Classification: Software and its engineering → Software creation and management → Software development process management Software and its engineering → Software creation and management → Software development process management → Software development methods → Agile software development. © 2022, University of Zagreb Faculty of Electrical Engineering and Computing. All rights reserved. Investigating the Role of Critical Success Factors in Achieving the Success of Agile Projects in the Gaza Strip agile methodology; critical success factors; project outcomes; software project Correlation methods; Cost benefit analysis; Customer satisfaction; Human resource management; Iterative methods; Project management; Quality control; Software design; Agile Methodologies; Critical success factor; Gaza Strip; Management software; Project managers; Project outcomes; Project success; Scope management; Software project; Success factors; Regression analysis",Value management
919,Integrated knowledge visualization and the enterprise digital twin system for supporting strategic management decision,"Purpose: This paper proposes an integrated knowledge visualization and digital twin system for supporting strategic management decisions. The concepts and applications of strategic architecture have been illustrated with a concrete real-world case study and decision rules of using the strategic digital twin management decision system (SDMDS) as a more visualized, adaptive and effective model for decision-making. Design/methodology/approach: This paper integrates the concepts of mental and computer models and examines a real case's business operations by applying system dynamics modelling and digital technologies. The enterprise digital twin system with displaying real-world data and simulations for future scenarios demonstrates an improved process of strategic decision-making in the digital age. Findings: The findings reveal that data analytics and the visualized enterprise digital twin system offer better practices for strategic management decisions in the dynamic and constantly changing business world by providing a constant and frequent adjustment on every decision that affects how the business performs over both operational and strategic timescales. Originality/value: In the digital age and dynamic business environment, the proposed strategic architecture and managerial digital twin system converts the existing conceptual models into an advanced operational model. It can facilitate the development of knowledge visualization and become a more adaptive and effective model for supporting real-time management decision-making by dealing with the complicated dependence of constant flow of data input, output and the feedback loop across business units and boundaries. © 2021, Emerald Publishing Limited. Integrated knowledge visualization and the enterprise digital twin system for supporting strategic management decision Data analytics; Decision support; Digital transformation; Digital twin; Dynamic capability; Innovation; Knowledge visualization; Performance management; Strategy; System dynamics (SD) ",Strategic alignment
920,Improving Project Management Decisions With Big Data Analytics,"A relationship between project management and knowledge management was observed with a detailed level of analysis in this chapter, as analytics tools and methods were presented to define new perspectives for these dynamics. After a theoretical review that advanced the level reached by a previous paper on the same topic a new theoretical background was completely worked, resulting in a base where a deeper way of analysis allowed, at the end, to study practical cases of rich association for PM and KM in practical, ready to apply situations. As a trend for next competitive cycles, tools, methods, and techniques that focus knowledge production for decision making are to be increasingly defined and applied, on one hand enabling organizations to propose new competitive structures and positioning, and on the other hand, presenting a more aggressive, faster, and demanding competitive environment. © 2022 by IGI Global. All rights reserved. Improving Project Management Decisions With Big Data Analytics  ",Strategic alignment
921,Modern HR Analytics: Digital Opportunities in Assessing the Effectiveness of Personnel Management,"The issues of assessing business performance, including assessing the effectiveness of personnel management, were actualized in the 1980s with the introduction and active development of strategic management. In the same period, theoretical and methodological grounds for conducting complex multi-method assessments were formed, which served as a starting point in the formation of modern HR analytics. The use of HR analytics is seen as a new stage in assessing the effectiveness of personnel management, the need for which arose with the emergence of talent management and individualization of approaches to personnel management. The extreme demand for HR analytics in the practice of management activities has led to the need to understand the possibilities of its application from the standpoint of an interdisciplinary approach. The objective is identification of the leading areas of modern HR analytics, opportunities, limitations and priorities for the use of modern digital tools for assessing the effectiveness of personnel management. Research methods are general scientific theoretical research methods; comparative analysis; SEO analysis. The leading directions of modern HR analytics have been identified and their high importance in assessing the effectiveness of personnel management has been shown; the opportunities, limitations and priorities of using modern digital capabilities for assessing the effectiveness of personnel management have been identified; on the basis of comparative analysis and SEO analysis, the priority practices of modern HR analytics have been identified and the current directions of its development have been shown. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG. Modern HR Analytics: Digital Opportunities in Assessing the Effectiveness of Personnel Management Digital tools for assessing personnel management effectiveness; HR analytics; Leading areas of HR analytics; Personnel management effectiveness ",Risk management
926,Use of Big Data Analysis in Data Management Aspects,"The concept of big data is spread all over all the sectors of industries. As it is concerning about the most important concept, the data, it need to be managed properly and effectively. By managing the data effectively, it will improve the decision making quality for better business. With the geometrical growth of data, appropriate solutions need to be studied and designed appropriately to handle and extract valuables and knowledge from targeted datasets. The strategic management level needs easy access of valuable insights from such varying and rapidly changing data, which ranges from daily transactions on customer interactions and social network. These values can be extracted using different data mining methods like association rule mining and cluster analysis. This value is provided with big data analytics, which is represented as an application of advanced analytics techniques of big data. The end point use of such finding is supply chain management, risk management fields of different domains. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. Use of Big Data Analysis in Data Management Aspects Big data analysis; Experimental analysis; Fraud detection; Quality management; Risk management; Supply chain management Advanced Analytics; Cluster analysis; Data Analytics; Data mining; Decision making; Information management; Quality control; Risk assessment; Risk management; Supply chain management; Big data analyse; Customers interactions; Data mining methods; Decisions makings; Experimental analysis; Fraud detection; Management level; Risks management; Rule mining; Strategic management; Big data",Strategic alignment
928,A Capability-Based Method for Modeling Resilient Data Ecosystems,"Modern information systems rely on data analytics and use various data sources to steer information processing and process execution activities. Capabilitydriven development is a method for the design and delivery of this kind of information systems. This chapter elaborates a method extension for capabilitybased modeling of data ecosystems for the purpose of ensuring their resilience. The ecosystem perspective is adopted because there is a need to understand the interactions among the various parties involved in capability delivery. The ecosystem model allows to analyze the impact on reliability and other properties of data providers on capability delivery resilience. The meta-model is elaborated together with a set of rules for analyzing the ecosystem model. The model is perceived as a property graph, and the network theory is used for the analysis. The ADOxx meta-modeling platform is used to implement the modeling tool, which is integrated with a graph database, where the model analysis is performed. The method and the tool are demonstrated using an example of a winter road maintenance ecosystem. © Springer International Publishing AG 2018. A Capability-Based Method for Modeling Resilient Data Ecosystems Capability management; Ecosystem modeling; Property graph; Resilience ",Strategic alignment
930,"Determinants of the use of predictive models in the management of investment portfolios, on the example of KGHM Polska Miedz S.A.","The authors present the determinants of the use of predictive analysis to support decision-making processes in the area of investment project portfolio management. The requirements are analyzed on the example of investment project portfolios of a mining company. The research is complemented by the description of already conducted tests, where predictive models have been created to determine most effective algorithms and key project attributes allowing to predict possible budget deviations. The proposed requirements for the implementation of the predictive analysis can be applied in other organizations managing projects according to project management methodologies with access to structured project data. © 2022 The Authors. Published by Elsevier B.V. Determinants of the use of predictive models in the management of investment portfolios, on the example of KGHM Polska Miedz S.A. investment projects; predictive analytics; Project management Budget control; Decision making; Financial data processing; Information management; Investments; Predictive analytics; Decision-making process; Effective algorithms; Investment programmes; Investments portfolios; Managing programmes; Mining companies; Predictive models; Project management methodology; Project portfolio; Project portfolio management; Project management",Financial management
931,Smart University: Digital Development Projects Based on Big Data,"A Smart University (SmU) consists of a number of traditional and smart components. Undoubtedly, it has the distinctive properties of a Smart University, which are discussed in detail in V.L. Uskov and his colleagues’ works. A Smart University is constantly increasing the level of its smartness through the improvement, development and implementation of Smart Component (SmC) projects. In the process of Smart University development, big data has been accumulated. So, the management processes are characterized by the active use of analytics tools. It is primarily done by smart analytics for work with this data. A Smart University uses the tools of intelligent systems, artificial intelligence technologies, neural networks and other Industry 4.0 technologies. The study deals with the issues of improving the management of Smart University projects through the information infrastructure development of project management. It is shown that the process of making managerial decisions in a Smart University should be built on approaches and technologies based on big data. The article also represents the conceptual vision of a projects’ formation portfolio for the development of Smart Components in a Smart University. It is shown the algorithm for using digital analytics in making management decisions based on big data. The originality of the study is based on the development of approaches to assessing the level of big data use in project management. A set of qualitative indicators has also been developed. It characterizes the level of Big Data technologies use in a management system of Smart University projects. The proposed developments can be applied to manage the university digital transformation and the transition to the concept of smart education. Expert methods were used for modeling. The proposed tools have been tested in the activities of Russian universities during the pandemic. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. Smart University: Digital Development Projects Based on Big Data Big Data; Mathematical model; Project management; Smart Components; Smart University; University digital transformation Decision making; Information management; Intelligent systems; Metadata; Project management; Analytic tools; Artificial intelligence technologies; Development programmes; Digital transformation; Management process; Project-based; Property; Smart components; Smart universities; University digital transformation; Big data",Strategic alignment
932,Application of AHP method for project selection in the context of sustainable development,"Any activity of an industrial enterprise will have an impact on the economic, social and environmental sectors. At present, the main business goal is to generate and maximize profits. The 2030 Agenda for Sustainable Development put the focus on maximizing profit also how profit is generated and what impact have profit-generating activity on society and the environment. Every activity in an industrial enterprise is preceded by decision-making. The multi-criteria nature of decision-making in the context of sustainable development (and its economic, social and environmental criteria) is a prerequisite for the application of multi-criteria optimization methods. The basic precondition to apply exact methods in decision-making is the manager who has knowledge of the exact methods and interest in their application. Based on different theories and decision-making approaches that do not arise from an exact decision-making approach, the decision-makers are unable to make rational decisions. The results of several conducted surveys of managers' decision-making show that managers decide on complex issues based on subjective reasoning or intuition (Liebowitz, in: Forum: intuition-based decision-making: the other side of analytics. https://analytics-magazine.org/forum-intuition-based-decision-making-the-other-side-of-analytics, 2015). The authors of the article focused on the issue of decision-making in the selection of a production project based on the criteria of sustainable development. The aim of the paper is to present the application of exact method—Analytical Hierarchical Process in decision process on above mentioned issue. © 2020, Springer Science+Business Media, LLC, part of Springer Nature. Application of AHP method for project selection in the context of sustainable development AHP; Decision-making; Multicriteria optimization; Project; Sustainable development Managers; Multiobjective optimization; Planning; Profitability; Sustainable development; Analytical hierarchical process; Decision makers; Decision process; Industrial enterprise; Multi-criteria; Multicriteria optimization; Project selection; Social and environmental; Decision making",Value management
933,Business Intelligence Approach and Sentiment Analysis as a Management Strategy Applied to Study Customer Satisfaction in the Hospitality Sector,"With the emergence of the Web 2.0, it is possible with user-generated content (UGC) in social media to express opinions about a customer experience related to a service or product. These customer reviews are very useful to understand the customer experience associated with a stay in a hotel, considering the different dimensions associated with hotel service and accommodation. Business intelligence and analytics are increasingly being considered as a management strategy to examine customer satisfaction about a hotel product or service and its positioning when compared to its economic sector. However, UGC can be difficult to analyze and compare with other ratings, so to fill this gap, a business intelligence environment was considered along with a text mining approach in a holistic decision-support system to help decision-making in the strategic management of the hotel. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. Business Intelligence Approach and Sentiment Analysis as a Management Strategy Applied to Study Customer Satisfaction in the Hospitality Sector Business intelligence; Digital economy; Hospitality reputation; Information and communication technology; Sentiment analysis; Tourism Artificial intelligence; Customer satisfaction; Decision making; Decision support systems; Hotels; Sales; Business-intelligence; Customer experience; Customers' satisfaction; Digital economy; Hospitality reputation; Information and Communication Technologies; Management strategies; Sentiment analysis; User-generated; Web 2.0; Sentiment analysis",Strategic alignment
934,Thriving information system through business intelligence knowledge management excellence framework,"In the current digitalization dilemma of an organization, there is a need for the business intelligence and knowledge management element for enhancing a perspective of learning and strategic management. These elements will comprise a significant evolution of learning, insight gained, experiences and knowledge through compelling theoretical impact for practitioners, academicians, and scholars in the pertinent field of interest. This phenomenon occurs due to digitalization transformation towards industry revolution 5.0 and organizational excellence in the information system area. This research focuses on the characteristic of a comprehensive performance measure perspective in an organization that conceives information assessment and key challenges of Business Intelligence and Knowledge Management in perceiving a relevant organizational excellence framework. The dynamic research focusing on the decision-making process and leveraging better knowledge creation. The future of organization excellence seemed to be convergent in determining the holistic performance measure perspective and its factors towards industry revolution 5.0. The research ends up with a typical basic excellence framework that will mash up some characteristics in designing an organizational strategic performance framework. The output is a conceptual performance measure framework for a typical decision-making application for organizational strategic performance management dashboarding. © 2022 Institute of Advanced Engineering and Science. All rights reserved. Thriving information system through business intelligence knowledge management excellence framework Business intelligence; Decision support system; Information management; Information system; Knowledge management ",Strategic alignment
935,"16th International Scientific-practical Conference on Mathematical Modeling and Simulation of Systems, MODS 2021","The proceedings contain 35 papers. The special focus in this conference is on Mathematical Modeling and Simulation of Systems. The topics include: Cybersecurity User Requirements Analysis: The ECHO Approach; performance Model for Convolutional Neural Networks; issue of Airplane Modification Mass Variations Based on Their Structural Modelling; Decision Support with Bayesian Influence Network During UAV Flight Control; numerical Analysis of the Gas Turbine Rotor Blades Thermal State Using a Refined Mathematical Model; prognostic Model of a Photovoltaic Power Plant; optimal Control of Buried Point Sources in a Two-Dimensional Richards-Klute Equation; Pillars for Establishing a Durable and Future-Proof IT Architecture Maturing Along with the NSC: Approaches from Continuous Integration to Service Mesh; modeling of Project Portfolio Management Process in Banking; modeling and Analysis of the Main Statistical Parameters of the Testing System of Special Equipment; fuzzy Game-Theoretic Modeling of a Multi-Agent Cybersecurity Management System for an Agricultural Enterprise; dynamic Malware Detection Based on Embedded Models of Execution Signature Chain; using Image Segmentation Neural Network Model for Motion Representation in Sport Analytics; Architecture of Distributed Blockchain Based Intrusion Detecting System for SOHO Networks; Usage of WBAN Systems and IoT Solutions in a Medical Environment; neural Network Models Ensembles for Generalized Analysis of Audit Data Transformations; adaptive Selection of Turbo Code Parameters in Wireless Data Transmission Systems; agent Functionals in Monitoring Information Systems; hash Method for Information Stream’s Safety in Dynamic Cooperative Production System; mathematical Modeling of Heat Field in a Six-Strand Tundish During Filling; modeling of Technological Machines Spatial Rigidity by Subsystems; preface. 16th International Scientific-practical Conference on Mathematical Modeling and Simulation of Systems, MODS 2021  ",Monitoring and control
940,SPMM: A Model Taxonomy for Designing and Managing Quality System,"Software engineering is a discipline of Computer Science in which the new sub-areas are constantly added, especially in the area of quality, data management, and architectural design. Nowadays software development languages and processes are rapidly changing to deliver high-quality software products, i.e., usable systems, hybrid, and fulfill users' needs. This paper aims to identify and classify different process models proposed by the researchers based on characteristics of software quality, data management, and software integration and redesign. From a study of several models through a systematic mapping study, we identify different parameters and presented them in a traceability matrix. The parameters are classified into six areas. This paper provides an in- depth theoretical insight into the models and characteristics. A systematic mapping study was conducted through a literature review. The methodology used in this paper is both qualitative and quantitative. Initially, through a systematic mapping study, we study different models working on different parameters. And then proposed a model that can cover all the aspects of software implementation and management. We select ERP systems for it. Later we perform the GAP analysis and statistical evaluation of the model. It has been observed that all of the models are area specific either focused on quality parameters or management issues or architectural-based. The proposed model covers all aspects. The primary research shows that industrialists also need a better model for quality implementation. Our statistical analysis can serve as a decision-making tool for them to add to their decision-making processes. The other could use it to further enhance the framework for quality management. This model will enhance further in the future for better implementation.  © 2013 IEEE. SPMM: A Model Taxonomy for Designing and Managing Quality System data analytics; GAP analysis; measuring parameters; software development; Software maturity model; software project management; software quality Computer software selection and evaluation; Decision making; Information management; Mapping; Project management; Quality management; Software design; GAP analyse; Measuring parameters; Security; Software; Software maturity models; Software project management; Software Quality; Systematic; Systematic mapping studies; Taxonomies",Strategic alignment
942,Artificial Intelligence and Machine Learning Used as an Enabler for Dynamic Risk Management,"Applying big data, data science, business process automation (BPA) and domain expertise to operational and project risk in the upstream O&G space, will create a paradigm when applied to wellbore construction. A scalable, dynamic risk ecosystem enables seamless integration of risk into all aspects of the well construction process, a cornerstone of this approach is interoperability at a system level. A lot of risk management is subjective, risk registers and mitigations are generated by workshopping with SME's. Risk scoring is often performed in a similar manner. The financial sector now applies data science techniques, for example in the fields of auditing and compliance. The aim of this paper is to discuss how these new techniques are integrated into a well-established existing risk management processes. The starting point for digitizing the process is data. A 'Risk Ecosystem' was developed with a risk engine at its core linked to curated input data and providing outputs through dedicated GUI's and direct links into offline planning and real-time operational software. The risk management process has become dynamic and integrated through the value chain. A major change is the integration with the real-time decision making process. Risk assessments combine analysis by hazard, risk, probability and barriers. When applying risk dynamically to well construction operations the barriers can be data, analysis, procedural and/or physical. The interplay of these barriers is the mitigation. In the risk engine barriers can be managed by the probability of effectiveness and the uncertainty of the input and output data. Artificial intelligence (AI) and machine learning (ML) can now enable risk and mitigation identification, supporting probability and uncertainty to be analysed in real-time to provide dynamic solutions. Data connectivity enables dynamic risk management where the risk updates when new data is available or an analysis is improved. The probability and uncertainty of analysis supports risk ranking. Dynamic risk management in planning leads through to risk management during operations, as risk changes and updates the source data can be revisited efficiently as it is connected through hazard, risk or mitigation. AI is triggered, by a change in the data, and the improved risk information is distributed through a revised mitigation scenario. The integration of ML recommends mitigations trained to global experience and continues to improve as SME's qualify mitigations. Exploration of the interaction of data management, AI, ML and personnel for risk management is important, achieving the correct balance reduces risk, making risk management more efficient. We propose that the improvement in data science for hazard, risk and mitigation identification supports the drive towards a risk management standard of a minimum risk analysis for the industry. In time this will lead to operational efficiency and consistency of performance. Copyright © 2022, International Petroleum Technology Conference. Artificial Intelligence and Machine Learning Used as an Enabler for Dynamic Risk Management  Construction; Ecosystems; Engines; Gasoline; Hazards; Human resource management; Information management; Interoperability; Machine learning; Risk analysis; Risk assessment; Risk perception; Uncertainty analysis; Artificial intelligence learning; Dynamic risks; Hazard risks; Input datas; Machine-learning; Management IS; Real- time; Risk management process; Risks management; Uncertainty; Risk management",Risk management
943,Nine Questions to Evaluate a Data Science Team's Process: Exploring a Big Data Science Team Process Evaluation Framework Via a Delphi Study,"While the lack of an effective team process is often noted as one of the key drivers for data science project inefficiencies and failures, there has been minimal research on how to evaluate a data science team's process. Without an evaluation framework, it is difficult for data science teams to understand their team process strengths and weaknesses. To help address this challenge, this exploratory research, via a Delpha study, identified nine key questions a data science team could answer to help evaluate their process. In short, the study identified questions evaluating the team's communication (within the team and with stakeholders). The study also identified team process questions (e.g., the use of iterations, life cycles and a prioritization process for potential tasks). Future research could explore how data science teams can best improve their process by leveraging and refining these questions as well as defining an overall data science project management evaluation framework. © 2022 IEEE. Nine Questions to Evaluate a Data Science Team's Process: Exploring a Big Data Science Team Process Evaluation Framework Via a Delphi Study Data Science; Project Management; Team Process Data Science; Human resource management; Life cycle; Petroleum reservoir evaluation; DELPHI study; Evaluation framework; Exploratory research; Prioritization process; Process Evaluation; Science projects; Team communication; Team process; Project management",Stakeholder management
944,Bridging Equipment Reliability Data and Risk-Informed Decisions in a Plant Operation Context,"Industry equipment reliability and asset management programs are essential elements that help ensure the safe and economical operation of nuclear power plants. The effectiveness of these programs is addressed in several industry-developed and regulatory programs. The Risk-Informed Asset Management project is tasked to develop tools in support of the equipment reliability and asset management programs at nuclear power plants. These tools are designed to create a direct bridge between component health and lifecycle data and decision-making (e.g., maintenance scheduling and project prioritization). This article provides a guide for specific use cases that the Risk-Informed Asset Management project is targeting. We have grouped uses cases into three main areas. The first area focuses on the analysis of equipment reliability data with a particular emphasis on condition-based data, such as test and surveillance reports and component monitoring data. The second area focuses on the integration of equipment reliability into system-plant reliability models to determine system-plant health and identify components critical to maintaining an operational system. Lastly, the third area manages plant resources, such as maintenance activities and replacement scheduling using optimization methods. Here, the primary focus is on supporting typical system engineer decisions regarding maintenance activity scheduling and component aging management. This is performed in a risk-informed context where the term “risk” is broadly constructed to include both plant reliability and economics. This framework combines data analytics tools to analyze equipment reliability data with risk-informed methods designed to support system engineer decisions (e.g., maintenance and replacement schedules, optimal maintenance posture) in a customizable workflow. © 2022 Probabilistic Safety Assessment and Management, PSAM 2022. All rights reserved. Bridging Equipment Reliability Data and Risk-Informed Decisions in a Plant Operation Context  Accident prevention; Asset management; Bridges; Condition based maintenance; Condition monitoring; Data Analytics; Decision making; Life cycle; Nuclear energy; Nuclear fuels; Reliability analysis; Asset management projects; Equipment reliability; Informed decision; Maintenance activity; Plant operations; Plant reliability; Reliability data; Reliability management; Reliability risks; System engineers; Nuclear power plants",Risk management
945,Artificial Intelligence and Analytics for Better Decision-Making and Strategy Management,"Since last six decades, artificial intelligence (AI) has empowered supercomputing and big data analytics. Nowadays, AI identifies the new challenges which influence defense, information system, reasoning, relationship with diagnosis, decision-making, strategy management, etc. with the rapid growth of big data technologies. AI-enabled superfast data procession systems are expanding and transforming the business. On the other hand, the interaction and integration of artificial intelligence support the decision-making in general and particulars. To support the human decision, some advance research is continuously going on for decision-making in the era of big data. However, the theoretical and conceptual development of AI technology gives more accurate management decision for the betterment of market. There is no doubt at all that the new AI faces the usual obstacles to progress unfamiliar and unauthentic technologies. Thus, the AI requires more investigation of various obstacles to give better results in decision-making and strategy management. This chapter delineates the basic of AI with its solicitation in healthcare, education, agriculture, transportation, smart city projects, manufacturing industries, and retail. It also includes the challenges of AI interaction with the management and organization for decision-making. Additionally, this chapter presents various approaches of decision-making and strategic management in business using AI such as marketing decision-making, customer relationship management (CRM), recommendation system, expert system, social computing, etc. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG. Artificial Intelligence and Analytics for Better Decision-Making and Strategy Management Artificial intelligence; Data analytics; Strategy management Big data; Commerce; Decision making; Expert systems; Information management; Metadata; Public relations; Data analytics; Data technologies; Decision making managements; Decision strategy; Decision-making strategies; Decisions makings; Defence information systems; Diagnosis decision; Rapid growth; Strategy management; Data Analytics",Value management
948,Corruption in Large Government Projects Not Only Inflates the Budget But Reduces Managerial Effectiveness,"Research on operations analytics has focused on the design and execution of processes and projects. For projects in particular, the emphasis is on factors such as plans, stakeholders, and uncertainty; and their effects on the outcomes of projects. Little attention is paid to a variable of considerable importance: ethical behaviour, particularly corruption in large projects. This matters because studies of corruption have shown that corruption inflates project budgets (by sometimes 30%) and thus imposes an unproductive tax. This chapter builds on an original dataset of 38 very large government projects in Nigeria to demonstrate that corruption not only inflates project budgets but also distorts decisions, rendering other project management success drivers less effective. The chapter demonstrates that corruption has negative interactions with the positive effect of project success drivers and illustrates on a detailed case example what these interactions look like in practice. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG. Corruption in Large Government Projects Not Only Inflates the Budget But Reduces Managerial Effectiveness Corruption; Government projects; Megaprojects; Project management ",Risk management
951,Technology in the Industrial Revolutions and Its Impact on the Key Performance Indicators of Organizations,"Context. The article considers topical issues of interconnection of life cycles of business, production, product models of enterprises and key indicators of their activities through the prism of the evolution of information systems, their impact on the pace of development of enterprises and the role that information systems play in industrial revolutions. Objective. The overall objective of the study is to assess the impact of information systems on the development of enterprises in the process of industrial revolutions by assessing the key performance indicators covering all perspectives (functional areas of enterprise development) of the balanced scorecard as one of the progressive systems of strategic management of organizations. Method. Formalization of key performance indicators through the mathematical apparatus was carried out using the methodology of strategic management of the enterprise - the balanced scorecard. Results. The study formalized the types of basic IS and software, their impact on the development processes of enterprises during the industrial revolutions by assessing certain key performance indicators in terms of the balanced scorecard methodology, which was first described using a mathematical apparatus. Conclusions. The main directions of transformation of modern production are defined by three global technological trends: network integration, intellectualization and flexible automation. The development and widespread implementation of IP has doubled the pace of technological revolutions and, in fact, determines all further enterprise development strategies within these technological trends. The next (fifth) industrial revolution will be defined exclusively by the development of data science and the transition to new system architectures of computing systems or their combinations, for example using neurosynaptic and quantum computers, which will allow using all the possibilities of industrial Internet of Things and Digital twin’s concepts in almost real time. © 2021 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0). Technology in the Industrial Revolutions and Its Impact on the Key Performance Indicators of Organizations balanced scorecard; CAD; CALS; ERP; industrial Internet of things; industrial revolutions; KPI; PLM Benchmarking; Computer aided design; Enterprise resource planning; Information management; Information systems; Information use; Life cycle; Real time systems; Strategic planning; Balanced scorecards; CALS; Enterprise development; Industrial internet of thing; Industrial revolutions; Key performance indicators; KPI; Mathematical apparatus; PLM; Strategic management; Internet of things",Financial management
952,Competency Model for Programming Courses in Information Technology Education (ITE) Programs from Industry Perspective: A Delphi Method,"This study aimed to develop a competency model for software development courses in ITE programs as perceived by the IT industry in the Philippines in the new normal. A review of previous studies revealed existing software engineering competency models being referenced by the IT industry along with the Commission on Higher Education (CHED) General Education Curriculum (GEC) being referenced by the academe. However, things drastically changed during this global pandemic which may have affected these current standards. Thus, this study aimed to determine the current competency needs in the IT industry in the new normal in relation to the programming courses being offered by the ITE programs in the Philippines; A mixed method three-round Delphi technique was used to solicit a unified expert opinion from the point of view of nine (9) IT industry experts in the Philippines. Twenty-three (23) technical skills and eleven (11) soft skills were extracted from online interviews using thematic analysis. Out of 23 technical skills, nineteen (19) have consensus. The findings produced a competency model for software development courses in the new normal consisting of must-have technical skills, game-changer technical skills, and must-have soft skills. The must-have technical skills include Web Development, Cyber Security, Cloud Computing, Agile Project Management, Mobile Development, and DevOps. The game changer technical skills include Data Science and Data Analytics, Artificial Intelligence and Machine Learning, the Internet of Things, and Research and Development. And the must-have soft skills include Communication skills, Results-Oriented, and Collaboration skills. © 2022 IEEE. Competency Model for Programming Courses in Information Technology Education (ITE) Programs from Industry Perspective: A Delphi Method competency model; Delphi method; Information Technology Education (ITE); programming courses; software development Artificial intelligence; Curricula; Cybersecurity; Engineering education; Project management; Software design; Competency model; Delphi method; Education programmes; Information technology education; IT industry; Philippines; Programming course; Soft skills; Technical skills; Data Analytics",Value management
953,ARTIFICIAL INTELLIGENCE AND PROJECT MANAGEMENT. PROSPECTIVE ANALYSIS IN DECISION MAKING; [LA INTELIGENCIA ARTIFICIAL Y LA DIRECCIÓN DE PROYECTOS. ANÁLISIS PROSPECTIVO EN LA TOMA DE DECISIONES],"Artificial intelligence (AI) is a combination of technologies, a driver for knowledge management and the great challenge facing society, in aspects such as information management. The White Paper on Artificial Intelligence, published by the European Commission in February 2020, also indicates the need to improve professional skills, better use of data, learning analytics and the application of predictive models for decision making. In this context, Project Management is a profession that is already affected by this revolution, if we take advantage of the benefits provided by AI systems, we can optimize management processes, improve the analysis of information generated in projects and optimize the decision making of the Project Manager. In this paper, a prospective analysis is made on these aspects, including the impact of AI on knowledge management and, especially, on decision making. © 2022 by the authors. Licensee AEIPRO, Spain. ARTIFICIAL INTELLIGENCE AND PROJECT MANAGEMENT. PROSPECTIVE ANALYSIS IN DECISION MAKING; [LA INTELIGENCIA ARTIFICIAL Y LA DIRECCIÓN DE PROYECTOS. ANÁLISIS PROSPECTIVO EN LA TOMA DE DECISIONES] Artificial intelligence; decision making; lessons learned; new trends ",Value management
955,Implementability of a Process Approach to Strategic Management in the Conditions of Digitalization,"The article describes a possible approach to solution of the problem of forming an information-logical structure of the strategic management of an organization based on the principles of the process approach. The methodological basis of the study is the concept that strategic tasks are best solved using framework templates for identifying, establishing, maintaining, controlling, and improving strategic management processes. As a result, the prerequisites for building simulation system-dynamic models based on business intelligence and big data analytics are formulated, which meets the demand for ways to transform complex data into sensitive, ready-to-use information and optimize strategic decision-making processes. It is proposed to reformat corporate reporting according to “the fivefold bottom line” principle, which, in addition to financial, social and environmental results, provides for the disclosure of data related to the loyalty of different categories of stakeholders and the organizational culture of the company. The article presents a model of the eco-environment and a because-and-effect diagram of the strategic management process in a modern organization. The scientific novelty of the results obtained lies in the development of the strategic management theory in the context of the requirements for its process maturity. It is concluded that the models and tools of the process approach can best solve the problem of improving strategic management in a modern organization. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG. Implementability of a Process Approach to Strategic Management in the Conditions of Digitalization Process approach; Strategic management; System dynamics ",Strategic alignment
956,Fatigue Life Prediction of Reinforced Concrete Using Artificial Neural Network,"Fatigue is a phenomenon of gradual, permanent internal changes in a material due to repeated or cyclic loading. The fatigue failure of structural elements may decreases the life of infrastructures, therefore the fatigue life of those structures should be considered. Highway and airfield pavements, bridge decks, offshore supporting structure, machinery foundation etc. are subjected to high cycle repeated loading. The randomness in parameters due to the heterogeneous nature of concrete due to fatigue loading leads to complexities in analysing fatigue failure of reinforced concrete. Probabilistic approach is more dependable for the prediction of fatigue life of reinforced concrete than deterministic approach as it can include variations and uncertainties. In recent years, artificial neural network emerged as a new promising computational tool which adopts a probabilistic approach for modelling complex relationships. The purpose of this study is to extract the data from fatigue tests conducted on reinforced concrete beam to create an artificial neural network predictive model. The developed model can able to predict the critical crack length of reinforced concrete members at which failure occurs by considering the fracture mechanics properties and material properties accountable for the softening behaviour of concrete as input. The developed ANN model and analytical model is capable of predicting the fatigue life of reinforced concrete with reasonable accuracy and in a faster approach. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG. Fatigue Life Prediction of Reinforced Concrete Using Artificial Neural Network Artificial neural network (ANN); Fatigue life; Probabilistic approach; Reinforced concrete Bridge decks; Concrete beams and girders; Concrete construction; Failure (mechanical); Fatigue of materials; Fatigue testing; Forecasting; Fracture mechanics; Machinery; Neural networks; Offshore oil well production; Predictive analytics; Project management; Safety engineering; Structural design; Complex relationships; Critical crack lengths; Deterministic approach; Fatigue life prediction; Prediction of fatigue lives; Probabilistic approaches; Reinforced concrete beams; Reinforced concrete member; Reinforced concrete",Capacity management
957,Mapping Relevant Petroleum Engineering Skillsets for the Transition to Renewable Energy and Sustainable Energy,"In the last century, the global energy mix has been primarily dominated by fossil fuels. However, commitments to tackling climate change have accelerated the conversations on transitioning to a lower-carbon future. This is beginning to influence governmental policies, investors' and shareholders interests, workforce preferences, academic curriculum adaptations, and student choices. Given the coordinated approach from government, industry, and academia necessary to facilitate this transition, it becomes increasingly essential to map the relevant skillsets for the energy transition. This paper aimed to identify the skill sets oil and gas students and professionals have that will be relevant in advancing a renewable and sustainable energy future. The study focused on students and professionals within Nigeria to address the conditions unique to the country and other fossil-fuel-dependent developing and emerging countries. First, a survey was conducted on students and young professionals to identify the knowledge gap in clean energy technologies and digitalization technologies. Subsequently, the skills oil and gas professionals have were mapped with the different renewable and sustainable energy technologies to identify overlaps between the oil and gas industry and low-carbon energy technologies. The results showed that the technical competencies of oil and gas industry professionals were most relevant in carbon storage, hydrogen storage, and geothermal energy. On the other hand, non-core oil and gas competencies such as project management, HSE (health, safety, and environment), and business development skills, cut across all low carbon technologies. These competencies appeared to be more relevant for renewable energy resources like solar, wind, and hydropower. Data science and digital skills cut across all the new energy technologies. The main deliverables of this study were a skillset map and a progressive curriculum that embraces digitalization, entrepreneurship, and clean energy technologies. This study provides a skillset map where students and professionals can identify their competency gaps for renewable and sustainable energy technologies. This will enable them (and organizations) to know how to develop upskilling and reskilling strategies and provide academia with insights on how to modify the current oil and gas engineering curriculum in universities. Although the paper drew on data-driven insights within Nigeria, the findings apply to schools and organizations globally. © 2022, Society of Petroleum Engineers. Mapping Relevant Petroleum Engineering Skillsets for the Transition to Renewable Energy and Sustainable Energy  Digital storage; Energy conservation; Engineering education; Fossil fuels; Gas industry; Gases; Hydrogen storage; Investments; Petroleum engineering; Project management; Students; Sustainable development; Technology transfer; Clean energy technology; Global energy; Nigeria; Oil and gas; Oil and Gas Industry; Renewable energies; Renewable energy technologies; Skill sets; Sustainable energy; Sustainable energy technology; Curricula",Financial management
959,Evaluating the critical success factors of data intelligence implementation in the public sector using analytical hierarchy process,"This study aims to fill a gap in the literature by identifying, defining, and evaluating the critical success factors that impact the implementation of data intelligence in the public sector. Fourteen factors were identified, and then divided into three categories: organization, process, and technology. We used the analytical hierarchy process, a quantitative method of decision-making, to evaluate the importance of the factors presented in the study using data collected from nine experts. The results showed that technology, as a category, is the most important. The analysis also indicated that project management, information systems & data, and data quality are the most important factors among all fourteen critical success factors. We discuss the implications of the analysis for practitioners and researchers in the paper. © 2021 Evaluating the critical success factors of data intelligence implementation in the public sector using analytical hierarchy process AHP; Data analytics; Data intelligence; Public sector; Success factors; Systems implementation Artificial intelligence; Decision making; Information management; Project management; Quality control; Analytical Hierarchy Process; Data analytics; Data intelligence; Methods of decision makings; Project management information system; Public sector; Quantitative method; Success factors; Systems implementation; Three categories; analytical hierarchy process; data quality; decision making; project management; public sector; Data Analytics",Value management
960,Empirically Derived Use Cases for Software Analytics,"[Background] Software engineering activities provide large volumes of data that software analytics tools can use to support decision-making. However, adopting such tools depends on the usefulness of the information provided regarding the needs of practitioners. While the needs of developers have been well-researched, the needs of managers are not getting as much attention. [Aims] This study provides an in-depth analysis of the needs of software practitioners involved in managerial decision-making from one organization that performs research, development, and innovation projects with industry partners. [Method] We identified and represented such needs as use cases by interviewing people in leadership positions and analyzing the collected data using Grounded Theory coding techniques, i.e., open and selective coding. [Results] Our analysis resulted in 19 software analytics use cases which we classified into four dimensions: quality, people, project management, and knowledge management. The use cases in the quality and project management dimensions were the most mentioned ones. [Conclusions] Although our results are particularly relevant to organizations similar to the one described herein, they aim to serve as input for implementing new analytics solutions by practitioners and researchers.  © 2022 University of Split, FESB. Empirically Derived Use Cases for Software Analytics Case Study; Grounded Theory; Software Analytics; Software Engineering; Software Measurement Behavioral research; Decision making; Knowledge management; Managers; Project management; Analytic tools; Case-studies; Decisions makings; Engineering activities; Grounded theory; In-depth analysis; Large volumes; Software analytic; Software Measurement; Software practitioners; Quality control",Strategic alignment
962,Strategic Design as an Effective Tool for Managing Digital Development of a Company,"This article is devoted to a new strategic management tool—strategic design. Modern conditions with rapidly changing technologies and consumer preferences, changing supply chains, emerging new markets, products, and new product safety requirements require the enterprise management strategy development to have adequate modern tools as well. The authors prove that such a tool can be a strategic design, which is based on design thinking and in-depth comprehensive analytics. It is analytics, as well as tracking the current and future customer needs, that allows the company to increase its competitiveness in both existing and new markets, including digital ones. This paper discusses strategic design stages preceding the development of a digital development strategy for a company. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. Strategic Design as an Effective Tool for Managing Digital Development of a Company  Commerce; Strategic planning; Condition; Consumers' preferences; Effective tool; Enterprise management; Management strategies; Market products; Product-safety; Safety requirements; Strategic design; Strategic management tools; Supply chains",Financial management
964,Alleviating Directional Well Trajectory Problems via Data Analytics,"A consistent leading because of drilling non-productive time (NPT) is the inability to steer the planned well trajectory trouble-free. Separate from downhole tool and drill bit failures, an unplanned trip to change the Bottom Hole Assembly (BHA) is required for up to one in every seven drilling runs. Root because analyses indicate potentially a quarter of all drilling NPT has poor planning or field execution as the failure mechanism, signifying scope for improvement. This paper aims to help guide optimal selection of RSS/ motor and bit, to ensure challenging wellpaths will be achieved with minimal NPT associated with BHA trips. Directional drilling analysis typically compares dogleg severity (DLS) for planned and actual trajectory. This metric is fundamentally direction-blind; absolute tortuosity is represented whether planned or unintentional. Without full context, DLS analysis can mask many steering issues. Typically, industry software does not measure how closely the steering inputs match their anticipated responses during a run. Strategic management and identification of zones with erratic toolface control, or strong formation/BHA tendencies is critical. The proposed 'derived steering' analytics method was applied to plan demanding 3D trajectories for an Extended Reach offshore campaign in Australia. Existing minimum curvature equations were repurposed to plot previous runs steering inputs and then infer efficiencies for each formation. Supervision was essential to counteract strong consistent right-hand BHA walk tendency for all the variety of wells studied. Multiple NPT events on previous campaigns had resulted from poor steering response in the shallow interbedded geology. In view of quantifiable field-specific risks, wellplans were refined to minimize tortuosity and maximize the design safety factor. The combination of highest anticipated dogleg response rotary steerable technology and bit selection was selected for steering assurance. Modelled tendencies per lithology were shared with wellsite supervisors, and recent drilling results essentially mimicked data analytics. Others operating in this field in the 21st century had drilled total meterage of 36,740m MD from 83 runs. Bit Gradings showed two 'Lost in Holes', one 'Drill String Failure', six trips for 'Downhole Tool Failures', seven for 'Penetration Rate', six to 'Change BHA', two for 'Hole Problems' and one for 'Downhole Motor Failure'. The current campaign's improved directional drilling offset analysis contributed towards significant avoidance of well delivery NPT to drill 28,061m in 34 runs. No trips were required to change BHA or bit because of inability to follow the trajectory, and field teams were able to pre-empt lithologyspecific challenges.  Copyright 2022, Society of Petroleum Engineers. Alleviating Directional Well Trajectory Problems via Data Analytics  Bottom-hole assembly; Drills; Failure (mechanical); Horizontal wells; Lithology; Offshore oil well production; Safety factor; Trajectories; Data analytics; Directional well; Dog leg severities; Down-hole tool; Drilling analysis; Failure mechanism; Non-productive time; Optimal selection; Root because analysis; Well trajectory; Infill drilling",Risk management
966,Wind Energy System: Data Analysis and Operational Management,"In the recent scenario, renewable energy system plays very vital role, and growth of such type of green industry is increased in very splendid way because green energy also decreases the pollution from the atmosphere. Wind energy system is one of the highly recognized renewable energy sources to generate the electricity in all over the world. This chapter introduces data analysis and operational management of wind energy system because in industry 4.0, data analytics and operation and strategic management are the very hot topic and are utilized for prediction analysis and total quality management of wind energy system. In this chapter, we analyzed frequency distribution of wind velocity of certain location and qualitative and quantitative analysis of wind energy system, and further result of given data is also assessed through R-language. After the completion of prefeasibility assessment of wind energy system, we explain optimum design of wind energy system through operational management process. A three-tier model of wind energy system for assessing the competitiveness of a location is presented, and location decision in wind energy distribution system must address the requirement of responsiveness. Wind energy component manufacturing company needs to spend a considerable amount of money to carry inventory. The cost of stores and the administrative cost related to maintaining inventory and accounting for it form a significant part of this cost. So arise such type of issues in this chapter to explain inventory and total quality management of wind energy system for increasing the system efficiency and reducing the loss. © The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG 2022. Wind Energy System: Data Analysis and Operational Management Decision science; R-language; Total quality management; Wind energy system ",Value management
968,"A Complete Overview of Analytics Techniques: Descriptive, Predictive, and Prescriptive","Analytics today is an area whose demand has reached a boom with every other organization using it to ponder upon major decisions. The data is growing exponentially day by day. The future of businesses is very much dependent on big data. This chapter reflects on the three types of analytics techniques used while discovering, interpreting, and communicating the meaningful patterns and trends in data, i.e., descriptive, predictive, and prescriptive analytics. Descriptive: Analytics technique that uses data mining to get insights on what has happened in the past.Predictive: Analytics technique that uses statistical methodologies and forecasting to know what is likely to happen in future.Prescriptive: Analytics technique that uses algorithms to know what should be done to affect what is likely to happen in future. Beginning with the brief idea of analytics, the chapter reflects on data mining along with the role of ML and AI in analytics. Techniques are compared stating the purposes they are used for. The big firms using them as a combination to grab every possible opportunity is discussed. These techniques being unique in their own implications have both the advantages and disadvantages. The chapter also discusses the various statistical methodologies, tools, and programming languages being used in these techniques. The overall thrust is to reflect on how organizations can adopt the new trend in order to completely change their operations and strategies to match up with the era where data is playing a huge part in taking informed decisions. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG. A Complete Overview of Analytics Techniques: Descriptive, Predictive, and Prescriptive Analytical strategic management; Analytical strategy; Analytics techniques; Application of tools; Business analytics; Decision science; Descriptive analytics; Predictive analytics; Prescriptive analytics; Tools Data mining; Statistical methods; Strategic planning; Analytic technique; Analytical strategic management; Analytical strategy; Application of tool; Business analytics; Decision science; Descriptive analytic; Prescriptive analytic; Statistical methodologies; Strategic management; Predictive analytics",Strategic alignment
970,OpenCEMS: An Open Solution for Easy Data Management in Connected Environments,"Automating the life cycle of data management projects is a challenging issue that has attracted the interest of both academic researchers and industrial companies. Therefore, several commercial and academic tools have been proposed to be used in a broad range of contexts. However, when dealing with data generated from connected environments (e.g., smart homes, cities), the data acquisition and management becomes more complex and heavily dependant on the environmental context thus rendering traditional tools less efficient and appropriate. In this respect, we introduce here OpenCEMS, an open platform for data management and analytics that can be used in various application domains and contexts, and more specifically in designing connected environments and analysing their generated/simulated data. Indeed, OpenCEMS provides a wide array of functionalities ranging from data pre-processing to post-processing allowing to represent and manage data from the different components of a connected environment (e.g., hardware, software) and to define the interactions between them. This allows to both simulate data with respect to different parameters as well as to contextualise collected data from the connected devices (i.e., consider environmental/sensing contexts). In this paper, we compare OpenCEMS with existing solutions and show how data is represented and processed. © 2022, Springer-Verlag GmbH Germany, part of Springer Nature. OpenCEMS: An Open Solution for Easy Data Management in Connected Environments Connected environments; Data analytics Automation; Data acquisition; Data handling; Environmental management; Information management; Intelligent buildings; Life cycle; Project management; Application contexts; Applications domains; Connected environment; Data analytics; Data preprocessing; Environmental contexts; Industrial companies; Management projects; Open platforms; Smart homes; Data Analytics",Strategic alignment
971,Can big data analytics help organisations achieve sustainable competitive advantage? A developmental enquiry,"Society is changing radically and fast, triggered by the digital revolution. Firms are thus looking for newer ways to attract, satisfy and retain those customers as their needs and aspirations change. Such newer ways constitute the firms’ quest to achieve competitive advantage. However, in this current digital era almost all the traditional “Porterian” competitive advantage barriers are difficult to sustain. Further, COVID has led to a rapid acceleration in the pace of digitalisation. Can the huge data generated in every digitally enabled entity of today be judiciously combined with other firm resources? Can this big data be transformed into meaningful knowledge which organisations can leverage to sustain and grow? Can big data provide a sustainable competitive advantage? In this conceptual paper, I use a knowledge-based approach analysed through resource-based view from the strategic management domain to search for answers to these questions. I introduce a holistic framework that integrates the following: (i) firm knowledge, (ii) managerial capabilities and decision-making, (iii) sustainable competitive advantage, (iv) big data analytics. I call this framework the “The Perpetual Model of BDA as a Sustainable Competitive Advantage”. I also propose a unique Analytic Maturity Model, which I call the SAMDDC-DAMPPC Model, which can help identify and classify firms based on their analytic maturity. © 2021 Elsevier Ltd Can big data analytics help organisations achieve sustainable competitive advantage? A developmental enquiry Big data analytics; Digital age; Knowledge-based view; Maturity model; Resource-based view; Sustainable competitive advantage Big data; Customer satisfaction; Data Analytics; Decision making; Knowledge based systems; Knowledge management; Sustainable development; 'current; Competitive advantage; Digital age; Digital era; Digital revolution; Knowledge-based approach; Knowledge-based views; Maturity model; Resource-based view; Sustainable competitive advantages; competitiveness; data set; decision making; holistic approach; industrial development; industrial practice; knowledge based system; sustainability; Competition",Strategic alignment
973,Integrating a Project Risk Model into a BI Architecture,"In today’s unpredictable and disruptive business landscape organizations face challenges that severely threatens their existence. To efficiently respond such challenges organizations must craft strategies to become more data-informed, agile, adaptative, and flexible. Integrating dynamic data analytical models in organizational structures to collect, analyze and interpret business data, is critical to organizations because it enables them to make more data-informed decisions and reduce bias in decision-making. In this work is illustrated the integration of a heuristic project risk-model used to identify project critical success factors into a typical organizational business intelligence architecture. The proposed integration enables organizations to efficiently and in a timely manner identify project collaborative risks by addressing people, environment, and tools, and generate actionable project-related knowledge that helps organizations to efficiently respond business challenges and achieve sustainable competitive advantages. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG. Integrating a Project Risk Model into a BI Architecture Artificial intelligence; BI architecture; Digital transformation; Industry 4.0; Machine learning; Project risk management ",Strategic alignment
974,Railway supply chain excellence through the mediator role of business intelligence: knowledge management approach towards information system,"The features of a holistic view in an organization create the data value of the Business Intelligence (BI) and Knowledge Management (KM) in viewing the big picture of organizational performance diagnostics framework. This research focuses on the specific features of railway supply chain performance in viewing the decision-making process and creating better knowledge formation. The intention of the study is to structure supplier performance using BI-KM framework development to determine holistic perspective factors. The outcomes indicate that BI and KM significantly increased the railway supply chain and significantly increased the information system. This BI-KM framework relates the current analytic characteristics in designing the railway supply chain towards information system in determining the strategic theme of the decision-making process of the decision support system together with system features, characteristics of data, the content of the themes, and the effect of the decision-making process and for executive strategic performance diagnostics tool that provides effective strategic decision making in supply chain performance. The quantitative research method uses SmartPLS software version 3.2.8 for empirical analysis through distributing survey questionnaires to 320 railway suppliers in Malaysia. Using a model-driven development framework, to measure the implementation success of the decision support system, the study is conducted in the railway supplier focusing on strategic management that helps to make the decision and facilitate the organizational success. © 2022 Growing Science Ltd. All rights reserved. Railway supply chain excellence through the mediator role of business intelligence: knowledge management approach towards information system Business Intelligence; Information System; Knowledge Management; Railway Supply Chain; Supplier Performance ",Strategic alignment
975,Artificial Intelligent Technologies for the Construction Industry: How Are They Perceived and Utilized in Australia?,"Artificial intelligence (AI) is a powerful technology that can be utilized throughout a construction project lifecycle. Transition to incorporate AI technologies in the construction industry has been delayed due to the lack of know-how and research. There is also a knowledge gap regarding how the public perceives AI technologies, their areas of application, prospects, and constraints in the construction industry. This study aims to explore AI technology adoption prospects and constraints in the Australian construction industry by analyzing social media data. This study adopted social media analytics, along with sentiment and content analyses of Twitter messages (n = 7906), as the methodological approach. The results revealed that: (a) robotics, internet-of-things, and machine learning are the most popular AI technologies in Australia; (b) Australian public sentiments toward AI are mostly positive, whilst some negative perceptions exist; (c) there are distinctive views on the opportunities and constraints of AI among the Australian states/territories; (d) timesaving, innovation, and digitalization are the most common AI prospects; and (e) project risk, security of data, and lack of capabilities are the most common AI constraints. This study is the first to explore AI technology adoption prospects and constraints in the Australian construction industry by analyzing social media data. The findings inform the construction industry on public perceptions and prospects and constraints of AI adoption. In addition, it advocates the search for finding the most efficient means to utilize AI technologies. The study helps public perceptions and prospects and constraints of AI adoption to be factored in construction industry technology adoption. © 2022 by the authors. Licensee MDPI, Basel, Switzerland. Artificial Intelligent Technologies for the Construction Industry: How Are They Perceived and Utilized in Australia? Automation; Big data; Digital twin; Industry 4.0; Machine learning; Robotics; Social media analytics ",Value management
976,"18th EAI International Conference on Computer Science and Education in Computer Science, CSECS 2022","The proceedings contain 24 papers. The special focus in this conference is on Computer Science and Education in Computer Science. The topics include: Real-Time and Near-Real-Time Services in Distributed Environment for IoT – Edge – Cloud Computing Implementation in Agriculture and Well-Being; On a Class of Minihypers in the Geometries PG (r, q); Integer Sequences in the HP Model of Dill; Challenges and Opportunities in ESG Investments; a Visual Tool to Study Sorting Algorithms and Their Complexity; Database Schemas Used in SQL University Courses – State of the Art; are Research Universities Meeting the Educational Challenge of the New Economy?; a Collaborative Learning Environment Using Blogs in a Learning Management System; integrating Agile Development Approaches and Business Analysis Foundational Knowledge into a Project Management Course; synopsis of Video Files Using Neural Networks: Component Analysis; ABA@BU Program Competitiveness and ABA Graduates Employability Support; plagiarism Abatement with Assignment Templates; reflections on the Applied Business Analytics Student Writing Project; digital Learning Technologies – Supporting Innovation and Scalability; Improving Student Employability with Python and SQL; Image Decluttering Techniques and Its Impact on YOLOv4 Performance; context-Switching Neural Node for Constrained-Space Hardware; Region-Based Multiple Object Tracking with LSTM Supported Trajectories; an Approach to Software Assets Reusing; Methodological Creation of HDRI from 360-Degree Camera - Case Study; bioinformatics: Model Selection and Scientific Visualization; Estimating COVID Case Fatality Rate in Bulgaria for 2020–2021. 18th EAI International Conference on Computer Science and Education in Computer Science, CSECS 2022  ",Monitoring and control
979,TMC 2021: 2021 International Workshop on Talent and Management Computing,"In today's competitive and fast-evolving business environment, it is a critical time for organizations to rethink how to deal with the talent and management related tasks in a quantitative manner. Indeed, thanks to the era of big data, the availability of large-scale talent data provides unparalleled opportunities for business leaders to understand the rules of talent and management, which in turn deliver intelligence for effective decision making and management for their organizations. In the past few years, talent and management computing have increasingly attracted attentions from KDD communities, and a number of research/applied data science efforts have been devoted. To this end, the purpose of this workshop, i.e., the 2021 International Workshop on Talent and Management Computing, is to bring together researchers and practitioners to discuss both the critical problems faced by talent and management related domains, and potential data-driven solutions by leveraging state-of-the-art data mining technologies.  © 2021 Owner/Author. TMC 2021: 2021 International Workshop on Talent and Management Computing group based decision making; professional social networks; strategic management; talent behavior modeling Data Science; Decision making; Business environments; Business leaders; Critical problems; Critical time; Data mining technology; International workshops; KDD community; State of the art; Data mining",Value management
982,Towards a Continuous Process Model for Data Science Projects,"Process models can assist in structuring and managing projects. For typical IT-projects, there are plenty process models which evolved over the last decades. Compared to them, data science process models focus on the specific challenges and aspects of data-based projects. They started evolving just before the turn of the millennium. This paper evaluates contents which could and should be included in data science process models to be useful for enterprises, especially when they are small and medium-sized or do not have their core competences in data science or IT. Regarding these contents, some existing models are analysed, providing an overview of their focus. Concluding, a vision for a continuous data science process model is given, which not only addresses the previously discussed contents, but also fulfils additional aspects to be useful in practise. © 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG. Towards a Continuous Process Model for Data Science Projects Artificial intelligence; Business analytics; Data analytics; Data mining; Data science; Economics; Industrial science; Methodology; Process model; Project management; Transformation process ",Value management
983,Concept for Enabling Customer-oriented Data Analytics via Integration of Production Process Improvement Methods and Data Science Methods,"Production companies are facing the major challenge of digitization. New technical developments enable new optimization potentials in production that can be leveraged in a highly competitive market. Data analysis is a good example of this, allowing large amounts of data to be used to support production in optimization projects. For data analysis the CRISP-DM approach has become generally accepted in science and practice. This process model offers a good support for data science projects with useful data analysis methods and tools. In practice, however, it can often be observed that aspects such as customer-oriented project definition and the integration of process knowledge in data science projects are difficult to achieve. This paper will present a solution for that challenge. The combination of process optimization methods from the Lean Management and Six Sigma toolbox as well as project management methods for each phase of the CRISP-DM approach support data science projects to customize the project definition and integrate process knowledge. © 2021 Elsevier B.V.. All rights reserved. Concept for Enabling Customer-oriented Data Analytics via Integration of Production Process Improvement Methods and Data Science Methods Data Analytics; Industry 4.0; Lean Management; Process Improvement Data integration; Industry 4.0; Information analysis; Lean production; Optimization; Process monitoring; Project management; Work simplification; CRISP-DM; Customers oriented; Data analytics; Lean management; Process Improvement; Process improvement methods; Process knowledge; Production process; Science methods; Science projects; Data Analytics",Strategic alignment
988,Business analytics in service operations—Lessons from healthcare operations,"We present an expanded framework for the use of business analytics in projects. To the commonly used descriptive, predictive, and prescriptive analytics, we add comparative analytics, wherein we compare the performance of systems under different interventions. This framework provides a conceptual roadmap for the implementation of business analytics projects. We then demonstrate this framework using recent operations research literature on analytics in healthcare, summarizing papers focusing on one of these aspects. Next, we discuss queue mining as an example of theory and practice illustrative of these aspects. We conclude there is room for further work by operations researchers and management scientists within business analytics projects generally and the healthcare industry more specifically. We argue future work should consider both theory and practice, especially within prescriptive analytics projects, where analysis through the lens of operations research and management science is imperative. We provide some thoughts on the current and future state of operations research and management science in business analytics. © 2021 Wiley Periodicals LLC. Business analytics in service operations—Lessons from healthcare operations business analytics; healthcare analytics; queue mining; service operations Health care; Operations research; Project management; Business analytics; Expanded frameworks; Health-care operations; Healthcare industry; Performance of systems; Service operations; Theory and practice; Through the lens; Advanced Analytics",Strategic alignment
989,Smart and Emerging Technologies: Shaping the Future of the Industry and Offsite Construction,"The interest in smart, emerging technologies and offsite construction methods continues to grow. While previous studies investigated different offsite construction aspects, there is still lack of research work that explored the current and future use of smart and emerging technologies, especially those used in offsite construction. This study addresses this knowledge gap through a multi-step research methodology. First, a list of 24 technologies was identified. Second, a survey was distributed to study the current use and the experts' future perceptions of the identified technologies in offsite construction. Third, descriptive statistics and computational clustering analysis were used to quantify and classify the current and expected future use of offsite construction technologies. The findings reflected that the industry will experience an increased reliance on all offsite construction technologies in the future. The outcomes also indicated that the top 10 technologies with the highest potential in the future include: (1) drones and remote monitoring; (2) smart sensors; (3) artificial intelligence, cognitive learning, and computer/machine vision; (4) extended reality (i.e., virtualization); (5) integrated real-time project management information systems; (6) wireless technology and nG networks; (7) big data, data analytics, and data ecosystem; (8) robotics; (9) internet of things (IoT); and (10) nD printing and additive manufacturing. This study adds to the body of knowledge by helping practitioners identify strategic technologies that will shape the future of the construction industry and offsite construction. © 2021 Computing in Civil Engineering 2021 - Selected Papers from the ASCE International Conference on Computing in Civil Engineering 2021. All rights reserved. Smart and Emerging Technologies: Shaping the Future of the Industry and Offsite Construction  Cognitive systems; Construction industry; Engineering education; Information management; Internet of things; Project management; Real time systems; 'current; Construction method; Construction technologies; Descriptive statistics; Emerging technologies; Knowledge gaps; Multisteps; Off-site construction; Research methodologies; Smart technology; Data Analytics",Financial management
990,Strategic management to model profitability of the primary dairy sector in Colombia,"In Colombia, the annual production of milk was 7,301 million liters in 2019. The dairy sector accounts for 1.6 percent of national GDP (gross domestic product) and 21.8 percent of agricultural GDP. However, the results for this sector have not been satisfactory as expected in the last decades as compared to the rest of Latin America. As we were able to identify, many problems have been reported in the primary productive sector such as: Lack of policies to improve productivity, inefficiency in production, inadequate technical production processes, huge variability in production prices, competitive disadvantages generated by the massive inflow of imported goods, among others, which contributes greatly to the reductions on dairy farms profitability. The latter implies the need for research on decision-making tools that reduce uncertainty in the dairy industry. The use of technical indexes aimed to control in evaluate dairy farms in terms of productivity and profitability, which can be useful to monitor and track changes and requirements for the Colombian dairy industry. In Colombia, there are no studies addressed at the determinants of profitability of the dairy sector. This has motivated the current work to study profitability using predictive analytics, through an integrated approach considering costs, productivity, economic and technical indexes. This project proposes a management model that supports decision-making on dairy farms by calculating profitability and generating alerts based on the sensibility of selected input variables. The model focuses on the determination of the because-effect relationships between the input and output variables (net annual income, profit margin, and return rates on the invested capital) to explain the determinant of profitability in the farm. First, we described the generalities of the primary link in the dairy sector in Colombia. Second, using a panel of experts we selected a set of variables related to feed, animals, human resources, fertilization, among others. We used principal component analysis (PCA) as a tool to reduce preselected inputs to those that explain better the variance and have the greatest impact on output variables. The results of this PCA worked as an initial model for multiple linear regression models as the primary methodology for establishing because-effect relationships between input and output variables (a model for each output variable) that can explain the profitability of the production in the farm. We used R Studio to operate both methodologies. Our final goal is to calculate the profitability considering our output variables. Furthermore, generate alerts on input variables based on scenario analysis to support decision-making in dairy farms. Our work team had access to an experimental farm of the Center of Agricultural Practices and Development of the Universidad de Antioquia located in the municipality of San Pedro de los Milagros, Antioquia, where we used productive records of this farm, developing indexes and generating new determinants of profitability if these were not available. We propose to validate this model on farms in other regions in Colombia in order to verify the capacity to calculate profitability in real management situations in the Colombian dairy productive sector. © IEOM Society International. Strategic management to model profitability of the primary dairy sector in Colombia Competitiveness; Dairy; Model; Productivity; Profitability ",Strategic alignment
992,"25th World Multi-Conference on Systemics, Cybernetics and Informatics, WMSCI 2021","The proceedings contain 90 papers. The topics discussed include: on the pitfalls of videoconferences for challenge-based face liveness detection; adopting agile practices: lessons learned transforming organizations that do not develop software; a novel IDS model oriented to drone nodes networks threated for DoS attacks; an optimal path planning approach for a two wheeled mobile robot using free segment and turning point algorithm; a cybersecurity data science demonstrator: machine learning in IoT network security; features of the case method application in the study of disciplines related to information technologies and IT project management; human intelligence (HI –Nous) and artificial intelligence (AI) In ESP/EAP teaching and editing of inter-disciplinary research for international communication – case studies and methods; forecasting of a technology using quantitative satellite lifetime data; interactive effects of object transfer between application windows displayed; and performance analysis of machine learning algorithms for sleep apnea detection using ECG. 25th World Multi-Conference on Systemics, Cybernetics and Informatics, WMSCI 2021  ",Financial management
993,Strategic Engineering Applied to Complex Systems within Marine Environment,"The paper proposes an example of Strategic Engineering approach applied to a complex system related to Marine Environment with special attention to traffic control. This case represents an application of innovative discipline in terms of Strategic Management based on Artificial Intelligence, Modeling and Simulation to support decision makers while operating into a dynamic environment. The authors propose this methodological approach using data and extra information based on the strong combination of Simulation with other techniques. In facts the adoption of Strategic Engineering improves Strategic Management capabilities within Organizations or Institutions and the proposed case study is based on a realistic scenario and developed through different elements, models and simulators. © 2021 SCS. Strategic Engineering Applied to Complex Systems within Marine Environment Artificial Intelligence; Data Analytics; Decision Making; Simulation; Strategic Engineering; Strategic Management Artificial intelligence; Data Analytics; Decision support systems; Environmental management; Information management; Strategic planning; Artificial simulation; Data analytics; Decision makers; Decisions makings; Intelligence models; Marine environment; Model and simulation; Simulation; Strategic engineering; Strategic management; Decision making",Monitoring and control
996,Optimizing Visual Studio Code for Python Development: Developing More Efficient and Effective Programs in Python,"Learn Visual Studio Code and implement its features in Python coding, debugging, linting, and overall project management. This book addresses custom scenarios for writing programs in Python frameworks, such as Django and Flask. The book starts with an introduction to Visual Studio Code followed by code editing in Python. Here, you will learn about the required extensions of Visual Studio Code to perform various functions such as linting and debugging in Python. Next, you will set up the environment and run your projects along with the support for Jupyter. You will also work with Python frameworks such as Django and go through data science specific-information and tutorials. Finally, you will learn how to integrate Azure for Python and how to use containers in Visual Studio Code. Optimizing Visual Studio Code for Python Development is your ticket to writing Python scripts with this versatile code editor. What You will Learn • Execute Flask development in Visual Studio Code for control over libraries used in an application • Optimize Visual Studio Code to code faster and better • Understand linting and debugging Python code in Visual Studio Code • Work with Jupyter Notebooks in Visual Studio Code Who This Book Is For Python developers, beginners, and experts looking to master Visual Studio Code. © 2021 by Sufyan bin Uzayr. All rights reserved. Optimizing Visual Studio Code for Python Development: Developing More Efficient and Effective Programs in Python Azure; Azure integration; Python; Python framework; Visual studio; VS Code ",Strategic alignment
997,Application of Big Data Technology in Cost Management and Control in Construction Project,"Currently, the competition of construction industry is increasingly fierce, the market is basically transparent, the traditional enterprise cost management model has been unable to adapt to the development of the construction market in the new era. In order to better control the cost and improve the benefit, construction enterprises must realize the technicalization, informationization and digitization in cost management. Cost management must be controlled by informationized and scientific management means. © Published under licence by IOP Publishing Ltd. Application of Big Data Technology in Cost Management and Control in Construction Project Big Data; Construction Projects; Cost Management Big data; Commerce; Competition; Construction industry; Data Science; Construction enterprise; Construction markets; Construction projects; Cost management and control; Data technologies; Informationization; Scientific management; Technicalization; Project management",Value management
999,America’s major league soccer: artificial intelligence and the quest to become a world class league,"Theoretical basis: Resource-based view (RBV) theory (Barney, 1991; Barney and Mackey, 2016; Nagano, 2020) states that a firm’s tangible and intangible resources can represent a sustainable competitive advantage (SCA), a long-term competitive advantage that is extremely difficult to duplicate by another firm, when it meets four criteria (i.e. not imitable, are rare, valuable and not substitutable). In the context of this case, we believe there are three sources of SCA to be discussed using RBV – the major league soccer (MLS) team player roster, the use of artificial intelligence (AI) technologies to exploit this roster and the league’s single-entity structure: • MLS players: it has been widely acknowledged that a firm’s human resource talent, which includes professional soccer players (Omondi-Ochieng, 2019), can be a source of SCA. For example, from an RBV perspective, a player on the Los Angeles Galaxy roster: > cannot play for any other team in any other league at the same time (not imitable and are rare), > would already be a competitive player, as he is acquired to play in the highest professional league in the country (valuable) and > it would be almost impossible to find a clone player matching his exact talent characteristic (not substitutable) anywhere else. Of course, the roster mix of players must be managed by a capable coach who is able to exploit these resources and win championships (Szymanski et al., 2019). Therefore, it is the strategic human resource or talent management strategies of the professional soccer team roster that will enable a team to have the potential for an SCA (Maqueira et al., 2019). • Technology: technology can also be considered a source of SCA. However, this has been a source of contention. The argument is that technology is accessible to any firm that can afford to purchase it. Logically, any MLS team (or for that matter any professional soccer team) can acquire or build an AI system. For many observers, the only obvious constraint is financial resources. As we discuss in other parts of the case study, there is a fan-based assumption that what transpired in major league baseball (MLB) may repeat in the MLS. The movie Moneyball promoted the use of sabermetrics in baseball when making talent selection (as opposed to relying exclusively on scouts), which has now evolved into the norm of using technology-centered sports analytics across all MLB teams. In short, where is the advantage when every team uses technology for talent management? However, if that is the case, why are the MLB teams continuing to use AI and now the National Basketball Association (NBA), National Football League (NFL) and National Hockey League are following suit? We believe RBV theorists have already provided early insights: > “the exploitation of physical technology in a firm often involves the use of socially complex firm resources. Several firms may all possess the same physical technology, but only one of these firms may possess the social relations, cultural traditions, etc., to fully exploit this technology to implementing strategies…. and obtain a sustained competitive advantage from exploiting their physical technology more completely than other firms” (Barney, 1991, p. 110). • MLS League Single-Entity Structure: In contrast to other professional soccer leagues, the MLS has one distinct in-built edge – its ownership structure as a single entity, that is as one legal organization. All of the MLS teams are owned by the MLS, but with franchise operators. The centralization of operations provides the MLS with formidable economies of scale such as when investing in AI technologies for teams. Additionally, this ownership structure accords it leverage in negotiations for its inputs such as for player contracts. The MLS is the single employer of all its players, fully paying all salaries except those of the three marquees “designated players.” Collectively, this edge offers the MLS unparalleled fluidity and speed as a league when implementing changes, securing stakeholder buy-ins and adjusting for tailwinds. The “socially complex firm resources” is the unique talent composition of the professional soccer team and most critically its single entity structure. While every team can theoretically purchase an AI technology talent management system, its application entails use across 30 teams with a very different, complex and unique set of player talents. The MLS single-entity structure though is the resource that supplies the stability required for this human-machine (technology) symbioses to be fully accepted by stakeholders such as players and implemented with precision and speed across the entire league. So, there exists the potential for each MLS team (and the MLS as a league) to acquire SCA even when using “generic” AI technology, as long as other complex firm factors come into play. Research methodology: This case relied on information that was widely reported within media, press interviews by MLS officials, announcements by various organizations, journal articles and publicly available information on MLS. All of the names and positions, in this case, are actual persons. Case overview/synopsis: MLS started as a story of dreaming large and of quixotic adventure. Back in 1990, the founders of the MLS “sold” the league in exchange for the biggest prize in world soccer – the rights to host the 1994 Fédération Internationale de Football Association World Cup before they even wrote up the business plan. Today, the MLS is the highest-level professional men’s soccer league competition in the USA. That is a major achievement in just over 25-years, as the US hosts a large professional sports market. However, MLS has been unable to attract higher broadcasting value for its matches and break into the highest tier of international professional soccer. The key reason is that MLS matches are not deemed high quality content by broadcasters. To achieve higher quality matches requires many inputs such as soccer specific stadiums, growing the fan base, attracting key investors, league integrity and strong governance, all of which MLS has successfully achieved since its inception. However, attracting high quality playing talent is a critical input the MLS does not have because the league has repeatedly cautioned that it cannot afford them yet to ensure long-term financial sustainability. In fact, to guarantee this trade-off, the MLS is one of the only professional soccer leagues with an annual salary cap. So, the question is: how does MLS increase the quality of its matches (content) using relatively low cost (low quality) talent and still be able to demand higher broadcast revenues? One strategy is for the MLS to use AI playing technology to extract higher quality playing performance from its existing talent like other sports leagues have demonstrated, such as the NFL and NBA. To implement such a radical technology-centric strategy with its players requires the MLS to navigate associated issues such as human-machine symbioses, risking fan acceptance and even altering brand valuation. Complexity academic level: The case is written and designed for a graduate-level (MBA) class or an upper-level undergraduate class in areas such as contemporary issues in management, human resource management, talent management, strategic management, sports management and sports marketing. The case is suitable for courses that discuss strategy, talent management, human resource management and brand strategy. © 2021, Emerald Publishing Limited. America’s major league soccer: artificial intelligence and the quest to become a world class league Artificial intelligence; Athlete privacy; Branding; Professional sports; Talent management ",Strategic alignment
1000,A Report on the Second International Workshop on Software Engineering for Artificial Intelligence (SE4AI 2021),"Computers control increasing numbers of objects in our daily life: phones, aircraft, cars, buildings, manufacturing machines, musical instruments, etc. In these so-called cyber-physical systems (CPSs), computers interact directly with the physical world through sensors and actuators. Those systems are becoming the key infrastructure and backbone of our society and are at the heart of revolutionary changes in our daily lives and economy. The sophistication and complexity of CPSs keep increasing since they must realize more functions with limited resources, which makes them increasingly difficult to build and manage. In particular, the cyber (software) part of these systems is growing rapidly and has become a key part of CPS, as they are the basis of operation for these systems. Artificial intelligence (AI) has a fundamental influence on the economy, administration, and society. AI is now also affecting software engineering, providing robust approaches for software development to analyze and evaluate complex software and its development processes. Repository mining, machine learning, big data analytics, and software visualization enable targeted insights and powerful predictions for software quality, software development, and software project management. The research community has shown a keen interest in this emerging field. This report presents a summary of the workshop held on February 2021 at KIT Bhubaneswar, co-located with the 14th Innovations in Software Engineering Conference (ISEC 2021).  © 2021 Owner/Author. A Report on the Second International Workshop on Software Engineering for Artificial Intelligence (SE4AI 2021) AI; Big Data; CPS; Simulation; Software Engineering Advanced Analytics; Aircraft instruments; Artificial intelligence; Computer software selection and evaluation; Data Analytics; Data visualization; Embedded systems; Project management; Software quality; Cyber physical systems (CPSs); International workshops; Manufacturing machine; Research communities; Revolutionary changes; Sensors and actuators; Software project management; Software visualization; Software design",Risk management
1001,The application of predictive analysis in decision-making processes on the example of mining company's investment projects,The authors present a method of supporting decision-making processes with the use of predictive analytics. The method is presented with an example of investment project portfolios of a mining company. In the research multiple predictive models have been tested to determine most effective algorithms and key project attributes allowing to predict possible budget deviations. The proposed approach can be applied in other organizations managing projects according to project methodologies with access to structured project data. © 2021 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0) Peer-review under responsibility of the scientific committee of KES International. The application of predictive analysis in decision-making processes on the example of mining company's investment projects Artificial inteligence; Decision-making processes; Predictive analytics; Project managment Budget control; Decision making; Artificial inteligence; Decision-making process; Effective algorithms; Investment programmes; Managing programmes; Managment; Mining companies; Predictive models; Project managment; Project portfolio; Predictive analytics,Financial management
1004,Expert-supported Data Science Projects for Production Integration of Expert Knowledge through Process Optimization Methods; [Expertengestützte Data-Science-Projekte für die Produktion Integration von Expertenwissen durch Prozessoptimierungsmethoden],"Digitization is both a challenge and an opportunity for production companies. New technical developments such as data collectors and analysis tools enable new optimization potential in production. A good example of this is the topic area of Data Science. Big Data analyses or Data Science projects can uncover optimization potential in production from large volumes of data. The CRISP-DM approach has become established in science and practice for data analysis. This procedure model offers good support for data science projects with useful methods and tools for data analysis. In practice, however, it is often observed that Data Science projects are often carried out by data experts and lack the linkage of long-standing process knowledge. In this paper, a combination of process optimization methods from the Lean Management and Six Sigma toolbox as well as project management methods for each phase of the CRISP-DM approach is presented as a solution to this challenge. © 2021 Carl Hanser Verlag. All rights reserved. Expert-supported Data Science Projects for Production Integration of Expert Knowledge through Process Optimization Methods; [Expertengestützte Data-Science-Projekte für die Produktion Integration von Expertenwissen durch Prozessoptimierungsmethoden] Data Analysis; Industry 4.0; Lean Management; Process Improvement Data Science; Optimization; Process control; Process engineering; Project management; Expert knowledge; Optimization potential; Procedure modeling; Process knowledge; Production companies; Project management method; Science projects; Technical development; Data integration",Strategic alignment
1006,The Innovation of Tourism Management Mode Based on Computer,"With the continuous expansion of the scale of tourism business, various project charges, customer conditions and travel routes in the process of tourism business operation are becoming increasingly complex. Business operators only rely on manual processing of a large number of data, which is easy to cause many problems such as missing information, high error rate of information, waste and idle of a large number of resources. The purpose of this paper is to study the innovation of tourism management model based on computer. The MVC framework studied in this paper can be built into a reusable system framework. Web application framework is required to be able to insert the basic components and component packages of web applications on J2EE architecture, so that the framework can be applied to other similar systems without any changes, and an efficient application system and reusable system framework can be constructed. This paper analyzes the tourism complaint management mode and tourism project management mode. The experimental results show that the processing process of tourism complaints management has been completed, accounting for 30%, indicating that for the tourism complaints that have been handled, the platform based on B / s network tourism management mode can get timely feedback data. © Published under licence by IOP Publishing Ltd. The Innovation of Tourism Management Mode Based on Computer B / S Network; Management Innovation; MVC Mode; Tourism Service Computer programming; Computer software reusability; Data Science; Information management; Leisure industry; Project management; Application systems; Business operation; Component packages; J2EE architecture; Manual processing; Missing information; Tourism management; Web application frameworks; Tourism",Stakeholder management
1007,Research on Intelligent Prediction and Forecast Model for Construction Period of Transmission and Transformation Engineering Based on BP Neural Network,"The construction period is one of the important control objectives of the construction and management of power transmission and transformation projects. If the construction period is not well controlled, it will lead to the failure of the project, because disputes between all parties to the construction, and the economic and social benefits of the project cannot be ignored. Negative Effects. Therefore, it is of great significance to carry out the research on the construction period prediction method to improve the economic efficiency of the construction of power transmission and transformation projects. Based on the systematic analysis of the factors affecting the construction period of power transmission and transformation projects, this paper constructs an intelligent prediction model for the construction period of power transmission and transformation projects based on BP neural network theory, and verifies the validity and prediction accuracy of the model through empirical analysis. Reasonable control of construction period provides reference and reference. © Published under licence by IOP Publishing Ltd. Research on Intelligent Prediction and Forecast Model for Construction Period of Transmission and Transformation Engineering Based on BP Neural Network  Backpropagation; Construction; Environmental technology; Forecasting; Neural networks; Pollution control; Power transmission; Predictive analytics; Construction period; Economic and social benefits; Economic efficiency; Intelligent prediction; Intelligent prediction model; Prediction accuracy; Systematic analysis; Transmission and transformation engineering; Project management",Capacity management
1008,Feature ranking and aggregation for bug triaging in open-source issue tracking systems,"The increasing complexity and team-based projects have lead to the rise of various project management tools and techniques. One of the important components of open-source project management is the usage of bug tracking systems. In the last few decades, software projects have experienced an inescapable appearance of bug reports. One of the main challenges in handling these incoming bugs is triaging of bug reports. Bug triaging can be considered as a mechanism for the election of a suitable software developer for a reported bug who will work towards resolving bug in a timely fashion. There exist several semi and fully automated bug triaging techniques in the existing literature. These techniques often consider varied bug parameters for prominent developer selection. Past researchers have concluded different parameters to be possessing prime importance in the optimal developer selection task. However, a common ranking scale depicting the importance among different bug parameters for bug triaging is not available. This paper presents a methodology to rank the non-textual bug parameters using feature ranking and aggregation techniques. The presented methodology has been evaluated on four open-source systems, namely, Mozilla Firefox, Eclipse, GNome and Open Office. From the experimental evaluation, it has been observed that the ranking of bug parameters is consistent among the different open-source projects of Bugzilla repository. © 2021 IEEE Feature ranking and aggregation for bug triaging in open-source issue tracking systems Aggregation; Bug Report; Bug Tracking System; Bug Triaging; Developer Recommendation; Feature Ranking; Mining Software Repository; Open Source Software Cloud computing; Data Science; Human resource management; Open source software; Project management; Tracking (position); Aggregation techniques; Bug tracking system; Experimental evaluation; Open source projects; Open source system; Project management tools; Software developer; Team-based projects; Open systems",Risk management
1009,Learning Experiences of Social Science Students in an Interdisciplinary Computing Minor,"The rapid growth of the digital economy and an associated increase in user-generated data has created a strong need for interdisciplinary computing professionals possessing both technical skills and knowledge of human behavior. To help meet this need and with funds from NSF IUSE, we developed an academic minor in Applied Computing for Behavioral and Social Sciences at San Jose State University. The minor involves a four-course sequence that includes programming fundamentals, data structures and algorithms, data cleaning and management, and a culminating project. At our institution and nationwide, social science students are more diverse than engineering students, with respect to gender, race, and ethnicity. By providing social science students with computing skills that complement their domain expertise, we aim to expand their career options and address the nation's need for a diverse, technology-capable workforce. We administered an exit survey on student learning experiences to two cohorts of students completing the minor. Given that the minor is new and that the first cohorts were relatively small, the number of students completing the survey was modest (n = 15). Results indicate that students were motivated to minor in Applied Computing by a desire to improve their data analysis skills and better prepare themselves for the job market/graduate school, as well as a belief that programming is a necessary skill for the future. A large majority of students indicated that their peers, instructors, and homework assignments supported their learning very well, whereas they found topics covered and course projects to be less supportive, followed by pacing of course content. With respect to career plans, a majority of students agreed that the minor provided them with their desired skills and allowed them to learn about careers in applied computing, and a large majority indicated that they planned to pursue a career utilizing applied computing. They expressed interest in fields such as human factors, data analytics, project management, teaching, clinical psychology, and various types of research. Finally, common themes that arose when providing advice to future students included not being shy in seeking help, tips for managing the level of course difficulty, encouragement to regularly practice, suggestions for how to master course content, and advice for adopting a successful mindset. These results will be instrumental in helping to optimize students' experiences in the minor, ranging from how we recruit new students to how we can better support their professional development. Given the largely positive experiences of our students and their plans to pursue careers involving applied computing, we believe that our approach of adding computing education alongside a social science degree demonstrates a promising model for meeting the increasing demand for diverse interdisciplinary computing workers in this digital age. © American Society for Engineering Education, 2021 Learning Experiences of Social Science Students in an Interdisciplinary Computing Minor  Behavioral research; Clinical research; Curricula; Data Analytics; Project management; Surveys; Teaching; Applied computing; Course contents; Course sequences; Digital economy; Human behaviors; Learning experiences; Rapid growth; San Jose State University; Technical skills; User-generated; Students",Risk management
1010,Beyond Business Analytics and Strategic Management: A Critical Review,"This paper examines the relationship between the notion of Beyond Business Analytics and the field of strategic management. The paper offers a critical review of three models: Michael Porter's model; the Balanced Scorecard model, and the Go versus Chess model. The paper also proposes lessons and conclusions, some of which are derived from an analysis of Musashi's book: ""The Book of Five Rings"" and Sun Tzu's book: ""The Art of War"". The paper particularly analyzes Musashi's proposition that ""It is important in strategy to know the enemy's sword and not be distracted by insignificant movements of his sword"", and .Sun Tzu's declaration: ""the skillful fighter puts himself into a position which makes defeat impossible, and does not miss the moment for defeating the enemy"". The paper also offers a salient historical example of the battle of Midway (1942) and the ability of the United States navy to makes the most of its intelligence apparatus which provided Admiral Nimitz with superior strategic position versus the Japanese Imperial Navy. © IEOM Society International. Beyond Business Analytics and Strategic Management: A Critical Review  ",Strategic alignment
1011,Improving Project Control by Utilizing Predictive Data Analytic Models,"Project progress is an apprehension for every project, as it indicates how the project is likely to meet the associated milestones. Utilizing collected data from archived projects can assist managers to envisage project progress. By leveraging the power of data analytics, this research attempts to highlight data trends based on data collected from 279 infrastructure projects in the UAE. Specifically, this research rigorously analyses the relationships between project budget, duration, and progress using K-means clustering techniques and hypothesis testing. We then provide predictive models using Autoregressive Integrated Moving Average - ARIMA and Multivariate regression models that allow managers to predict with a 99.15% accuracy the monthly progress of an infrastructure project over the next 3 months. This research paper provides project managers with a comprehensive framework that combines data analytics techniques with agility practices to predict short term project progress in order to take proactive measures on the different influencing factors.  © IEEE 2022. Improving Project Control by Utilizing Predictive Data Analytic Models Clustering; Hypothesis Testing; Predictive models; Project Management; Project progress Budget control; Data Analytics; K-means clustering; Managers; Regression analysis; Clusterings; Data analytics; Data trend; Hypothesis testing; Infrastructure project; Power; Predictive models; Project budget; Project control; Project progress; Project management",Monitoring and control
1012,Improving Effectiveness and Efficiency,"This chapter covers total quality management (TQM) with respect to the Lean and Six Sigma methods used to improve the effectiveness and efficiency of hospitality operations. In the first section, TQM is discussed. In the second section, Lean and Six Sigma techniques are examined individually. Then, Lean and Six Sigma are connected with an example of a housekeeping case study that applies both methodologies. In the third section, business analytics are explored and statistical process control analysis is demonstrated using a hotel room cleanliness example. The fourth section summarizes the concepts of change management, which is critical for embracing the philosophies of TQM. Finally, project management is discussed in the fifth and last section. © 2021 Emerald Publishing Limited. Improving Effectiveness and Efficiency change management; DMAIC; Lean; project management; quality; Six Sigma; Total quality management ",Capacity management
1015,Application of Blockchain in Construction Inspection Activities,"Construction management is a complex and time-consuming process, and effective quality management is mandatory to ensure overall project quality. Unfortunately, current methods are cumbersome and rife with error. Therefore, the application of electronic and blockchain-based technology for autonomous inspection operations is proposed, and its advantages to auxiliary management operations are demonstrated. The application of blockchain technology in the construction industry using appropriate electronic aids can improve operational efficiency over traditional paper-based methods, thus reducing the time required by engineers and improving quality management. © 2021 ACM. Application of Blockchain in Construction Inspection Activities autonomous inspection; Blockchain; construction industry; Ethereum; quality management Blockchain; Construction industry; Data Science; Electronics industry; Quality management; Construction inspection; Construction management; Electronic aids; Management operation; Operational efficiencies; Paper based methods; Project quality; Project management",Strategic alignment
1016,Strategic engineering applied to complex systems within marine environment,"The paper proposes an example of Strategic Engineering approach applied to a complex system related to Marine Environment with special attention to traffic control. This case represents an application of innovative discipline in terms of Strategic Management based on Artificial Intelligence, Modeling and Simulation to support decision makers while operating into a dynamic environment. The authors propose this methodological approach using data and extra information based on the strong combination of Simulation with other techniques. In facts the adoption of Strategic Engineering improves Strategic Management capabilities within Organizations or Institutions and the proposed case study is based on a realistic scenario and developed through different elements, models and simulators. © 2021 Society for Modeling & Simulation International (SCS). Strategic engineering applied to complex systems within marine environment Artificial Intelligence; Data Analytics; Decision Making; Simulation; Strategic Engineering; Strategic Management Artificial intelligence; Data Analytics; Decision support systems; Environmental management; Information management; Strategic planning; Artificial simulation; Data analytics; Decision makers; Decisions makings; Intelligence models; Marine environment; Model and simulation; Simulation; Strategic engineering; Strategic management; Decision making",Monitoring and control
1018,Risk assessment of innovative projects: Development of forecasting models,"The purpose of this article is development a systematic dynamic complex model of generation and risk assessment of innovative project, on the bases of which a scenario modeling of many risks influence arising at certain stages of project implementation in the target area. For article purpose realization, the paper proposes a complex toolkit for modeling the innovative projects risks in the direction of their impact on performance indicators in scenarios, which involves the implementation of the following stages of modeling: Stage 1. Collection and processing of project data; Stage 2. Evaluation of innovation project efficiency indicators; Stage 3. Formation and assessment of many risks of innovation project by components, nature and impact strength; Stage 4. Modeling of innovative project development scenarios. The implemented set of models makes possible to compare all components of efficiency and riskiness, which determine the integrated aggregate level of project risk by components of life cycle risks, target project risks and scenarios depending on environmental factors and managers propensity to take risks, and solves the problem of positioning the real indicators state of innovation project efficiency in a comparative dynamic context based on three-level assessment, due to structural elements of risks and identification of possible and promising deadlines and time horizons by stages of the project life cycle, critical paths and reserves which allow us to achieve the main goal of improving the innovative projects implementation efficiency. © 2021 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0). Risk assessment of innovative projects: Development of forecasting models Business entities; Challenges; Efficiency criterion; Innovative project; Life cycle; Risks; Scenario modeling; Simulation model Efficiency; Impact strength; Life cycle; Machine learning; Predictive analytics; Risk perception; Environmental factors; Innovation projects; Innovative projects; Performance indicators; Project implementation; Project life cycle; Structural elements; Three-level assessment; Risk assessment",Financial management
1019,Job analyses of earth science data librarians and data managers,"This study's purpose is to capture the skills of Earth science data managers and librarians through interviews with current job holders. Job analysis interviews were conducted of 14 participants-six librarians and eight data managers-to assess the types and frequencies of job tasks. Participants identified tasks related to communication, including collaboration, teaching, and project management activities. Data-specific tasks included data discovery, processing, and curation, which require an understanding of the data, technology, and information infrastructures to support data use, reuse, and preservation. Most respondents had formal science education and six had a master's degree in Library and Information Sciences. Most of the knowledge, skills, and abilities for these workers were acquired through on-the-job experience, but future professionals in these careers may benefit from tailored education informed through job analyses. © 2021 American Meteorological Society For information regarding reuse of this content and general copyright information, consult the AMS Copyright Policy. Job analyses of earth science data librarians and data managers Data quality control; Data science; Databases; Education Earth sciences; Job analysis; Libraries; Managers; Project management; Data discovery; Earth science data; Formal science; Information infrastructures; Library and information science; Management activities; Master's degree; Specific tasks; Data handling",Stakeholder management
1020,Research on Integrated Scheduling Algorithm of Inbound and Depart Flights in Airport Based on Integrated Operation,"In the past decade, with the development of economy, flight traffic flow is increasing, and the phenomenon of flight congestion and delay is increasing. The departure command and approach command lack of efficient linkage and coordination, the tower and approach determine the insertion and release intervals and time slots through manual coordination communication. The time slot resource interval cannot be fully used, so it is urgent to study a set of integrated flight scheduling algorithm based on integrated operation to improve the scene operation efficiency. From the perspective of process management system construction, this paper describes the construction background, construction content, technical innovation and existing problems of the project. The research results effectively promote the research progress in this field in China and lay a foundation for practical operation. © 2021 ACM. Research on Integrated Scheduling Algorithm of Inbound and Depart Flights in Airport Based on Integrated Operation Air traffic; core technology; Inbound and Depart Flights; Integrated Operation; Scheduling Algorithm Data Science; Construction contents; Existing problems; Integrated Operations; Integrated scheduling algorithms; Operation efficiencies; Process management systems; Research results; Technical innovation; Project management",Financial management
1022,"25th World Multi-Conference on Systemics, Cybernetics and Informatics, WMSCI 2021","The proceedings contain 90 papers. The topics discussed include: on the pitfalls of videoconferences for challenge-based face liveness detection; adopting agile practices: lessons learned transforming organizations that do not develop software; a novel IDS model oriented to drone nodes networks threated for DoS attacks; an optimal path planning approach for a two wheeled mobile robot using free segment and turning point algorithm; a cybersecurity data science demonstrator: machine learning in IoT network security; features of the case method application in the study of disciplines related to information technologies and IT project management; human intelligence (HI –Nous) and artificial intelligence (AI) In ESP/EAP teaching and editing of inter-disciplinary research for international communication – case studies and methods; forecasting of a technology using quantitative satellite lifetime data; interactive effects of object transfer between application windows displayed; and performance analysis of machine learning algorithms for sleep apnea detection using ECG. 25th World Multi-Conference on Systemics, Cybernetics and Informatics, WMSCI 2021  ",Financial management
1024,Estimating production and warranty cost at the early stage of a new product development project,"The paper is concerned with estimating the cost of production and warranty in the context a new product development project. All possible variants of the trade-off between costs are sought within the company's resources and requirements for a product development project. A company and its projects can be specified in terms of variables and constraints that constitute the systems approach for a problem related to cost optimization. This problem is described in the form of a constraint satisfaction problem and implemented with the use of parametric modelling and constraint programming techniques. The paper also presents a method for estimating the cost of prototyping, faulty products in manufacturing and the after-sales stage, and simulating variants that ensure the desirable level of costs. An example shows the applicability of the proposed approach in the context of a product development project. © 2021 The Authors. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0) Estimating production and warranty cost at the early stage of a new product development project Business analytics; Computational intelligence for business; Data-driven decision making; Decision support and control; Project management Artificial intelligence; Constraint satisfaction problems; Constraint theory; Cost benefit analysis; Cost estimating; Decision making; Economic and social effects; Product development; Project management; Business analytics; Computational intelligence for business; Data driven decision; Data-driven decision making; Decision control; Decision supports; Decisions makings; New product development projects; Product development programs; Production cost; Decision support systems",Strategic alignment
1033,Risks of Data Science Projects - A Delphi Study,"Risk is one of the most crucial components of a project. Its proper evaluation and treatment increase the chances of a project's success. This article presents the risks in Data Science projects, assessed through a study conducted with the Delphi technique, to answer the question, ""What are the risks of Data Science projects"". The study allowed the identification of specific risks related to data science projects, however it was possible to verify that over a half of the most mentioned risks are similar to other types of IT projects. This paper describes the research from expert selection, risk identification and analysis, and the first conclusions. © 2021 Elsevier B.V. All rights reserved. Risks of Data Science Projects - A Delphi Study Data Science; Delphi study; project risk management; project sucess; risk assessment Risk assessment; Risk management; DELPHI study; Delphi technique; IT project; Project risk management; Project success; Project sucess; Risk Identification; Risks assessments; Science projects; Data Science",Risk management
1034,Big data project management approaches in the Czech Republic,"The amount of data grows exponentially every year and the information obtained from their analysis is an asset of every business. Therefore, more and more companies invest in Big Data implementation and use. However, the technology for processing this data is very complex and it is very difficult for companies to successfully master and operate it as to fully utilize its potential. Projects focused on this issue often end in failure. There are many critical factors affecting success of these projects. One of the most critical factors is appropriate project management. The paper aims to identify the approach which is most often used by companies in the Czech Republic to manage Big Data projects, as well as to assess how successful these projects are. The research is based on data from a research which included a questionnaire survey. The paper determines the most frequently used approach to Big Data project management in the Czech Republic, including the resulting success of such projects as perceived by respondents. © 2021 IDIMT 2021 - Pandemics: Impacts, Strategies and Responses, 29th Interdisciplinary Information Management Talks All rights reserved. Big data project management approaches in the Czech Republic Agile; Big data; Czech Republic; Data analytics; Data mining; Data volumes; Project management; Project success Big data; Information management; Project management; Surveys; Critical factors; Czech Republic; Data implementation; Questionnaire surveys; Data handling",Value management
1035,MinViME/Minimum Viable Model Estimator[Formula presented],"MinViME is a python package and web application that allows scientists and engineers to estimate a lower bound on the performance characteristics of a machine learning model for a problem with well quantified characteristics. It is used to evaluate the feasibility of machine learning projects, prioritize among multiple competing projects, and run simulations to explore the structure of the difficulty landscape in a multi-dimensional space of potential problems. © 2021 The Author(s) MinViME/Minimum Viable Model Estimator[Formula presented] Data science; Estimation; Machine learning; Project management ",Strategic alignment
1036,Innovation and investment factors in the state strategic management of social and economic development of the country: Modeling and forecasting,"The aim of the article is the modeling of impact of innovative and investment factors on the social and economic development of Ukraine, and forecasting Ukraine's GDP growth in the short term to improve the state strategic management. The model-based approach moves away from classical regression analysis and instead uses combination of production functions and regression analysis. The forecast was made for a short-term period by means of Box-Jenkins method (ARIMA) with the use of Statistica. By means of the regression model, there was established that forecast values of GDP volume over the long term will gradually decrease. It was found out that the highest priority factors of a competitiveness according to the innovative component of Ukraine are the amount of scientific and technological works, a number of innovative technologies and technological processes introduced, fixed investments, a number of innovative products sold. The effective investments in basic capital and increase of sold innovative products volume will also because GDP growth. All these require the development and realization of the preventive measures, improvement of state programs of social and economic development and implementation of state policy. © 2021 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0). CEUR Workshop Proceedings (CEUR-WS.org) Innovation and investment factors in the state strategic management of social and economic development of the country: Modeling and forecasting Economic development; Forecast; GDP; Innovative and investment factors; Production function method; Regression analysis; State programs of social Data Science; Economics; Engineering education; Forecasting; Investments; Machine learning; Regression analysis; Strategic planning; Innovative component; Innovative technology; Model based approach; Modeling and forecasting; Production function; Social and economic development; Strategic management; Technological process; Economic and social effects",Strategic alignment
1037,"25th World Multi-Conference on Systemics, Cybernetics and Informatics, WMSCI 2021","The proceedings contain 90 papers. The topics discussed include: on the pitfalls of videoconferences for challenge-based face liveness detection; adopting agile practices: lessons learned transforming organizations that do not develop software; a novel IDS model oriented to drone nodes networks threated for DoS attacks; an optimal path planning approach for a two wheeled mobile robot using free segment and turning point algorithm; a cybersecurity data science demonstrator: machine learning in IoT network security; features of the case method application in the study of disciplines related to information technologies and IT project management; human intelligence (HI –Nous) and artificial intelligence (AI) In ESP/EAP teaching and editing of inter-disciplinary research for international communication – case studies and methods; forecasting of a technology using quantitative satellite lifetime data; interactive effects of object transfer between application windows displayed; and performance analysis of machine learning algorithms for sleep apnea detection using ECG. 25th World Multi-Conference on Systemics, Cybernetics and Informatics, WMSCI 2021  ",Financial management
1038,Neural networks based software development effort estimation: A systematic mapping study,"Developing an efficient model that accurately predicts the development effort of a software project is an important task in software project management. Artificial neural networks (ANNs) are promising for building predictive models since their ability to learn from previous data, adapt and produce more accurate results. In this paper, we conducted a systematic mapping study of papers dealing with the estimation of software development effort based on artificial neural networks. In total, 80 relevant studies were identified between 1993 and 2020 and classified with respect to five criteria: publication source, research approach, contribution type, techniques used in combination with ANN models and type of the neural network used. The results showed that, most ANN-based software development effort estimation (SDEE) studies applied the history-based evaluation (HE) and solution proposal (SP) approaches. Besides, the feedforward neural network was the most frequently used ANN type among SDEE researchers. To improve the performance of ANN models, most papers employed optimization methods such as Genetic Algorithms (GA) and Particle Swarm Optimization (PSO) in combination with ANN models. Copyright © 2021 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved Neural networks based software development effort estimation: A systematic mapping study Artificial Neural Networks; Software Development Effort Estimation; Systematic Mapping Study Feedforward neural networks; Genetic algorithms; Mapping; Particle swarm optimization (PSO); Predictive analytics; Project management; Ann models; Optimization method; Predictive models; Research approach; Software development effort; Software project; Software project management; Systematic mapping studies; Software design",Strategic alignment
1039,The role of DMSS analytics tools in software project risk management,[No abstract available] The role of DMSS analytics tools in software project risk management  ,Risk management
1040,The case for information fiduciaries: The implementation of a data ethics checklist at Seattle Children's Hospital,"There is little debate about the importance of ethics in health care, and clearly defined rules, regulations, and oaths help ensure patients' trust in the care they receive. However, standards are not as well established for the data professions within health care, even though the responsibility to treat patients in an ethical way extends to the data collected about them. Increasingly, data scientists, analysts, and engineers are becoming fiduciarily responsible for patient safety, treatment, and outcomes, and will require training and tools to meet this responsibility. We developed a data ethics checklist that enables users to consider the possible ethical issues that arise from the development and use of data products. The combination of ethics training for data professionals, a data ethics checklist as part of project management, and a data ethics committee holds potential for providing a framework to initiate dialogues about data ethics and can serve as an ethical touchstone for rapid use within typical analytic workflows, and we recommend the use of this or equivalent tools in deploying new data products in hospitals.  © 2020 The Author(s) 2021. Published by Oxford University Press on behalf of the American Medical Informatics Association. All rights reserved. For permissions, please email: journals.permissions@oup.com. The case for information fiduciaries: The implementation of a data ethics checklist at Seattle Children's Hospital business ethics; checklist; data ethics; ethical analysis; hospital ethics Checklist; Codes of Ethics; Data Science; Ethics, Clinical; Ethics, Professional; Hospital Information Systems; Hospitals, Pediatric; Washington; business ethics; checklist; child; human; professional standard; review; workflow; checklist; ethics; hospital; hospital information system; medical ethics; Washington",Financial management
1041,Training the next industrial engineers and managers about industry 4.0: A case study about challenges and opportunities in the covid-19 era,"Training the next generation of industrial engineers and managers is a constant challenge for academia, given the fast changes of industrial technology. The current and predicted development trends in applied technologies affecting industry worldwide as formulated in the Industry 4.0 initia-tive have clearly emphasized the needs for constantly adapting curricula. The sensible socioeconomic changes generated by the COVID-19 pandemic have induced significant challenges to society in general and industry. Higher education, specifically when dealing with Industry 4.0, must take these new challenges rapidly into account. Modernization of the industrial engineering curriculum combined with its migration to a blended teaching landscape must be updated in real-time with real-world cases. The COVID-19 crisis provides, paradoxically, an opportunity for dealing with the challenges of training industrial engineers to confront a virtual dematerialized work model which has accelerated during and will remain for the foreseeable future after the pandemic. The paper describes the methodology used for adapting, enhancing, and evaluating the learning and teaching experience under the urgent and unexpected challenges to move from face-to-face university courses distant and online teaching. The methodology we describe is built on a process that started before the onset of the pandemic, hence in the paper we start by describing the pre-COVID-19 status in comparison to published initiatives followed by the real time modifications we introduced in the faculty to adapt to the post-COVID-19 teaching/learning era. The focus presented is on Industry 4.0. subjects at the leading edge of the technology changes affecting the industrial engineering and technology management field. The manuscript addresses the flow from system design subjects to implementation areas of the curriculum, including practical examples and the rapid decisions and changes made to encompass the effects of the COVID-19 pandemic on content and teaching methods including feedback received from participants. © 2021 by the authors. Licensee MDPI, Basel, Switzerland. Training the next industrial engineers and managers about industry 4.0: A case study about challenges and opportunities in the covid-19 era Business intelligence; Coronavirus; COVID-19; Cyber-physical systems; Digital health; Education; Educational activities; Industrial engineering; Industry 4.0; Internet of things; Project management; Project-based learning COVID-19; Curriculum; Education, Distance; Humans; Pandemics; SARS-CoV-2; Curricula; E-learning; Engineers; Industry 4.0; Managers; Teaching; Development trends; Engineering and technology; Industrial technology; Learning and teachings; Real time modifications; Socio-economic change; Technology change; University course; curriculum; education; human; pandemic; Personnel training",Value management
1043,Project management for a machine learning project,"We will briefly revisit classical project management approaches in the power generation industry before giving a brief introduction of agile and scrum. It will be made clear how these approaches differ and why scrum might be the better choice when applied to machine learning and data science projects. The scrum framework with its most relevant components will be explained briefly using a real world example, while stressing the importance of management and change culture while executing these machine learning projects. At last we will show what is important when scaling your ideas from simple pilot to a product that will be adopted in your entire organization. © 2021 Elsevier Inc. All rights reserved. Project management for a machine learning project Agile; Change management; Digitalization; Machine learning; Power generation industry; Project management; Scrum ",Capacity management
1044,"Knowledge Project - Concept, Methodology and Innovations for Artificial Intelligence in Industry 4.0","AI is one of the biggest megatrends towards the 4th industrial revolution. Although these technologies promise business sustainability as well as product and process quality, it seems that the ever-changing market demands, the complexity of technologies and fair concerns about privacy, impede broad application and reuse of Artificial Intelligence (AI) models across the industry. To break the entry barriers for these technologies and unleash its full potential, the knowlEdge project will develop a new generation of AI methods, systems, and data management infrastructure. Subsequently, as part of the knowlEdge project we propose several major innovations in the areas of data management, data analytics and knowledge management including (i) a set of AI services that allows the usage of edge deployments as computational and live data infrastructure as well as a continuous learning execution pipeline on the edge, (ii) a digital twin of the shop-floor able to test AI models, (iii) a data management framework deployed along the edge-to-cloud continuum ensuring data quality, privacy and confidentiality, (iv) Human-AI Collaboration and Domain Knowledge Fusion tools for domain experts to inject their experience into the system, (v) a set of standardisation mechanisms for the exchange of trained AI models from one context to another, and (vi) a knowledge marketplace platform to distribute and interchange trained AI models. In this paper, we present a short overview of the EU Project knowlEdge - Towards Artificial Intelligence powered manufacturing services, processes, and products in an edge-to-cloud-knowledge continuum for humans [in-the-loop], which is funded by the Horizon 2020 (H2020) Framework Programme of the European Commission under Grant Agreement 957331. Our overview includes a description of the project's main concept and methodology as well as the envisioned innovations. © 2021 IEEE. Knowledge Project - Concept, Methodology and Innovations for Artificial Intelligence in Industry 4.0 Artificial intelligence; Data analytics; Human-AI collaboration; Industry 4.0; Machine learning; Smart process manufacturing Cloud analytics; Commerce; Industry 4.0; Knowledge management; Machine learning; Project management; Business sustainability; Data analytics; Human-artificial intelligence collaboration; Industrial revolutions; Intelligence models; Machine-learning; Megatrends; Process manufacturing; Smart process; Smart process manufacturing; Data Analytics",Strategic alignment
1045,Managing and Composing Teams in Data Science: An Empirical Study,"Data science projects have become commonplace over the last decade. During this time, the practices of running such projects, together with the tools used to run them, have evolved considerably. Furthermore, there are various studies on data science workflows and data science project teams. However, studies looking into both workflows and teams are still scarce and comprehensive works to build a holistic view do not exist. This study bases on a prior case study on roles and processes in data science. The goal here is to create a deeper understanding of data science projects and development processes. We conducted a survey targeted at experts working in the field of data science (n=50) to understand data science projects' team structure, roles in the teams, utilized project management practices and the challenges in data science work. Results show little difference between big data projects and other data science. The found differences, however, give pointers for future research on how agile data science projects are, and how important is the role of supporting project management personnel. The current study is work in progress and attempts to spark discussion and new research directions. © 2021 IEEE. Managing and Composing Teams in Data Science: An Empirical Study agile practices; Data science; project management; teamwork Data Science; Human resource management; Information management; Agile practices; Case-studies; Empirical studies; Holistic view; Project process; Project team; Science development; Science projects; Teamwork; Work-flows; Project management",Value management
1049,Deep learning with small datasets: using autoencoders to address limited datasets in construction management,"Large datasets are necessary for deep learning as the performance of the algorithms used increases as the size of the dataset increases. Poor data management practices and the low level of digitisation of the construction industry represent a big hurdle to compiling big datasets; which in many cases can be prohibitively expensive. In other fields, such as computer vision, data augmentation techniques and synthetic data have been used successfully to address issues with limited datasets. In this study, undercomplete, sparse, deep and variational autoencoders are investigated as methods for data augmentation and generation of synthetic data. Two financial datasets of underground and overhead power transmission projects are used as case studies. The datasets were augmented using the autoencoders, and the project cost was predicted using a deep neural network regressor. All the augmented datasets yielded better results than the original dataset. On average the autoencoders provide a model score improvement of 7.2% and 11.5% for the underground and overhead datasets, respectively. MAE and RMSE are lower for all autoencoders as well. The average error improvement for the underground and overhead datasets is 22.9% and 56.5%, respectively. Variational autoencoders provided more robust results and represented better the non-linear correlations among the attributes in both datasets. The novelty of this study is that presents an approach to improve existing datasets and thus improve the generalisation of deep learning models when other approaches are not feasible. Moreover, this study provides practitioners with methods to address the limited access to big datasets, a visualisation method to extract insights from non-linear correlations in data, and a way to improve data privacy and to enable sharing sensitive data using analogous synthetic data. The main contribution to knowledge of this study is that it presents a data augmentation technique for transformation variant data. Many techniques have been developed for transformation invariant data that contributed to improving the performance of deep learning models. This study showed that autoencoders are a good option for data augmentation for transformation variant data. © 2021 Elsevier B.V. Deep learning with small datasets: using autoencoders to address limited datasets in construction management Autoencoders; Deep learning; Machine learning; Predictive analytics; Variational autoencoders Construction industry; Deep neural networks; Information management; Large dataset; Learning systems; Metadata; Privacy by design; Project management; Average errors; Construction management; Data augmentation; Learning models; Management practices; Non-linear correlations; Sensitive datas; Transformation invariants; Deep learning",Strategic alignment
1050,Innovative Application of Computer Technology in Engineering Management,"With the rapid development of China's economy, computers have been well developed at a certain level. In most of our project management process, if we can effectively use computer technology, to a certain extent, we can improve the quality and efficiency of the project. Based on this, this paper studies the innovative application of computer technology in engineering management. In this paper, an engineering team is selected as the research object. Based on the theory of computer technology and RDF graph data flow division algorithm, the engineering resource utilization rate, project management efficiency and engineering difficulty of the engineering team before and after the application of computer technology for project management are compared. The research results show that before the application of computer technology for project management, the resource utilization rate of engineering team is 72.5%, the project management efficiency is 61.6%, and the engineering construction difficulty is 76.3%. After the application of computer technology to project management, the data have changed, the resource utilization rate has increased to 88.3%, the pipeline efficiency has increased to 79.3%, and the engineering difficulty has been reduced to It can be seen from the data changes that the same engineering team has changed significantly before and after the application of computer technology in project management, which not only saves engineering resources, but also effectively improves the engineering efficiency. © Published under licence by IOP Publishing Ltd. Innovative Application of Computer Technology in Engineering Management Computer Technology; Engineering Management; Innovation Strategy; Partition Algorithm of RDF Graph Data Flow Computation theory; Computer resource management; Data flow analysis; Data Science; Efficiency; Flow graphs; Graph algorithms; Human resource management; Information management; Engineering constructions; Engineering efficiency; Engineering management; Engineering resources; Management efficiency; Project management process; Resource utilizations; Theory of computers; Project management",Financial management
1054,"Data Science, Decision Theory and Strategic Management: A Tale of Disruption and Some Thoughts About What Could Come Next","This paper intends to assume a comprehensive and far-sighted enough perspective on the pillar themes of the Conference. Data Science (DS), Decision Theory (DT) and Strategic Management (SM) are three relevant and interrelated fields that have undergone substantive changes in recent years. The purpose of the paper is, first, to examine the revolution that occurred in DS, DT and MS and, then, to discuss the leap that happened in the process data are linked to decisions and these, in turn, to strategies. In short, data moved from a situation of scarcity, fragmentation and poor quality to a situation of abundance, structure and extreme richness; Decision Theory moved from Olympic rationality and certainty to bounded rationality, uncertainty and cognitive biases; Strategic Management moved from clear and straightforward course of action to an open, incremental and “many best ways” process. So, once upon a time, (few) data were the input for decisions based on sound criteria and led to a definite route to be followed by the organization. Nowadays, a multitude of data are available to decision-makers who are more or less aware of the flaws affecting their behavior with the whole process ending in simple claims of strategic agility and flexibility. Finally, some reflections are introduced about the consequences of such changes, the challenges digital technology poses to companies and society in the coming years and about a tentative logic to cope with such complexity. © The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2021. Data Science, Decision Theory and Strategic Management: A Tale of Disruption and Some Thoughts About What Could Come Next  ",Value management
1056,Big Data analytics in Agile software development: A systematic mapping study,"Context: Over the last decade, Agile methods have changed the software development process in an unparalleled way and with the increasing popularity of Big Data, optimizing development cycles through data analytics is becoming a commodity. Objective: Although a myriad of research exists on software analytics as well as on Agile software development (ASD) practice on itself, there exists no systematic overview of the research done on ASD from a data analytics perspective. Therefore, the objective of this work is to make progress by linking ASD with Big Data analytics (BDA). Method: As the primary method to find relevant literature on the topic, we performed manual search and snowballing on papers published between 2011 and 2019. Results: In total, 88 primary studies were selected and analyzed. Our results show that BDA is employed throughout the whole ASD lifecycle. The results reveal that data-driven software development is focused on the following areas: code repository analytics, defects/bug fixing, testing, project management analytics, and application usage analytics. Conclusions: As BDA and ASD are fast-developing areas, improving the productivity of software development teams is one of the most important objectives BDA is facing in the industry. This study provides scholars with information about the state of software analytics research and the current trends as well as applications in the business environment. Whereas, thanks to this literature review, practitioners should be able to understand better how to obtain actionable insights from their software artifacts and on which aspects of data analytics to focus when investing in such initiatives. © 2020 Elsevier B.V. Big Data analytics in Agile software development: A systematic mapping study Agile software development; Artificial intelligence; Data analytics; Literature review; Machine learning; Software analytics Advanced Analytics; Application programs; Big data; Data Analytics; Life cycle; Project management; Software testing; Agile software development; Business environments; Development cycle; Literature reviews; Software artifacts; Software development process; Software development teams; Systematic mapping studies; Software design",Strategic alignment
1057,The Coaching Black Box: Risk Mitigation during Change Management,"A case study of strategic renewal in the Chinese education market, this paper explores a non-directive coaching model and its impact on risk mitigation, knowledge exchange and innovation in strategic renewal through the application of multi-tiered coaching and manager coaches. Through an ethnographic action research methodology, we ask “Can coaching mitigate organisational risk and increase the likelihood of positive outcomes in change management?” and “Can managers, acting as internal coaches, increase knowledge socialisation and mitigate risk in the change management process?” The paper finds that there is no inherent failure rate in the change management process and that a strategic management approach can mitigate risk liberating managers and organisations to seek to create the collaborative environments that support organisational learning and strategic renewal, thus moving beyond a narrative of failure to one of strategic empowerment and a strategic management approach to risk mitigation. We conclude that a data-driven approach to organisational learning and Professional Learning Communities helps teams to ask the right questions and to mitigate risk through better aligning the organisation to its strategic reality, exploiting organisational learning to achieve competitive advantage and ensuring that systems and processes continue to match the emerging strategic reality. © 2021 by the authors. The Coaching Black Box: Risk Mitigation during Change Management change management; coaching; innovation; knowledge management; knowledge socialisation; manager coach; people analytics; risk mitigation; strategic renewal; team coaching ",Risk management
1059,Fifty years of information management research: A conceptual structure analysis using structural topic modeling,"Information management is the management of organizational processes, technologies, and people which collectively create, acquire, integrate, organize, process, store, disseminate, access, and dispose of the information. Information management is a vast, multi-disciplinary domain that syndicates various subdomains and perfectly intermingles with other domains. This study aims to provide a comprehensive overview of the information management domain from 1970 to 2019. Drawing upon the methodology from statistical text analysis research, this study summarizes the evolution of knowledge in this domain by examining the publication trends as per authors, institutions, countries, etc. Further, this study proposes a probabilistic generative model based on structural topic modeling to understand and extract the latent themes from the research articles related to information management. Furthermore, this study graphically visualizes the variations in the topic prevalences over the period of 1970 to 2019. The results highlight that the most common themes are data management, knowledge management, environmental management, project management, service management, and mobile and web management. The findings also identify themes such as knowledge management, environmental management, project management, and social communication as academic hotspots for future research. © 2021 Elsevier Ltd Fifty years of information management research: A conceptual structure analysis using structural topic modeling Generative models; Information management; Structural topic models; Text analytics; Topic modeling Knowledge management; Project management; Conceptual structures; Generative model; Organizational process; Service management; Social communications; Text analysis; Topic Modeling; Web management; Environmental management",Strategic alignment
1060,Application Analysis of Computer Technology in Construction Project Schedule Control in Information Age,"Construction industry is one of the pillar industries in China, which has made great contributions to China's economic development. However, because the overall management level of the construction industry in China is still relatively low at present, the construction procedures of many projects are complex and involve many factors. In order to make the construction proceed smoothly, it is necessary to control the progress of construction projects reasonably. Therefore, the purpose of this paper is to analyze the application of computer technology in construction project schedule control in the information age. In this paper, according to the actual situation, the project management software Project 2019 in computer technology is used to control and manage the progress of the project. Project 2019 can work out a set of optimal construction schedule within the specified time limit, compare it with the actual schedule, find out the deviation and its causes, make reasonable analysis and make corresponding adjustments. At the end of this paper, the construction projects controlled by the project management software Project 2019 are compared with those not used, and the actual effects of both are analyzed. The experimental results show that the project management software Project 2019 can effectively control the progress of the construction project reasonably, and can make corresponding adjustments according to the actual situation, which can accelerate the overall progress of the project by about 10%, which is of great significance for improving the existing construction project progress.  © Published under licence by IOP Publishing Ltd. Application Analysis of Computer Technology in Construction Project Schedule Control in Information Age Computer Technology; Construction Project; Information Age; Project 2019 Computer control; Computer software; Construction industry; Data Science; Application analysis; Computer technology; Construction procedures; Construction projects; Information age; Management level; Optimal construction; Project management software; Project management",Risk management
1061,Using strategy analytics to measure corporate performance and business value creation,"Strategic analytics is a relatively new field in conjunction with strategic management and business intelligence. Generally, the strategic management field deals with the enhancement of the decision-making capabilities of managers. Typically, such decision-making processes are heavily dependent upon various internal and external reports. Managers need to develop their strategies using clear strategy processes supported by the increasing availability of data. This situation calls for a different approach to strategy, including integration with analytics, as the science of extracting value from data and structuring complex problems. Using Strategy Analytics to Measure Corporate Performance and Business Value Creation discusses how to tackle complex business dynamics using optimization techniques and modern business analytics tools. It covers not only introductory concepts of strategic analytics but also provides strategic analytics applications in each area of management such as market dynamics, customer analysis, operations, and people management. It unveils the best industry practices and how managers can become expert strategists and analysts to better measure and enhance corporate performance and their businesses. This book is ideal for analysts, executives, managers, entrepreneurs, researchers, students, industry professionals, stakeholders, practitioners, academicians, and others interested in the strategic analytics domain and how it can be applied to complex business dynamics. © 2021 by IGI Global. All rights reserved. Using strategy analytics to measure corporate performance and business value creation  ",Strategic alignment
1063,Data Visualization on the Life Cycle of Science and Technology Projects,"Science and technology project management system is a fundamental tool in modern governments which is required for the effective management of research arrangement, financial resources, project implement and researcher communication. However, despite its successful development, there are still many visualized application scenarios that require customized methods to explore data integration, and to establish visualization of project life cycle. To that end, this paper focuses on science and technology project data model and business data model based on four phases of management workflow. Furthermore, we propose a project life cycle model, a three-layered data structure for combining all the data of a project, to facilitate dynamic integration among project data and business data. In addition, we present a system architecture including its functional layers and data flow, which improves project life cycle visualization, and especially displays the details of project and business data in a prototype web page. This research contributes to project management business visualization by providing data models and system architecture, and helps project managers in making decisions, managing business operations and tracking the progress of tasks. © 2021 IEEE. Data Visualization on the Life Cycle of Science and Technology Projects business visualization; data model; project life cycle; science and technology project Advanced Analytics; Big data; Cloud computing; Computer architecture; Data integration; Data visualization; Life cycle; Project management; Visualization; Websites; Application scenario; Business visualization; Effective management; Financial resources; Project life cycle; Science and Technology; Science and technology project managements; System architectures; Information management",Financial management
1067,Project Management for a Machine Learning Project,"We will briefly revisit classical project management approaches in the oil and gas sector before giving a brief introduction of agile and scrum. It will be made clear how these approaches differ and why scrum might be the better choice when applied to machine learning and data science projects.The scrum framework with its most relevant components will be explained briefly using a real world example, while stressing the importance of management and change culture while executing these machine learning projects. At last, we will show what is important when scaling your ideas from simple pilot to a product that will be adopted in your entire organization. © 2021 Elsevier Inc. All rights reserved. Project Management for a Machine Learning Project agile; change management; digitalization; machine learning; oil and gas industry; project management; scrum ",Strategic alignment
1070,Design of Project Cost Information Management System Based on Intelligent Construction,"With the rapid development of computer science and technology, artificial intelligence technology has penetrated into all disciplines and fields. In this paper, the application of artificial intelligence in the field of valuation is systematically studied. Taking the land price of a residential land sample as an example, the digital land price model based on Surfer is established, and further research ideas are put forward. © Published under licence by IOP Publishing Ltd. Design of Project Cost Information Management System Based on Intelligent Construction Artificial intelligence technology; Surfer; the field of valuation Artificial intelligence; Data Science; Project management; Artificial intelligence technologies; Computer science and technologies; Intelligent constructions; Land prices; Project cost; Information management",Value management
1071,Digital transformation of the construction design based on the building information modeling and internet of things,"This study is devoted to the problem of digital transformation in the construction industry. An original scheme of systematic approach to digital modeling of the design of a construction enterprise is proposed. The integration of Building Information Modeling (BIM) and Internet of Things (IoT) for digital modeling in design activity is analyzed in detail. The concept of introducing lifecycle management system construction objects using BIM implementation in directions renovation projects is proposed. The necessity of evolution of information technology IoT from smart things to smart planet is presented.The basic structure of the BIM platform is described, which consists of four components. There are Cloud Computing, Big Data analytics, Internet of Things and Blockchain information technologies. The result of the study is a model of a digital project company for management of the life cycle of a construction object. © 2021 CEUR-WS. All rights reserved. Digital transformation of the construction design based on the building information modeling and internet of things Big Data Analytics; BIM; Blockchain; Building Information Modeling; Construction design; Digital transformation; Internet of Things; IoT Big data; Blockchain; Construction; Construction industry; Data Analytics; Information theory; Internet of things; Life cycle; Metadata; Project management; Structural design; Block-chain; Building Information Modelling; Construction design; Construction enterprise; Design activity; Digital modeling; Digital transformation; Enterprise IS; Lifecycle management; Architectural design",Financial management
1073,Features of creating of employees' working hours interactive system,"This article analyzes the existing technologies and software tools for accounting of working time of employees, which showed the relevance of the study and identified the main shortcomings of existing approaches. During the research, a mathematical description of the subject area was performed using the algebra of algorithms, which provided the means to minimize the created models by the number of unfiterms, and thus provided the means to display and optimize the system structure. An object-oriented design of the software system was performed, which consisted of constructing a set of diagrams (a diagram of use cases, classes and activities) according to the UML standard. In accordance with the analysis of available design methods and technologies, a system was developed that is designed to visually monitor the working time of the employee and track related parameters. Further research will be aimed at creating additional software modules (payroll, biometric control), their verification and coordination of operation. © 2021 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0). CEUR Workshop Proceedings (CEUR-WS.org) Features of creating of employees' working hours interactive system Algebra of algorithms; Buddy punch; Monitoring; Project management; Working time Computer software; Data Science; Machine learning; Verification; Wages; Algebra of algorithms; Biometric control; Interactive system; Mathematical descriptions; Object oriented design; Software modules; Software systems; System structures; Object oriented programming",Monitoring and control
1074,"8th International Conference on Augmented Reality, Virtual Reality and Computer Graphics, AVR 2021","The proceedings contain 52 papers. The special focus in this conference is on Augmented Reality, Virtual Reality and Computer Graphics. The topics include: Immersive Insights: Virtual Tour Analytics System for Understanding Visitor Behavior; Immersive VR as a Promising Technology for Computer-Supported Mindfulness; AR Scribble: Evaluating Design Patterns for Augmented Reality User Interfaces; interdisciplinary Collaboration in Augmented Reality Development - A Process Model; interoperable Dynamic Procedure Interactions on Semantic Augmented Reality Browsers; Creating Immersive Play Anywhere Location-Based Storytelling Using Mobile AR; efficient Augmented Reality on Low-Power Embedded Systems; visualizing Building Energy Measurement Data in Mixed Reality Applying B.I.M.; VIAProMa: An Agile Project Management Framework for Mixed Reality; developing a Gesture Library for Working in a Virtual Environment; the Experience “Mondrian from Inside”. An Immersive and Interactive Virtual Reality Experience in Art; Initial Evaluation of an Intelligent Virtual Museum Prototype Powered by AI, XR and Robots; virtual Reality in Italian Museums: A Brief Discussion; uncovering the Potential of Digital Technologies to Promote Railways Landscape: Rail to Land Project; mobile Extended Reality for the Enhancement of an Underground Oil Mill: A Preliminary Discussion; design of a Serious Game for Enhancing Money Use in Teens with Autism Spectrum Disorder; proposed System for Orofacial Physiotherapy Based on a Computational Interpretation of Face Gestures to Interact with a 3D Virtual Interface; A Virtual Reality Based Application for Children with ADHD: Design and Usability Evaluation; a New Technique of the Virtual Reality Visualization of Complex Volume Images from the Computer Tomography and Magnetic Resonance Imaging. 8th International Conference on Augmented Reality, Virtual Reality and Computer Graphics, AVR 2021  ",Risk management
1076,An investigation into the impact of artificial intelligence on the future of project management,"The purpose of the study is to investigate the impact of Artificial Intelligence on the future of Project Management. This study provides detailed conceptual information about Artificial Intelligence and different perspectives. Artificial Intelligence is defined as the new technical discipline, which would develop an application system, a technological method in order to simulate the expansion and extension of human intelligence. This research is a review that discusses how artificial intelligence affects project management. The paper has discussed various benefits of AI adoption and its implementation. The results show that technology and AI cannot replace the human mind. Machine and other AI robots can automate tools and tasks, but at the end of the day, machines need human help to operate and monitor. © 2021 IEEE. An investigation into the impact of artificial intelligence on the future of project management Artificial Intelligence; Automation; Machine Learning; Project Management Data Science; End effectors; Project management; Application systems; End of the days; Human intelligence; Human mind; Technological methods; Artificial intelligence",Value management
1077,Improving engineering students’ writing/presentation skills using laboratory/mini-project report,"Effective learning has many and different disciplines, one of them is the improvement of students’ soft skills which include different abilities, such as presentation, communicating, teamwork, time/project management and effective writing. This paper addresses the improvement of writing skills, particularly when writing a technical laboratory or mini-project report is used. The report should contain all the factors that enhance students’ writing using the appropriate language and conventions. Here, we investigate the students’ performance and response when applying the proposed guidelines and recommendations for the suggested structure of mini-projects or laboratory reports. The analysis contains a discussion of a case study conducted over 12 consecutive terms to assess student performance in four different courses using the proposed report structure. Moreover, it includes a discussion about the students’ response for the questionnaire which explores students’ satisfaction with the assigned lab tasks and proposed structure for five consecutive terms for a specific course, namely electrical machines. © The Author(s) 2019. Improving engineering students’ writing/presentation skills using laboratory/mini-project report ABET; academic analytics; communication skills; effective learning; electrical machines; lab report; laboratory; presentation skills; Soft skills; writing skills Electric machinery; Human resource management; Laboratories; ABET; academic analytics; Communication skills; Effective learning; Electrical machine; lab report; Presentation skills; Soft skills; Writing skills; Students",Strategic alignment
1079,Using Technology to Structure and Scaffold Real World Experiential Learning in Distance Education,"Team projects allow students to apply their technical skills to a real-world context and build twenty-first century competencies, including teamwork, project management and communication skills. However, the complex challenges that such experiential learning projects present for students and faculty can limit the scale of implementation. This article argues that Virtual Business Projects (VBP), a model of team-based experiential learning where teams of students complete a virtual business project for an industry sponsor, can mitigate these problems by leveraging instructional technology and learning analytics. The VBP model is deployed in multiple universities, which have provided more than 2500 Virtual Business Projects since 2015. We will discuss how innovative technology, embedded in thoughtful learning design, supports experiential learning by taking advantage of features such as customizable scaffolding, automated reflection and feedback loops, and learning analytics. © 2020, The Author(s). Using Technology to Structure and Scaffold Real World Experiential Learning in Distance Education Experiential learning; Higher education; Industry engagement; Instructional design; Team based projects ",Strategic alignment
1080,Digital inclusion in Nothern England: Training women from underrepresented communities in tech: A data analytics case study,"The TechUPWomen programme takes 100 women from the Midlands and North of England, particularly from underrepresented communities, with degrees or experience in any subject area, retrains them in technology and upon graduation guarantees an interview with a company. The retraining programme, developed by the Partner Universities in conjunction with the Industrial Partners, has modules at level 6/7 including: Technology: coding, data science, cyber security, machine learning, agile project management; Workplace readiness skills: public speaking, clear communication, working as a team. In this paper, we introduce, for the first time, the TechUPWomen programme, and we analyse its temporal evolution and special features via a data analytics nowcasting approach. Deepening these women's experience with applied upskilling includes one-to-one mentoring (100-100), strong networking, residentials, close industry connection with two directions (non-technical technical) and four job-focussed final tracks: business analyst, agile project manager, data scientist, developer. TechUPWomen also has significant representation of traditionally underrepresented communities, with focus on enabling instead of teaching approach. Beside the originality of the unique combination of features of the programme, this is, to the best of our knowledge, the first analysis based on data analytics of a women in tech(nology) retraining programme, based on nowcasting. Results show that the approach is effective; topic analysis shows that frequent topics include joy, BAME, networking, residential, industry, learning. © 2020 IEEE. Digital inclusion in Nothern England: Training women from underrepresented communities in tech: A data analytics case study Computer Science Education; Data Analytics; Digital Inclusion; TechUPWomen; Underrepresented Communities Data Analytics; Data Science; E-learning; Education computing; Human resource management; Personnel training; Professional aspects; Security of data; Agile project management; Business analysts; Digital inclusion; Industrial partners; Project managers; Public speaking; Teaching approaches; Temporal evolution; Project management",Monitoring and control
1082,"Interdisciplinary Conference on Innovation, Design, Entrepreneurship, And Sustainable Systems, IDEAS 2019","The proceedings contain 47 papers. The special focus in this conference is on Innovation, Design, Entrepreneurship, And Sustainable Systems. The topics include: The positive impacts of using the service design approach for expanding the innovative potential on business; low cost solution for home brewing and small brewing business using raspberry pi; relating design management and project management: Application of the pm mind map tool in the creative design process; development of the innovative design of an automatic equipment to aid in physical rehabilitation; stuttering and the use of facebook as a tool for interaction between people who stutter: A content analysis; exploratory study on the behavior of the brazilian financial market using google trends; systematic review of data that uses project e in the development of digital products focusing on user experience; system architecture of a robotics airship; affordance based design: The affordance digital stimuli tool to stimulate creativity during product design; efficient methods for system design analytics; analysis of the application of a product development method called meta project; systematic approach to develop physical models as creative stimuli in conceptual design phase; development of an automatic machine for sensor manufacturing by the gop technique; educating city: A media for social innovation; virtual reality as an educational tool for elementary school; the “mount your school” case and the contribution of design thinking in public education in brazil; faculdade zumbi dos palmares case study of racial integration in the advertising area; engrena ita: Alliance for innovation prospection on gear technologies; preface. Interdisciplinary Conference on Innovation, Design, Entrepreneurship, And Sustainable Systems, IDEAS 2019  ",Strategic alignment
1084,Competitive intelligence: a prescription for US health-care?,"Purpose: This paper aims to illustrate how integrating competitive intelligence (CI) into a US health-care firm can aid in information sharing and building knowledge for the organization. Design/methodology/approach: This study is exploratory using a systematic literature review to develop a conceptual model applied to the US health-care industry. Findings: This research presents key propositions of CI’s role in the CI process along with the C-suite’s role in supporting a process and culture to ultimately, gain competitive advantage through the knowledge-based view. Practical implications: With the growing volume of data, a unified system and culture within a firm is paramount. The US health-care system is a privatized industry that has become more competitive stifling information sharing. The need for prompt and accurate decision-making has become an imperative. Crises, like the current COVID-19 pandemic, only exacerbate the issue. This model offers a blue print for executives to build a CI function and encourage information sharing. Originality/value: Previous research has focused on the CI process and its value. Yet, little research is found on how to integrate CI into a firm and its role through the CI process. This study builds a conceptual model addressing integration and the flow of information to knowledge along with key firm dynamics to nurture the function. Although the model is applied specifically to US health care, it offers application to most any industry. © 2020, Emerald Publishing Limited. Competitive intelligence: a prescription for US health-care? Competitive intelligence; Data analytics; Decision-making in health-care; Learning organizations; Strategic management ",Strategic alignment
1088,Factors that influence the selection of a data science process management methodology: An exploratory study,"This paper explores the factors that impact the adoption of a process methodology for managing and coordinating data science projects. Specifically, by conducting semi-structured interviews from data scientists and managers across 14 organizations, eight factors were identified that influence the adoption of a data science project management methodology. Two were technical factors (Exploratory Data Analysis, Data Collection and Cleaning). Three were organizational factors (Receptiveness to Methodology, Team Size, Knowledge and Experience), and three were environmental factors (Business Requirements Clarity, Documentation Requirements, Release Cadence Expectations). The research presented in this paper extends recognized factors for IT process adoption by bringing together influential factors that apply to data science. Teams can use the developed process adoption model to make a more informed decision when selecting their data science project management process methodology. © 2021 IEEE Computer Society. All rights reserved. Factors that influence the selection of a data science process management methodology: An exploratory study  Data Science; Human resource management; Project management; Business requirement; Environmental factors; Exploratory data analysis; Influential factors; Knowledge and experience; Organizational factors; Process management methodology; Semi structured interviews; Information management",Value management
1089,Data-Driven Machine Learning Approach to Integrate Field Submittals in Project Scheduling,"Construction projects are data-rich environments. However, those data are usually captured for site-specific reasons, e.g., the filing and approval of inspection requests, with little regard to how they can be leveraged for improved project management. Typically, scheduling techniques rely on general probability estimates, which do not capture the details of the site processes causing schedule deviations. This paper illustrates how machine learning techniques can mine project data to forecast delay in the midst of the project. The proposed method uses concrete pouring requests as an example of a site data stream and implements a random forest predictive model to forecast the likelihood of acceptance for these requests. Embedded in the proposed approach is an analysis that allows for the addition of probabilistic time delays associated with the forecast of rejected requests. The methodology was tested on a real-world case study, allowing for the comparison between a project duration estimate based on critical path method (CPM) with static buffers and a project duration obtained using the proposed method. The results show a difference of 10% between the two durations. The paper shows how using data streams from a construction site with machine learning techniques can enhance project duration estimates in execution. © 2020 American Society of Civil Engineers. Data-Driven Machine Learning Approach to Integrate Field Submittals in Project Scheduling Data analytics; Field submittals; Machine learning; Scheduling Decision trees; Forecasting; Machine learning; Predictive analytics; Scheduling; Turnaround time; Construction projects; Critical path method; Machine learning approaches; Machine learning techniques; Predictive modeling; Probability estimate; Project scheduling; Scheduling techniques; Data streams",Strategic alignment
1090,Machine Learning Approach to Failure Mode Prediction of Reinforced Concrete Infilled Frames,"Earthquake damage assessment studies conducted throughout the world have already established the importance of considering the contribution of reinforced concrete infilled frames in the response of structures subjected to sudden lateral loads. Still, much clarity needs to be made on the behaviour, and failure mechanisms of RC infilled frames when subjected to such large and sudden earthquake loads. A data-driven machine learning approach to the prediction of failure modes of RC infilled frames is suggested in this paper. An exhaustive database consisting of experimental results done throughout the world was gathered. A failure mode classification system consisting of three predominant failure modes is proposed. Suitable parameters are identified for the purpose of machine learning modelling. Machine learning algorithms like AdaBoost, CatBoost, KNN, Decision Trees were used to predict the failure modes. An open-source dynamic model is created, which could be updated once new data is available from experiments. Google provides a free TensorFlow enabled Jupyter notebook for machine learning (Google Colabs). The same was used in this study as it supports remote access from different locations, and the model would always remain in the cloud, making it instantly accessible. Three performance measures were used in this study to evaluate the performance of the various machine learning models: accuracy, precision, and recall. The results obtained indicate that for complex structural interaction problems having (a number of dependent parameters) machine learning modelling techniques, in which the dataset is allowed to speak for itself, can be successfully employed. © 2021, Springer Nature Switzerland AG. Machine Learning Approach to Failure Mode Prediction of Reinforced Concrete Infilled Frames Data-driven modelling; Earthquake; Lateral loads; Machine learning; RC infilled frame Adaptive boosting; Concrete construction; Damage detection; Decision trees; Earthquakes; Failure modes; Forecasting; Machine learning; Predictive analytics; Project management; Reinforced concrete; Safety engineering; Structural design; Structural frames; Earthquake damages; Failure mode classifications; Machine learning approaches; Machine learning models; Modelling techniques; Performance measure; Prediction of failures; Structural interactions; Failure (mechanical)",Strategic alignment
1092,The promise of implementing machine learning in earthquake engineering: A state-of-the-art review,"Machine learning (ML) has evolved rapidly over recent years with the promise to substantially alter and enhance the role of data science in a variety of disciplines. Compared with traditional approaches, ML offers advantages to handle complex problems, provide computational efficiency, propagate and treat uncertainties, and facilitate decision making. Also, the maturing of ML has led to significant advances in not only the main-stream artificial intelligence (AI) research but also other science and engineering fields, such as material science, bioengineering, construction management, and transportation engineering. This study conducts a comprehensive review of the progress and challenges of implementing ML in the earthquake engineering domain. A hierarchical attribute matrix is adopted to categorize the existing literature based on four traits identified in the field, such as ML method, topic area, data resource, and scale of analysis. The state-of-the-art review indicates to what extent ML has been applied in four topic areas of earthquake engineering, including seismic hazard analysis, system identification and damage detection, seismic fragility assessment, and structural control for earthquake mitigation. Moreover, research challenges and the associated future research needs are discussed, which include embracing the next generation of data sharing and sensor technologies, implementing more advanced ML techniques, and developing physics-guided ML models. © The Author(s) 2020. The promise of implementing machine learning in earthquake engineering: A state-of-the-art review earthquake engineering; Machine learning; seismic fragility assessment; seismic hazard analysis; structural control; system identification and damage detection Computational efficiency; Damage detection; Data Science; Data Sharing; Decision making; Earthquakes; Engineering education; Engineering geology; Machine learning; Materials handling; Project management; Structural dynamics; Construction management; Earthquake mitigations; Hierarchical attributes; Science and engineering; Seismic hazard analysis; State-of-the art reviews; Traditional approaches; Transportation engineering; algorithm; artificial intelligence; computer simulation; dynamic response; earthquake engineering; machine learning; seismic response; structural analysis; Earthquake engineering",Value management
1093,Bayesian belief network-based project complexity measurement considering causal relationships,"This research proposes a Bayesian belief network-based approach to measure the project complexity in the construction industry. Firstly, project complexity nodes are identified for model development based on the literature review. Secondly, the project complexity measurement model is developed with 225 training samples and validated with 20 test samples. Thirdly, the developed measurement model is utilized to conduct model analytics for sequential decision making, which includes predictive, diagnostic, sensitivity, and influence chain analysis. Finally, EXPO 2010 is used to testify the effectiveness and applicability of the proposed approach. Results indicate that (1) more attention should be paid on technological complexity, information complexity, and task complexity in the process of complexity management; (2) the proposed measurement model can be applied into practice to predict the complexity level for a specific project. The uniqueness of this study lies in developing project complexity measurement model (PCMM) with the because-effect relationships taken into account. This research contributes to (a) the state of knowledge by proposing a method that is capable of measuring the complexity level under what-if scenarios for complexity management, and (b) the state of practice by providing insights into a better understanding of causal relationships among influencing factors of complexity in construction projects. © 2020 The Author(s). Bayesian belief network-based project complexity measurement considering causal relationships Bayesian belief network; Influence chain analysis; Project complexity measurement model (PCMM); Sensitivity analysis Bayesian networks; Computational complexity; Construction industry; Decision making; Predictive analytics; Project management; Sensitivity analysis; Because-effect relationships; Chain analysis; Complexity management; Construction projects; Information complexity; Project complexity; Sequential decision making; Technological complexity; Complex networks",Strategic alignment
1094,Improving agile development from perspective of design-informing model,"Agile development is usually used to solve the problem of inflexibility which the other project management models, like the Waterfall model and the V model, may meet. However, conventional agile development still faces many problems under rapid iterations. Frequent iteration lacks efficient communication a large community, resources to share information, and quality of product. For better project management, this article describes a method, called the design-informing model (DIM), to improve the agile development. The design-informing model consists of four parts, user mental mode, developer mental model, system model, and environmental model. At the end of the paper, some discussions will be conducted to evaluate DIM in practice. © 2020 IEEE Improving agile development from perspective of design-informing model Agile development; Model optimization; Project management; Software engineering Data Science; Iterative methods; Agile development; Efficient communications; Environmental model; Improving agile development; Mental model; Quality of product; System modeling; Waterfall model; Project management",Governance
1095,"Innovation, Social Networks, and Service Ecosystems: Managing Value in the Digital Economy","This book examines the ways in which value is created in the digital economy from a social networks and service ecosystems perspective. Focusing on innovation, this project explores analytics, Big Data, and privacy with respect to service management and value creation. It debunks these technology-centric buzzwords by relating cross-disciplinary research topics from seminal sociology, business, management, marketing, information systems, organizational, and technology theory under the common theme of plasticity, which is the ability of a system to take and retain form. A keen understanding of plasticity is the route to success in the digital economy. This book, aimed at academics, graduate students and practitioners in fields related to innovation, service research, and strategic management, offers a holistic perspective on innovation that is informed by scholarly research from multiple disciplines. © The Editor(s) (if applicable) and The Author(s), under exclusive licence to Springer Nature Switzerland AG 2020. Innovation, Social Networks, and Service Ecosystems: Managing Value in the Digital Economy artificial intelligence; big data; digital platforms; innovation ecosystems; innovation management; institutional theory; internet of things; machine learning; service innovation; systems thinking; value proposition ",Strategic alignment
1096,Teaching software engineering methods with agile games,"With agile methodology and international collaboration having become industry standards not only in software engineering, how would we enable students in higher education to gather these experiences? One possible approach presents our intensive course on agile software development, applying games in engineering education, within the framework of a one-week pan-European exchange program. First, we briefly introduce the program that is held twice a year, offering about 60 scientific intensive courses in each session. Topics come from the research fields at the partner institutions and focus on engineering, natural sciences, and mathematics. Roughly 4, 000 students from 24 universities in 15 European countries participate in the program annually. Second, we report on the concept and design of our intensive course on agile software development. We apply a flipped classroom setting to acquire the basic knowledge; we use game-based learning for in-class exercises; we foster project-based and collaborative learning for practical implementation. Finally, we outline the project work in our course and share the most inspiring student projects diving into data-driven visualizations and learning analytics. Thus, the participants gain insights into the real-life application of agile project management methods and international collaboration in heterogeneous teams despite the limited course length. © 2020 IEEE. Teaching software engineering methods with agile games Agile Methodologies Teaching; Game-based Learning; International Student Teams; Project-based Learning; Teaching case Engineering education; Human resource management; International cooperation; Project management; Software design; Students; Agile Methodologies; Agile project management; Agile software development; Collaborative learning; Game-based Learning; International collaborations; Partner institutions; Real-life applications; Curricula",Risk management
1097,Core competencies for clinical informaticians: A systematic review,"Background: Building on initial work carried out by the Faculty of Clinical Informatics (FCI) in the UK, the creation of a national competency framework for Clinical Informatics is required for the definition of clinical informaticians’ professional attributes and skills. We aimed to systematically review the academic literature relating to competencies, skills and existing course curricula in the clinical and health related informatics domains. Methods: Two independent reviewers searched Web of Science, EMBASE, ERIC, PubMed and CINAHL. Publications were included if they reported details of relevant competencies, skills and existing course curricula. We report findings using the Preferred Reporting Items for Systematic Reviews and Meta-Analysis (PRISMA) statement. Results: A total of 82 publications were included. The most frequently used method was surveys (30 %) followed by narrative descriptions (28 %). Most of the publications describe curriculum design (23 %) followed by competency definition (18 %) and skills, qualifications & training (18 %). Core skills surrounding data, information systems and information management appear to be cross-cutting across the various informatics disciplines with Bioinformatics and Pharmacy Informatics expressing the most unique competency requirements. Conclusion: We identified eight key domains that cut across the different sub-disciplines of health informatics, including data, information management, human factors, project management, research skills/knowledge, leadership and management, systems development and evaluation, and health/healthcare. Some informatics disciplines such as Nursing Informatics appear to be further ahead at achieving widespread competency standardisation. Attempts at standardisation for competencies should be tempered with flexibility to allow for local variation and requirements. © 2020 Elsevier B.V. Core competencies for clinical informaticians: A systematic review Bioinformatics; Clinical; Core competencies; Health; Healthcare data science; Informatics; Pharmacy; Requirements; Skills Curriculum; Humans; Information Management; Leadership; Medical Informatics; Nursing Informatics; Professional Competence; Curricula; Health; Medical informatics; Project management; Research and development management; Standardization; Academic literature; Clinical informatics; Core competencies; Curriculum designs; Nursing informatics; Systematic Review; Systems and information; Systems development; bioinformatics; Cinahl; curriculum; data science; Embase; human; informatician; information system; leadership; medical informatics; Medline; narrative; nursing informatics; pharmacy (discipline); review; skill; standardization; systematic review; Web of Science; medical informatics; meta analysis; nursing informatics; professional competence; Information management",Financial management
1098,Leveraging cloud-based analytics in active well defense projects and automated pressure response analyses,"Fracture-driven interactions in horizontal wells are receiving considerable attention due to the costly negative effects on the full field development of unconventional reservoirs. Operators have tested various methods to minimize these interactions. Active well defense (AWD) describes the process of pumping into existing wells while completing new wells in a unit. Detailed evaluations of active well defense projects are cumbersome due to the extremely large volume of data generated from multiple sources, further complicated by the fact that the data are often stored in variable formats. This paper demonstrates that near real-time evaluation of an active well defense project is possible. Minimization of fracture-driven interactions has been accomplished by a two-fold approach: optimization of the completion design for the new wells and improvements to the active defense process. Building upon previous successful projects (Bommer et al. 2017; Bommer and Bayne 2018), this case study is based upon a new 16-well data set. The workflow developed allows near real-time optimization of the defense process. An improved understanding of the impact of design and execution parameters (rates, pressures, fluids, diverters) on fracture-driven interactions is possible using only the time-series data traditionally gathered during fracturing and well defense operations. Increasing the number of fracture initiation points along the lateral by maximizing perforation cluster efficiency is the first step toward minimizing fracture-driven interactions. A common tool is the use of dynamic diversion. Operators apply various diversion techniques in multi-stage fracturing to increase cluster efficiency. The ability to assess diverter performance in time-series data is valuable when optimizing fracture operations. Active well defense is the second step in minimizing fracture-driven interactions (FDI). In this case study legacy wells are defended by pumping treated water into the legacy wells while completing new wells in the unit. FDIs are monitored with high resolution gauges while the pump-in rates into the legacy wells are dynamically adjusted based upon the pressure responses. Post-project evaluations involve multiple time-series data streams containing an extremely large amount of data. The raw data (.CSV files) are collected and analyzed using a cloud-based application optimized for time-series frac data. Combining the frac treatment data from the new well with the legacy well defense data and applying advanced analytics techniques, it is possible to identify trends and quantify the effectiveness of the active well defense process. Finally, well records and historical production data are combined with the treatment data to demonstrate the overall economic benefit of the process. © 2020, Society of Petroleum Engineers Leveraging cloud-based analytics in active well defense projects and automated pressure response analyses  Data Analytics; Fracture; Horizontal wells; Hydraulic fracturing; Network security; Petroleum reservoir evaluation; Project management; Pumps; Time series; Water treatment; Case-studies; Cloud-based; Defence programs; Field development; Large volumes; Near-real time; Pressure response; Response analysis; Time-series data; Unconventional reservoirs; Efficiency",Risk management
1099,Using hybrid modelling to simulate and analyse strategies,"Purpose: This paper aims to explore the applicability and strengths of proposing the three-paradigm hybrid simulation (HS) approach to developing and analysing strategies. The objective of the modelling effort is to conceptually illustrate its use in strategic planning by combining with the threat-opportunity-weakness-strength (TOWS) matrix, which builds a bridge between strategic management with the operations research community. The authors also aim to introduce a modelling framework to help model designers to apply HS to their own business issues. Design/methodology/approach: The paper presents a process to develop a HS model associated with the development of strategies using the TOWS matrix. Findings: After developing the model and testing four strategies, the best option for the supermarket to increase market share and sales is implementing the strength–opportunity strategy, which involves online shopping to adapt to the digital world. Research limitations/implications: First, some modelling assumptions are used to simplify the development process, but they need further validation. Second, the real data collection is limited. Third, the personal learning edition of the simulation software is not a comprehensive version and has some limitations. Practical implications: The hybrid model and the scenario planning introduced, in this study, could allow decision makers to rehearse the potential strategy before actual implementation. The framework is easy to implement to other business and industry. Originality/value: This study links HS with strategic management, which has not been performed previously and evaluates the capability of HS in strategic planning. The functionality of the modelling platform has been tested for simulating a completely dynamic system. © 2019, Emerald Publishing Limited. Using hybrid modelling to simulate and analyse strategies Business analytics; Business strategy; Modelling; Simulation ",Strategic alignment
1100,Software requirements elicitation techniques selection method for the project scope management,"Project Scope Management is one of the ten knowledge areas described in PMBOK. It refers to the set of processes that ensures a project-s scope is accurately defined and mapped. Elicitation is a critical part of the “Collect Requirements” process of the Scope Management that helps to derive and extract information from stakeholders or other sources. The results of elicitation are used as inputs for requirement analysis and management activities. Multiple elicitation techniques may be applied alternatively or in conjunction with other techniques to accomplish the elicitation. Business analysts can modify existing techniques or create new ones to adjust the project context. The selection of the best-suited techniques influences the business analysis approach, which is an important part of the scope management plan. This paper is intended to analyze the current practice of elicitation techniques application in the software development projects, define factors influencing technique selection based on the two-classification Machine Learning model, and predict the usage of a particular elicitation technique depending on the project attributes and business analyst background. We conducted a survey study involving 328 specialists from Ukrainian IT companies. Gathered data was used to build and evaluate the prediction models. © 2021 Copyright for this paper by its authors. Software requirements elicitation techniques selection method for the project scope management Business rule analysis; Collect requirements; Elicitation technique; Machine learning; Observation; Project scope management; Prototyping; Requirements management plan Application programs; Predictive analytics; Project management; Requirements engineering; Elicitation techniques; Extract informations; Machine learning models; Management activities; Requirement analysis; Software development projects; Software requirements; Technique selection; Software design",Strategic alignment
1102,Innovation Project Risk Analytics: A Preliminary Finding:  Use risk analytics for new product development for high-risk innovation projects.,[No abstract available] Innovation Project Risk Analytics: A Preliminary Finding:  Use risk analytics for new product development for high-risk innovation projects.  ,Risk management
1103,Strategic Design as an Effective Tool for Managing Digital Development of a Company,"This article is devoted to a new strategic management tool—strategic design. Modern conditions with rapidly changing technologies and consumer preferences, changing supply chains, emerging new markets, products, and new product safety requirements require the enterprise management strategy development to have adequate modern tools as well. The authors prove that such a tool can be a strategic design, which is based on design thinking and in-depth comprehensive analytics. It is analytics, as well as tracking the current and future customer needs, that allows the company to increase its competitiveness in both existing and new markets, including digital ones. This paper discusses strategic design stages preceding the development of a digital development strategy for a company. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. Strategic Design as an Effective Tool for Managing Digital Development of a Company Digital development; Roadmap; Strategic design; Strategic management Commerce; Supply chains; Consumer preferences; Design thinking; Development strategies; Effective tool; Enterprise management; Product safety; Strategic design; Strategic management tools; Planning",Strategic alignment
1105,A comparison of computational thinking approaches in hci-seo design: Implications to teaching and learning ste(a)m,"Search engine optimization has often been through tagging (metadata descriptions) and appropriate placement of these metadata in inherent document structures e.g. XML. This paper presents a complement whereby the structure and information design based on design thinking and computational thinking results in more effective scoping of user requirements and leaner, agile design. This form of human-computer interaction-search engine optimization is much used in successful e-commerce websites due to Data Science. Comparison between the standard 4 CT aspects approach and Brennan and Resnick’s 3 CT aspects approach and implications to STE(A)M teaching and learning are investigated through a meta-analysis of two Project Management course assignments. Significance of the paper is direct link and greater specificity between design thinking, computational thinking, human-computer interaction, Project Management and search engine optimization within an entrepreneurial project management framework. © 2020, The Education University of Hong Kong. All rights reserved. A comparison of computational thinking approaches in hci-seo design: Implications to teaching and learning ste(a)m Computational thinking approaches; Design optimization; Design thinking; Higher education; STE(A)M ",Governance
1106,Midst: An enhanced development environment that improves the maintainability of a data science analysis,"With the increasing ability to generate actionable insight from data, the field of data science has seen significant growth. As more teams develop data science solutions, the analytical code they develop will need to be enhanced in the future, by an existing or a new team member. Thus, the importance of being able to easily maintain and enhance the code required for an analysis will increase. However, to date, there has been minimal research on the maintainability of an analysis done by a data science team. To help address this gap, data science maintainability was explored by (1) creating a data science maintainability model, (2) creating a new tool, called MIDST (Modular Interactive Data Science Tool), that aims to improve data science maintainability, and then (3) conducting a mixed method experiment to evaluate MIDST. The new tool aims to improve the ability of a team member to update and rerun an existing data science analysis by providing a visual data flow view of the analysis within an integrated code and computational environment. Via an analysis of the quantitative and qualitative survey results, the experiment found that MIDST does help improve the maintainability of an analysis. Thus, this research demonstrates the importance of enhanced tools to help improve the maintainability of data science projects. © 2020, SciKA. Midst: An enhanced development environment that improves the maintainability of a data science analysis Data science; Data science development environment; Maintainability; Project management; Visual programming ",Monitoring and control
1107,Multi objective optimization of mix proportion of foam concrete,"As an environmentally friendly material, foamed concrete (FC) has attracted wide attention in the field of green concrete. However, a large number of trial tests are needed to design its proper performance. Intelligent prediction of the properties of foamed concrete is an urgent problem to be solved. Based on the experimental data, this paper uses genetic algorithm to optimize the parameters of support vector regression model (GA-SVR). Taking the raw materials (water, cement, sand, fly ash, lime, foaming agent) as input variables and taking strength, dry density and water absorption as the prediction objectives, the GA-SVR prediction model of compressive strength of foamed concrete is established. Then, the NSGA-II and TOPSIS methods are used for multi-objective optimization of the foam concrete to obtain the optimal mix proportion. The result shows that the mix proportion based on multi-objective optimization algorithm can effectively improve the strength of foam concrete, reduce the quality and water absorption. The results presented could facilitate and enhance the use of SVR and NSGA-II methods in other civil engineering-related problems. © 8th International Symposium on Project Management, ISPM 2020. All rights reserved. Multi objective optimization of mix proportion of foam concrete Foam concrete; GA-SVR; Multi-objective optimization; NSGA-II Autoclaved aerated concrete; Compressive strength; Concrete mixtures; Fly ash; Forecasting; Genetic algorithms; Lime; Predictive analytics; Project management; Support vector regression; Water absorption; Green concrete; Input variables; Intelligent prediction; Mix proportions; Prediction model; Support vector regression models; TOPSIS method; Urgent problems; Multiobjective optimization",Monitoring and control
1108,Project Management for Innovation Projects – State of Art,"It has been largely researched what the factors which determine successful project management are. Most of the research show a huge dependency between project success and type of the projects in terms of size, industry, scope, market, etc. Much research analyze the type of project management and all specifics related to its application. This paper focuses on project management specifically for innovation projects as they are extremely different types of projects which require distinct project management approach. The purpose of the research is to determine the current state of art of the topic by revealing the research achievements in the science literature. The research applies a systematic literature analysis through a traditional approach as well as an advanced technique for digital systematic literature review so to reveal the current status of the state of art of the topic Project management for innovation projects. The systematic literature analysis went through 299 research papers from the Scopus database so to uncover the current accomplishment. The results are of interest to scientists and practitioners primarily from the management sciences, but are also extremely suitable for practitioners from any other industry related to innovation development. © 2020, Springer Nature Switzerland AG. Project Management for Innovation Projects – State of Art Advanced analytics; Innovation management; Project management; Science management; Systematic literature review; Technology management Arts computing; Innovation projects; ITS applications; Literature analysis; Project success; Research achievements; Research papers; Systematic literature review; Traditional approaches; Project management",Stakeholder management
1110,Construction Collaborative Management Method Based on BIM and Control Calculation,"BIM Technology is an important method to solve the problem of coordination in the construction process of engineering projects. The construction project construction collaborative management method combined with control computing can be visualized in real-time and can dynamically control the calculation and simulation process. Through the analysis of the important role of the combination of control type calculation and BIM for construction collaborative management, the application of based on BIM theory, engineering collaborative management theory, and multi-objective optimization theory, the multi-objective optimization model based on IFC is established, and the combination scheme of control computing, coevolutionary algorithm, and BIM is designed to realize the calculation of multi-objective optimization model. The collaborative management model of the construction stage is constructed to integrate the multi-objective optimization process and the collaborative workflow of all participants in the construction stage. The construction collaborative model can effectively realize the collaborative work among the participants in the construction stage, the collaborative optimization of multi-objective, and the real-time visualization and real-time control of the process. © 2021, The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. Construction Collaborative Management Method Based on BIM and Control Calculation BIM; Collaborative management; Construction management; Control computing Advanced Analytics; Architectural design; Big data; Computation theory; Embedded systems; Multiobjective optimization; Project management; Real time control; Smart city; Co-evolutionary algorithm; Collaborative management; Collaborative optimization; Collaborative workflow; Construction process; Construction projects; Multi-objective optimization models; Real time visualization; Process control",Strategic alignment
1111,The performance effects of big data analytics and supply chain ambidexterity: The moderating effect of environmental dynamism,"The importance of big data analytics–enabled dynamic capability has been at the forefront of research for information systems management, operations management, and strategic management community. Prior studies have reported on the influence of big data analytics–enabled dynamic capability (BDA) for improved organizational agility and organizational performance, but there has been a paucity of literature regarding the role of big data analytics–enabled dynamic capability in untangling the supply chain ambidexterity dilemma and organizational performance. To address these research gaps, this paper draws on the dynamic capability view of the organization under the contingent effect of environmental dynamism. We tested our research hypotheses using 281 surveys, gathered using a pre-tested questionnaire. Our results suggest that BDA has positive effects on improving supply chain agility (SCAG), supply chain adaptability (SCAD) and performance measures (cost performance and operational performance). However, we noted that hypotheses regarding the moderating effect of environmental dynamism (ED) on the paths joining BDA and SCAG/SCAD were not supported. To address these unexpected results, we conducted post hoc analysis to explain the rationale behind the insignificant moderating effects of ED on the paths joining BDA and SCAG/SCAD. We found that the effects of BDA on SCAG/SCAD were higher under intermediate levels of environmental dynamism but comparatively weak when the environmental dynamism is low or high. Hence, we can argue that big data analytics can help enhance supply chain agility, supply chain adaptability, and organizational performance, but these effects are contingent upon the level of environmental dynamism. Moreover, a non-linear, inverse YOU-shaped moderating effect of environmental dynamism exists. Collectively, these findings provide a theory-based understanding of the organizational level of usage of big data analytics and its effects on supply chain agility, supply chain adaptability, and organizational performance. Moreover, they further shape our understanding of how big data analytics–enabled dynamic capabilities yield differential results under the moderating effect of environmental dynamism. Hence, we believe that our results will be useful for managers who are highly optimistic about the usage of these emerging technologies and their effects on supply chain characteristics. Finally, we have outlined our study limitations and offered numerous research directions. © 2019 Elsevier B.V. The performance effects of big data analytics and supply chain ambidexterity: The moderating effect of environmental dynamism Big data analytics; Dynamic capability; Organizational performance; Supply chain adaptability; Supply chain agility; Supply chain ambidexterity Advanced Analytics; Big data; Enterprise resource management; Information management; Joining; Supply chains; Surveys; Dynamic capabilities; Emerging technologies; Environmental dynamisms; Operational performance; Organizational agility; Organizational levels; Organizational performance; Supply chain agility; Data Analytics",Value management
1112,DIY Smart Home: The Development of an Exemplary Internet of Things Infrastructure for Computer Science Education,"The generic term smart home describes among other things the increase of quality of life, security and reduction of energy consumption in our homes and workplaces through automated processes. These processes enable educators to work up realistic and tangible aspects of computing science using everyday examples. Programming, networks, communication protocols, web design and development, electrical engineering, data science, CAD and even project management can be combined with smart home in terms of content. For this reason, we have used open source tools to develop a cost-effective infrastructure that combines the above-mentioned aspects in a single kit. This kit is based on various in-house developed sensors and actuators based on the ESP8266, a Raspberry Pi as the central unit and Node-RED as the framework for linking the hardware. This hardware can be connected without writing code or previous knowledge, enables a quick experience of success while offering much potential for internal differentiation in teaching. © 2020 ACM. DIY Smart Home: The Development of an Exemplary Internet of Things Infrastructure for Computer Science Education computer science education; ESP8266; internet of things; IoT; Node-RED; smart home Automation; Computer aided design; Computer hardware; Cost effectiveness; Education computing; Energy utilization; Engineering research; Internet of things; Open source software; Project management; Automated process; Computer Science Education; Computing science; Cost effective; Open source tools; Quality of life; Sensors and actuators; Web design and development; Engineering education",Risk management
1113,4D inspection: A comprehensive platform to digitize pipeline construction inspection and generate data driven continuous improvement,"Beginning in 2018, TC Energy began an effort to digitize inspection and construction management and internally named this concept the Dynamic, Digital, Data and Diagnostics platform otherwise known as ""4D Inspection"". The 4D Inspection platform is built upon Project Consulting Service's Epilogue® energy infrastructure construction management solution and is intended to evolve inspection reporting to a digital platform to create standardized construction reporting embedded with real-time compliance validations for efficient and effective management of field construction issues, progress tracking, and over-all construction quality monitoring. Currently, this effort is nearing the end of its first year of a multi-year implementation plan, the platform is in use on projects in both Canada and the United States spanning four different time zones with more than 595 inspectors on over $7.5 billion in capital projects. Even with implementation still underway, this concept's key functionality, like automated inspection report document control, simplified photo capture with geo-tagging and automated daily progress reporting, has provided immediate benefits of more thorough, more reliable, and more efficient construction data than ever gathered using previous data collection processes. With improved data accuracy and detail, TC Energy gains insight and even foresight into how best to advance construction quality and safety while also impacting overall project costs and schedule. Copyright © 2020 ASME 4D inspection: A comprehensive platform to digitize pipeline construction inspection and generate data driven continuous improvement Analytics; Construction management; Digitization; Inspection; Mobile inspection forms; Pipeline construction; Project dashboard; Technology adoption Information services; Inspection; Offshore oil well production; Offshore pipelines; Safety engineering; Compliance validation; Construction management; Construction quality; Continuous improvements; Data collection process; Efficient construction; Energy infrastructures; Pipeline construction; Project management",Financial management
1114,Digitalization of a Systematic Literature Review Process – Lean Startup and Data Analytics Solution for Scholars,"The paper aims at analyzing how Lean Startup as a product and project management approach can be used in the context of Logic-Based Program Synthesis product development. The research is interdisciplinary and connects some technological and managerial aspects of data analytics, product development, and technology management. Both the concepts of Lean Startup and Logic-Based program synthesis have very similar approach for problem-solving and developing solutions. By combining them, the paper shows results, which lead to: 1.) Designing a Method for a common process for developing a Lean Startup Program Synthesis (LSPS) application development model and 2.) Probating a concrete application development through the identified stages. The developed application’s purpose is the digitalization of systematic literature analysis via data analytics technics and the use of Qlik Sense software. Potential readers are scholars who are interested in digitalizing and automating the performance of systematic literature analysis replacing the usual reading of full papers with a cross wording filtering amongst words in titles, abstracts and author keywords of science papers. © 2020, Springer Nature Switzerland AG. Digitalization of a Systematic Literature Review Process – Lean Startup and Data Analytics Solution for Scholars Data analytics; Lean startup; Science digitalization; Science innovation; Systematic literature review; Technology management Application programs; Computer circuits; Engineering education; Logic programming; Product development; Project management; Research and development management; Application development; Concrete applications; Developed applications; Developing solutions; Literature analysis; Logic based program synthesis; Systematic literature review; Technology managements; Data Analytics",Value management
1118,Managing complex engineering projects: What can we learn from the evolving digital footprint?,"The challenges of managing large complex engineering projects, such as those involving the design of infrastructure, aerospace and industrial systems; are widely acknowledged. While there exists a mature set of project management tools and methods, many of today's projects overrun in terms of both time and cost. Existing literature attributes these overruns to factors such as: unforeseen dependencies, a lack of understanding, late changes, poor communication, limited resource availability (inc. personnel), incomplete data and aspects of culture and planning. Fundamental to overcoming these factors belies the challenge of how management information relating to them can be provided, and done so in a cost eﬀ; ective manner. Motivated by this challenge, recent research has demonstrated how management information can be automatically generated from the evolving digital footprint of an engineering project, which encompasses a broad range of data types and sources. In contrast to existing work that reports the generation, veriﬁcation and application of methods for generating management information, this paper reviews all the reported methods to appraise the scope of management information that can be automatically generated from the digital footprint. In so doing, the paper presents a reference model for the generation of managerial information from the digital footprint, an appraisal of 27 methods, and a critical reﬂection of the scope and generalisability of data-driven project management methods. Key ﬁndings from the appraisal include the role of email in providing insights into potential issues, the role of computer models in automatically eliciting process and product dependencies, and the role of project documentation in assessing project norms. The critical reﬂection also raises issues such as privacy, highlights the enabling technologies, and presents opportunities for new Business Intelligence tools that are based on real-time monitoring and analysis of digital footprints. © 2019 Managing complex engineering projects: What can we learn from the evolving digital footprint? Big Data; Business Intelligence; Knowledge Workers; Project Management Big data; Competitive intelligence; Information analysis; Project management; Automatically generated; Enabling technologies; Knowledge workers; Management information; Project documentation; Project management method; Project management tools; Resource availability; Human resource management",Financial management
1120,Data Science Roadmapping: Towards an Architectural Framework,"The availability of big data and related technologies enables businesses to exploit data for competitive advantage. Still, many industries face obstacles while leveraging data science to overcome business problems. This paper explores the development of a roadmapping approach to address data science challenges. Towards this goal, we customize technology roadmapping by synthesizing roadmapping, big data, data science, and data-driven organization literature. The resulting data science roadmapping approach links business strategy with data-related, technological, and organizational resources. It also enables communication, stakeholder buy-in, and project prioritization. While most of the existing studies illustrate prebuilt roadmaps, this study focuses on the process of roadmapping. The application of the roadmapping process rather than a particular roadmap provides the benefits above. © 2020 IEEE. Data Science Roadmapping: Towards an Architectural Framework big data; business strategy; data science; Industry 4.0; roadmapping Big data; Competition; Decision making; Industrial management; Address datum; Architectural frameworks; Business problems; Business strategy; Competitive advantage; Project prioritization; Roadmapping; Technology roadmapping; Data Science",Stakeholder management
1121,Seeking 'strategy' in business intelligence literature: Theorizing BI as part of strategy research,"This paper connects the business intelligence (BI) literature with research in strategic management by plotting the existing research strands on BI: environmental scanning, competitive intelligence, executive information systems, and business intelligence, against the strategic dimensions of a) orientation (External vs. Internal), b) focus (Content vs. Process), and c) practice realms. The article accordingly offers a new re-conceptualization of BI as a strategic artifact across four strategic clusters: BI as a system, BI as a planned process, BI as a product, and BI as a decisional paradigm. This conceptual article contributes to the literature by integrating disparate views on BI and placing them within the content, process, and practice streams of strategy research. © 2020 SciPost Physics. All rights reserved. Seeking 'strategy' in business intelligence literature: Theorizing BI as part of strategy research Business intelligence (BI); Conceptualization; Definition; Literature review; Strategy as practice; Strategy content; Strategy process; Strategy realms ",Strategic alignment
1123,Data science in the design of public policies: dispelling the obscurity in matching policy demand and data offer,"Data Science (DS) is expected to deliver value for public governance. In a number of studies, strong claims have been made about the potential of big data and data analytics and there are now several cases showing their application in areas such as service delivery and organizational administration. The role of DS in policy-making has, on the contrary, still been explored only marginally, but it is clear that there is the need for greater investigation because of its greater complexity and its distinctive inter-organizational boundaries. In this paper, we have investigated how DS can contribute to the policy definition process, endorsing a socio-technical perspective. This exploration has addressed the technical elements of DS - data and processes - as well as the social aspects surrounding the actors’ interaction within the definition process. Three action research cases are presented in the paper, lifting the veil of obscurity from how DS can support policy-making in practice. The findings highlight the importance of a new role, here defined as that of a translator, who can provide clarity and understanding of policy needs, assess whether data-driven results fit the legislative setting to be addressed, and become the junction point between data scientists and policy-makers. The three cases and their different achievements make it possible to draw attention to the enabling and inhibiting factors in the application of DS. © 2020 The Author(s); Data Science; Policy; big data; framing, Knowledge Management; Information Systems Management; Information Management; Human Resource Management; Business Management; Strategic Management; Risk Management, Information science, Business © 2020 The Author(s) Data science in the design of public policies: dispelling the obscurity in matching policy demand and data offer Big data; Business; Business management; Data science; Framing; Human resource management; Information management; Information science; Information systems management; Knowledge management; Policy; Risk management; Strategic management ",Governance
1124,The impact of entrepreneurship orientation on project performance: A machine learning approach,"Recent studies in project management have shown the important role of entrepreneurship orientation of the individuals in project performance. Although identifying the role of entrepreneurship orientation as a critical success factor in project performance has been considered as an important issue, it is also important to develop a measurement system for predicting performance based on the degree of an individual's entrepreneurial orientation. In this study, we use predictive analytics by proposing a machine learning approach to predict individuals' project performance based on measures of several aspects of entrepreneurial orientation and entrepreneurial attitude of the individuals. We investigated this relationship using a sample of 185 observations and a range of machine learning algorithms including lasso, ridge, support vector machines, neural networks, and random forest. Our results showed that the best method for predicting project performance is lasso. After identifying the best predictive model, we then used the Bayesian Information Criterion and the Akaike Information Criterion to identify the most significant factors. Our results identify all three aspects of entrepreneurial attitude (social self-efficacy, appearance self-efficacy, and comparativeness) and one aspect of entrepreneurial orientation (proactiveness) as the most important factors. This study contributes to the relationship between entrepreneurship skills and project performance and provides insights into the application of emerging tools in data science and machine learning in operations management and project management research. © 2020 Elsevier B.V. The impact of entrepreneurship orientation on project performance: A machine learning approach Entrepreneurship orientation; Machine learning; Predictive analytics; Project performance; Supervised learning Decision trees; Forecasting; Learning algorithms; Learning systems; Predictive analytics; Project management; Supervised learning; Support vector machines; Akaike information criterion; Bayesian information criterion; Critical success factor; Entrepreneurial attitude; Entrepreneurial orientation; Machine learning approaches; Project management research; Project performance; Machine learning",Capacity management
1125,A framework for big data integration within the strategic management process based on a balanced scorecard methodology,"The purpose of this research is to study the impact of big data initiatives on strategic management processes. While the majority of strategic management disciplines have had research dedicated to the use of strategic management theories to understand how big data affect organizational performance, the body of research on big data lacks academic work capable of examining how to integrate big data into the strategic management process. The main contributions of this work are: (1) it highlights the strategic use of big data; (2) it analyses the main frameworks/models proposed by scholars that support the use of big data as a strategic management tool, and outlines this research gap; and (3) it proposes a new framework that integrates big data within the strategic management process based on a balanced scorecard methodology. © 2021 A framework for big data integration within the strategic management process based on a balanced scorecard methodology Balanced scorecard; big data; big data analytics; big data framework; business intelligence; strategic management; strategic management process ",Strategic alignment
1127,Understanding the intellectual structure and evolution of Competitive Intelligence: a bibliometric analysis from 1984 to 2017,"Competitive Intelligence (CI) is a relatively novel discipline that is generating a growing interest in the field of strategic management. Its novelty, approach and origin have meant that the definition of the CI entails different interpretations and utilities depending on the audience. This research develops a conceptual analysis of CI in literature by quantifying the bibliometric performance indicators, identifying the main authors, countries, journals and research areas and evaluating the intellectual structure and evolution of the discipline using SciMAT as bibliometric analysis software. The bibliometric performance analysis is focused on the citation-based impact of the scientific output, while the science mapping illustrates the evolution of the research themes that build the discipline through the use of bibliometric network analysis techniques. To this purpose, the publications related to CI from 1984 to 2017 available at Scopus have been retrieved (5,275 publications). Finally, it offers a framework to support future researches. © 2019, © 2019 Informa UK Limited, trading as Taylor & Francis Group. Understanding the intellectual structure and evolution of Competitive Intelligence: a bibliometric analysis from 1984 to 2017 Business intelligence; competitive intelligence; science mapping analysis; strategic intelligence Scopus; network analysis; numerical model; performance assessment; strategic approach; technological change; technological development",Monitoring and control
1128,Study on foundation pit construction cost prediction based on the stacked denoising autoencoder,"To accurately predict the construction costs of foundation pit projects, a model based on the stacked denoising autoencoder (SDAE) is constructed in this work. The influencing factors of foundation pit project construction costs are identified from the four attributes of construction cost management, namely, engineering, the environment, the market, and management. Combined with Chinese national standards and the practice of foundation pit project management, a method of the quantization of the influencing factors is presented. 60 deep foundation pit projects in China are selected to obtain 13 main characteristic factors affecting these project construction cost by using the rough set. Then, considering the advantages of the SDAE in dealing with complex nonlinear problems, a prediction model of foundation pit project construction costs is created. Finally, this paper employs these 60 projects for a case analysis. The case study demonstrates that, compared with the actual construction costs, the calculation error of the proposed method is less than 3%, and the average error is only 1.54%. In addition, three error analysis tools commonly used in machine learning (the determination coefficient, root mean square error, and mean absolute error) emphasize that the calculation accuracy of the proposed method is notably higher than those of other methods (Chinese national code, the multivariate return method, the BP algorithm, the BP model optimized by the genetic algorithm, the support vector machine, and the RBF model). Therelevant research results of this paper provide a useful reference for the prediction of the construction costs of foundation pit projects.  Copyright © 2020 Lanjun Liu et al. Study on foundation pit construction cost prediction based on the stacked denoising autoencoder  Backpropagation; Cost engineering; Errors; Forecasting; Foundations; Genetic algorithms; Learning systems; Mean square error; Predictive analytics; Support vector machines; Calculation accuracy; Characteristic factors; Chinese national standard; Construction cost managements; Determination coefficients; Foundation pit constructions; Project construction; Root mean square errors; Project management",Monitoring and control
1129,Knowledge Management Systems for Structural Engineering Design in the Era of Emerging Technological Advancements,"As construction projects become larger, more complex, and fast-paced, the role of knowledge management (KM) is fast growing in engineering firms, which are characterized as knowledge-intensive by nature. To remain competitive and ensure project success, engineers are seeking new tools and methods to capture both explicit and implicit knowledge that can then be retrieved in a timely and efficient way. This paper aims to discuss knowledge management systems (KMS) as related to structural engineers, in the context of large, international, and multi-disciplinary engineering firms. The methodology used consists of a review of literature to identify the macro, meso, and micro level factors influencing the development of structural engineering KMS. The various strategies and steps related to the development of the KMS are then discussed. The challenges of implementing such systems are further examined in light of emerging technologies and of the complex interrelations between the different entities of large international firms. The case of a top-twenty global engineering design group is presented. Furthermore, it is argued that firms often limit KMS to their technological aspects undermining the need for a detailed and structured framework to guide the KMS initiative. Technological advancements such as knowledge discovery in databases, data analytics, artificial intelligence, platforms, and web 2.0 technologies should be rather viewed as useful tools to render the KM experience faster and more user friendly. They do not replace the need to elaborate a reliable framework based on the specific needs of structural engineers, and which can act as the backbone of KMS. Firms should also adopt strategies to promote a learning organization culture and empower engineers to become actively engaged in the KMS experience, thus ensuring an optimum retention of knowledge within their organizations. © 2020 American Society of Civil Engineers. Knowledge Management Systems for Structural Engineering Design in the Era of Emerging Technological Advancements  Artificial intelligence; Data Analytics; Engineers; Knowledge based systems; Knowledge management; Structural design; User experience; Construction projects; Emerging technologies; Knowledge discovery in database; Knowledge management system; Learning organizations; Multi disciplinary engineerings; Technological advancement; Technological aspects; Project management",Financial management
1134,Proposed metamodels transformation from Predictive methodologies to Agile methodologies,"Project management models are constantly evolving, among the most widely answered models we find Agile models or else called 'Adaptive' and 'Predictive' models. A study by the Standish group confirms that Agile projects achieve the desired result three times more than projects that are carried out by conventional (predictive) Methodologies. Thus, many organizations tend to apply the new project management model or migrate from the standard model to the agile one. In this article, building on previous research, we will build on the principles of MDA to define a metamodel of Agile and Predictive Methodologies, and then we will affect a transformation that will help organizations transform project management. © 2020 IEEE. Proposed metamodels transformation from Predictive methodologies to Agile methodologies Agile Project Management; ATL; Digital transformation; MDA; Metamodeling; Predictive Project Management; Project Management; Transformation Decision support systems; Project management; Agile Methodologies; Agile models; Meta model; The standard model; Predictive analytics",Strategic alignment
1135,Data science and big data technologies role in the digital economy,"This article explores the role of Data Science and Big Data technology in the modern digital economy. The author states that large and medium companies from retail trade and service sector show increased interest in using them. These technologies are actively used by banks, mobile operators and large manufacturing companies to analyze data on equipment failures and to reduce downtime, which allows reducing costs. The role of Big Data technology is to be a liquid product and a necessary condition to increase the profitability of enterprises through personalized customer service and predictive analytics. For today's Russian digital economy, it is very important to legalize a single definition of Big Data andto achieve the emergence of special data exchanges. © 2020 Sergey V. Novikov. Data science and big data technologies role in the digital economy Big Data technology; Digital economy; Innovative technologies; Machine learning; Project management based on Data Science ",Value management
1136,Big data analytics in innovation processes: which forms of dynamic capabilities should be developed and how to embrace digitization?,"Purpose: The purpose of this paper is to analyze, from a dynamic capabilities perspective, the role of big data analytics in supporting firms' innovation processes. Design/methodology/approach: Relevant literature is reviewed and critically assessed. An interpretive methodology is used to analyze empirical data from interviews of big data analytics experts at firms within digitally related sectors. Findings: This study shows how firms leverage big data to gain “richer” and “deeper” data at the inter-sections between the digital and physical worlds. The authors provide evidence for the importance of counterintuitive strategies aimed at developing innovative products, services or solutions with characteristics that may initially diverge, even significantly, from established customer/user needs. Practical implications: The authors’ findings offer insights to help practitioners manage innovation processes in the physical world while taking investments in big data analytics into account. Originality/value: The authors provide insights into the evolution of scholarly research on innovation directed toward opportunities to create a competitive advantage by offering new products, services or solutions diverging, even significantly, from established customer demand. © 2021, Rosita Capurro, Raffaele Fiorentino, Stefano Garzella and Alessandro Giudici. Big data analytics in innovation processes: which forms of dynamic capabilities should be developed and how to embrace digitization? Big data analytics; Digitalization; Dynamic capabilities; Innovation; Strategic management ",Strategic alignment
1137,A future prospect for European collaboration on advanced analytics in economy and society,"Analytical Reasoning by applying machine learning approaches, artificial intelligence, NLP and visualizations allow to get deep insights into the different domains of various stakeholders and enable to solve complex tasks. Thereby the tasks are very heterogenous and subject of investigation in the different areas of application. These tasks or challenges should be defined by the stakeholders themselves and lead through a deep investigation to advanced analytical approaches. We therefore set up a strategic alliance of research, enterprises and societal organization with the goal of a strong collaboration to identify in a first step these challenges and workout technological solutions for each application scenario. We give in this paper a first draft of current challenges and technological advancements. The main contribution of this paper is next to an accurate description of the current challenges in the analytics domain, also the description of an agenda how these challenges can be solved. Furthermore, a process is explained, how the strategic alliance should act and organize their work to realize beneficial and useful analytical solutions. Copyright © 2020 for this paper by its authors. A future prospect for European collaboration on advanced analytics in economy and society Business intelligence; European network; Research collaboration; Strategic management; Trend analytics Artificial intelligence; Analytical approach; Analytical reasoning; Application scenario; Economy and society; Machine learning approaches; Strategic alliance; Technological advancement; Technological solution; Advanced Analytics",Strategic alignment
1138,Factors Affecting Knowledge Sharing Intentions among Construction Workers: The Case of Lebanon,"Construction knowledge is tacit; thus, it resides in the minds of its bearers. The flow of this knowledge depends more on the attitudes and intentions of individuals than it does on the wishes and desires of upper management. Consequently, exploring the drivers of these individual attitudes and intentions is critical for understanding how knowledge flows in construction organizations. This paper builds the case for studying the current state of knowledge sharing among construction workers. It also presents the preliminary findings of a survey of construction labor working on a set of projects in Lebanon. The survey focuses on the different drivers of knowledge sharing intentions among construction workers, namely social, internal, and organizational drivers. Social drivers include factors such as reciprocal relationships. Internal drivers describe individual attitudes, whereas organizational drivers relate to fairness and affiliation with the organization. Preliminary findings indicate that workers in the Lebanese construction companies have generally positive intentions to share knowledge with their colleagues. This paper is part of a research initiative that aims at using data analytics and advanced statistical models to determine the underlying relations among these drivers and their impact on knowledge sharing. The ultimate findings aim to provide regional contractors with strategies for fostering knowledge sharing thereby improving productivity on construction sites. © 2020 American Society of Civil Engineers. Factors Affecting Knowledge Sharing Intentions among Construction Workers: The Case of Lebanon  Construction industry; Data Analytics; Industrial management; Knowledge management; Surveys; Construction companies; Construction knowledge; Construction organizations; Construction sites; Construction workers; Knowledge-sharing; Research initiatives; Upper management; Project management",Strategic alignment
1140,Data Science Team Roles and Need of Data Science: A Review of Different Cases,"The paper first looks at the benefits of well-known roles and then discusses the relative lack of structured roles within the data science community, possibly because of the field’s novelty. The paper reports extensively on five case studies which discuss five separate attempts to establish a standard set of roles. The paper then leverages the findings of these case studies to discuss the use in online job posts for data science positions. While some positions often appeared, such as data scientist and software engineer, no role in all five case studies was regularly used. The paper concludes, however, by acknowledging the need to build a structure for data science workforce that students, employers, and academic institutions can use. This framework would allow organizations to more accurately employ their data science teams with the desired skills. © The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2021 Data Science Team Roles and Need of Data Science: A Review of Different Cases Big data; Data science; Data science roles; Project management Engineering; Industrial engineering; Academic institutions; Case-studies; First look; Science community; Team roles; Data Science",Value management
1143,Empowering Team Science across the Translational Spectrum with the UAB Biomedical Research Infrastructure Technology Enhancement (U-BRITE),"In response to a need for diverse computing support for translational science teams, the Informatics Institute at the University of Alabama at Birmingham (UAB) has developed a prototype platform called UAB Biomedical Research Infrastructure Technology Enhancement (YOU-BRITE). This platform provides project management functionality, high-volume data storage, access to clinical data, processing of data through custom pipelines, and high-performance computing in an environment that is compliant with privacy regulations. The project was designed and developed with the help of four biomedical sciences teams, each with their own-omics data, clinical data, and research questions. This paper describes YOU-BRITE's architecture (accessible at https://ubrite.org/) and the experience of the members of four teams who were its initial users. Our experience provides useful guidance for future data reuse and an open science model of collaborative biomedical research.  © 2020 IEEE. Empowering Team Science across the Translational Spectrum with the UAB Biomedical Research Infrastructure Technology Enhancement (YOU-BRITE) data science; high-performance computing; infrastructure; omics; research; team science Clinical research; Data integration; Data Science; Digital storage; Engineering research; Information use; Project management; User experience; Biomedical research; Biomedical science; Clinical data; High performance computing; Management functionality; Privacy regulation; Research questions; University of Alabama at Birmingham; Information management",Risk management
1144,Mo.Re.Farming: A hybrid architecture for tactical and strategic precision agriculture,"In this paper we propose an innovative architecture, called Mo.Re.Farming, for handling agricultural data in an integrated fashion and supporting decision making in the precision agriculture domain. This architecture is oriented to data analysis and is inspired by Business Intelligence 2.0 approaches. It is hybrid in that it couples traditional and big data technologies to integrate heterogeneous data, at different levels of detail, from several owned and open data sources; its goal is to demonstrate that such integration is feasible and beneficial in supporting situ-specific and large-scale analyses. The proposed architecture has been developed in the context of the Mo.Re.Farming project, aimed at providing a Decision Support System for agricultural technicians in the Emilia-Romagna region and to enable analyses related to the use of water and chemical resources. The architecture is fully deployed and serves as a hub for agricultural data in Emilia-Romagna; the integrated data are made available in open access mode and can be accessed through web interfaces and through a set of web services. The paper describes the architecture from the technological and functional points of view and discusses the Mo.Re.Farming project outcomes and lessons learnt. © 2020 Elsevier B.V. Mo.Re.Farming: A hybrid architecture for tactical and strategic precision agriculture BI 2.0; Data integration; Precision agriculture Agricultural robots; Artificial intelligence; Chemical analysis; Data handling; Decision making; Decision support systems; Information analysis; Precision agriculture; Project management; Web services; Business intelligence 2.0; Data technologies; Functional points; Heterogeneous data; Hybrid architectures; Integrated fashion; Large-scale analysis; Proposed architectures; Open Data",Governance
1145,A framework for managing uncertainty in information system project selection: an intelligent fuzzy approach,"Information System Project (ISP) Selection is the most significant strategic consideration to each management of organizations, in terms of business intelligence (BI) as critical factors must be considered from large volume of information, known as Big Data (BD). In today’s competitive environment, the main objective is selecting suitable and effective ISP to reduce the risk of investment, maximize overall performance of management of organization by mitigating uncertainty. As these types of decisions generally involve several criteria and it is often necessary to compromise among possibly conflicting factors, the multiple criteria decision making (MCDM) becomes a useful approach to solve this kind of problem. This paper establishes a novel intelligent model by integrating fuzzy Shannon entropy and Fuzzy Technique for Order Preference by Similarity to Ideal Solution Method (FTOPSIS) techniques as a decision tool for solving MCDM problem using linguistic values, which smoothly aids decision makers dealing with uncertain or incomplete information without losing existing quantitative information. The novelty of this paper is to propose a framework of BI in management of an organization to determine suitable ISP where all the meaningful information, relevant knowledge and visualization retrieved by analyzing BD based on decision making to enhance any organizational performance worldwide. © 2019, © 2019 International Society of Management Science and Engineering Management. A framework for managing uncertainty in information system project selection: an intelligent fuzzy approach BD; BI; FTOPSIS; fuzzy entropy; ISP; uncertainty ",Strategic alignment
1146,Iot-based energy analytic platform for foundry units,"Energy efficiency practices also help in reducing the energy cost of a manufacturing plant. Energy is one of the top three cost components of any manufacturing facility. In the last decade, the general awareness level in energy efficiency has improved very rapidly and as a result, metering system for tracking energy consumption is improved as compared to the last decade. Energy Data Analytics means the processing of energy, production, and operational data to report energy performance of the industry. It saves the time and resources required to analyze data and prepare reports and help to discover the operational inefficiencies and their impact, hidden energy-saving potential. © Springer Nature Singapore Pte Ltd 2021. Iot-based energy analytic platform for foundry units Internet of things (IoT); Machine to machine (M2M); Project management unit (PMU); Specific energy consumption Data Analytics; Energy utilization; Internet of things; Manufacture; Cost components; Energy cost; Energy performance; Energy saving potential; Manufacturing facility; Manufacturing plant; Metering systems; Operational data; Energy efficiency",Stakeholder management
1147,"Data science and artificial intelligence in project management: The past, present and future","The contemporary landscape of Project Management is continually morphing as a response to the changing demands as well as the availability of developing technology. This article discusses and uncovers the current state of Data Science and Artificial Intelligence in relation to the Project Management sector. Drawing from this context, the possible future applications and uses are then delineated. Several trends are identified, and the overall findings are synthesized to provide a realistic impression of what the future topography of Project Management holds. The increasingly sophisticated applications of technology have been found to assist in streamlining the current industry-standard procedures. © 2020 Editora Mundos Sociais. All rights reserved. Data science and artificial intelligence in project management: The past, present and future Artificial Intelligence; Data Science; Project Management ",Risk management
1148,Measuring dynamic capabilities in new ventures: exploring strategic change in US green goods manufacturing using website data,"Entrepreneurial scholarship suggests that a small firm’s ability to grow is a function of its capacity to sense and respond to changes in the market as well as the broader environment for the firm’s goods and services. Developing detailed measures of internal capabilities at a large scale, however, is often hampered by limitations in the availability of data from conventional sources, low survey response rates and panel attrition. The emergence of new information sources, including big data sets derived from the online activities of firms, coupled with advanced computational approaches, raises fresh analytical possibilities. In this exploratory study, we turn to freely accessible website data to gauge internal capabilities, specifically for market sensing and responding. To operationalize the construct of seizing, the paper uses an application of topic modeling, a text mining approach commonly used in computer science, on archived website data from the Wayback Machine for two time periods, 2008–2009 and 2010–2011, to explain sales growth for green goods enterprises in two later time periods, from 2010 to 2012. We find an endogenous inverse YOU-shaped relationship exists between market seizing and sales growth. Increasing levels of focus on a firm’s local geographic area also predict sales growth. We consider these findings in light of the practitioner literature on firm agility and pivoting and discuss opportunities for future work using website data to study entrepreneurship and the strategic management of innovation. © 2019, The Author(s). Measuring dynamic capabilities in new ventures: exploring strategic change in US green goods manufacturing using website data Big data; Dynamic capabilities; Entrepreneurship; SMEs; Text mining; Website analytics Advanced Analytics; Big data; Data mining; Enterprise resource management; Green computing; Sales; Strategic planning; Surveys; Websites; Computational approach; Dynamic capabilities; Entrepreneurship; Exploratory studies; Information sources; SMEs; Strategic management; Text mining; Industrial management",Capacity management
1150,Automation of Rail Gate Control with Obstacle Detection and Real Time Tracking in the Development of Bangladesh Railway,"It has been noticed that a lot of fatalities of lives occur every day due to manually operated rail gates all over Bangladesh. These happen mainly at places where the rail road passes through a city, locality or unmanned gates of the crossing zones. Currently, gatemen mainly operate on the assumption of a train departure schedule from the station to reach a crossing zone. But at times there are departure delays or maybe the train reaches crossing zones earlier, leaving the gateman unprepared to close the gate. Accidents are more likely to happen in such cases, causing severe damage to human lives and properties near the rail crossings. This paper presents the development and implementation of automatic rail gate control system as well as real time monitoring of train and obstacle detection for developing countries like Bangladesh. This research project was carried out using Arduino Nano along with IR Sensor, IR LED, Flame Sensor, Servo Motor, Ultrasonic sensor, DC Gear Motor and USB UART Board. This project is a combination of old technology with recent wireless technology and analytics to provide the best possible service to the nation. This paper also suggests the effectiveness of real time information of train position. The main objective of the proposed humanitarian project is to ensure the efficiency, quality, time management, and most importantly public safety, using wireless based communication network for the development of the railway industry in Bangladesh.  © 2020 IEEE. Automation of Rail Gate Control with Obstacle Detection and Real Time Tracking in the Development of Bangladesh Railway Automatic Railway System; Automatic Train Stop; Rail Gate Automation; Real Time Monitoring; Real Time Tracking Accident prevention; Automatic train control; Developing countries; Man machine systems; Obstacle detectors; Project management; Railroad accidents; Railroads; Ultrasonic applications; Humanitarian projects; Obstacle detection; Railway industry; Real time monitoring; Real time tracking; Real-time information; Time management; Wireless technologies; Rails",Risk management
1151,Comparison of mining prediction with real mining as a tool for strategic management,This paper responds to published scientific papers which compile econometric models of extraction of selected mineral resources. Mining prediction models can be new tools to increase the competitiveness of mining enterprises. Comparing the results of mining prediction and real data is important for further research on the issue. Specification of the results will lead to better managerial decisions in strategic and operational management. The results of the paper can lead to the clarification of the so-called random component in econometric models and the refinement of the assembled models. Random components are different from macroeconomic indicators. These components cannot be quantitatively captured in calculations in terms of econometric models. These components are influenced by the economic models of the mineral extraction prediction. The aim of the paper is to estimate random components in the future when using mining predictions as support for managerial decisions. Other mining activities may react differently to other random components. © 2020 International Multidisciplinary Scientific Geoconference. All rights reserved. Comparison of mining prediction with real mining as a tool for strategic management Comparison of results; Econometric models; Managerial decisions; Strategic management Competition; Extraction; Forecasting; Managers; Maps; Mineral resources; Remote sensing; Surveying; Macroeconomic indicators; Managerial decision; Mineral extraction; Mining enterprise; Operational management; Random components; Scientific papers; Strategic management; Predictive analytics,Strategic alignment
1153,Convergence and digital fusion lead to competitive differentiation,"Purpose: Organizations are consistently seeking innovative strategies and novel pathways to enhance business processes and create differentiation. The global business ecosystem is changing and there is growing demand for multi-modal digital technologies, big data consolidation and data analytics to harness a cost-competitive agile system. Technological convergence and integration of digital systems is one of the preferred methodologies that facilitates new and effective workflows and revives business processes. The progressive interlinking of digital technologies with business operations leads to the convergence and blending of management disciplines, devices and applications. The growing inconsistencies in managerial understanding regarding the benefits of convergence prompts a comprehensive examination of digital convergence pathways, identifying the impacts on converging entities and business objectives. The State bank of India (SBI) mega-merger case study was selected to investigate the pragmatic framework of digital convergence and to understand the impacts on interlinked entities such as: business operations, strategic management, project team that support value creation and competitive differentiation. The purpose of this paper is to focus on the phenomena of techno-fusion of emerging technologies creating new opportunities, business models and unique strategies for global banking and financial service organizations. Design/methodology/approach: This study applies the qualitative, inductive research method using critical reflection of before and after the implementation of convergence and digital integration strategies. The SBI case study employs this research strategy based on the premise that banks must stay agile and highly responsive to the changing environment to enhance its value proposition and competitive differentiation objectives. The study methodology incorporates cooperative inquiry and multiple levels of analysis using data collection techniques of exhaustive review of archives, informal interviews, questionnaires and observations to identify the synergistic process improvement pathway. The study is grounded on the concept that the convergence of diverse business pathways involves innovative and interlinked project, strategic and information technology (IT) workflows that results in open innovative systems. Findings: The studies identify that organizational innovation and creative solutions are a result of ecosystem turbulence, environmental force diversity, competitive pressure and the need for differentiation. Organizations that harness the power of digital fusion and convergence of management, systems and data generate a competitive advantage. The technological convergence strategy pulls multiple business and technology processes (project, strategic, IT, Cloud, AI and business process management) at the organizational, divisional or functional level generating new opportunities and threats, new business models and unique growth strategies for global banking and financial services organizations. Organizations that fully integrate techno-fusion of business and digital strategies produce synergistic effects and enhance adaptability, innovation and resiliency in the face of competitive challenges. Research limitations/implications: Additional areas that can be explored further as an extension of this study are listed below: identifying factors to improve the speed of convergence; the current results are limited to large size organizations where formal management and technology functions are distinctive. Similar studies on smaller organizations are warranted. Originality/value: This study focuses on the evolving field of technology innovation, which is increasingly being intertwined with business operations. Innovative digital technology is enabling the convergence of the disciplines of management, digital devices and applications. This facilitates the creation of a pragmatic framework that supports convergence of business operations, strategic management and digital fusion which leads to value creation and competitive differentiation. The techno-fusion of emerging technologies and digital strategies generates new opportunities and threats, new business models and unique growth strategies for organizations. © 2019, Emerald Publishing Limited. Convergence and digital fusion lead to competitive differentiation Agile synergistic interaction; Business and technology management; Convergence; Digital fusion; Digital technology; Open innovation ",Strategic alignment
1154,Sustainable competitive advantage driven by big data analytics and innovation,"Big data analytics (BDA) is one of themain pillars of Industry 4.0. It has become a promising tool for supporting the competitive advantages of firms by enhancing data-driven performance. Meanwhile, the scarcity of resources on aworldwide level has forced firms to consider sustainable-based performance as a critical issue. Additionally, the literature confirms that BDA and innovation can enhance firms' performance, leading to competitive advantage. However, there is a lack of studies that examine whether or not BDA and innovation alone can sustain a firm's competitive advantage. Drawing on previous studies and dynamic capability theory, this study proposes that big data analytics capabilities (BDAC), supported by a high level of data availability (DA), can improve innovation capabilities (IC) and, hence, lead to the development of a sustainable competitive advantage (SCA). This study examines the proposed hypotheses by surveying 117 manufacturing firms and analyzing responses via partial least squares-structural equation modeling (PLS-SEM) statistical software. Findings reveal that BDAC relies significantly on the degree of DA and has a significant role in increasing IC. Furthermore, the analysis confirms that IC has a significant and direct effect on a firm's SCA, while BDAC has no direct effect on SCA. This study provides valuable insights for manufacturing firms to improve their sustainable business performance and theoretical and practical insights into BDA implementation issues in attaining sustainability in processes. © 2020 by the authors. Sustainable competitive advantage driven by big data analytics and innovation Big data analytics (BDA); Data availability (DA); Dynamic capabilities; Firm performance; Innovation; Strategic management; Sustainable competitive advantage (SCA) ",Strategic alignment
1155,Data Mining and Analytics for Construction Project Cost Data Management System,"This paper mainly discusses the theory of data mining, discusses the application of decision tree technology in the construction project cost data management system. This paper mainly studies the following contents: firstly, the development of informatization and data mining of construction project cost data management is introduced, and then the management problems of project cost data management are introduced; secondly, the concept and steps of data mining are introduced, and then the common algorithms of data mining are briefly introduced, This paper describes the specific application of data mining in related industries. © 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG. Data Mining and Analytics for Construction Project Cost Data Management System Data mining; Decision tree; Forecast; Project cost Decision theory; Decision trees; Information management; Intelligent systems; Project management; Trees (mathematics); Construction project costs; Decision tree technologies; Informatization; Management IS; Management problems; Project cost; Data mining",Strategic alignment
1156,Increasing the viability of project selection and prioritization: The role of business analytics and sensemaking,"Purpose: This briefing is prepared by an independent writer who adds their own impartial comments and places the articles in context. Design/methodology/approach: This paper aims to review the latest management developments across the globe and pinpoint practical implications from cutting-edge research and case studies. Findings: Business analytics (BA) can help firms better select and prioritize project work they undertake. Data generated through analytics permits awareness of changes in the operating environment and can be used to inform sensemaking and alongside intuition to enhance decision-making. Originality/value: The briefing saves busy executives and researchers hours of reading time by selecting only the very best, most pertinent information and presenting it in a condensed and easy-to-digest format. © 2021, Emerald Publishing Limited. Increasing the viability of project selection and prioritization: The role of business analytics and sensemaking Business analytics; Project selection; Sensemaking; Start-ups ",Strategic alignment
1157,Analyzing the adoption of recent IT technologies in undergraduate engineering project course,"Outcome-based professional education processes are a new standard for all stakeholders and knowledge economy. Majority of the time the IT services/consulting industry demands graduates who shall be billable from day one of joining the enterprise. According to the National Employability Report Engineers 2019, only 3% of engineers possess new-age skills in areas such as AI, data analytics and mobile technologies. Academic project works shall be seen in the light of aforesaid observations and therefore suggests treating this as an opportunity to initiate undergraduates to more hands-on experiential learning considering the advancements in technology and market need. The objective of this paper is to identify the gap between the needs of the market/job trends and undergraduate CS/IT student projects areas. The second objective is to identify the factors affecting the project selection process. Our empirical study in general spanning over seven Engineering Institutes in western Maharashtra. The statistical data is collected from a controlled group of 4000 students. The data consists of project topics completed by students in the last nine years (from the year 2011 to the year 2019). Two indicators are used to understand the IT market. First, Gartner strategic technologies present the worldwide trend of technologies. Second, student project areas are compared with job trends in India. The student’s feedback on technology adoption theory presents the challenges in adoption of recent IT technologies in academic projects. 62% of students have reported unavailability of resources at the institute to encourage the adoption of the recent technologies. The student’s perception about the adoption of recent technologies needs to change. © 2021, Rajarambapu Institute Of Technology. All rights reserved. Analyzing the adoption of recent IT technologies in undergraduate engineering project course CS/IT academic project selection; CS/IT job/market trend; Gartner strategic technologies ",Capacity management
1161,Design requirements of a modern business Master’s degree course: perspectives of industry practitioners,"Contemporary industry practices must be appropriately reflected in designing modern-day teaching and learning programmes. Existing studies are limited to systematic methodologies for accumulating contemporary practice requirements and using that data to inform the design of educational programmes, even though various, local approaches for doing so often exist in higher education institutions. Going beyond this and adopting design-based research (DBR) principles, this paper introduces industry practitioner perspectives of contemporary- practice need for conceptualisation and design of a new business master’s degree programme. Outlining industry-demand as a driving force in stimulating a new business data-analytics programme at a medium-sized Australian metropolitan university, the study utilises open-ended interviews with five senior data analytics professionals to find a new matrix of industry expectations. The emerged elements are open-sourced tools based general technical knowledge; specific industry certifications or special skills; technology integration knowledge; cross-industry knowledge such as marketing; project management/agility and decision-making utilising appropriate supporting knowledge. Based on these findings, key learning objectives, an initial structure of the programme and specialisation subjects is proposed for further evaluation through convergent interviewing. We anticipated that the entire design process could be reusable for other similar situations for designing new practical courses in higher education sector. © 2020, Springer Science+Business Media, LLC, part of Springer Nature. Design requirements of a modern business Master’s degree course: perspectives of industry practitioners Analytics; Big data; Business discipline; Course design; Curriculum; Higher education ",Governance
1163,Business intelligence for generating comprehensive report in electronic completion and handover,"Project completion is a common and best practice for oil and gas and construction industry. It provides a comprehensive completion approach and gives total confidence to asset owner to operate the facility handed over by construction contractors. While the nature variation of asset hierarchy is unique from one type of asset to another, a project completion software must have prominent ability to adapt wide range of variations with unlimited level of hierarchy. One of the approaches to overcome this is implementing a tree-model concept to accommodate flexible hierarchy. Unfortunately, the package is loaded with complexity to retrieve data and takes longer join operation. This paper proposes a business intelligence approach to analyze and make an optimum reporting retrieval using data warehouse. This is implemented in 4 steps following Kimball methods. The objective of this paper is to generate model by using data warehouse starting the extract, transform and load process on the flexible tree model hierarchy. It can be used to generate report and comprehensive dashboard especially progress report of project and work schedule as needed in the oil and gas and construction industry. © 2020 ASTES Publishers. All rights reserved. Business intelligence for generating comprehensive report in electronic completion and handover Business Intelligence; Data Warehouse; Project Completion; Project Management System; Tree Model Structure ",Capacity management
1164,Project Cost Prediction of Overhead Line Based on Big Data Analysis of Power Grid Engineering,"Many factors affect the cost level of overhead lines, such as different macroeconomics, natural conditions, technical conditions and external construction environment, and overhead line engineering cost prediction is an important part of project management and control. In order to obtain more accurate prediction results, this paper is based on gray correlation Analyze and screen the factors that affect the cost of overhead line projects, then use the particle swarm improved support vector machine algorithm to establish a smart transmission line project cost prediction model, and use the actual samples of power grid engineering big data to verify the effectiveness of the proposed method. Reasonable determination of power grid investment and decision-making provides a basis. © 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG. Project Cost Prediction of Overhead Line Based on Big Data Analysis of Power Grid Engineering Cost prediction; Grey relational analysis; Overhead line engineering; PSO-SVM model Big data; Cost benefit analysis; Decision making; Economics; Electric power transmission networks; Forecasting; Overhead lines; Predictive analytics; Project management; Security of data; Support vector machines; Accurate prediction; Construction environment; Management and controls; Natural conditions; Power grid investments; Smart transmission lines; Support vector machine algorithm; Technical conditions; Cost engineering",Monitoring and control
1165,Machine Learning of Concrete Temperature Development for Quality Control of Field Curing,"Understanding the relationship between concrete temperature development and field curing time helps to control material quality, improve construction efficiency, and enhance research on concrete design. However, it is difficult to precisely predict temperature trends when placing concrete because there are many influencing factors and uncontrollable ambient variables in the curing process. To forecast the short-term temperature trends reliably and automatically, this research proposes a temperature measurement and quality prediction (TMQP) system to proactively evaluate the development trajectory of concrete quality and the temperature changes at the center and surface of the cross section of concrete structural members. The TMQP system includes radio-frequency identification (RFID) temperature sensors for recording the temperature data and Big Data analytics (BDA) combined with the machine-learning method of classification and regression tree (CART) for measuring and predicting of temperature development. The results indicate that the system has over 98% reliability on the correlation coefficients between the predicted temperatures and actual temperatures based on 240 h of continuous experiments and 190 h of documented data. This entire research design is applicable to various concrete construction projects and sheds light on how BDA and machine learning can help construction engineers and managers to control concrete curing and take preventive measures to avoid concrete surface cracks.  © 2020 American Society of Civil Engineers. Machine Learning of Concrete Temperature Development for Quality Control of Field Curing Big Data analytics; Material quality; Nonlinear regression; Prediction; Temperature data series Concrete placing; Curing; Data Analytics; Forecasting; Machine learning; Project management; Quality control; Radio frequency identification (RFID); Temperature measurement; Trees (mathematics); Classification and regression tree; Concrete construction projects; Concrete structural members; Construction efficiency; Construction engineers; Correlation coefficient; Machine learning methods; Temperature development; Concretes",Strategic alignment
1166,Performance Evaluation of High-Tech Enterprises’ Science and Technological Input Based on BP Neural Network,"Performance evaluation of science and technological input is the basis and premise of establishing competition, incentive and supervision mechanism in science and technology project management. It also is a reliable basis for scientific decision-making and ensuring the accomplishment of scientific and technological tasks. To a certain extent, high-tech enterprises also show that the government attaches importance to scientific and technological innovation and is committed to promoting scientific research performance with high-tech. Standing at the macro level, this paper discusses the system construction of performance evaluation of high-tech enterprises’ science and technology investment. In this paper, BP neural network is used to study and establish a performance prediction model of science and technological input, verify the feasibility of the model and find out a scientific and reasonable evaluation model method. This paper provides a scientific and feasible guidance scheme and information basis for the regional government’s decision-making on science and technological investment in high-tech enterprises. © 2020, Springer Nature Singapore Pte Ltd. Performance Evaluation of High-Tech Enterprises’ Science and Technological Input Based on BP Neural Network BP neural network; Performance evaluation; Science and technological input Backpropagation; Decision making; Integrated circuits; Predictive analytics; Project management; Evaluation modeling; Performance prediction models; Science and Technology; Science and technology project managements; Scientific and technological innovations; Scientific decisions; Scientific researches; Supervision mechanisms; Neural networks",Risk management
1167,A Probabilistic Approach for Predicting the Fatigue Life of Concrete,"The fatigue failure of structural elements subjected to repeated cyclic loading may reduce the life of infrastructures. Heterogeneous nature of concrete and random factors in fatigue testing lead to great variability in fatigue life of concrete. As deterministic approach depends on certain parameters and initial conditions, it is not reliable for the prediction of fatigue life of concrete. In this study, a probabilistic approach using artificial neural network is utilised to predict the fatigue life of plain concrete. An artificial neural network predictive model was developed utilising the data from fatigue tests conducted on plain concrete beams of three different sizes mainly small, medium and large. The model is trained using the available experimental data of small and medium specimen and is validated using available experimental data reported on large specimens. The developed model is able to predict the number of cycles of failure of concrete by considering material and fracture mechanics properties responsible for the softening behavior of concrete as input. This approach is advantageous over other methods as it includes the randomness in the fatigue of concrete and will be able to predict the fatigue life of concrete with reasonable accuracy. © 2021, Springer Nature Switzerland AG. A Probabilistic Approach for Predicting the Fatigue Life of Concrete Artificial neural network (ANN); Fatigue life; Probabilistic approach Concrete beams and girders; Concrete testing; Failure (mechanical); Fatigue testing; Forecasting; Fracture mechanics; Neural networks; Predictive analytics; Project management; Structural design; Deterministic approach; Mechanics property; Prediction of fatigue lives; Predictive modeling; Probabilistic approaches; Reasonable accuracy; Softening behavior; Structural elements; Fatigue of materials",Capacity management
1168,"A review of accident prediction in construction: Tools, issues, and solutions","The frequent occurrence of accidents in construction industry puts forward higher requirements for the prevention of accidents. The key to the prevention of accidents is the accurate prediction of accidents, and corresponding preventive measures are formulated according to the predicted results. The feasibility of accident prediction has been verified in the fields of traffic safety, aviation safety and so on. This paper analyzes the present situation of accident prediction research in construction industry from the aspects of theory, method and content of accident prediction research, and points out the defects of the existing research. The purpose is to make a systematic summary of the existing research for the future research direction. The results of literature analysis show that accurate and effective accident prediction is reflected in three aspects:(1) Objective and true data;(2) Scientific selection of predictive variables and outcome variables; (3) Construction of prediction model. These are the issues that future research should focus on. © 8th International Symposium on Project Management, ISPM 2020. All rights reserved. A review of accident prediction in construction: Tools, issues, and solutions Accident prediction; Construction industry; Prediction model; Research status Accidents; Construction industry; Forecasting; Predictive analytics; Project management; Accident prediction; Accurate prediction; Future research directions; Literature analysis; Predictive variables; Present situation; Prevention of accidents; Preventive measures; Traffic control",Strategic alignment
1171,Recent Technologies in Construction; A Novel Search for Total Cost Management of Construction Projects.,"There is a continuous rise in the implementation of information-driven technology in construction, especially for effective control and management of construction resources. However, the enormous opportunities that these technologies provide have not been adequately utilized for construction cost management. There are so many innovations in the current IR 4.0 era, where digitization and connected systems are the trend of activities in engineering and other allied industries. These innovations are being adopted in the construction industry, to enhance the efficiency of construction processes. The areas of the current trend include Internet of things [IoT], Augmented and Virtual Reality [AR and VR], 5D-BIM, Autonomous Equipment, Artificial Intelligence and Machine Learning, and Predictive Analytics. This paper explores the applications and benefits of these areas of new technological trends in construction project management with emphasis laid in cost management of construction projects. Eighty-one[81] recent publications in journal articles, textbooks, web pages and conference proceedings published between 2008 and 2019 were reviewed in order to articulate and reveal areas of application of the technologies in project cost management and control. The paper exposes areas of application of the current ICT trends which construction managers and project cost managers should harness. © Published under licence by IOP Publishing Ltd. Recent Technologies in Construction; A Novel Search for Total Cost Management of Construction Projects.  ",Value management
1172,An improved extreme gradient boosting approach to vehicle speed prediction for construction simulation of earthwork,"Construction simulation is an effective tool to provide schedule plans. Vehicle speed is one of the most significant factors in earthwork construction simulation. However, neglecting the strong correlation with contextual factors, random distribution methods will lead to inaccurate prediction of vehicle speed. To address such issues, an improved extreme gradient boosting (XGBoost) approach to vehicle speed prediction is proposed for earthwork construction simulation. Firstly, to improve the global searching ability, an improved grey wolf optimization algorithm (IGWO) is put forward. Secondly, XGBoost is optimized by IGWO to construct an IGWO-XGBoost model. Then, the prediction model is embedded in the earthwork construction simulation model. The case study proves that the simulation results of the proposed method are more consistent with an actual construction schedule. It is expected that the vehicle speed prediction embedded into a simulation program facilitated an accurate development of schedule plan, thereby improving the efficiency of construction management. © 2020 Elsevier B.V. An improved extreme gradient boosting approach to vehicle speed prediction for construction simulation of earthwork Construction simulation; Contextual factors; Earthwork; Improved extreme gradient boosting; Improved grey wolf optimization algorithm; Vehicle speed prediction Excavation; Forecasting; Foundations; Project management; Speed; Vehicles; Construction management; Construction schedules; Construction simulation; Contextual factors; Global searching ability; Optimization algorithms; Random distribution; Vehicle speed predictions; Predictive analytics",Strategic alignment
1174,Predicting the success of news: Using an ML-based language model in predicting the performance of news articles before publishing,"Traditional recommendation systems have limited possibilities to optimise business value in editorial decision making in news production, as they select the recommendations only from the content whose production has been decided editorially in the daily news process or content from existing content inventories. This paper explores an approach to use predictive analytics to make it possible to optimise story assignment and editing in daily editorial work based on selected business objectives already before publishing. In this case study exploration, we use the 'constructive approach' as a method to provide solutions to concrete business problems with a scientific approach. We contribute by experimenting a novel method combining elements from several scientific domains like strategic management and system dynamics. We conclude that with language analysis using recurrent neural networks, we were able to predict the success of a news story published on a digital channel in a way that fulfils the 'weak market test' criteria of the constructive approach. A company with whom the model was developed considered it valuable enough to decide to move it from exploration to be further developed and used in real news production. © 2020 ACM. Predicting the success of news: Using an ML-based language model in predicting the performance of news articles before publishing constructive approach; digital news media; editorial predictive analytics; recurrent neural networks Concrete industry; Construction; Decision making; Forecasting; Predictive analytics; Business objectives; Business problems; Constructive approach; Digital news; Language analysis; News productions; Strategic management; System Dynamics; Recurrent neural networks",Strategic alignment
1175,Geospatially distributed safety and performance benefits for projects of a transportation system,"Prioritization of system improvements, such as in transportation systems, typically uses multi-objective analysis and cost-benefit analysis but overlooks how geospatial association will influence initiatives. Recent YOU.S. regulations have mandated project coordination processes that seek opportunities to reduce cost and improve the safety of infrastructure projects by identifying opportunities for collective construction activities. In this paper, a method is developed to evaluate the geospatial factors in the consideration of transportation project prioritization and coordination. The prioritization of projects is intended to effectively administer resources for improving safety through transportation infrastructure projects. A spatial association factor (Gi*) is assigned to projects, identifying the geospatial proximity and ranking of other projects under consideration. This finds opportunities of project coordination and addresses the geographic diversity of safety investments. The approach is demonstrated for a selection of 1,573 intersection improvement projects under consideration by a YOU.S. state department of transportation. The results and methodology are of interest to systems and enterprises that balance multiple investment projects with geographic attributes. © 2020 IEEE Geospatially distributed safety and performance benefits for projects of a transportation system Geospatial information; Intersection safety; Resilience analytics; Risk management; Spatial association; Systems evaluation; Transportation infrastructure Construction industry; Investments; Collective constructions; Geographic diversity; Infrastructure project; Multi-objective analysis; Performance benefits; Transportation infrastructures; Transportation projects; Transportation system; Cost benefit analysis",Capacity management
1176,Construction cost management strategy based on BIM technology and neural network model,"The construction of construction projects is an important industry of national social and economic development, and price management control is an important part of construction projects, and has become an important factor for major construction companies in China to manage construction projects. At present, the internal construction price management is not the best, nor the most ideal. Few investments exceed the budget, mainly due to defects in effective construction price management, lack of advanced technology and lack of prospects for prepayment, which make it difficult to match the actual and expected results of construction project price management. The actual results are always unsatisfactory. In this paper, the engineering cost estimation model is studied, and the neural network comprehensive prediction model is established to improve the accuracy and application technology of the prediction model. By using the building of BIM technology and neural network model, and effectively using the price advantage of ICT, it is used in the construction industry, and the cost is strictly controlled, so as to bring huge profits to the enterprise and promote the development of the enterprise. © 2021 - IOS Press. All rights reserved. Construction cost management strategy based on BIM technology and neural network model BIM technology; construction engineering; cost management; neural network model Architectural design; Budget control; Construction industry; Cost engineering; Cost estimating; Economics; Neural networks; Predictive analytics; Advanced technology; Application technologies; Comprehensive prediction; Construction companies; Construction cost managements; Construction projects; Neural network model; Social and economic development; Project management",Monitoring and control
1177,Developing a channel strategy decision support framework for a diesel engine supplier in Mozambique,"The global economic competitiveness has forced many organizations to consider the emerging and developing markets for future growth opportunities. The Southern African country Mozambique was identified as such an opportunity. However, although the country has experienced more sustained historic economic growth than its neighboring countries, it is also riddled with corruption and bureaucracy. Due to the complex and challenging Mozambique business environment, this research project utilized SWOT and PESTLE analytical techniques to develop a decision support framework to support the organizational growth strategies in Mozambique. The approach allowed for both internally and externally focused data collection and evaluation. The qualitative data were collected during structured interviews with selected captains-of-industry in the Mozambican context. In the development of the decision support framework, key factors were identified, evaluated, and ranked in order of severity. Key internal factors found centered on human resources, including management style, skilled labor availability and retention, while external factors centered potential growth in the mining and gas industry, government interventions and stability, and lack of skilled labor availabity. Taking cognizance of these factors enables a systematic framework to guide the strategy development in the Mozambican marketplace. It is recommended that this framework should be a living framework, continuously evolving as new strategic information becomes available. © Henco du Plooy, Pieter Buys, 2020. Developing a channel strategy decision support framework for a diesel engine supplier in Mozambique Business intelligence; Emerging markets; PESTLE analysis; Strategic management; SWOT analysis ",Strategic alignment
1179,"10th Annual International IEOM Conference, IEOM 2020","This proceedings contains 318 papers. IEOM Society International focus on the latest developments and advancements in the fields of Industrial Engineering and Operations Management. The conference topics covering industrial issues/applications and theoretical research, which include: Artificial Intelligence; Automation and Control; Business Management; Case Studies; Construction Management; Cyber Security; Data Analytics and Big Data; Decision Sciences; Defense; Design and Analysis; E-Business/E-Service; E-Manufacturing; Energy; Engineering Education; Engineering Management; Entrepreneurship and Innovation; Environmental Engineering; Financial Engineering; Healthcare Operations and Healthcare Engineering; Human Factors and Ergonomics; Industrial Management; Industry 4.0; Industry Best Practices; Information Technology; Inventory Control; IoT; Lean Six Sigma; Logistics; Maintenance Services; Manufacturing; Modeling and Simulation; Operations Excellence; Operations Management; Operations Research; Product Lifecycle Management (PLM); Production Engineering; Production Planning and Management; Project Management; Quality and Reliability; Service Engineering and Service Management; Supply Chain Management; Sustainability; Systems Dynamics; Systems Engineering; Technology Management; Transportation and Traffic; Waste; Work Design, Measurement and ISO, etc. The key terms of this proceedings include Building Information Modelling (BIM), artificial intelligence technique, airline disruption management, vehicle routing problem, SWOT analysis, offshore wind power systems, Analytical Hierarchy Process (AHP), nonlinear regression analysis, Military Camouflage designs, portable electromagnetic radiation protective. 10th Annual International IEOM Conference, IEOM 2020  ",Capacity management
1180,How mega is the mega? Exploring the spillover effects of Wechat using graphical model,"WeChat, an instant messaging app, is considered a mega app because of its dominance in terms of use among Chinese smartphone users. Little is known, however, about its externality in the broader app market. This work estimates the spillover effects of WeChat on the other top 50 most frequently used apps in China, using users’ weekly app usage data. Given the challenge of determining causal inference from observational data, we apply a graphical model and an econometric method to estimate the spillover effects in two steps: (1) we determine the causal structure by estimating a partially ancestral diagram, using a fast causal inference algorithm; and (2) given the causal structure, we find a valid adjustment set and estimate the causal effects by an econometric model with the adjustment set for controlling noncausal effects. Our findings show that the spillover effects of WeChat are limited; in fact, only two other apps, Tencent News and Taobao, receive positive spillover effects from WeChat. In addition, we show that if researchers fail to account for the causal structure that is determined from the graphical model, it is easy to fall into the trap of confounding bias and selection bias when estimating causal effects. The findings generate managerial implications in terms of app usage patterns, strategic management of mega apps on an app platform, and app promotional strategies for app platform managers and app developers. Copyright: © 2019 INFORMS How mega is the mega? Exploring the spillover effects of Wechat using graphical model App analytics; Causal inference; Econometrics; Graphical model; Machine learning; Spillover effects; WeChat Economics; Inference engines; Learning systems; Managers; Statistics; Causal inferences; Econometrics; GraphicaL model; Spillover effects; WeChat; Graphic methods",Strategic alignment
1181,Analysis of the application of military big data in equipment quality information management,"This At present, big data has risen to the national strategy. Big data is fully integrated into the military field, becoming the driving force of military scientific research, the core element of construction management, and an important resource for war success. This paper mainly expounds the basic connotation of big data technology and military big data, and analyzes the application of military big data in equipment quality information management, and proposes information collection, storage, analysis, processing, exchange and feedback on equipment quality information management. The countermeasures provide methods and basis for military big data in equipment information management. © 2019 IEEE. Analysis of the application of military big data in equipment quality information management equipment quality information; management; military big data Advanced Analytics; Big data; Cloud computing; Digital storage; Management; Military electronic countermeasures; Project management; Quality control; Construction management; Data technologies; Equipment information; Equipment quality; Fully integrated; Information collections; National strategies; Scientific researches; Information management",Strategic alignment
1186,Using Big Data in E-tourism Mobile Recommender Systems: A project approach,"This paper describes main modern tendencies for the design and development of e-tourism recommender systems with big data analytics. This study is an attempt to systematize and summarize knowledge about the possibilities of using e-tourism big data in mobile e-tourism recommender systems. In particular, to analyze the sources and types of tourist data generated by the tourist gadget, that can be related to e-tourism big data. This research focuses on the first stage of the project lifecycle for creating a mobile recommender system using e-tourism big data to filter those that best meet the interests of a particular user. Some solutions have been designed and methodological tools analyzed for more efficient use of various types of e-tourism big data from a user's gadget to be operated by a recommender system. In this study, big data for the e-tourism industry will be considered not only as a set of approaches, tools and methods for processing structured and unstructured touristic data of huge volumes. Copyright © 2020 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0) Using Big Data in E-tourism Mobile Recommender Systems: A project approach Big data; Context analysis; E-tourism; Mobile recommender systems; Trip support Big data; Data Analytics; Life cycle; Project management; Tourism; Context analysis; Design and Development; eTourism; Methodological tools; Mobile recommender systems; Project approach; Project lifecycle; Tools and methods; Recommender systems",Strategic alignment
1188,Crowdsourcing-based learning: The effective smart pedagogy for STEM education,"Multiple innovative teaching and learning strategies, that strongly support the advanced technology-based education, emerged in recent years. However, there are no published data analytics and validated outcomes regarding how good those innovative teaching approaches support the main concepts of Smart Pedagogy. This paper presents the outcomes of the ongoing research, design and development project at the InterLabs Research Institute, Bradley University (Peoria, IL, USA) aimed to identify, analyze, test, implement and recommend various components of Smart Pedagogy for STEM education. Those components may include but are not limited to 1) learning- by-doing, active learning or experiential learning, 2) games-based learning and gamification of learning, 3) collaborative learning, 4) analytics-based learning, 5) flipped classroom, 6) crowdsourcing-based learning, 7) badging-based learning, and 8) productive failure-based learning. The obtained research outcomes about application of crowdsourcing-based learning in several STEM undergraduate and graduate courses are presented in this paper. Project outcomes undoubtedly demonstrate students' keen interest in active use of crowdsourcing-based learning in Sciences courses, in particular, and in highly technological Smart Pedagogy in STEM education, in general. © 2019 IEEE. Crowdsourcing-based learning: The effective smart pedagogy for STEM education Crowdsourcing based learning; Formative and summative evaluation; Smart pedagogy; STEM education Crowdsourcing; Curricula; Data Analytics; Engineering education; Project management; Teaching; Advanced technology; Collaborative learning; Design and Development; Experiential learning; Formative and summative evaluation; Innovative teaching; Smart pedagogy; STEM education; STEM (science, technology, engineering and mathematics)",Risk management
1189,Critical success factors for business intelligence implementation in an enterprise resource planning system environment using dematel: A case study at a cement manufacture company in Indonesia,"This paper is aimed at evaluating critical success factors in Business Intelligence (BI) implementation in an Enterprise Resource Planning (ERP) environment. The data analysis method used in this paper is the Decision Making Trial and Evaluation Laboratory Model (DEMATEL). The study has been conducted on a cement manufacturing strategic holding company that has implemented ERP since 2010. This research is done through literature review and interviews with the head of the BI development team as the expert for this research, before distributing questionnaires to Information and Communication Technologies (ICT) Team and BI stakeholders. The questionnaire has been addressed to 18 respondents consisting of the BI development team and stakeholders, which include the Strategic Planning Division, Business Development Division, Transformation Management Division, and Accounting Division. There are 13 factors evaluated, consisting of 4 factors related to the organization, four factors related to the development process, three factors related to the technology, and two factors associated with the external environment. The most critical factor for organizational criteria is Top management support. The most important factor for process criteria is Effective project management. The most important factor for technology criteria is System reliability, flexibility, and scalability. The most important factor for environment criteria is the selection of a vendor. © University of Tehran, Faculty of Management. Critical success factors for business intelligence implementation in an enterprise resource planning system environment using dematel: A case study at a cement manufacture company in Indonesia Business intelligence; Critical success factor; DEMATEL ",Value management
1190,Engineering Project Health Management: A Computational Approach for Project Management Support through Analytics of Digital Engineering Activity,"Due to the situational and contextual individuality of engineering work, the in-progress monitoring and assessment of those factors that contribute to the success and performance in a given scenario poses a distinct and unresolved challenge, with heavy reliance on managerial skill and interpretation. Termed engineering project health management (EPHM), this paper presents a novel approach and framework for monitoring of engineering work through data-driven and computational analytics that in turn support the managerial interpretation and generation of higher level, context-specific understanding. EPHM is formed through the first adaptation of integrated vehicle health management (IVHM) to the field of engineering management; an approach that has been used to-date for the machine monitoring and predictive maintenance. The approach is applied to four industrial cases, which demonstrates the generation of project-specific information. The approach thereby acts to increase understanding of an engineering activity and a work state, and is complementary to existing managerial toolsets and approaches. A key tenet of the adaption of IVHM is to place the manager in a central role, supporting their professional judgment while reducing investigative effort. © 2018 IEEE. Engineering Project Health Management: A Computational Approach for Project Management Support through Analytics of Digital Engineering Activity Engineering management; integrated vehicle health management (IVHM); process monitoring and control; project management; project performance; project success factors Decision making; Decision theory; Health; Job analysis; Maintainability; Managers; Monitoring; Open systems; Process monitoring; Project management; Complexity theory; Engineering management; Integrated vehicle health managements; Process monitoring and control; Project performance; Project success factors; Task analysis; Research and development management",Monitoring and control
1191,Exploring the potential improvement of quality control in the construction industry with the use of digital technology,"The construction industry has traditionally relied on paper to manage quality records, resulting in the capture and analysis of data being challenging, preventing historical performance analytics that lead to better outcomes. This research is aimed at critiquing the application of digital technologies for the potential improvement of quality management in the construction phase of civil engineering projects. To address the aim, two objectives emerge. The first is reviewing digital technologies that are available for use in quality control that could assist in the reduction of defects. The second is seeking the viewpoint from construction professionals to develop a quality management framework employing the most applicable digital technologies. In support of the objectives, a qualitative research approach involves multiple sources of data collection, gained from literature and interviews. Participants include digital engineering specialists, designers, main contractors and subcontractors. Applicable digital quality applications are identified as electronic document management systems, personal digital assistants, building information modelling (BIM), mobile construction application products (apps), clash mitigation using BIM, real time performance information, point clouds of as-built construction, three-dimensional vision on mobile phones and barcodes, among others. This holistic and collaborative approach facilitates personnel to make better decisions in the use of quality data. Implications for practice indicated that training, visible use of digital technologies and the provision of an effective common data environment are paramount in instigating digital applications. Employing this digital engineering in the construction phase should encourage the continuing journey to greater automation of the building activities themselves with an improvement in quality and productivity. © 2019 Association of Researchers in Construction Management, ARCOM 2019 - Proceedings of the 35th Annual Conference. All rights reserved. Exploring the potential improvement of quality control in the construction industry with the use of digital technology Digital technology; Information management; Total quality management Architectural design; Construction; Construction industry; Contractors; Information management; Information services; Personal digital assistants; Quality control; Real time systems; Three dimensional computer graphics; Total quality management; Building Information Modelling; Civil engineering projects; Construction professionals; Digital technologies; Electronic document management system; Historical performance; Quality management frameworks; Three dimensional vision; Project management",Risk management
1192,Comprehensive information management analysis of construction project based on BIM,"Informatization management of construction projects refers to the application of information technology in various stages of the project construction process based on Computer technology, Internet technology, Internet of Things technology and Communication technology. Collect, store, and process information from different parties, and control various aspects such as project cost, schedule, quality, and safety to improve management efficiency and level. Apply building information models to construction projects and combine construction projects with modern information technology to realize the information management of all participants, the whole goal, the whole factor and the whole life cycle of the construction project. Exploring the path and method of realizing comprehensive informationization of construction projects from three aspects: combining modern information technology, Digital Twin and BIM-based collaboration. The realization of comprehensive information management of BIM-based construction projects is of great significance for better promoting the application of BIM and promoting the development of intelligent buildings and information management of construction projects in China. © Springer Nature Singapore Pte Ltd. 2020. Comprehensive information management analysis of construction project based on BIM Building information modeling; Comprehensive information; Construction project management; Intelligent buildings Advanced Analytics; Architectural design; Big data; Construction; Digital twin; Embedded systems; Information management; Information theory; Intelligent buildings; Life cycle; Quality control; Smart city; Application of information technologies; Building Information Model - BIM; Communication technologies; Comprehensive information; Construction project management; Construction projects; Internet of things technologies; Modern information technologies; Project management",Value management
1193,Technology environment and crowdfunding platforms in Brazil,"Purpose: The purpose of this paper is to analyze the influence of the technological segment of the general environment in crowdfunding platforms’ operations in Brazil. Design/methodology/approach: By means of a qualitative and exploratory approach, the research comprised the execution of a panel of experts via semi-structured interview scripts. For the data analysis, a content analysis with the software NVivo 9 was conducted. Findings: One of the main results concerning this influence in the industry would be the expansion of internet access as key factor to scalability of operations and the use of analytics for developing markets. In addition, the cultural aspect emerges as facilitator for platforms access, thus the influence of technological segment cannot be analyzed without considering the cultural segment of the general environment. Research limitations/implications: Research limitations relate to the qualitative approach; although valuable insights were obtained for strategic policy in crowdfunding platforms, generalization is not possible. Moreover, the limited number of experts in the panel who agreed to participate may have been an obstacle for richer results. Practical implications: Among some implications to the strategic management of crowdfunding platforms in the country are investment prioritization in analytics, governance and transparency of operations and marketing. Analytics will enable more effective insertion in supporting communities and better selection strategies of projects with attributes of success; governance and marketing will aid platforms to reduce cultural resistance on the part of the potential users. Social implications: Regulations regarding crowdfunding platforms as well as socio-cultural segment of the strategic environment are key aspects in fostering co-creation among participants and in bringing scale to crowdfunding operations; they may be mediated by technology. Thus, analytics along with marketing initiatives related to addressing shared practices in communities will have a significant impact on the adoption of crowdfunding. Furthermore, such task should be more intense than in developed economies where internet infrastructure and quality access are widespread. Originality/value: Although various contributions have been made to the theme of crowdfunding, there has not been identified any paper addressing future influences of the strategic general environment, such as the technological segment, to the operations of crowdfunding platforms, especially in the Brazilian context. © 2019, Afonso Lima and Francisco Fabiano Mapurunga Araújo. Technology environment and crowdfunding platforms in Brazil Sharing economy; Strategic management ",Capacity management
1196,The effects of turnover on expert effort estimation,"Turnover of the personnel represents a serious issue for management of software projects. The buildup of competences and phasing in of the people into the project requires both time and effort. This paper presents a case study of a large in-house agile software development project. The research goal was to determine the effects that turnover has on the expert effort estimation. In order to do this, paper examines relations across empirical data on a studied project. Study findings are the following: a) it is necessary to distinguish types of turnover, b) the general and planned turnover do not necessarily have a negative effect on estimation accuracy, and c) the unplanned turnover can have a significant negative impact on the reliability of the estimates and therefore should be treated with special attention. Results suggest that these facts should be taken into account both by the management and human resources. © 2020, University of Zagreb, Faculty of Organization and Informatics. All rights reserved. The effects of turnover on expert effort estimation Effort estimation; Employee turnover; HR analytics; Software engineering; Software project management; Statistical analysis ",Risk management
1197,Strategic management and sustainable organizations: A path driven by technology; [Gestión estratégica y organizaciones sustentables: Un camino conducido por la tecnología],"At present, organizations operate in a changing environment, requiring truthful and timely information to make changes in their conduct that allow their sustainability over time. This research consists of systematizing the strategic management process based on a balanced scorecard for the administrative processes of the Integrated Security Service ECU 911 Ibarra Center, establishing indicators by areas and perspectives that support decision making. It is a descriptive, applied research, with non-experimental design and mixed approach. Observation, interview and survey were used as information gathering techniques. The system was developed using Extreme Programming methodology (XP), PHP language, YII framework, MySQL database, Higcharts and PHO storm. The strategic management software built generates reports dynamically, improving visualization, decreasing response times and reducing errors. More than 80% of users express the utility of the tool. © 2019, Associacao Iberica de Sistemas e Tecnologias de Informacao. All rights reserved. Strategic management and sustainable organizations: A path driven by technology; [Gestión estratégica y organizaciones sustentables: Un camino conducido por la tecnología] Balanced scorecard; Business intelligence; Data visualization; Decision making; Management software ",Strategic alignment
1199,Innovators 5 forces approach to increase the strategic accuracy of technological SME-Innovations,"We live in times of rapid technological advancements across industries facilitated by innovative trends such as the digitalization of supply chains, massive data analytics and the concurrent development of products with increasing technical complexity and functionality. The need for firms to advance research and development (R&D) processes proves to be a challenge particularly for highly specialized small and medium-sized enterprises. Innovator_Institut developed a practical approach, based on the traditional 5-Forces Model by Michael E. Porter. This approach aims to increase the strategic accuracy of technological SME innovations by considering the effect of innovations on industry structure. © Centar for Quality. Innovators 5 forces approach to increase the strategic accuracy of technological SME-Innovations Data-driven innovation; SME; Strategic management; Time-to-market ",Strategic alignment
1202,Document clustering for knowledge synthesis and project portfolio funding decision in R&D organizations,"The paper discusses a method of using document clustering for information/knowledge synthesis and decision facilitation in R&D organisations. The emerging methodologies of machine learning, artificial intelligence and data science in conjunction with fuzzy mathematics can be optimally exploited to catalyse development of information bank for research organisations. This knowledge ecosystem can be utilized by the proposed mechanism to accelerate and reinforce interdisciplinary research for R&D organisations and empower them to make efficacious information-driven decisions related to project portfolio selection and proposal funding. © 2019, National Institute of Science Communication and Information Resources (NISCAIR). All rights reserved. Document clustering for knowledge synthesis and project portfolio funding decision in R&D organizations Decision Support System (DSS); Fuzzy clustering; Information synthesis; Soft clustering ",Strategic alignment
1203,RATIONAL SELECTION OF WORK PERFORMERS FOR INFORMATION SYSTEM DESIGN PROJECTS; [РАЦИОНАЛЬНЫЙ ВЫБОР ИСПОЛНИТЕЛЕЙ НА ПРОЕКТАХ ПО СОЗДАНИЮ ИНФОРМАЦИОННЫХ СИСТЕМ],"The paper deals with the process of rational selection of a work performer for development and implementation tasks on IT projects with corporate information systems of military-industrial and civil enterprises. The subject of this work is the development of methods, algorithms and software for collecting and analyzing statistical data about available work performers for certain tasks in behalf of the further decision support for a project manager. For the purpose of analyzing and processing data about developers and other employees we use information from RedMine project management system, as one of the most popular and common tool. Based on the information collected from the RedMine database for each available performer, a vector criterion is constructed. It becomes the basis for the project manager to estimate one’s suitability for the next task based on his own preference function, some versions of which are considered in this work. The scientific significance of this study lies in the fact that this approach can be applied in a multi-agent model for the process of development and implementation of information systems, which serves for optimal planning and control in the project management activities. Practical relevance consists in automatic performance of the person responsible for the distribution of tasks among the performers via formalized mathematical approach that provides a rational loading plan for available employees that leads to economic benefit. © 2019 ITMO University. All Rights Reserved. RATIONAL SELECTION OF WORK PERFORMERS FOR INFORMATION SYSTEM DESIGN PROJECTS; [РАЦИОНАЛЬНЫЙ ВЫБОР ИСПОЛНИТЕЛЕЙ НА ПРОЕКТАХ ПО СОЗДАНИЮ ИНФОРМАЦИОННЫХ СИСТЕМ] data science; decision support; information system; multi-agent modeling; preference function ",Strategic alignment
1205,Data-driven decisions in employee compensation utilizing a neuro-fuzzy inference system,"Artificial intelligence assists organizations to carry out strategic management decisions especially in talent management. A firm’s overall compensation management is defined by its pay philosophy and process that has been a key component in employee engagement and satisfaction that also correlates with firm success. This neuro-fuzzy inference system was able to design an objective compensation algorithm that objectively identified relevant variables for qualified applicants in the hiring and selection stage that will be the baseline of an employee’s initial salary. The output is a salary grade matrix that allows adjustment discretion according to the standards of the HR department who may have preference to either one of the variables. This will now simultaneously function as an operational framework in the performance management stage for current employees and serve as a benchmark during annual salary reviews. An artificial neural network employed all parameters in the categorical traits in the performance evaluation of employees that targets errors that are not normally detected in the traditional review method that is subjected to preferential bias, favoritism or irregularities. The ANN structure output produced 5 numerical decisions to upgrade, maintain and downgrade the salary grade that will coincide with both organizational objectives and HR compensation policies. © 2019, World Academy of Research in Science and Engineering. All rights reserved. Data-driven decisions in employee compensation utilizing a neuro-fuzzy inference system Artificial intelligence; Compensation; HR analytics; Management; Neural-Fuzzy Inference System; Performance Evaluation; Salary Grade ",Value management
1206,Managing for competency with innovation change in higher education: Examining the pitfalls and pivots of digital transformation,"Digital transformation recently converged on organizations as a new paradigm—a must-have exemplar—to enable competitive advantage. While the effects of digital transformation and their analytics, along with platform technologies, are becoming pronounced in companies, there is still a need to examine their implications on higher education. In light of the dynamics of digital transformation, how can higher education better manage the shift toward newer competencies and the need for innovation presented by the emergence of digital technologies? In this article, I examine the issues around the need for this balance—often defined in strategy as ambidexterity, or the need to address both competency with innovation—by outlining the historical trajectories that led to this problem in higher education, identifying three common pitfalls that higher education programs and administrators face, and tying these issues to higher education's absorptive capacity. To resolve these pitfalls, this article builds upon absorptive capacity frameworks for education practitioners and strategies as a prospective change management tool. © 2019 Kelley School of Business, Indiana University Managing for competency with innovation change in higher education: Examining the pitfalls and pivots of digital transformation Digital transformation; Exploration and exploitation; Higher education; Managing change; Organizational ambidexterity; Strategic management ",Risk management
1207,Sport Facility Operations Management: A Global Perspective,"Now in a fully revised and updated third edition, Sport Facility Operations Management goes beyond the basic theories of sport facility management to include relevant practical professional experiences connecting facilities, people, and technology. This is a comprehensive and engaging textbook introducing cutting-edge concepts and best practice in sport facility operations management. Each chapter contains real-world case studies and discussion questions, innovative 'Technology Now' and new 'Facility Focus' features, and 'In the Field' segments about what is going on in the industry. This new edition also provides new content in the areas of project management, social and digital media, revenue generation and diversifi cation, performance analytics, and impacts and legacies. This is a vital resource for sport management educators and students, especially those studying facility management. It is also an interesting read for industry professionals working in sport facility management, from grassroots and community complexes to global mega stadiums and arenas. Dedicated online materials include PowerPoint presentations for each chapter; multiple-choice and essay questions; online appendices with diagrams, schematics, manuals, and forms; a glossary; and a sample master syllabus. © 2020 Eric C. Schwarz, Stacey A. Hall and Simon Shibli. All rights reserved. Sport Facility Operations Management: A Global Perspective  ",Risk management
1208,Research on passenger flow prediction of Beijing subway based on spatiotemporal correlation analysis,"Urban rail transit has significant advantages such as large traffic volume, fast speed and high comprehensive benefits. It has become the most important component of urban traffic construction management and urban traffic congestion solution. Based on the actual passenger flow data of the Beijing subway Line 2 within one day, this paper analyzes the unbalanced distribution of passenger flow in space and time by using the method and theory of short-term traffic flow prediction. The gray correlation between the overall passenger flow of Line 2 and the passenger flow data of each site in one day was compared, and four stations with the highest gray correlation were selected for subsequent passenger flow forecasting. Then, based on the passenger flow of Line 2, four stations and corresponding total passenger flow data are taken as training samples, and the time is from 4:00 to 18:00. Taking the data of the four stations with the highest correlation as input, the short-term prediction of the passenger flow from 18:00 to 19:00 is used as the output. © 2019 IEEE. Research on passenger flow prediction of Beijing subway based on spatiotemporal correlation analysis BP neural network; gray system; passenger flow analysis; passenger flow forecasting; Subway Line 2 Advanced Analytics; Big data; Cloud computing; Computation theory; Forecasting; Light rail transit; Motor transportation; Neural networks; Project management; Street traffic control; BP neural networks; Construction management; Gray system; Passenger flow predictions; Passenger flows; Spatiotemporal correlation; Subway lines; Urban traffic congestion; Traffic congestion",Monitoring and control
1209,Data mining approach to effort modeling on agile software projects,"Software production is a complex process. Accurate estimation of the effort required to build the product, regardless of its type and applied methodology, is one of the key problems in the field of software engineering. This study presents the approach to effort estimation on agile software project using local data and data mining techniques, in particular k-nearest neighbor clustering algorithm. The applied process is iterative, meaning that in order to build predictive models, sets of data from previously executed project cycles are used. These models are then utilized to generate estimate for the next development cycle. Used data enrichment process, proved to be useful as results of effort prediction indicate decrease in estimation error compared to the estimates produced solely by the estimators. The proposed approach suggests that similar models can be built by other organizations as well, using the local data at hand and this way optimizing the management of the software product development. © 2020 Slovene Society Informatika. All rights reserved. Data mining approach to effort modeling on agile software projects Agile scrum; Data mining; Effort estimation; K-nearest neighbor; Project management; Software engineering Clustering algorithms; Iterative methods; Nearest neighbor search; Predictive analytics; Software engineering; Accurate estimation; Complex Processes; Effort Estimation; Estimation errors; K-nearest neighbor clustering; Predictive models; Software product development; Software production; Data mining",Strategic alignment
1210,Data science in the business environment: customer analytics case studies in SMEs,"Purpose: A vast amount of complex data is being generated in the business environment, which enables support for decision-making through information processing and insight generation. The purpose of this study is to propose a process model for data-driven decision-making which provides an overarching methodology covering key stages of the business analytics life cycle. The model is then applied in two small enterprises using real customer/donor data to assist the strategic management of sales and fundraising. Design/methodology/approach: Data science is a multi-disciplinary subject that aims to discover knowledge and insight from data while providing a bridge to data-driven decision-making across businesses. This paper starts with a review of established frameworks for data science and analytics before linking with process modelling and data-driven decision-making. A consolidated methodology is then described covering the key stages of exploring data, discovering insights and making decisions. Findings: Representative case studies from a small manufacturing organisation and an independent hospice charity have been used to illustrate the application of the process model. Visual analytics have informed customer sales strategy and donor fundraising strategy through recommendations to the respective senior management teams. Research limitations/implications: The scope of this research has focused on customer analytics in small to medium-sized enterprise through two case studies. While the aims of these organisations are rather specific, they share a commonality of purpose for their strategic development, which is addressed by this paper. Originality/value: Data science is shown to be applicable in the business environment through the proposed process model, synthesising micro- and macro-solution methodologies and allowing organisations to follow a structured procedure. Two real-world case studies have been used to highlight the value of the data-driven model in management decision-making. © 2020, Emerald Publishing Limited. Data science in the business environment: customer analytics case studies in SMEs Business analytics; Business strategy; Data analytics; Decision-making; Modelling ",Strategic alignment
1211,How Digital Information Transforms Project Delivery Models,"This study articulates how increasingly pervasive digital information transforms project delivery models. It builds on and extends the literature on innovation and knowledge codification, analyzing London’s evolving digital innovation ecosystem across 15 years of industry/government initiatives and infrastructure megaprojects. Findings suggest profound and ongoing changes in digitally enabled project delivery models. Novel contributions are: first, to identify new generations of integrated solutions; second, to articulate changes in supply chains and relationships with owners, operators, and end users; and third, to recognize the growing importance of digital workflows and analytics, rather than documents. There are implications for project management practice and scholarship. © 2019 Project Management Institute, Inc. How Digital Information Transforms Project Delivery Models digital information; digital maturity; infrastructure megaprojects; innovation ecosystems; integrated solutions; project delivery models ",Capacity management
1212,Jockeying for Position in CEO Letters: Impression Management and Sentiment Analytics,"This paper evidences the strategic positioning of positive and negative words within a CEO letter as a subtle form of impression management. We find that managers tend to present information in such an order that the reader of the CEO letter has a more positive perception of the underlying message. We uncover a smile in the frequency of positive words within the letter, and a half smile in the intratextual distribution of negative words, with a prevalence of negative words at the beginning of the letter. We also find a significant positive association between this qualitative impression management and the use of abnormal accruals in earnings management. We propose sentiment analytics that can compensate for the strategic management of narrative structure and find that the proposed position weighted sentiment has more predictive power for the firm performance over the next year. © 2018 Financial Management Association International Jockeying for Position in CEO Letters: Impression Management and Sentiment Analytics  ",Monitoring and control
1213,Data Science in Public Mental Health: A New Analytic Framework,"Understanding public mental health issues using data science and finding solutions based on the findings from the data science projects can be complex and requires advanced techniques, compared to conventional data analysis projects. It is important to have a comprehensive project management process to ensure that project associates are competent and have enough knowledge to implement the data science process. Therefore, this paper presents a new framework that mental health professionals can use to solve challenges they face using data science. Although a large number of research papers have been published on public mental health, few have addressed the use of data science in public mental health. Recently, Data Science has changed the way we manage, analyze and leverage data in healthcare industry. Data science projects differ from conventional data analysis, primarily because of the scientific approach used during data science projects. One of the motives for introducing a new framework is to motivate healthcare professionals to use 'Data Science' to address the challenges of mental health. Having a good data analysis framework and clear guidelines for a comprehensive analysis is always a plus point. It also helps to predict the time and resources needed in the early in the process to get a clear idea of the problem to be solved. © 2019 IEEE. Data Science in Public Mental Health: A New Analytic Framework Data Mining; Data Science; Mental Health; Public Health; Visual Data Exploration Data handling; Data Science; Health care; Project management; Public health; Analysis frameworks; Comprehensive analysis; Comprehensive projects; Finding solutions; Health care professionals; Healthcare industry; Mental health; Visual data exploration; Data mining",Risk management
1215,"Project portfolio selection problems: A review of models, uncertainty approaches, solution techniques, and case studies","Project portfolio selection has been the focus of many scholars in the last two decades. The number of studies on the strategic process has significantly increased over the past decade. Despite this increasing trend, previous studies have not been yet critically evaluated. This paper, therefore, aims to presents a comprehensive review of project portfolio selection and optimization studies focusing on the evaluation criteria, selection approach, solution approach, uncertainty modeling, and applications. This study reviews more than 140 papers on project portfolio selection research topic to identify the gaps and to present future trends. The findings show that not only the financial criteria but also social and environmental aspects of project portfolios have been focused by researchers in project portfolio selection in recent years. In addition, meta-heuristics and heuristics approach to finding the solution of mathematical models have been the critical research by scholars. Expert systems, artificial intelligence, and big data science have not been considered in project portfolio selection in the previous studies. In future, researchers can investigate the role of sustainability, resiliency, foreign investment, and exchange rates in project portfolio selection studies, and they can focus on artificial intelligence environments using big data and fuzzy stochastic optimization techniques. © 2019 The Author(s). Project portfolio selection problems: A review of models, uncertainty approaches, solution techniques, and case studies Case studies; Evaluation criteria; Project portfolio selection; Selection approach; Solution approach; Uncertainty approach ",Value management
1216,Agile projects and big data,"In this paper, we explore the Agile approach to IT and big data project management and explain the benefit of agile methodologies for Big Data projects. The primary benefit of adopting an Agile methodology is its fluidity, this enables projects to adapt to changing assumptions, hypotheses, and requirements in a transparent way. Organizations are modernizing by using big data to power important decisions. They are refining agile approaches to Big Data problems and develop new techniques. An Agile approach is beneficial for big data projects because it allows analysts to gain valuable insights quickly, even from large data sets. This is possible by the way Agile projects break data sets down into smaller increments and is assisted by the continuous testing process. The paper reviews benefits of agile methodologies and explain how applying Agile IT methodology to Data Science Projects and Big Data Projects and why does Data Science need Agile methodologies. © 2019 Academic Conferences and Publishing International Limited. All rights reserved. Agile projects and big data Agile project; Big data; Big data project management; Data science Data Science; Knowledge management; Project management; Agile approaches; Agile Methodologies; Continuous testing; Data problems; Large datasets; Science projects; Big data",Risk management
1217,Business analytics for strategic management: Identifying and assessing corporate challenges via topic modeling,"Strategic management requires an assessment of a firm's internal and external environments. Our work extends the body of management tools (e.g., SWOT analysis or growth-share matrix) by proposing an automated text mining framework. Here we draw on narrative materials from firms (e.g., financial disclosures) and perform topic modeling so as to identify the key issues faced by an organization. We then quantify the use of language along two dimensions: risk and optimism. This reveals a firm's strengths and weaknesses by identifying business units, activities, and processes subject to risk, while also comparing it with competitors or the market. © 2018 Elsevier B.V. Business analytics for strategic management: Identifying and assessing corporate challenges via topic modeling Business analytics; Firm performance; Latent Dirichlet allocation; Strategic management; Text mining; Topic modeling Advanced Analytics; Statistics; Strategic planning; Business analytics; Firm Performance; Latent Dirichlet allocation; Strategic management; Text mining; Topic Modeling; Data mining",Strategic alignment
1218,Upgrading the business intelligence system by implementing the decision tree model in the R software package,"Business Intelligence is the key and basis of a modern understanding of management. The organizations that are able to manage their data resources, information and knowledge are more successful and competitive than the others. These organizations, as a rule, rely on modern strategic management concepts and develop business intelligence systems. Certain changes have taken place in the world of research in recent years, and open-source software packages are now most commonly used for statistical surveys. The R package in particular is gradually becoming the dominant platform for companies that are unable to spend too much on software. The R package has gathered a huge community of enthusiasts who are constantly building upon the latest developments in statistics and data mining at no cost to the end user. The main problem related to this approach lies in the data sources, because the R package is not able to store large amounts of data, as it was designed for data analysis and the respective data is mostly stored on other platforms. The aim of this paper is to find a common solution and correlation between BI, which is based on data warehouses and programs for statistical data processing, and the open-source R package on the other hand in order to obtain timely information in the shortest possible time, according to different criteria, by applying the decision tree model. Decision makers will be able to use the proposed solution to make decisions with confidence even if they do not possess the pertinent IT knowledge. © 2020, National Institute for R and D in Informatics. Upgrading the business intelligence system by implementing the decision tree model in the R software package Business intelligence; Decision tree; Machine learning; ARE ",Governance
1220,The potential of digital technology to improve construction productivity,"Despite increasing adoption of digital technology in construction, productivity reports have remained disappointing. To develop insights into the reasons of this contradiction, the present paper suggests drawing on organisational competitiveness literature considering that the factors-Affecting-productivity are conveniently captured within that literature. Through a questionnaire survey, the paper analyses the views of managers in the UK construction industry regarding the effect of Building Information Modelling (BIM) and Big Data Analytics (BDA) on organisational competitiveness. The results are then traced back to the factors-Affecting-productivity for discussion. It is concluded that digitalisation enables performance improvements that can be tied to productivity gains, but this relies on the presence of certain skills and knowledge, which require training. It is also concluded that the lack of impact of digitalisation on some of the factors-Affecting-productivity may be limiting the impact of digitalisation on the overall productivity, thus leading to a stagnating productivity. © 2019 Association of Researchers in Construction Management, ARCOM 2019 - Proceedings of the 35th Annual Conference. All rights reserved. The potential of digital technology to improve construction productivity Big data; BIM; Competitiveness; Digital technology; Productivity Architectural design; Big data; Competition; Construction industry; Data Analytics; Project management; Surveys; Building Information Modelling; Competitiveness; Construction productivity; Digital technologies; Factors affecting productivity; Productivity gain; Questionnaire surveys; UK construction industry; Productivity",Financial management
1221,Maintaining and sustaining a telehealth-based ecosystem,"Telehealth and telemedicine have since long been described as something exotic, out of this world with tele- as the key component. Many projects have had problems related to high costs of systems, poor orientation of the stakeholders, connectivity, and obsolescence as newer technologies came up, with low or complete disuse. Justification was hence for situations where this was the only choice like during space travel. Of late, benefits are realized within hybrid systems where a telecomponent serves a smaller but specific purpose to cover gaps in existing or new projects. The overall target of health-care delivery or improvement is thereby maintained. Examples include collection of health data, efficiency, time saving, and decrease the out of pocket costs for travel to avail care. This chapter discusses processes required to integrate the ""tele"" component into health-care projects. These require planning, project management strategy including budgeting, advocacy and media support, orientation and training of staff, test runs, maintenance, and other administration issues. © 2020 Elsevier Inc. All rights reserved. Maintaining and sustaining a telehealth-based ecosystem Accounting and book keeping; Analytics; Appointment roster; Needs assessment; Project management; Routinization ",Stakeholder management
1223,Data science approach for it project management,"Majority of the IT companies realized that ability to analyse and use data, could be one of the key factors for increasing of number of successful projects, portfolios, programs. Key performance indicators based on data analysis helps organizations be more prosperous in a long term perspective. Also, statistical data are very useful for monitoring and evaluation of project results which are very important for managers, delivery directors, CTO and others high level management of company. The Data Science methods could make more efficient project management in several of business problems. Analysis of historical data from the project life-cycle based on Data Science models could provide more efficient benefits for different stakeholders. Differential of the project data vector with target as an integral evaluation of the project success which allow for the complex correlations between separate features. Therefore, the influence of features importance and override creatures could be decreased on the target. This study propose new approach based on Data Science providing more efficient and accurately project management, taking into account best practices and project performance data. © 2019 Janis Grabis, Bohdan Haidabrus, Serhiy Protsenko, Iryna Protsenko, Anna Rovna. Data science approach for it project management Business processes; Data analysis; Machine learning; Project management Benchmarking; Data handling; Data reduction; Data Science; Learning systems; Life cycle; Business Process; Complex correlation; Integral evaluation; It project managements; Key performance indicators; Long-term perspective; Monitoring and evaluations; Project performance; Project management",Monitoring and control
1225,Design and Implementation of Business Intelligence Dashboard for Project Control at the Port Harbor-Company,"This study aims to assist resource management and the project controller at Port Harbor Company. Port Harbor Company is an IT Consulting. Problems that often occur in IT Consulting are related to the project cost, resource management and control of each project. Problems that occur at the Port Harbor Company are poor resource management, spending costs that exceed a predetermined budget and how to see the status of ongoing projects. Using Business Intelligence and the dashboard as a front end will help the project controller and resource management manage resources, manage project costs and monitor ongoing projects. In this study, the development and implementation of Business Intelligence and dashboards using the AS SOON AS POSSIBLE method are expected to be more structured and can solve the Port Harbor Company problem. © 2020, Springer Nature Singapore Pte Ltd. Design and Implementation of Business Intelligence Dashboard for Project Control at the Port Harbor-Company Business Intelligence; Business Object; ERP; Project Management Budget control; Competitive intelligence; Controllers; Database systems; Enterprise resource planning; Information analysis; Natural resources management; Ports and harbors; Project management; Resource allocation; Business objects; Design and implementations; Front end; Project control; Project cost; Resource management; Costs",Monitoring and control
1226,Leveraging internet of things and big data analytics initiatives in European and American firms: Is data quality a way to extract business value?,"Big data analytics (BDA) and the Internet of Things (IoT) tools are considered crucial investments for firms to distinguish themselves among competitors. Drawing on a strategic management perspective, this study proposes that BDA and IoT capabilities can create significant value in business processes if supported by a good level of data quality, which will lead to a better competitive advantage. Responses are collected from 618 European and American firms that use IoT and BDA applications. Partial least squares results reveal that better data quality is needed to unlock the value of IoT and BDA capabilities. © 2019 Elsevier B.V. Leveraging internet of things and big data analytics initiatives in European and American firms: Is data quality a way to extract business value? Big data analytics; Dynamics capability theory; Internet of things; Knowledge-based theory; Strategic management Advanced Analytics; Big data; Competition; Data Analytics; Knowledge based systems; Least squares approximations; Strategic planning; Business Process; Business value; Competitive advantage; Dynamics capability; Internet of thing (IOT); Knowledge based; Partial least square (PLS); Strategic management; Internet of things",Strategic alignment
1227,Global halal food discourse on social media: a text mining approach,"No previous studies have explored the distinct role played by halal food discourse on social media. This study fills this research gap by investigating the structure, dynamics and sentiments related to halal food tweets using methods borrowed from text mining and social network analytics. Based on sample of 11,700 world-wide random tweets, we used both the NRC and the AFINN lexicons to gauge worldwide sentiments towards halal food. Although results show a generally positive sentiment, the tweets also reflect a general concern about animal welfare. Community detection techniques revealed a strong homophily effect among different halal food communities. This effect occurs when actors in an environment similar to a virtual room deal with topics of common interest or discuss common agenda. Our findings hold important implications for different stakeholders as they underscore the relational nature of halal food networks and the importance of the strategic management of social media as a critical communication tool. © 2020, © 2020 Journal of International Communication. Global halal food discourse on social media: a text mining approach Halal food; sentiment analysis; social media; topic modelling; Twitter ",Stakeholder management
1229,KairÓS: Predictive model based on context histories for time management in projects; [KairÓS: Modelo preditivo baseado em histÓricos de contextos para o gerenciamento de tempo em projetos],"This paper presents a computational model entitled Kairós forprediction and recommendation in project schedules. The modeluses context prediction mechanisms based on task data and projectsstored during its execution. The recommendations are made to themanager in a proactive manner, considering best practices inproject management and learning with the approval or rejection ofeach recommendation. A prototype was implemented based on theproposed model, and through it, an evaluation was carried out usingsimulated use cases with real data from a large company. Theresults showed that the model was able to predict with precision of93% if a task would be completed with delay, with 87% accuracy. © 2019 Association for Computing Machinery. KairÓS: Predictive model based on context histories for time management in projects; [KairÓS: Modelo preditivo baseado them histÓricos de contextos para o gerenciamento de tempo them projetos] Context histories; Prediction; Project management; Project schedules; Recommendation; Time management; Ubiquitouscomputing Forecasting; Information systems; Information use; Project management; Ubiquitous computing; Best practices; Computational model; Context predictions; Large companies; Predictive modeling; Project schedules; Recommendation; Time management; Predictive analytics",Risk management
1230,Machine learning in oil and gas; a SWOT analysis approach,"Digitalization of workflows using machine learning and advanced analytics is the new go-to strategy to add business value in the oil and gas industry. Enterprises strive to embrace these new technologies; but struggle to put their models in production, deliver tangible results and obtain favorable returns on investment. This paper reviews some of the recent developments and practices in this area and offers a SWOT analysis for strategic management and technology enablement. It is concluded that to reap the full benefits of ML in mission-critical tasks, oil and gas industry must leverage the latest technology developments, have a consistent strategic focus and build agile and collaborative teams of data scientists and domain experts. © 2019 Elsevier B.V. Machine learning in oil and gas; a SWOT analysis approach Analytics; IoT; Machine learning; SWOT; Technology; Transfer learning Advanced Analytics; Gas industry; Learning systems; Analytics; Collaborative teams; Mission critical tasks; Oil and Gas Industry; Returns on investment; Strategic management; SWOT; Transfer learning; corporate strategy; gas industry; investment; machine learning; management; oil industry; technological development; Machine learning",Strategic alignment
1231,A survey of requirements for Thailand's industry 4.0: The perspectives from academics and entrepreneurs,"The main objective of this study is to explore various requirements for improving skills and knowledges needed for Thailand's 4.0 industry. The findings are used to develop the Master's Degree Program in Industrial Engineering for Thailand Sustainable Smart Industry (MSIE 4.0). The aim of this created program-based needs of instructors and employers is to serve high skilled and graduated students for commercial entrepreneurs. Likewise, academics can improve their teaching and learning approaches via the curriculum. In this study, the respondents are instructors and employers, who have working experience ranged from 0 to 10 years and greater. There are 225 participants from Bangkok and its suburbs, North, South, Northeast and West of Thailand agree to participate in the study. The respondents are allowed to assess their needs based on 16 required skills and knowledges: enterprise management in digital economy, project management for industry 4.0, smart operations management, quality management for extended enterprise, sustainable supply chain management, digital factory, advanced optimization, intelligent decision support systems, applied data analytics, cyber-physical industrial systems, collaborative manufacturing systems, additive manufacturing for industry 4.0, innovative product design and development, human-centric design for operator 4.0, customer experience-driven design, and leadership communication and people development in digital era. The results can promote the short training courses, which can be used to enhance the knowledge of instructors, graduates, and employers accurately. Moreover, the course learning outcomes (CLOs) of each subject in MSIE4.0 are focused and designed based on the guidelines of this study. © 2020 University of Minho. All rights reserved. A survey of requirements for Thailand's industry 4.0: The perspectives from academics and entrepreneurs Curriculum Requirements; Industry 4.0; Required Skills; Student Learning Outcomes ",Strategic alignment
1232,New toys and tricks for old dogs,"Nuclear industry stands to benefit by adapting recent developments in robotics, drones, cloud computing, digital communication, machine-learning and data analytics, virtual reality [10], 4D construction management tools and photogrammetry, and in additive manufacturing (AM). Even better tools will be available as AR becomes more mature, and becomes available for industrial applications. While it is challenging to stay abreast with these rapidly changing technologies, the rewards are expected to be high. © 2019 American Nuclear Society. All rights reserved. New toys and tricks for old dogs  Data Analytics; Digital communication systems; Nuclear industry; Project management; Toys; 4D construction management; Digital communications; 3D printers",Risk management
1233,An Analysis of Log Management Practices to reduce IT Operational Costs Using Big Data Analytics,"The security operations center (SOC) is a team that is related to many people, processes, and all the technologies. They must monitor the situation through detection, quarantine, and remediation of IT threats. They must consider the potential events in IT operations, whether it is a malicious threat or how it may otherwise affect the business. They need to have adequate staff to ensure that all relevant screens are monitored and tracked. Consequently, IT operational costs have increased according to the number of people employed, as well as the technology tools invested in building SOC teams. This paper proposes a log management architecture using Big Data Analytics to make a new generation of SOC. We have integrated all necessary notification tools from 'many' to 'one screen.' All monitoring modules have been combined for increasing automation, including replacing new hires in routine operational tasks. Furthermore, based on the results of this analysis, it is found that this form of work is a useful solution that can cover all the necessary tasks better than the original work model. Moreover, the cost analysis in operational reporting has shown that IT expenses decreased by 60% while the return on investment (ROI) shows that the solution can be cost-effective within six months. Consequently, a satisfaction survey shows this solution has a good satisfaction score (4.5 / 5) because employees have more work flexibility and therefore feel less stressed and overburdened. © 2019 IEEE. An Analysis of Log Management Practices to reduce IT Operational Costs Using Big Data Analytics Big Data Analytics; Component; Network Operation Center; Operations Management; Strategic Management Advanced Analytics; Big data; Cost effectiveness; Data Analytics; Engineering research; Information management; Component; Log managements; Network operation centers; Number of peoples; Operational tasks; Operations management; Security operations; Strategic management; Cost benefit analysis",Strategic alignment
1234,A multi perspective framework for enhanced supply chain analytics,"Supply chain analytics, especially in the field of food supply has become a strategic business function. Monthly executive sales and operation planning meetings utilize supply chain analytics to inform strategic business decisions. Having identified gaps in the strategic management of food supply chains, a multi perspective supply chain analytics framework is developed incorporating process and data attributes to support decision making. Using Design Science as the research methodology, a novel framework with a supporting IT artefact is built and presented with early evaluation results. The resulting multi perspective supply chain analytics framework equips practitioners to identify strategic issues, providing important decision support information. The case study further illustrates the framework has applicability across all integrated food supply chains. This research has highlighted gaps in the application of process science to the supply chain management domain, particularly in the area of simultaneous assessment of process and data. The outcomes contribute to research in this domain providing a framework that will enhance the significant reference modelling and operational management work that has occurred in this field. © Springer Nature Switzerland AG 2020. A multi perspective framework for enhanced supply chain analytics Food supply chains; Multi perspective supply chain analytics; Process and data analytics Decision making; Decision support systems; Enterprise resource management; Food supply; Assessment of process; Decision supports; Multi-perspective frameworks; Operation planning; Operational management; Research methodologies; Strategic business; Strategic management; Supply chain management",Strategic alignment
1235,Smarter Crossing Analytics System to Predict and Anticipate the Student Behavior for Self Automatic Adaptation of Academic Learning,"In contrast to several domains (such as industry and economy) that have used the development of Big Data, artificial intelligence approaches and decision support systems, the field of education does not know a revolutionary deployment of this technology. In this perspective, we suggest an innovative approach for observing and analysing students’ behaviour during their university studies. For example, to decrease the number of young graduates who are finding various difficulties in their professional launch, related to their training path not suitable to socio-economic market. In this paper, we will propose an original work, which consists to build patterns from the analysis of collected data to design a new system able to anticipate the future state of a student in professional level, by combining psychological and moral data with academic and professional data. These patterns help students in the future to make less risky decisions. The main objective of our paper is to create a strategic management and control system that helps students to make decisions about their study path by proposing adaptations and reorientations. The system is a decision-making system based on Neural Networks’ algorithms. The primitive test of the system that was conducted on a sample of graduate students from Moroccan Universities (70 graduates), has proved an accuracy of 75% in terms of adaptation and reorientation. © Springer Nature Switzerland AG 2020. Smarter Crossing Analytics System to Predict and Anticipate the Student Behavior for Self Automatic Adaptation of Academic Learning Artificial intelligence; Big data; Decision support systems; Education; MOOC; Neural networks; Students’ behaviour Artificial intelligence; Big data; Decision making; Decision support systems; Economics; Education; Intelligent systems; Learning systems; Neural networks; Planning; Professional aspects; Sustainable development; Analytics systems; Automatic adaptation; Decision-making systems; Graduate students; Innovative approaches; MOOC; Professional levels; Strategic management; Students",Strategic alignment
1236,Design of a customized enterprise resource planning system for a private basic education school,"Enterprise Resource Planning (ERP) system for a private basic education school focuses on the analyses of the school's resources to aid the administration in developing better economic plans and decisions. The ERP system integrates admission, enrolment, billing, inventory, human resource, and attendance systems through a shared database that supports multiple functions and allows the generation of analytics of resources that can be used by different departments to support them in both strategic and operational plans. The system analyses the interplay of school operations with resource utilization and provides reports to enhance supplies, human resources, finance, and other assets process flow. The business intelligence components of the system enable real-time, interactive access, analysis, and visualization of enrolment trends, employee turnover rate, school supplies reorder point, school services budget allocation, the status of school facilities, breakeven point analysis, aging of unpaid accounts and real-time gate attendance monitoring of students and employees. Developmental research was used in the development of the system using Rapid application development (RAD) in agile project management. The study was piloted in the School of Our Lady of La Salette, Inc., a private basic education school. The overall compliance of the developed system to the International Organization for Standardization (ISO) 25010 Software Quality Standards was a very great extent. The developed Customized ERP system in this study can provide a useful tool for the private basic education institution to support the administrators in effectively managing economic resources. © 2020 by Advance Scientific Research. Design of a customized enterprise resource planning system for a private basic education school Enterprise Resource Planning; ERP Model; Integrated System; ISO 25010 Software Quality Standards; Rapid Application Development ",Financial management
1237,Evaluating and comparing entrepreneurial ecosystems using SMAA and SMAA-S,"This paper focuses on the entrepreneurial ecosystem as a set of interdependent and coordinated factors in a territory enabling entrepreneurship. To date, academic research has failed to produce methodologies for evaluating and comparing entrepreneurial ecosystems from different perspectives that can highlight the underlying factors. Moreover, there is a lack of empirical analysis that discriminates between factors according to their importance. Taking into account these two gaps, the aim of the paper is twofold. First, it proposes the application of an accurate, robust and reliable measurement technique, namely stochastic multicriteria acceptability analysis (SMAA). It considers the variability of weights that can be assigned to the different factors, producing a probabilistic ranking to obtain a comparison among entrepreneurial ecosystems. This ranking is more reliable than a single ranking proposed by the usual composite indices that take into account a single vector of weights. Second, the paper presents a new methodology, SMAA for strategic management analytics and assessment, or SMAA squared (SMAA-S), which detects the relation between entrepreneurial ecosystem factors and growth-oriented start-ups in a territory. The results show that the most relevant entrepreneurial ecosystem factors enabling the birth and activity of high-growth start-ups, and so impacting on technology, economy and society, can be identified in cultural and social norms, government programs, and internal market dynamics. © 2018, Springer Science+Business Media, LLC, part of Springer Nature. Evaluating and comparing entrepreneurial ecosystems using SMAA and SMAA-S Entrepreneurial ecosystem; Start-ups; Stochastic multicriteria acceptability analysis; Strategic management analytics and assessment Stochastic systems; Strategic planning; Economy and society; Government projects; Probabilistic ranking; Reliable measurement; Stochastic multicriteria acceptability analyses (SMAA); Stochastic multicriteria acceptability analysis; Strategic management; Underlying factors; Ecosystems",Monitoring and control
1238,"A Bioinformatics Primer to Data Science, with Examples for Metabolomics","With the increasing importance of big data in biomedicine, skills in data science are a foundation for the individual career development and for the progress of science. This chapter is a practical guide to working with high-throughput biomedical data. It covers how to understand and set up the computing environment, to start a research project with proper and effective data management, and to perform common bioinformatics tasks such as data wrangling, quality control, statistical analysis, and visualization, with examples on metabolomics data. Concepts and tools related to coding and scripting are discussed. Version control, knitr and Jupyter notebooks are important to project management, collaboration, and research reproducibility. Overall, this chapter describes a core set of skills to work in bioinformatics, and can serve as a reference text at the level of a graduate course and interfacing with data science. © 2020, Springer Science+Business Media, LLC, part of Springer Nature. A Bioinformatics Primer to Data Science, with Examples for Metabolomics Bioinformatics; Cloud computing; Data management; Data science; Data visualization; Metabolomics; Quality control; Scripting Cloud Computing; Computational Biology; Data Management; Data Science; Database Management Systems; Databases, Factual; Humans; Metabolomics; Software; big data; bioinformatics; biomedicine; career; cloud computing; data science; data visualization; metabolomics; quality control; reproducibility; skill; biology; cloud computing; database management system; factual database; human; information processing; procedures; software",Risk management
1239,Integration of the Management Information System for Competitive Positioning,"The accelerating pace of technical innovation, coupled with a turbulent socio-political global environment, has created opportunities and challenges for companies in terms of competitiveness and sustainability particularly the manufacturing sector. Manufacturers in the developing world struggle to create knowledge and practice-based management information systems that will allow them to operate competitively in the global market. This paper explores the integration of organizational information systems for competitive positioning, using a case study of a manufacturing company operating in Sub-Saharan Africa. Our findings suggest the need for an integrated management information system that incorporates management practices based on research, knowledge management, and organizational learning and capabilities. © 2020 The Authors. Published by Elsevier B.V. Integration of the Management Information System for Competitive Positioning business intelligence; business process reengineering; IT-enabling business processes; knowledge management; strategic management ",Strategic alignment
1242,ICSSP 2018—Special issue introduction,"The International Conference on Software and System Processes (ICSSP) provides a leading forum for the exchange of research outcomes and industrial best practices in process development from software and systems disciplines. ICSSP 2018 was held in Gothenburg, Sweden, May 26 to 27, 2018, colocated with the 40th International Conference on Software Engineering (ICSE). The theme of ICSSP 2018 was studying “Demands on Processes, Processes on Demand” by recognizing the demands on processes that include the need for both well-developed plans and incremental deliveries (agile and hybrid processes), utilization of increased automation (model-based engineering and DevOps), higher degrees of customer collaboration, comprehensive analysis of existing products for reuse (open source and COTS), and performance requirements of enterprise-level architectures. This special issue includes the revised and extended versions of the five highest ranked full research papers and industry experience papers of ICSSP 2018, including the two award-winning papers. © 2019 John Wiley & Sons, Ltd. ICSSP 2018—Special issue introduction agile methods; continuous development; data science; deployment; hybrid systems development; product duality; project management ",Risk management
1243,Analysing the use of business simulation to build entrepreneurial leaders: The case of UAE learners,"The purpose of this study is to measure the relationship between developing entrepreneurial leaders and their successful performance in business simulation. At Higher Colleges of Technology (HCT) in the United Arab Emirates (UAE), learners do Strategic Management and business simulation courses during the third year of their Bachelor's degree. By the time they do the business simulation course, they will already have taken at least one or two courses in each of marketing, management, accounting, business statistics, international business, finance and innovation and entrepreneurship. These students already have an introductory exposure to the main concepts that comprise the business core. Theoretically, they are ready to test their knowledge, understanding and skills at the applied learning level. Thus business simulation is seen as a capstone course for the business-core courses. For this study, comparative performance metrics in the student group project, student self-reflective statements, overall placement in the simulation game and total shareholder return of a simulated mobile phone company are matched to the data visualisation graphics. Data visualisation is enabled by KH Coder, an unsupervised machine learning, quantitative text-analysis process. Technologies used in the analysis include correlation co-occurrence networks, centrality co-occurrence networks and multidimensional scaling. A linkage is struck between high performance in the business simulation course via a set of metrics and the visual content of the graphics, representing students' understanding of the simulation. The high performing teams in the simulation-those who best modelled overall competitive and financial success in the game-exhibited a unique pattern of understanding. Using a pattern-matching approach, a clear data structure emerges that separates high-performing teams from their competitors. This study documents success via modelling innovation and entrepreneurship through participation in strategic management and business simulation courses. The results highlight the importance of learner participation in overall innovation and entrepreneurship education, and the financial literacy gained from participating in business simulation. While business simulation is currently a hot topic, limited research is available from the Middle East connecting entrepreneurship and innovation to success in business simulation. This study takes a data analytics approach to demonstrate the value of participating in business simulation. There is a strong connection between performing well in business simulation and the unique pattern that emerges from the data analytics of the text of what students feel they have learned. © 2019 Primrose Hall Publishing Group. Analysing the use of business simulation to build entrepreneurial leaders: The case of UAE learners Business simulation; Content analysis; Entrepreneurial finance; Innovation; Machine learning; Modelling; Natural language processing; Total shareholder return ",Monitoring and control
1246,Chapter 9: Information technology issues in France,"The information technology (IT) industry in France realizes the importance of developing highly innovative IT-based business solutions in order to meet the challenges of fierce global competition. The new FrenchTech eco-system, strongly supported by the French government, is enhancing this trend. Against this backdrop, we examined the important organizational, technological and individual concerns of IT employees in France. The three topmost organizational IT issues are: Revenue generating IT innovations, security and privacy, and project management. The top-three technology issues are: Business intelligence and analytics, customer relationship management systems, and mobile and wireless systems. In general, the IT employees are satisfied with their jobs and the IT profession, and plan to stay with the current employer in the near term. © 2020 The Author(s). Chapter 9: Information technology issues in France  ",Risk management
1247,"Applications of an interaction, process, integration and intelligence (IPII) design approach for ergonomics solutions","This paper first reviews current ergonomics design approaches in delivering digital solutions to achieve a unified experience from interaction and business process design perspectives. Then, it analyses the opportunities that new technologies may bring in for enhancing current ergonomics design approaches from integration and intelligence design perspectives. To address the challenges in today’s ergonomics practices in delivering digital solutions, an interaction, process, integration and intelligence (IPII) design approach is proposed. A case study is presented that implemented the IPII approach. The quantitative data gathered from the case study demonstrates that the IPII approach has achieved significant advantages in reaching the goal of a unified experience and operational benefits for delivering digital solutions. The IPII approach also demonstrates improvements compared to today’s ergonomics design approaches, such as user-centred design, for digital solutions. Finally, the paper highlights the contributions of the IPII approach for future ergonomics practices in delivering digital solutions. Practitioner Summary: In addition to the interaction design for the UI of digital solutions, as is the case in current typical ergonomics practice, the IPII adds three additional design components: process, integration and intelligence design. The case study demonstrates the advantages of the IPII, providing an enhanced approach for designing digital solutions. Abbreviations: IPII: interaction, process, integration and intelligence; IEA: International Ergonomics; Association; HFE: human factors/ ergonomics; HCD: human-centred design; UX: user experience; UI: user interface; ISO: International Organization for Standardization; UCD: user-centred design; ERP: enterprise resource planning; E2E experience: end-to-end experience; UXD: user experience design; AI: artificial intelligence; ML: machine learning; HCI: human-computer interaction; IaaS: infrastructure as a service; PaaS: platform as a service; SaaS: software as a service; CRM: customer relation management; SCM: supply chain management; HCM: human capability management; BI: business intelligence; BOMA: Bill of Materials Application; POC: proof of concept; TCM: transition change management; SMEs: subject matter experts; PMO: program management office; UAT: user acceptance test; iBPMS: intelligent business process management suite. © 2019, © 2019 Informa UK Limited, trading as Taylor & Francis Group. Applications of an interaction, process, integration and intelligence (IPII) design approach for ergonomics solutions ergonomics design approach; human–computer interaction; intelligence design; Product design; user experience Artificial Intelligence; Equipment Design; Ergonomics; Humans; User-Computer Interface; Acceptance tests; Application programs; Artificial intelligence; Association reactions; Computer resource management; Enterprise resource planning; Ergonomics; Human computer interaction; Human resource management; Infrastructure as a service (IaaS); Integration; Learning systems; Platform as a Service (PaaS); Process design; Product design; Public relations; Software as a service (SaaS); Software testing; Supply chain management; User interfaces; Business process design; Business process management; Computer interaction; Customer relation management; Ergonomics design; International organization for standardizations; User experience; User experience design; artificial intelligence; computer interface; equipment design; ergonomics; human; procedures; User centered design",Governance
1249,Sanitary sewer infrastructure O&M and renewal prioritization from utility-wide data analytics,"The Gwinnett County Department of Water Resources has established an advanced, multipronged approach to short- and long-term asset management for its 3,000-mile gravity sanitary sewer system, with the goals of predicting 1) areas with high probability of operational failure, 2) areas with high probability of structural failure, and 3) deterioration curves for the various pipe cohorts, informing the long-term renewal process. An application of software-based analytics is described that represents a step forward in decision-making used to drive asset inspection, maintenance and capital project prioritization. The components above are jointly addressed in a mutually informative, permanently available software, providing clear, defensible, quantified overall prioritization of capital and operational expenses. Advanced analytics are brought together in a multi-criteria, optimized framework designed to continuously update itself and provide both operational and structural support on demand. The paper presents the analytics methods used, a detail of key results and their discussion, and provides an overview of the multi-criteria framework deployed to support the prioritization of assets, sub-basins and projects. © 2019 Water Environment Federation Sanitary sewer infrastructure O&M and renewal prioritization from utility-wide data analytics Asset management; Flow metering; Infiltration/Inflow (I/I); Inspections; Operations and Maintenance (O&M); Sanitary sewer; Work orders Advanced Analytics; Application programs; Asset management; Data Analytics; Decision making; Deterioration; Digital storage; Fracture mechanics; Inspection; Sanitary sewers; Water resources; Department of Water Resources; Flow metering; Operational expense; Operational failures; Operations and maintenance; Sanitary sewer systems; Sewer infrastructure; Work order; Failure (mechanical)",Strategic alignment
1250,Multi-attribute dependent bug severity and fix time prediction modeling,"A software bug is characterized by many features/attributes out of which some are entered during the time of bug reporting whereas others are entered during the bug fixing. Severity is an important bug attribute and critical factor in deciding how soon it needs to be fixed. During the initial period of bug reporting, its severity changes and get stabilizes over a period of time. Severity identification is a major task of triagers, whose success affects the bug fix time. The prediction of bug fix time will help in estimating the maintenance efforts and better software project management. We investigated the association among the bug attributes and built multi-attribute based classification and regression models for bug severity and fix time prediction. Bug severity and fix time prediction models have been built using the combinations of different independent bug attributes. We have used different classification and regression techniques, namely Support Vector Machine (SVM), Naïve Bayes (NB), k-Nearest Neighbors (k-NN), Ordinal Regression (OR), Fuzzy Linear Regression (FLR), Fuzzy Multi Linear Regression (FMLR), Multiple Linear Regression (MLR), Support Vector Regression (SVR) and k-Nearest Neighbors Regression (k-NNR) to build the models. Our models are tested on the real world datasets from famous open source project: Mozilla. k-NN gives better performance than NB and SVM in terms of precision and f-measure for bug severity prediction. In terms of goodness of fit, SVR is better than MLR and k-NNR for bug fix time prediction. The proposed mechanism is able to predict severity and fix time for newly reported bugs. Empirical results reveal that the multi-attribute based classification and regression models work well for bug severity and fix time prediction. The two newly derived attributes Summary weight and Bug age are found to be good predictors of severity across all the used techniques. In case of bug fix time prediction, Bug age is found to be a good predictor. © 2019, The Society for Reliability Engineering, Quality and Operations Management (SREQOM), India and The Division of Operation and Maintenance, Lulea University of Technology, Sweden. Multi-attribute dependent bug severity and fix time prediction modeling Bug age; Bug fix time; Bug severity; Prediction model; Summary weight Forecasting; Linear regression; Motion compensation; Nearest neighbor search; Open source software; Project management; Support vector machines; Support vector regression; Bug age; Bug fixes; Bug severity; Prediction model; Summary weight; Predictive analytics",Capacity management
1253,Information System Guided Supply Chains and their Visual Analytics in Integrated Project Management,"From a digital ecosystem perspective, sustainability is a manifestation of a composite entity with multiple data attribute dimensions. The data relationships may emerge between geographically distributed supply chain management ecosystems and their linked human, economic and environment ecologies. The ecosystems may exhibit inherent connections and interactions. For making connections more resilient, we characterize models that serve multiple industries through numerous data associations, even in Big Data scales. In the context of Integrated Project Management (IPM), the knowledge of boundaries between systems is mysterious, analysing diverse ecosystems through a sustainable framework can uncover new insights of inherent connections. The purpose of this research is to develop a holistic information system approach, in which multidimensional data and their connectivity are analysed, recognizing the ontological cogency, uniqueness of ecosystems and their data sources. The research outcome has facilitated the tactical development of strategies for ameliorating the sustainability challenges in the IPM contexts. © 2019 Nimmagadda, Reiners, Wood & Mani. Information System Guided Supply Chains and their Visual Analytics in Integrated Project Management Information systems; integrated project management; supply chains; visual analytics Ecosystems; Information management; Information systems; Information use; Supply chain management; Sustainable development; Visualization; Data association; Data attributes; Data relationships; Digital ecosystem; Human economics; Human environment; Integrated programs; Integrated project management; Multiple data; Visual analytics; Project management",Strategic alignment
1255,"Values statements: The missing link between organizational culture, strategic management and strategic communication","Purpose: This paper aims to examine 611 values statements to determine if values statements contain characteristics of organizational culture as provided by Denison and Mishra (1995). The general hypothesis given is there is a relationship between values statements and culture characteristics. Four testable hypotheses, one for each of Denison and Mishra’s (1995) characteristics, are created and tested. Design/methodology/approach: The process in testing the hypotheses had two components. First, a taxonomy of the values statements had to be determined. This involved using natural language characteristics rather than predetermined classes to create a taxonomy based on the language itself. Second, a custom dictionary for each characteristic had to be created based on Neuendorf (2017) to perform content analysis. Once the values statements were coded with a taxonomic classification and with culture characteristics, a Chi-Square test was performed to determine a relationship between the statement type and the cultural characteristic, and then a multinomial regression test was performed to determine strength and direction of the relationships. Findings: The tests for all four hypotheses produced significant results in the Chi-Square test. The multinomial regression tests showed primarily that Business statements types often lack adaptability and stakeholder involvement cultural elements. Additionally, Religion statement types are positively related to adaptability and mission. Research limitations/implications: This paper creates a taxonomy and supplies the rules for classification. Values statements can now be classified using those rules and the classification used in analysis. Additionally, as values statements span over organizational culture, strategic management and strategic communication, these statements become a focal point for studying multiple topics across these fields. More particularly, finding the negative relationship between the Adaptability characteristic with the Business statement type and the involvement characteristic with the Business statement type may provide a cultural explanation for many mixed result studies on organizational success. Practical implications: Organizational culture can be displayed by way of values statements and can potentially affect organizational strategy and organizational communication. Wording is extremely important in creating a values statement, and that statement must clearly reflect the cultural values of the organization. Originality/value: First, this paper creates a taxonomy of values statements that is far more complete than anything created before. Second, by examining language, this paper discovers a link between organizational culture, strategic management and strategic communication. © 2018, Emerald Publishing Limited. Values statements: The missing link between organizational culture, strategic management and strategic communication Organizational culture; Strategic management; Taxonomy; Text analytics; Text mining; Values statements ",Strategic alignment
1256,Speech-acts based analysis for requirements discovery from online discussions,"Online discussions about software applications and services that take place on web-based communication platforms represent an invaluable knowledge source for diverse software engineering tasks, including requirements elicitation. The amount of research work on developing effective tool-supported analysis methods is rapidly increasing, as part of the so called software analytics. Textual messages in App store reviews, tweets, online discussions taking place in mailing lists and user forums, are processed by combining natural language techniques to filter out irrelevant data; text mining and machine learning algorithms to classify messages into different categories, such as bug report and feature request. Our research objective is to exploit a linguistic technique based on speech-acts for the analysis of online discussions with the ultimate goal of discovering requirements-relevant information. In this paper, we present a revised and extended version of the speech-acts based analysis technique, which we previously presented at CAiSE 2017, together with a detailed experimental characterisation of its properties. Datasets used in the experimental evaluation are taken from a widely used open source software project (161120 textual comments), as well as from an industrial project in the home energy management domain. We make them available for experiment replication purposes. On these datasets, our approach is able to successfully classify messages into Feature/Enhancement and Other, with F-measure of 0.81 and 0.84 respectively. We also found evidence that there is an association between types of speech-acts and categories of issues, and that there is correlation between some of the speech-acts and issue priority, thus motivating further research on the exploitation of our speech-acts based analysis technique in semi-automated multi-criteria requirements prioritisation. © 2018 Elsevier Ltd Speech-acts based analysis for requirements discovery from online discussions Classification techniques; Online discussions; Requirements engineering; Sentiment analysis; Speech-acts analysis Application programs; Classification (of information); Data mining; Engineering research; Filtration; Learning algorithms; Learning systems; Linguistics; Natural language processing systems; Open source software; Open systems; Petroleum reservoir evaluation; Project management; Requirements engineering; Sentiment analysis; Social networking (online); Speech; Classification technique; Experimental evaluation; Home energy managements; Online discussions; Open source software projects; Requirements elicitation; Speech acts; Web-based communication; Speech analysis",Stakeholder management
1257,On the Radical De- and Re-Construction of Today's Enterprise Applications,"The office applications (spreadsheets, text processing, presentation programs) used in today's enterprises are silos. So are the ERP (enterprise resource planning) systems. Tons of time is wasted to manually pump around data between office applications, ERP systems and the many further enterprise applications (CRM, DMS, BI etc. etc.) [1,2]. The whole situation is fragile (error prone), blocks business process re-engineering [3, 4] and prohibits (potentially massive) enterprise data analytics. Enterprise content management systems [5] help a little but do not fundamentally improve the situation: they do not crack the silos. The same is with the many enterprise application integration approaches (ad-hoc glue code [6], SOA platforms [7, 8, 9, 10, 11, 12], BPM suites [13, 14, 15, 16, 17, 18, 19, 20, 21]): nice-to-have, but no game changers. Enterprise wikis [22] could help somehow, but their potential is not well understood by today's CIOs [1]; and again: they are silos. In this keynote, the presenter takes a different approach and fundamentally rethinks today's enterprise application landscape. He explains the vision of a so-called massive backbone application (or system-2 application in terms of Stafford Beer's Viable System Model [23, 24, 25]). He deconstructs today's enterprise applications into their essential particles and recombines them coherently into the new approach: (i) collaborative editing of semi-structured data (wikis) [26], (ii) data types (data+operations) [27, 28], (iii) access rights mechanisms [29], (iv) version control [30, 31, 32], (v) expression-level transclusion [33, 34] of typed data [35] and (vi) an integrated querying language [35]. With such a backbone application, today's enterprise applications become deeply standardized views (or perspectives) [36, 37] on a huge, single underlying information structure. Furthermore, (web) weaving [38, 39] of features becomes a first class citizen with such a backbone application. The presenter explains, how such a backbone application can radically improve the way we work. © 2019 The Authors. Published by Elsevier B.V. On the Radical De- and Re-Construction of Today's Enterprise Applications backbone application; BI; big data; BPM; CRM; DMS; ECM; ERP; office applications; SOA; transclusions; version control; Viable Systems Model; web weaving; wikis Application programs; Big data; Bismuth; Data Analytics; Helium; Information management; Information systems; Information use; Management information systems; Military electronic countermeasures; Project management; Text processing; Office applications; transclusions; Version control; Viable systems models; wikis; Enterprise resource planning",Monitoring and control
1259,Investigating the data science skill gap: An empirical analysis,"With big data analytics constantly growing in importance for contemporary organizations so does the need for skilled professionals. Perhaps the most critical item noted in the age of data is the lack of people with the required skill-set to turn raw data into actionable insight. Building on this pressuring issue, the objective of this paper is to survey the status quo of technical and business-related data analytics skills in a range of different industries and identify the most important skills that will be needed in the next few years. To do so, this study builds on a sample of 202 survey responses from key executives from Norwegian firms. Our analysis reveals the level of skill-fulfilment in for technically and business-oriented employees in a number of key industries. In addition, we use survey data from an additional sample of 27 executives and interviews with 6 managers and provide a ranking of the perceived importance of data analytics-related skills according to respondents in three categories, technical skills, business and project management skills, and soft skills. Our study concludes with findings regarding the skill-gap that exists in the domain of data science as well as suggestions on how to fulfil these needs, indicating specific subject-areas that are of heightened importance. © 2019 IEEE. Investigating the data science skill gap: An empirical analysis Big data analytics; Business analytics; Data scientist; Empirical; Employees Advanced Analytics; Big data; Engineering education; Personnel; Project management; Surveys; Additional samples; Business analytics; Business-oriented; Data scientist; Empirical; Empirical analysis; Project management skills; Technical skills; Data Analytics",Value management
1263,PROJECT MANAGEMENT PROPOSAL WITH AGILE METHODOLOGY: ELDE CASE STUDY; [PROPUESTA DE GESTIÓN DE PROYECTOS CON METODOLOGÍA AGILE: CASO DE ESTUDIO PROYECTO ELDE],"As has been seen in [1] it is possible to change the paradigm for both small and medium-sized companies, proving theoretically that it is possible to change the style of project management from a classical approach to an agile approach. In this new approach to project management, the use of different management programs that have been emerging in recent times (Wrike, slack, ...) is raised. This new management system enhances the constant analysis of their development. In [2] they propose an analysis of the factors that contribute most to the success of the projects, contributing to the storage of data, business intelligence and the development of projects at the analytical level. In the present article we propose a case of application in a real research project called ""ELDE"". Various entities such as research centers, companies and universities participate in the present project. We present the parameters that lead us to success with AGILE in this Case study. © 2019 by the authors. Licensee AEIPRO, Spain. PROJECT MANAGEMENT PROPOSAL WITH AGILE METHODOLOGY: ELDE CASE STUDY; [PROPUESTA DE GESTIÓN DE PROYECTOS CON METODOLOGÍA AGILE: CASO DE ESTUDIO PROYECTO ELDE] Agile; ELDE; investigation; Project management ",Risk management
1264,Detecting IoT Applications Opportunities and Requirements Elicitation: A Design Thinking Based Approach,"IoT development is complex. To reduce this complexity, IoT platforms provide a set of resources and functionalities to enable application development and support its execution. In this work, we present a human-centered approach for requirements elicitation and mapping them to application resources in IoT platforms, using empathy, definition and ideation methods. A previous study by the authors has identified 11 categories of resources provided by 47 IoT platforms to developers in their application layers. From this set, 6 categories were selected for this work: schedulers and triggers, message and notification triggers, big data and analytics, artificial intelligence and machine learning, dashboards, and services. We invited 18 members of 8 projects for a workshop and divided them in 4 teams, according their project areas, which are: Industry 4.0 (6 participants), Environmental Disasters (4 participants), Environmental Management (3 participants) and Pollution (5 participants). We divided the workshop in 3 phases: warm-up, with user journey mapping, requirements identification using “how might we” questions as a trigger and requirements clustering the questions by the 6 selected categories of resources or an extra category named “others” for those which could not be related to any previous category. Our contribution for the IoT application development is an approach for turning easier requirements elicitation using DT techniques, covering the stages of empathise, definition and ideation, with well-available materials and considering the resources present at application layer of IoT platforms. © 2020, Springer Nature Switzerland AG. Detecting IoT Applications Opportunities and Requirements Elicitation: A Design Thinking Based Approach Design Thinking; Human centered design; Internet of Things; Requirement gathering Advanced Analytics; Application Layer; Artificial intelligence; Environmental management; Human computer interaction; Human resource management; Mapping; Project management; Requirements engineering; Application development; Design thinking; Environmental disasters; Ideation methods; IOT applications; Requirements elicitation; Warm up; Internet of things",Strategic alignment
1266,Online modules to introduce students to solar array control using neural nets,"The growth in the field of machine learning (ML) can be attributed in part to the success of several algorithms such as neural networks as well as the availability of cloud computing resources. Recently, neural networks combined with signal processing analytics have found applications in renewable energy systems. With machine learning tools for solar array systems becoming popular, there is a need to train undergraduate students on these concepts and tools. In our undergraduate signal processing classes, we have developed self-contained modules to train students in this field. We specifically focused on developing modules with built-in software for applying neural nets (NN) to solar array systems where the NNs are used for solar panel fault detection and solar array connection topology optimization which are essentially ML classification tasks. We initially developed software modules in MATLAB and also developed these models on the user-friendly HTML-5 JavaDSP (JDSP) online simulation environment. J-DSP allows us to create and disseminate web-based laboratory exercises to train undergraduate students from different disciplines, in neural network applications. In this paper, we describe our efforts to enable students understand the properties of the main features of the data used, the types of ML algorithms that can be applied on solar energy systems, and the statistics of the overall results. The modules are injected in our undergraduate DSP class. The project outcomes are assessed using pre and post quizzes and student interviews. © American Society for Engineering Education, 2019 Online modules to introduce students to solar array control using neural nets  Engineering education; Fault detection; Machine learning; MATLAB; Project management; Signal processing; Simulation platform; Solar cell arrays; Solar energy; Students; Classification tasks; Neural network application; Online simulation; Renewable energy systems; Solar energy systems; Student interviews; Undergraduate students; Web based laboratories; Neural networks",Monitoring and control
1267,Project Management as a Tool for Smart University Creation and Development,"The paper suggests using project management as a tool for smart university for creation and development. The methodological and information infrastructure of smart university project management is considered; it will simplify and speed up the process of management decision making and it more visible to the project team members, the project office, and the university management. Mathematical models of project management support are proposed. They can be applied in the project offices’ activities for managing groups of projects for the development of smart university. It is proposed to use business process management system (BPMS) as a core of the information support infrastructure of smart university and smart technology integration center. The use of an application programming interface (API) in the knowledge management system is also considered. The project management experience for smart university creation and development is studied on the example of Togliatti State University. © 2020, The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. Project Management as a Tool for Smart University Creation and Development API; Learning analytics; Mathematical model; Project management; Project office; Smart university Application programming interfaces (API); Computer systems programming; E-learning; Human resource management; Knowledge based systems; Knowledge management; Project management; Business process management systems; Information infrastructures; Information support; Knowledge management system; Management decision-making; Project management experience; Smart technology; Smart universities; Decision making",Strategic alignment
1268,Choosing effective internet marketing tools in strategic management,"User buying decision models are continually changing. Few people spend money spontaneously-people watch reviews, read reviews, compare prices, and only after they receive the relevant information are they ready to move on to the next stage. Therefore, the definition of useful Internet marketing tools is one of the main steps in strategic management. Along with the classic tools, such as website, email marketing, SMM, SEO and SEM, content marketing, it is necessary to implement modern, sharply gaining popularity in recent years: chatbots and instant messengers, optimization for voice search, audio content. Particular attention should be paid to end-to-end analytics, which can track the effectiveness of each advertising channel, even offline. Based on the indicators, it is necessary to optimize advertising budgets and adjust the management strategy. © BEIESP. Choosing effective internet marketing tools in strategic management Email marketing; Internet Marketing; SEM; SEO; SMM; Strategic Management ",Strategic alignment
1269,An experience report for running an REU program in an iSchool,"In this article, we report on our experiences and lessons learned from training undergraduate students in data science research in an iSchool, comparing this program to the experience of running a similar program in a computer science department. The undergraduate research training programs described were supported by the National Science Foundation (United States) through the Research Experiences for Undergraduates (REU) program. Through investigating the research tasks, reading materials, lectures, tutorials, research reports, publications, and project evaluation results, we summarize the differences in research focus of the same program running in an iSchool and in a Computer Science department. We develop a group of research activities that can be adopted for effectively training undergraduate researchers in an iSchool. Furthermore, we propose an enhancement of the undergraduate data science curriculum based on the experiences and lessons we learned from running the REU programs. © Springer Nature Switzerland AG 2020. An experience report for running an REU program in an iSchool Computer science; Data analytics; Data science; Information retrieval; Research Experiences for Undergraduates Data Science; Engineering education; Project management; Experience report; National Science Foundations; Project evaluation; Research activities; Research Experiences for Undergraduates programs; Science curriculum; Undergraduate research; Undergraduate students; Students",Risk management
1270,Project and Resource Optimization (PRO) for IT Service Delivery,"This paper identifies the needs and challenges of IT service project delivery. A hierarchical Project and Resource Optimization (PRO) architecture is presented to provide a comprehensive and systematic roadmap for coping with the decision needs at the strategic, tactical, operational and executional levels. We highlight the data-driven feature of PRO with emphasis on the modeling and algorithmic methdologies to provide dynamic and adaptive decision-support. © 2019, Springer Nature Switzerland AG. Project and Resource Optimization (PRO) for IT Service Delivery Analytics; Data-driven; Mathematical programming; Project management; Resource management ",Strategic alignment
1271,Data science and project engineering management techniques; [Técnicas de gestión de la ciencia de datos e ingeniería de proyectos],"The like most queries in information science itself, the solution is not straight forward. However, by mobilization yourself with a broader understanding of assorted project management approaches and the way they will be applied specifically for information science, you are a lot of doubtless to seek out or develop a strategy that works. This is presumably the website dedicated to information science and engineering management so learn why information science must evolve its own project management approaches, explore methodologies within the information science project management guide, and dive into specific topics within the diary. In the context of medical care, organization area unit deploying a lot of and a lot of information science comes. This show of some project management methodology which will guide the new generations of comes. The methodologies area unit terribly common in IT, information science often needs newer methodologies like adjective or extreme. This post can provides a fast review of these methodologies, show strengths and weakness of every one. First it is necessary to category the methodology in two ways: Aim and answer method. We are going too selected if the goal is totally clear from the start as in building construction or it is not clear as in research. Second, if the answer method is defined and pre-establish or if method area unit ad-hoc and discovered within the approach of develop the project. © 2019, Universidad del Zulia. All rights reserved. Data science and project engineering management techniques; [Técnicas de gestión de la ciencia de datos e ingeniería de proyectos] Data science; Data science techniques; Engineering management; Project Engineering Management techniques ",Risk management
1272,Machine Learning Algorithms for Construction Projects Delay Risk Prediction,"Projects delays are among the most pressing challenges faced by the construction sector attributed to the sector's complexity and its inherent delay risk sources' interdependence. Machine learning offers an ideal set of techniques capable of tackling such complex systems; however, adopting such techniques within the construction sector remains at an early stage. The goal of this study was to identify and develop machine learning models in order to facilitate accurate project delay risk analysis and prediction using objective data sources. As such, relevant delay risk sources and factors were first identified, and a multivariate data set of previous projects' time performance and delay-inducing risk sources was then compiled. Subsequently, the complexity and interdependence of the system was uncovered through an exploratory data analysis. Accordingly, two suitable machine learning models, utilizing decision tree and naïve Bayesian classification algorithms, were identified and trained using the data set for predicting project delay extents. Finally, the predictive performances of both models were evaluated through cross validation tests, and the models were further compared using machine-learning-relevant performance indices. The evaluation results indicated that the naïve Bayesian model provides a better predictive performance for the data set examined. Ultimately, the work presented herein harnesses the power of machine learning to facilitate evidence-based decision making, while inherent risk factors are active, interdependent, and dynamic, thus empowering proactive project risk management strategies. © 2019 American Society of Civil Engineers. Machine Learning Algorithms for Construction Projects Delay Risk Prediction Classification; Complex systems; Confusion matrices; Construction projects; Cross validation; Delay risk analysis; Machine learning; Predictive data analytics; Risk identification; Time delay Bayesian networks; Classification (of information); Construction industry; Data Analytics; Decision making; Decision trees; Forecasting; Large scale systems; Learning systems; Machine learning; Predictive analytics; Risk analysis; Risk assessment; Risk management; Time delay; Trees (mathematics); Confusion matrices; Construction projects; Cross validation; Evidence- based decisions; Exploratory data analysis; Machine learning models; Project risk management; Risk Identification; Learning algorithms",Strategic alignment
1273,A model designed for HSE big data analysis in petroleum industry,"HSE (Health, Safety, and Environment) management is one of the most concerned matters of every business, especially in petroleum Industry. Currently, analyzing the origin of accident and tracing the responsibility of accident commonly happened after the accident due to the lack of analytical theories, methods and models. This paper presents a HSE big data analysis framework which is capable of analyzing historical data of HSE management to promote the practicality and scientificity of HSE management. This paper has done much research of HSE data analytics. Based on the features of HSE management in petroleum Industry, it elaborate the open source projects and its applicable scenes in data analytics. Then, it gives suggestions of choosing open source projects to establish data analytics platform under given conditions. Last, by using data warehouse, data mining, machine learning and pattern identification technology, a HSE big data analytics framework was presented in this paper. This framework includes the level of data acquisition, data storage, data processing, data analysis, and data application. The efficient use of this model can help to untangle doubts of HSE big data analytics, discover the regularity and characteristics of accidents, and enhance supervision and warning of safety production. © 2019, International Petroleum Technology Conference A model designed for HSE big data analysis in petroleum industry Big Data; Data Analytics; Data features; HSE Management Accident prevention; Accidents; Advanced Analytics; Big data; Data acquisition; Data Analytics; Data mining; Data warehouses; Digital storage; Gasoline; Petroleum analysis; Petroleum industry; Project management; Analysis frameworks; Analytical theory; Data application; Data feature; HSE management; Open source projects; Pattern identification; Safety production; Data handling",Strategic alignment
1274,"Proceedings - 2019 ACM/IEEE 14th International Conference on Global Software Engineering, ICGSE 2019","The proceedings contain 25 papers. The topics discussed include: country stereotypes, initial trust, and cooperation in global software development teams; ShIFt - software identity framework for global software delivery; data science and development team remote communication: the use of the machine learning canvas; investigating the adoption and application of large-scale scrum at a German automobile manufacturer; challenges in adopting continuous delivery and DevOps in a globally distributed product team: a case study of a healthcare organization; understanding key business processes for business process outsourcing transition; offshore outsourcing costs: known or still hidden?; using product line engineering in a globally distributed agile development team to shorten release cycles effectively; empirical analysis of critical success factors for project management in global software development; understanding the different levels of challenges in global software development; and collaboration in global software development: an investigation on research trends and evolution. Proceedings - 2019 ACM/IEEE 14th International Conference on Global Software Engineering, ICGSE 2019  ",Risk management
1277,Senior public leaders’ perceptions of business intelligence,"Purpose: The purpose of this paper is to address the perceptions senior public leaders in local government have regarding the need for business intelligence and their perceptions of the extent to which their organizations are capable of effectively assimilating business intelligence. Design/methodology/approach: The data are from a survey on local governments’ need for and capability to use business intelligence, with a response rate of 50.5 percent, and semi-structured interviews. The survey method originates from private sector research but is adapted to local government conditions in Sweden. Findings: The leaders’ perceptions about the need for business intelligence were fragmented. Their perceptions regarding its use were even more fragmented, both between different municipalities and within municipalities. Research limitations/implications: The survey is adapted to local government conditions in Sweden and may need further changes to fit other settings. The adaptation and renewal of questions can lead to summation errors in relation to the original survey. Practical implications: The paper highlights some of the strategic areas where senior public leaders need to advance their business intelligence and prioritize specific organizational capabilities. The dominant logic, enhancing an inward-looking approach, seems to prevent a more thoroughgoing business analysis. Originality/value: The adaptation of a method that is mainly used in the private sector can give new perspectives to senior public leaders regarding the need for and use of business intelligence and can help them identify the factors that can affect the complexity and volatility in local government settings. © 2019, Emerald Publishing Limited. Senior public leaders’ perceptions of business intelligence Business intelligence; Dominant logic; Local government; Senior public leaders; Strategic management ",Strategic alignment
1279,Spatio-temporal assessment of energy consumption and socio-economic drivers in rural areas in Poland,"The new project of the Polish Energy Policy until 2040 presents a comprehensive vision of modernization of the country's energy system. One of the directions is to develop a diffused energy system based on energy clusters at the local and sub-regional scale. Clusters are expected to increase renewable energy production and consumption mainly in rural areas, as well as improve the quality of life of local communities and ensure sustainable livelihood. Effective delineation and strategic management of energy clusters should be based on evidence-based analyses. Therefore, the aim of the paper is to assess the spatial and temporal diversification of energy use in Poland in connection to socio-economic drivers from 2004 (Polish access to the European Union) to 2017. The 314 poviat units (county) are examined (NUTS 4 level). Multidimensional data exploration and geovisualisations are performed with use of an innovative location intelligence system of combined geographical information systems (GIS) and business intelligence (BI) software. The paper presents the multivariable data analysis and mapping of selected indicators of: (1) consumption of low-voltage electricity per capita in rural households, (2) population by age and (3) place of residence, (4) residential buildings development, and (5) economic growth reflected in personal income taxes. The obtained results support regional policy makers in energy clusters' location and their strategic management. The results of the study can be used to mitigate disproportions between regions affected by unsustainable suburbanisation, depopulation and ageing society, continuous growth of energy consumption as well as economic inequalities. The study was carried out at the beginning of 2019 as an input for public discussion on ongoing works on the project of the Polish Energy Policy until 2040. © 2019 Latvia University of Agriculture. All rights reserved. Spatio-temporal assessment of energy consumption and socio-economic drivers in rural areas in Poland Energy clusters; Location intelligence; Rural areas Energy policy; Energy utilization; Information analysis; Location; Population statistics; Regional planning; Rural areas; Strategic planning; Taxation; Economic inequality; Energy clusters; Location intelligences; Multidimensional data; Personal income taxes; Residential building; Strategic management; Sustainable livelihood; Economic analysis",Financial management
1281,Towards an improved ASUM-DM process methodology for cross-disciplinary multi-organization big data & analytics projects,"The development of big data & analytics projects with the participation of several corporate divisions and research groups within and among organizations is a non-trivial problem and requires well-defined roles and processes. Since there is no accepted standard for the implementation of big data & analytics projects, project managers have to either adapt an existing data mining process methodology or create a new one. This work presents a use case for a big data & analytics project for the banking sector. The authors found out that an adaptation of ASUM-DM, a refined CRISP-DM, with the addition of big data analysis, application prototyping, and prototype evaluation, plus a strong project management work with an emphasis in communications proved the best solution to develop a cross-disciplinary, multi-organization, geographically-distributed big data & analytics project. © 2018, Springer Nature Switzerland AG. Towards an improved ASUM-DM process methodology for cross-disciplinary multi-organization big data & analytics projects Analytics; ASUM-DM; Big data; CRISP-DM; Process methodology; Project management Data mining; Knowledge management; Project management; Software prototyping; Analytics; ASUM-DM; Banking sectors; CRISP-DM; Cross-disciplinary; Data mining process; Non trivial problems; Project managers; Big data",Strategic alignment
1282,Massive open online courses in software engineering education,"Despite the popularity of MOOCs in providing opportunities for socialization, collaboration, and professional improvement, there has been little research exploring them in the context of Software Engineering Education (SEE). The purpose of this study is to provide a better understanding of practices and challenges when developing academic software engineering MOOCs. To this end, we research (i) how MOOCs in SEE are designed and (ii) what learning design considerations educators and learning designers have when developing SEE-related MOOCs. To address the first research question, a systematic mapping is performed to outline the current landscape. The second research question is answered by analyzing and reflecting on practical experiences acquired during the design of a MOOC on Agile Software Development. The design of the course was based on the Pedagogical Design Pattern Framework for MOOCs, an approach to design for learning that was defined in our previous studies. For this course, two environments were considered: (a) course-time on the Tim Tec MOOC platform for deeper engagement with active learning activities, such as Project-based learning; (b) a Facebook group as a social and collaborative learning space, including scenarios based on problem-based learning to activate prior knowledge. Some of the pedagogical strategies adopted to motivate self-regulated learning included self-introduction video, diary, diagnostic self-assessments, development of small projects alone and in pairs. Our findings provide evidence that there are still several technological and pedagogical challenges that need to be addressed so as to enhance the MOOCs learning experience for SEE. Technological issues identified include the development of tools and educational games, which should be integrated into MOOCs, and the use of learning analytics to support motivation, user experience, and more active learning in specific topics, such as Project Management, Agile Methods, and Requirements Engineering. In turn, the most significant and needed improvement to the pedagogical aspects is a re-thinking of the virtual moment in the MOOC platform so as to optimize it through activities that promote active learning and contextualized assessments. © 2017 IEEE. Massive open online courses in software engineering education Agile software development; Design patterns; Learning design; Lifelong learning; MOOCs; Software engineering education Artificial intelligence; Curricula; E-learning; Project management; Software design; Software engineering; Teaching; Agile software development; Design Patterns; Learning designs; Life long learning; MOOCs; Engineering education",Strategic alignment
1283,Prioritization of business analytics projects using interval type-2 fuzzy AHP,"Because of emerging technologies, a vast amount of data can be stored and processed very easily. These advances also affect companies and many new projects are being proposed. Business analytics is the umbrella term for these projects and it denotes to the skills, technologies, activities aiming at assessment and exploration of past performance to gain an understanding for better decision making. Data and analytical models are the two main pillars of business analytics. Business analytics project can be grouped into three main groups: (i) descriptive analytics, efforts to understand what has happened in the company, (ii) predictive analytics, efforts to figure out the result of an future event, and (iii) prescriptive analytics use mathematical and computational sciences to suggest decision options to take advantage of the results of descriptive and predictive analytics. In this study a prioritization method for possible business analytics projects using Type-2 fuzzy AHP is proposed. Proposed model is composed of six criteria namely, strategic value, competitiveness, customer relations, improved decision-making, improved operations, and data quality. © 2018, Springer International Publishing AG. Prioritization of business analytics projects using interval type-2 fuzzy AHP Business analytics; Interval type-2 fuzzy sets; Multicriteria decision making; Project selection; Type-2 fuzzy AHP Computation theory; Decision making; Fuzzy logic; Fuzzy sets; Hierarchical systems; Pattern matching; Public relations; Business analytics; Interval type-2 fuzzy sets; Multi criteria decision making; Project selection; Type-2 fuzzy; Predictive analytics",Value management
1285,Data mining of project management data: An analysis of applied research studies,"Data collected and generated through and posterior to projects, such as data residing in project management software and post-project review documents, can be a major source of actionable insights and competitive advantage. This paper presents a rigorous methodological analysis of the applied research published in academic literature, on the application of data mining (DM) for project management (PM). The objective of the paper is to provide a comprehensive analysis and discussion of where and how data mining is applied for project management data and to provide practical insights for future research in the field. © 2017 Association for Computing Machinery. Data mining of project management data: An analysis of applied research studies Business analytics; Data mining; Industrial applications; Project management Competition; Data mining; Industrial applications; Academic literature; Applied research; Business analytics; Competitive advantage; Comprehensive analysis; Post-project review; Project management software; Project management",Strategic alignment
1286,"Proceedings of the International Conference on Industrial Engineering and Operations Management Bangkok, IEOM 2019","This proceedings contains 494 papers. The conference aims to provides the ideas and recent developments in the field of industrial engineering, systems engineering, service engineering, manufacturing engineering, quality and reliability engineering, operations research, engineering management, operations management and operations excellence. The theme of the conference is ""Achieving and Sustaining Excellence in Operations, Systems and Service"". Also the conference cover the following topics which include: Artificial Intelligence (AI); Automation and Agility; Business Management; Case Studies; Construction Management; Cyber Security; Data Analytics and Big Data; Decision Sciences; Defense and Aviation; Design and Analysis; E-Business/E-Service; E-Manufacturing; Energy; Engineering Education; Engineering Management (THEM); Entrepreneurship; Environmental Engineering and Management; Financial Engineering; Healthcare Operations and Services; Human Factors and Ergonomics; Industrial Services; Industry Practices and Solutions; Information Technology and Information Systems (IT & IS); Innovation; Inventory Control and Management; Lean; Logistics; Maintenance Services; Manufacturing; Material Flow Cost Accounting (MFCA); Modeling and Simulation; Operations Management (OM); Operations Research (OR); Optimization; Product Design and Development; Product Lifecycle Management (PLM); Production Planning and Control (PPC); Project Management; Quality; Reliability; Service Engineering and Service Management; Six Sigma; Statistics; Supply Chain Management (SCM); Sustainability and Green Systems; Sustainability in Supply Chain, Enterprise Operations and Strategies including Food Supply Chain; Sustainable Manufacturing; Systems Dynamics; Systems Engineering; Technology Management; Total Quality Management (TQM); Transportation and Traffic; Waste Management; Work Design, Measurements, Standards and ISO, etc. The key terms of this proceedings include aquaculture manufacturing firm, artificial neural network, mechanical solar tracker design, Excellent Performance Realization Methodologies (EPRMs), green reverse logistics network design, cotton supply chain management, laser induced fluorescence spectra (LIF), stochastic modeling, applied thermodynamics, dynamic modeling and fuzzy logic. Proceedings of the International Conference on Industrial Engineering and Operations Management Bangkok, IEOM 2019  ",Capacity management
1287,The influence of the Digital Divide on Big Data generation within supply chain management,"Purpose: The purpose of this paper is to examine the influence of the Digital Divide (DD) and digital alphabetization (DA) on the Big Data (BD) generation process, to gain insight into how BD could become a useful tool in the decision-making process of supply chain management (SCM). Similarly, the paper aims to recognize and understand, from a value-creation perspective, the correlation between DD and BD generation and between DD and SCM. Design/methodology/approach: The approach utilized in the present study consists of two steps: first, a systematic literature review was conducted aiming at finding out to determine the existing relationship between “Big Data Analytics” (BDA), “SCM” and the “DD”. A total of 595 articles were considered, and analysis showed a clear relationship among BDA, SCM, and DD. Next, the Vector autoregressive (VAR) approach was applied in a case study to prove the correlation between DD (as part of internet usage) and internet acquisitions, and in general terms the relationship between DD and Trade. Internet usage and internet acquisition in imports and exports at the European level were considered as variables in an empirical study of European trade. The novelty of this two-tiered approach consists in its application of a systematic literature review, the first of its kind, to generate inputs for the longitudinal case study of imports and exports at the EU level. In turn, the case study tested the accuracy of the theorized relationship among the main variables. Findings: By analyzing the connection between DD and internet acquisitions, a positive and long-lasting impulse response function was revealed, followed by an ascending trend. This suggests that a self-multiplying effect is being generated, and it is reasonable to assume that the more individuals use the internet, the more electronic acquisitions occur. We can thus reasonably conclude that the improvement of the BD and SCM process is strongly dependent on the quality of the human factor. Tackling DA is the new reading key in the decision-making process: quantifying the added value of the human factor in SCM is challenging and is an ongoing process, based on the opportunity cost between automation in decision-making or relying on the complexity of human factors. Research limitations/implications: One of the biggest limits in our research is the lack of the time series available on consumer orientations and preferences. Data on the typology of customer preferences, and how they are shaped, modified, or altered, were non-accessible, though big companies may have access to this data. The present longitudinal study on European trade helps clarify how and to what extent BDA, SCM, and DD are inter-related. The modeling of the theoretical framework likewise highlights several identifiable benefits for companies of adopting BDA in their business processes. Practical implications: Understanding the obstacles to DD in trade companies and states, and identifying their influence on firm performance, serves to orient the decision-making process in SCM toward reducing DD to generate important economic benefits. Enhancing internet usage may accelerate longer-term investments in human resources, offering developing countries unprecedented opportunities to enhance their educational systems and to improve their economic policies, widening the range of opportunities for businesses and poor states. Social implications: BD generation will undeniably influence microeconomic decisions: they will become evaluation tools of more efficient economic progress in small and/or large economies. However, an economically efficient society will be achievable only in those countries in which qualified human resources can generate and manage BD, to unlock its potential. This twofold effect will surely affect the socio-economic and geopolitical situation. The economic progress of conventional countries may vacillate if it is not adequately flanked by qualified human resources able to progress the information and communication technology (ICT) prevalent in contemporary economies. Consequently, the social impact of investments in ICT capacity building will necessarily affect future socio-economic scenarios. New indicators will become necessary to measure the conventional progress, and one of them will surely be DD. Originality/value: The novelty of the present study is twofold: first, it is the first meticulous meta-analysis developed using a very wide analysis of the published literature to highlight a previously hidden relationship among DD, BD, and SCM. This comparative approach made it possible to build a theoretical framework for the real evaluation of the impact of BDA on different organizational elements, including SCM. Second, the research emphasizes the need to reform and reshape the studies on BDA, convincing companies that it is necessary to understand that the obstacles (DD and DA, i.e. internet usage) must be addressed with conscious decision-making processes, strategically and resolutely, to transform points of weakness into opportunities. © 2018, Emerald Publishing Limited. The influence of the Digital Divide on Big Data generation within supply chain management Buyer-supplier relationships; Case study; Europe; Management research; Performance measurements; Strategic management; Supply chain competences; Supply chain innovation ",Value management
1288,A Systematic Framework for Assessing the Quality of Information in Data-Driven Applications for the Industry 4.0,"Managing and improving the quality of information generated in data-driven empirical studies is of central importance for Industry 4.0. A fundamental and necessary condition for conducting these activities is to be able to measure the quality of information - “If you can not measure it, you can not improve it” (Lord Kelvin). It is somewhat surprising that, with so many efforts devoted to take the most out of the available data resources, not much attention has been paid to this key aspect. Therefore, in this article we described and apply a framework, the InfoQ framework, for evaluating, analyzing and improving the quality of information generated in the variety of data-driven activities found in the Chemical Processing Industry (CPI). This systematic framework can be used by anyone involved in conducting these activities, irrespectively of the context and the specific goals to achieve. For instance, it can either be used to provide a preliminary assessment of the project risk, by analyzing the adequacy of the data set and analysis methods to achieve the intended goal, as well as to perform a SWOT analysis on an ongoing project, to improve it and increase the quality of information generated, i.e., increasing its InfoQ. The framework is applied to a real world case study in order to illustrate its implementation, utility and relevance. The author recommend its routine adoption, as part of the Definition stage in any data-driven task, such as in Lean Six Sigma projects, exploratory studies, on-line and off-line process monitoring, predictive modelling and diagnostic & troubleshooting activities. © 2018 A Systematic Framework for Assessing the Quality of Information in Data-Driven Applications for the Industry 4.0 Big Data; Industry 4.0; InfoQ; Predictive analytics; Quality of information Big data; Chemical analysis; Chemical industry; Industry 4.0; Predictive analytics; Process engineering; Process monitoring; Risk assessment; Chemical processing industry; Data-driven applications; Exploratory studies; InfoQ; Predictive modelling; Preliminary assessment; Quality of information; Systematic framework; Quality control",Strategic alignment
1289,Sustaining innovation performance in SMEs: Exploring the roles of strategic entrepreneurship and IT capabilities,"Recent research questions our understanding of the processes at play regarding information technology (IT) capabilities' contribution to innovation performance, particularly under environmental uncertainty. Strategic entrepreneurship (SE) or the interface of entrepreneurship and strategic management, which aims to answer the very question of how firms create value or wealth and sustain success in increasingly competitive and dynamic environments, is deemed the appropriate catalyst to further explore this link. Thus, this study attempts to examine the driving effects of IT capabilities on product innovation performance (PIP) by exploring the mediating role of SE. Data were collected from 164 small- and medium-sized enterprises (SME) information communication technology (ICT) firms in Canada. Partial least squares (PLS) regression tested the hypotheses derived from the research model, and data exploration and analysis including visual analytics were performed in R. Results confirm that IT capabilities drive PIP and thereby create firm level value. Secondly, SE had a direct impact on PIP, and SE partially mediates IT capabilities effect on PIP. To date, SE research has mostly been conceptual in nature making this study one of the few to empirically capture the phenomena and highlight its link to sustainable innovation performance. © 2018 by the authors. Sustaining innovation performance in SMEs: Exploring the roles of strategic entrepreneurship and IT capabilities Information technology capabilities; Product innovation performance; SME firms; Strategic entrepreneurship Canada; entrepreneur; hypothesis testing; information technology; innovation; least squares method; performance assessment; small and medium-sized enterprise; uncertainty analysis",Stakeholder management
1290,The impact of digital transformation on development of Latvian insurance companies' digitalization strategies and shift of perception values,"The insurance market is going through revolutionary change - Internet of Things, Telematics and sensor technologies, Machine Learning (MA) with intelligent automation, growth of Cyber insurance, Big Data analytics and online ecosystems with cloud computing will significantly change the landscape of the insurance services market. In this paper, a multi-criteria decision-making methodology (MCDM), Analytical Hierarchical Process (AHP), is applied in the Latvian insurance market to assess the strategic decision making criteria. As soon as nowadays managers have high risk of contradiction in pairwise comparisons due to market dynamics and high level of uncertainty, the proposed evaluation model is based on the use of the consistency-driven linguistic methodology. The subject of the study is the influence of digitalization on customer preferences and development of Latvian insurance companies' strategies. The purpose of this work is to study the development of insurance company strategies based on the shift of the perception values of clients, employees and owners of insurance companies in the era of digital transformation. According to the research results digital transformation is showing shift of customer perception values: simple product gives a greater benefit to the fact that owner only has to enter minimum information, so it is recommended to use smart-functions provided by digital opportunities to meet consumer's expectations. Within the scope of the survey, e-services perception values are mostly associated with the availability of product descriptions and comparison (aggregators), simple payment or online advisers. Copyright © 2018 International Business Information Management Association (IBIMA). The impact of digital transformation on development of Latvian insurance companies' digitalization strategies and shift of perception values Customer feedback; Digital transformation; Digitalization; Digitalization strategy; Insurance; Strategic management Data Analytics; Decision making; Financial markets; Information management; Learning systems; Linguistics; Planning; Sales; Customer feedback; Digital transformation; Digitalization; Digitalization strategy; Strategic management; Insurance",Strategic alignment
1293,"Proceedings - Frontiers in Education Conference, FIE","The proceedings contain 303 papers. The topics discussed include: remote laboratory exercises and tutorials for spectrum-agile radio frequency systems; smart start: designing impact-driven projects; increase your project's success through coordinated communication: research and practice; using course networking to enhance student engagement outside the classroom; entrepreneurial minded learning in app development courses; signal processing and machine learning concepts using the reflections echolocation app; Simvascular as an instructional tool in the classroom; beginning to understand variation in teaching approaches to game-based learning; a serious game to support the drug misuse prevention for teenagers students; DATARPG: improving student motivation in data science through gaming elements; using threshold concepts to restructure an electrical and computer engineering curriculum: troublesome knowledge in expected outcomes; design and implementation of an enterprise integrated project environment: experience from an information systems program; integrating NEMS/MEMS with IoT applications into an innovative ECE senior elective course; improving communication in multicultural teams - a web-based model and its application in project management education; development of a method to study real-time engineering writing processes; and assessing students' higher education performance in minority and non-minority serving universities. Proceedings - Frontiers in Education Conference, FIE  ",Risk management
1294,Exploring project management methodologies used within data science teams,"There are many reasons data science teams should use a well-defined process to manage and coordinate their efforts, such as improved collaboration, efficiency and stakeholder communication. This paper explores the current methodology data science teams use to manage and coordinate their efforts. Unfortunately, based on our survey results, most data science teams currently use an ad hoc project management approach. In fact, 82% of the data scientists surveyed did not follow an explicit process. However, it is encouraging to note that 85% of the respondents thought that adopting an improved process methodology would improve the teams' outcomes. Based on these results, we described six possible process methodologies teams could use. To conclude, we outlined plans to describe best practices for data science team processes and to develop a process evaluation framework. © 2018 Association for Information Systems. All rights reserved. Exploring project management methodologies used within data science teams Big data; Data science; Process methodology; Project management Big data; Information systems; Information use; Project management; Surveys; Best practices; Data science; Improved process; Process Evaluation; Project management methodology; Stakeholder communications; Team process; Human resource management",Risk management
1295,"18th Online World Conference on Soft Computing in Industrial Applications, WSC 2018","The proceedings contain 20 papers. The special focus in this conference is on Soft Computing in Industrial Applications. The topics include: Intelligent land cover detection in multi-sensor satellite images; a knowledge extraction framework for call center analytics; Soft LMI-based H∞ control with time delay; sharing without losing and donation: Two new operators for evolutionary algorithm with variable length chromosome; optimizing quantitative and qualitative objectives by user-system cooperative evolutionary computation for image processing filter design; tabu search algorithm for the vehicle routing problem with time windows, heterogeneous fleet, and accessibility restrictions; applying multi-objective particle swarm optimization for solving capacitated vehicle routing problem with load balancing; some modifications in binary particle swarm optimization for dimensionality reduction; collaborative project management framework for partner network initiation in machining domain; adaptive information processing in computer-aided product development; reflections upon measurement and uncertainty in measurement; Experimental FuzzyWA aggregated location selection model for very large photovoltaic power plants in global grid in the very early engineering design process stage; Experimental FOWA aggregated location selection model for VLCPVPPs in MENA region in the very early engineering design; fuzzy demand forecast classification and fuzzy pattern recognition for distributed production; modeling excess carbon dioxide emissions from traffic congestion in urban areas; an experimental case study on fuzzy logic modeling for selection classification of private mini hydropower plant investments in the very early investment stages in Turkey; an experimental study on fuzzy expert system: Proposal for financial suitability evaluation of commercial and participation banks in power plant projects in Turkey. 18th Online World Conference on Soft Computing in Industrial Applications, WSC 2018  ",Strategic alignment
1296,The use of academic libraries in turbulent times: Student library behaviour and academic performance at the University of Cape Town,"Purpose: The purpose of this paper is to explore how an innovation in the University Management Information System was leveraged to incorporate library data by an initially sceptical strategic management team. The rationale was to extract evidence of correlations between library use and student achievement. This kind of information is of particular interest to the institution, which is at present dealing with crises popularly summarised in the slogan “#FeesMustFall” among students who suffer from the effects of poverty and exclusion in higher education. Comment is offered on some of the relationships between student library behaviour before, during and after the nationwide disruptions that destabilised universities and threatened their survival at the end of 2016, just before the final examination period. Design/methodology/approach: Data were extracted from the data warehouse from the comparative demographic perspectives of students’ degrees of disadvantage in an effort to uncover any hitherto hidden patterns of library use. Findings: The use of the library as expressed by footfall and loans was mapped against students’ pass rates and their collective GPA, indicating that increased library use correlates positively with better academic performance. Some of the initial correlations between student library behaviour before, during and after the nationwide disruptions that destabilised universities and threatened their survival at the end of 2016 just before the final examination period are explored. The effects that library closures (under threat of damage) at a critical time in the academic year might have had on library use and on student performance are interrogated. Practical implications: Students on financial aid, which was used as an indicator of disadvantage, come from schools and environments where access to information technology and libraries is very limited, so that library habits are either poorly established or not at all. At the University of Cape Town (UCT), considerable support is in place for students to encourage the development of library habits. An analysis of available data indicates that students who have acquired library habits regardless of unfavourable financial circumstances do not exhibit behaviour and academic outcomes markedly different from that of their more privileged peers. Originality/value: Combining library data with data from the university data warehouse is a new approach in South Africa. It is an approach that is of value both to the library and the institution at large and has brought meaningful insights into the role the academic library might be seen to play in promoting student academic achievement. © 2018, Emerald Publishing Limited. The use of academic libraries in turbulent times: Student library behaviour and academic performance at the University of Cape Town Analytics; Impact; Innovative methods; Library use; Performance indicators; Value ",Strategic alignment
1297,Leveraging predictive analytical for business value : Theoretical foundations,"Challenges and dilemmas exist on how to maximise the value of Business Intelligence and Analytics (BI&A) in a complex and dynamic organisational environment. The expectation of BI&A is to improve decision making for core business processes that drive business performance. A multi-disciplinary review of theories from the domains of strategic management, technology adoption and economics claim that people, tasks, technology and structures need to be aligned for BI&A to add value to decision making. However, these elements interplay, making it difficult to determine how they are configured. The fit as gestalts approach is adopted to examine the relationships between these elements and also determine how best they can be aligned. This approach will give a comprehensive account and holistic view as to what extent these factors complement each other. This will help identify the ideal combination of factors that will result in BI&A adding value to decision making which in turn results in better organisational performance. © 2016 IEEE. Leveraging predictive analytical for business value : Theoretical foundations Alignment; Business Intelligence; Data Analysis; Predictive Analytics; Strategy Alignment; Behavioral research; Competitive intelligence; Computation theory; Data reduction; Digital storage; Information analysis; Management science; Predictive analytics; Business performance; Business value; Organisational; Organisational performance; Strategic management; Strategy; Technology adoption; Theoretical foundations; Decision making",Strategic alignment
1301,Improvement of investment processes in mining company by implementation of project management system,"The paper presents the experience gained from implementing Project Management System in investment processes of mining company on an example of KGHM Polska Miedź S.A. The customized system architecture has been developed to effectively support project management area negatively influenced by the extensive planning phases, delayed reporting, hampered portfolio management and hard accessible data sources. Specific attention is paid to the positive impact Project Management System has on project managers, project management offices and decision-making processes on strategic level. Expansion of Business Intelligence tools and databases led to further development includ-ing: balancing and consolidation of portfolios, electronic documentation workflow and detailed statistical reports. Research has been set up to incorporate predictive analysis capa-ble of estimating real budget, scope and time required for successful completion of project’s tasks. © 2019 Taylor & Francis Group, London. Improvement of investment processes in mining company by implementation of project management system  Budget control; Financial data processing; Information management; Investments; Mineral industry; Operations research; Project management; Decision making process; Electronic documentation; Portfolio managements; Project management offices; Project management system; Project managers; Statistical report; System architectures; Decision making",Strategic alignment
1303,Critical success factors for the implementation of business intelligence systems,"This article focuses on critical success factors during the implementation of a business intelligence system. The existing literature was reviewed, and critical success factors were extracted. Subsequently, the critical success factors that occur in practice were collected through qualitative expert interviews that are analysed through a qualitative content analysis. The critical success factors found in literature are afterwards compared with those that have been collected during the expert interviews. It was found that many of the critical success factors were mentioned in the literature and in the expert interviews as well, such as a strong management support, a light-weight approach, user acceptance, the project team and data quality. In addition, the performance of the business intelligence system, the definition of standards, terminology and key performance indicators as well as an institutionalization and integration of business intelligence were mentioned in the expert interviews. © 2018, IGI Global. Critical success factors for the implementation of business intelligence systems Business Intelligence; Business Intelligence System; Critical Success Factors; Project Management ",Capacity management
1306,"10th Conference of the European Society for Fuzzy Logic and Technology, EUSFLAT 2017 and 16th International Workshop on Intuitionistic Fuzzy Sets and Generalized Nets, IWIFSGN 2017","The proceedings contain 170 papers. The special focus in this conference is on European Society for Fuzzy Logic and Technology. The topics include: Higher degree fuzzy transform: application to stationary processes and noise reduction; sheffer stroke fuzzy implications; towards fuzzy type theory with partial functions; dynamic intuitionistic fuzzy evaluation of entrepreneurial support in countries; dynamic intuitionistic fuzzy evaluation of entrepreneurial support in countries; an interval valued hesitant fuzzy clustering approach for location clustering and customer segmentation; six sigma project selection using interval neutrosophic TOPSIS; integrated call center performance measurement using hierarchical intuitionistic fuzzy axiomatic design; prioritization of business analytics projects using interval type-2 fuzzy AHP; optimized fuzzy transform for image compression; compositions consistent with the modus ponens property used in approximate reasoning; comparative study of type-1 and interval type-2 fuzzy systems in the fuzzy harmony search algorithm applied to benchmark functions; analysis of different proposals to improve the dissemination of information in university digital libraries; using fuzzy sets in a data-to-text system for business service intelligence; an approach to fault diagnosis using fuzzy clustering techniques; universal generalized net model for description of metaheuristic algorithms; global quality measures for fuzzy association rule bases; particle swarm optimization with fuzzy dynamic parameters adaptation for modular granular neural networks; edge detection based on ordered directionally monotone functions and learning in comparator networks. 10th Conference of the European Society for Fuzzy Logic and Technology, EUSFLAT 2017 and 16th International Workshop on Intuitionistic Fuzzy Sets and Generalized Nets, IWIFSGN 2017  ",Strategic alignment
1310,XEW 2.0: Establishment of a new competitive intelligence system for big data analytics,"Competitive Intelligence is a strategic management of information, which aims to provide collaborative decision-making. In other words, competitive intelligence is a mapping of the surrounding business environment. Nowadays, every organization needs a competitive intelligence system in order to enhance its position in the market, or simply to survive, as well as to be able to track every single change and to provide the right response to it in a real time scale. We propose in this paper a new Competitive Intelligence System for Big Data Analytics (CIS-BG: XEW 2.0). © 2005 – ongoing JATIT & LLS. XEW 2.0: Establishment of a new competitive intelligence system for big data analytics Big data analytics; Big data visualisation; Competitive intelligence system; XEW 2.0 ",Strategic alignment
1311,"As Technologies for Nucleotide Therapeutics Mature, Products Emerge","The long path from initial research on oligonucleotide therapies to approval of antisense products is not unfamiliar. This lag resembles those encountered with monoclonal antibodies, gene therapies, and many biological targets and is consistent with studies of innovation showing that technology maturation is a critical determinant of product success. We previously described an analytical model for the maturation of biomedical research, demonstrating that the efficiency of targeted and biological development is connected to metrics of technology growth. The present work applies this model to characterize the advance of oligonucleotide therapeutics. We show that recent oligonucleotide product approvals incorporate technologies and targets that are past the established point of technology growth, as do most of the oligonucleotide products currently in phase 3. Less mature oligonucleotide technologies, such as miRNAs and some novel gene targets, have not passed the established point and have not yielded products. This analysis shows that oligonucleotide product development has followed largely predictable patterns of innovation. While technology maturation alone does not ensure success, these data show that many oligonucleotide technologies are sufficiently mature to be considered part of the arsenal for therapeutic development. These results demonstrate the importance of technology assessment in strategic management of biomedical technologies. © 2017 The Authors As Technologies for Nucleotide Therapeutics Mature, Products Emerge antisense; data analytics; FDA approval; miRNA; oligonucleotide therapeutics; ribozyme; RNAi; small interfering RNA; technology forecasting; technology management antisense oligonucleotide; aptamer; CRISPR associated protein; microRNA; monoclonal antibody; oligonucleotide; ribozyme; small interfering RNA; Article; biomedical technology assessment; drug approval; drug research; gene targeting; gene technology; priority journal",Strategic alignment
1312,Identifying the key drivers for teams to use a data science process methodology,"While data science teams do not yet typically use a standard team process methodology, researchers are starting to explore process methodologies that improve team performance. However, little has been done to understand what might be the key acceptance factors for teams to implement a data science process methodology. To address this gap, the Diffusion of Innovation Theory is used as a theoretical lens to identify factors that might drive an organization to adopt a data science process methodology. The results of this qualitative research effort found ten factors that can influence a team to use, or not use, a data science process methodology. In short, eight positive factors were found with respect to relative advantage and compatibility and two negative factors were identified with respect to complexity. While more work is required to validate and refine these factors, the derived acceptance model can help teams as they consider adopting an improved data science process methodology. © 26th European Conference on Information Systems: Beyond Digitization - Facets of Socio-Technical Change, ECIS 2018. All Rights Reserved. Identifying the key drivers for teams to use a data science process methodology Big Data; Data Science; Project Management Big data; Data Science; Information systems; Information use; Project management; Acceptance models; Diffusion of innovation theory; Qualitative research; Team performance; Team process; Digital storage",Value management
1313,Strategic analytics: Integrating management science and strategy,"Defines common ground at the interface of strategy and management science and unites the topics with an original approach vital for strategy students, researchers and managers Strategic Analytics: Integrating Management Science and Strategy combines strategy content with strategy process through the lenses of management science, masterfully defining the common ground that unites both fields. Each chapter starts with the perspective of a certain strategy problem, such as competition, but continues with an explanation of the strategy process using management science tools such as simulation. Facilitating the process of strategic decision making through the lens of management science, the author integrates topics that are usually in conflict for MBAs: strategy and quantitative methods. Strategic Analytics features multiple international real-life case studies and examples, business issues for further research and theory review questions and exercises at the end of each chapter. Strategic Analytics starts by introducing readers to strategic management. It then goes on to cover: managerial capabilities for a complex world; politics, economy, society, technology, and environment; external environments known as exogenous factors (PESTE) and endogenous factors (industry); industry dynamics; industry evolution; competitive advantage; dynamic resource management; organisational design; performance measurement system; the life cycle of organisations from start-ups; maturity for maintaining profitability and growth; and finally, regeneration. Developed from the author’s own Strategy Analytics course at Warwick Business School, personal experience as consultant, and in consultation with other leading scholars Uses management science to facilitate the process of strategic decision making Chapters structured with chapter objectives, summaries, short case studies, tables, student exercises, references and management science models Accompanied by a supporting website Aimed at both academics and practitioners, Strategic Analytics is an ideal text for postgraduates and advanced undergraduate students of business and management. © 2019 John Wiley & Sons Ltd. All rights reserved. Strategic analytics: Integrating management science and strategy  ",Strategic alignment
1315,Strategic mapping: relationships that count,"Purpose: The purpose of this paper is to study how the design of a strategy map can be supported by measures expressing the customers’ perceptions about strategic factors and their related determinants. In particular, managers are provided with a fact-based test useful to revise prior knowledge and beliefs. Design/methodology/approach: A case study is used to describe the adoption of the partial least squares path modelling (PLS-PM) approach to structural equation modelling in order to compare competing strategy maps and select the one that best fits customer perceptions. A focus group was organised to design the strategy maps, which were tested through a survey of 600 randomly selected resellers. Findings: The empirical-based validation of a causal map by using PLS-PM may effectively stimulate a revision of managers’ collective perceptions about a phenomenon characterised by implicit knowledge, as in the case of customer needs. Research limitations/implications: The case-study company operates in a business-to-business environment, and thus only the needs of direct customers have been included in the analysis. Final users’ needs should also be considered, even if different solutions are required for data collection. Practical implications: The proposed approach provides a set of indicators which allow managers to identify strategic priorities, thus facilitating decision making and strategic planning. Originality/value: In the strategic management literature, few attempts have been made to operationalise the complex and multidimensional latent constructs of a strategy map combining managers’ implicit knowledge and empirical validation in a “holistic” manner. The adoption of PLS-PM is relatively new in testing the accuracy of causal maps. © 2018, Emerald Publishing Limited. Strategic mapping: relationships that count Business analytics; Knowledge discovery; Partial least squares path modelling; Strategy maps ",Strategic alignment
1319,In search of a framework for personal healthcare management in oncology,"This research solves a real problem of global concern and high practical value for healthcare. It designs a theoretical methodology and a framework on lifestyle management (Lifestyle Analyzer); centered on chronic patients, specifically in oncology patients; the target is to identify a lifestyle that promotes a healthy life by classifying between two groups: The first promotes illness and the second a health or remission state. In line with World Health Organization it understands health is more than the absence of disease. The theoretical structure takes as reference intellectual capital management to approach complexity of cancer from a holistic view. The approach is based on case studies of oncological patients and a 500.000 anonymized patient data in Barcelona. We carry out a design science research based on qualitative methods. The main findings are the following: 1) There are common chronic illness on previous stages of cancer; 2) There are common symptoms without a direct because of principal cancers tumor; 3) After a lifestyle changes, there are improvement in symptoms and chronic illness. The framework helps people to understand the pathways of illness. It also contributes to the improvement of life quality for patients. The originality of this study is to propose a framework to support the complexity of cancer combining three scopes: Healthcare Analytics, Intellectual Capital Management and Healthcare management. © 2018 Academic Conferences Limited. In search of a framework for personal healthcare management in oncology Big data; Chronic disease; Healthcare analytics; Intellectual capital; Self-management program; Strategic management Big data; Health care; Hospital data processing; Knowledge management; Oncology; Chronic disease; Design-science researches; Health-care managements; Intellectual capital; Self management; Strategic management; Theoretical structure; World Health Organization; Diseases",Strategic alignment
1320,The Design of Flexible Workflow in Scientific Research Management System,"In scientific research management projects, if the process model changes, without a workflow technology, the codes need to be modified and the workload of modification will increase significantly. After introducing a flexible workflow technology, what can be done is to directly define the new process model. This paper mainly introduces ECA rules on the basis of workflow meta-model to design a flexible workflow: the ECA rules will express execution logics of the basic workflow, and the basic elements of the process model in the approval process of scientific research projects will be represented by ECA. The design of the flexible workflow in scientific research management system will include flexible workflow engine and the process approval platform for scientific research project management with ECA rule analysis. The flexible workflow implemented in this paper can better guide the workflow engine to adapt to the current environment, and decide the different execution process according to the specific process instance contexts. The method proposed in this paper better supports the needs of scientific research project management. © 2018 IEEE. The Design of Flexible Workflow in Scientific Research Management System ECA rules; flexible workflow; process approval; software architecture; workflow Advanced Analytics; Engines; Software architecture; ECA rule; Execution process; Flexible workflows; Process instances; Scientific researches; workflow; Workflow meta-models; Workflow technology; Project management",Risk management
1322,What construction topics do they discuss in social media? A case study of weibo in China,"As a traditional industry such as the construction, it is hard to collect data for management and improvement. Traditional ways such as questionnaire survey, interviews, or focus meetings to collect data are both time- And cost-consuming. Recently, with the rapid development of social media services, data can be collected and extracted for topic analysis to provide officials and managers with fresh perspectives on participants in the construction management. In this paper, a topic analysis systematic framework is proposed. This system collected user messages from social media sites, establishes and compares different clusters' topics and keywords in their messages. This paper generated valuable information and knowledge in the construction domain. As an initial trial, this study selected social media of Weibo because of its wide usage in China. Four clusters which include construction workers, construction companies, construction unions, and construction media were analyzed. For each user, the crawler is used to collect the Weibo messages from his/her web page. On average, there are 135 messages collected for each user. This research then analyzed these data in the following aspects to dig out information behind data: keywords, hashtags, and topic modeling. Detailed findings, benefits, and barriers to incorporating social media data analytics in the construction industry, along with future research, were discussed. This paper benefits the academia by testing an alternative way of studying the construction population, which further will help decision makers better understand the real situations of the construction industry. © 2018 American Society of Civil Engineers (ASCE). All rights reserved. What construction topics do they discuss in social media? A case study of weibo in China Big data; China; Social media; The construction industry; Topic analysis; Weibo Big data; Construction industry; Data acquisition; Data mining; Decision making; Project management; Surveys; China; Construction companies; Construction management; Questionnaire surveys; Social media; Social media services; Topic analysis; Weibo; Social networking (online)",Value management
1323,Student attitudes toward information systems graduate program design and delivery,"This study examines student preferences regarding graduate management information systems (MIS) education. One hundred and eighty four graduate students responded to a survey exploring student attitudes towards degree program content, delivery format, and peer group interaction. Study results indicate that students prefer a program with an even mix of business and technical coursework taught by full-time faculty featuring frequent guest lectures by industry professionals. The most often cited business courses that should be required include quantitative business analysis, operations management, strategy, and leadership, and the most often identified management information systems courses that should be required were internships, business intelligence, data warehousing, management information systems fundamentals, and information technology project management. The study also explored how students with and without prior work experience differed in their preferences, which will help administrators and faculty with insights and tools to design more effective programs of study. © 2018 by the Information Systems & Computing Academic Professionals, Inc. (ISCAP). Student attitudes toward information systems graduate program design and delivery Curriculum design & development; Enrollment; Program assessment & design; Program improvement; Program promotion; Student attitudes; Student expectations; Student perceptions Curricula; Data warehouses; Information management; Information systems; Information use; Management information systems; Project management; Teaching; Curriculum designs; Enrollment; Program assessment; Program improvement; Program promotion; Student attitudes; Student expectations; Student perceptions; Students",Governance
1325,The role of business intelligence in sustainability reporting for South African higher education institutions,"Purpose: This paper aims to show that business intelligence (BI) is a key component of a sustainability-reporting framework for higher education institutions (HEIs). Design/methodology/approach: Four questionnaires were administered to Registrars and managers at 21 South African HEIs and at selected international HEIs. The data analysis entailed both descriptive and inferential statistics. Findings: The study confirmed that factors such as management buy-in, the availability of BI reports and the provision of reporting guidelines were positively related to effective strategic planning. The study shows that the use of BI by South African HEIs is still at a low maturity level. Research limitations/implications: The case study used is the Nelson Mandela University in Port Elizabeth, South Africa. The implications are relevant for all 26 HEIs in South Africa. Practical implications: HEIs must invest in technological tools, including BI to provide information in understandable and usable formats for management and other relevant stakeholders. Social implications: BI reporting can assist all stakeholders to obtain the relevant and required information relating to HEI operations and strategic management initiatives and activities. Originality/value: The study concludes that HEIs ought to invest in BI technologies that can assist the sustainability reporting process to ensure stakeholder satisfaction and regulatory compliance. © 2018, Emerald Publishing Limited. The role of business intelligence in sustainability reporting for South African higher education institutions Business intelligence; Governance; Higher education institutions; Integrated reporting; Sustainability reporting ",Strategic alignment
1327,An ICT Project Case Study from Education: A Technology Review for a Data Engineering Pipeline,"The paper presents a brief technology survey of existing tools to implement data ingestion pipelines in a classical Data Science project. Given the emergent nature of technologies and the challenges associated with any Big Data project, we propose to identify and discuss the main components of a data pipeline, from a data engineering perspective. The data pipeline is showcased with a case study from an ICT university project, where several teams of master students competed towards designing and implementing the best solution for a manufacturing data pipeline. The project proposes a research-based multidisciplinary approach to education, aiming at empowering students with a novel role in the process of learning, that of knowledge creators. Therefore, on the one hand, the paper discusses the main components of a Big Data pipeline and on the other hand it shows how these components are addressed and implemented within a concrete ICT project from education, realized in tight relation with the IT industry. © 2019, Springer Nature Switzerland AG. An ICT Project Case Study from Education: A Technology Review for a Data Engineering Pipeline Big Data; Collaborative work; Data pipeline; ICT project management; Research-informed education; Virtual teams Big data; Engineering education; Engineering research; Human resource management; Information systems; Information use; Project management; Students; Collaborative Work; Data engineering; Data pipelines; Multi-disciplinary approach; Process of learning; Science projects; Technology review; Virtual team; Pipelines",Value management
1328,Citizen Data Scientist: A Design Science Research Method for the Conduct of Data Science Projects,"Firms are seeking to gain greater understanding of and insights into more and more massive quantities of data collected and stored in disparate public and private databases. To effectively and efficiently deploy project resources to the data science search activity and to consequently build and evaluate innovative artifacts, firms are finding that a Design Science Research (DSR) approach can extend into the Data Science (DS) project domain through an iterative, evaluative project management method for the diagnosing, design, implementation, and evolution of data science artifacts. Importantly, DSR also provides a guided, emergent search paradigm that can be integral to finding hidden insights in massive data where the problem and solution domains are both frequently poorly understood at the outset of the DS inquiry. This article examines a case for using the elaborated action design research (eADR) method to inform the DS project management (PM) approach in situ with a Fortune 100 Global Manufacturer. The innovative DS PM approach resulted in multiple innovative DS solution artifacts built and evaluated by a dozen DS PM teams at the firm over the first two years of the DS PM deployment. © 2019, Springer Nature Switzerland AG. Citizen Data Scientist: A Design Science Research Method for the Conduct of Data Science Projects Action design research; Data science; Data science project management; Design science; Digital innovation; Informal learning Data Science; Design; Information systems; Information use; Iterative methods; Project management; Design research; Design science; Digital innovations; Informal learning; Science projects; Information management",Strategic alignment
1329,A study on the provision of national R & D information service through user analysis,"The NTIS has a very high used according to the accumulated page views over the last 10 years but whether it can be used easily and conveniently by users must be reviewed. This study will find the issues of NTIS by analyzing its service structure status and usage by user types. In the current NTIS service, users must access several different services to access the functions because it is structured by national ARE & D information and this study analyzes status of NTIS service and authority System and then classifies NTIS functions by the purpose of service use. To survey the usage status of NTIS, this study utilized Google analytics functions. The survey period is 3 months and the range is the entire NTIS service. The NTIS main portal was used the most, followed by project management service. Usage status analysis shows there are clearly distinctive characteristic by user type but they are not huge enough to influence the service. And it is difficult to distinguish frequently used functions and unused functions by user types. Several noticeable characteristics by user types are as follows. Department/project managing institutions users used project management service and national ARE & D standard information management service more than other services to manage national ARE & D information. Also, this type of users mainly used functions such as assignment search, statistical information and restricted information. Researchers often used functions of registering and managing research facility and equipment as well as searching scientist/engineer registration numbers and similar assignments. For company users and general users, only the performance search was added unlike other user types and other characteristics were not found. If NTIS improves its service as suggested in this study, service users will be able to search and view various information through integrated search, easily access the functions to use during ARE & D activities and use the NTIS easily and conveniently through stronger personalized functions. © Medwell Journals, 2018. A study on the provision of national ARE & D information service through user analysis Google analytics; National ARE & D information; NTIS; Service improvement plan; Usage status; User type ",Strategic alignment
1331,Best practices in structuring data science projects,"The goal of Data Science projects is to extract knowledge and insights from collected data. The focus is put on the novelty and usability of the obtained insights. However, the impact of a project can be seriously reduced if the results are not communicated well. In this paper, we describe a means of managing and describing the outcomes of the Data Science projects in such a way that they optimally convey the insights gained. We focus on the main artifact of the non-verbal communication, namely project structure. In particular, we surveyed three sources of information on how to structure projects: common management methodologies, community best practices, and data sharing platforms. The survey resulted in a list of recommendations on how to build the project artifacts to make them clear, intuitive, and logical. We also provide hints on tools that can be helpful for managing such structures in an efficient manner. The paper is intended to motivate and support an informed decision on how to structure a Data Science project to facilitate better communication of the outcomes. © 2019, Springer Nature Switzerland AG. Best practices in structuring data science projects Data science; Management methodologies; Project management; Tools Architecture; Information systems; Information use; Project management; Surveys; Tools; Data science; Data-sharing platforms; Informed decision; Management methodologies; Non-verbal communications; Project artifacts; Project structure; Sources of informations; Information management",Risk management
1332,Data lakes in business intelligence: Reporting from the trenches,"The data lake approach has emerged as a promising way to handle large volumes of structured and unstructured data. This big data technology enables enterprises to profoundly improve their Business Intelligence. However, there is a lack of empirical studies on the use of the data lake approach in enterprises. This paper provides the results of an exploratory study designed to improve the understanding of the use of the data lake approach in enterprises. I interviewed 12 experts who had implemented this approach in various enterprises and identified three important purposes of implementing data lakes: (1) as staging areas or sources for data warehouses, (2) as a platform for experimentation for data scientists and analysts, and (3) as a direct source for self-service business intelligence. The study also identifies several perceived benefits and challenges of the data lake approach. The results may be beneficial for both academics and practitioners. Further, suggestions for future research is presented. © 2018 The Authors. Published by Elsevier Ltd.. Data lakes in business intelligence: Reporting from the trenches BI architecture; Big data; Business intelligence; Data lake Big data; Competitive intelligence; Data warehouses; Information analysis; Information systems; Information use; Lakes; Project management; Data technologies; Empirical studies; Exploratory studies; Large volumes; Perceived benefits; Self-service business intelligences; Unstructured data; Information management",Governance
1334,The dark side of successful data intensive projects: Function creep and stakeholder creep,"In this article, we investigate the unintended consequences of initiating a large data analytic (DA) initiative in healthcare. DA is promising in terms of aiding decision making both for healthcare decisions and administrative decisions. However, while the (re)use of large amounts of data has lots of potential, it also has a dark side in terms of privacy, ethics, and control. Through the analysis of a longitudinal case study (2003-2015) of the emergence, expansion, and collapse of a large-scale DA project in the Danish healthcare sector, we demonstrate how high expectations for DA caused a surge of stakeholders and features that ultimately resulted in the project's demise. We call these surges stakeholder creep and function creep and theorize their emergence and consequences for DA in organizations. © 26th European Conference on Information Systems: Beyond Digitization - Facets of Socio-Technical Change, ECIS 2018. All Rights Reserved. The dark side of successful data intensive projects: Function creep and stakeholder creep Data Analytics; Function Creep; Project Management; Stakeholder Creep Data Analytics; Decision making; Health care; Information systems; Information use; Project management; Data intensive; Health-care decisions; Healthcare sectors; Large amounts of data; Large data; Longitudinal case study; Unintended consequences; Creep",Strategic alignment
1336,Digitalisation and big data mining in banking,"Banking as a data intensive subject has been progressing continuously under the promoting influences of the era of big data. Exploring the advanced big data analytic tools like Data Mining (DM) techniques is key for the banking sector, which aims to reveal valuable information from the overwhelming volume of data and achieve better strategic management and customer satisfaction. In order to provide sound direction for the future research and development, a comprehensive and most up to date review of the current research status of DM in banking will be extremely beneficial. Since existing reviews only cover the applications until 2013, this paper aims to fill this research gap and presents the significant progressions and most recent DM implementations in banking post 2013. By collecting and analyzing the trends of research focus, data resources, technological aids, and data analytical tools, this paper contributes to bringing valuable insights with regard to the future developments of both DM and the banking sector along with a comprehensive one stop reference table. Moreover, we identify the key obstacles and present a summary for all interested parties that are facing the challenges of big data. © 2018 by the authors. Licensee MDPI, Basel, Switzerland. Digitalisation and big data mining in banking Banking; Big data analytics; Data mining; Survey Big data; Customer satisfaction; Data Analytics; Banking sectors; Big data analytic; Current research status; Customers' satisfaction; Data analytic tools; Data analytics; Data intensive; Data-mining techniques; Research and development; Strategic management; Data mining",Strategic alignment
1337,Smart office: A data-driven management tool for mechanized tunneling construction,"Mechanized tunneling construction with TBM is among the most innovative areas in the construction industry, however, tunnel construction process remains severely under-digitized. While vast data is recorded by construction equipment (e.g., TBM, conveyor system) and human interaction (e.g., site engineers, operators), leveraging the power of data to increase productivity and improve the construction process is overlooked. In this paper, the concept of a unified analytics center to gather, integrate, and analyze data from disparate databases on a construction site and then contextualize this information into a visual, meaningful representation provides a robust tool to enhance instant, and far-off decision-making for all levels of site and office personnel is presented. Dugway Storage Tunnel project is considered as a case study. Throughout the construction period of the tunnel, vast quantities of data from TBM sensors is streamed directly from TBM’s PLC. Data is then pushed into Power BI, which is an analytics service provided by Microsoft. This system connects the project personnel (e.g., Project manager, Construction manager, Project engineer, and site Engineers) to a broad range of data via easy-to-use dashboards, interactive reports, and meaningful interactive visualizations that bring data to life. © 2019 Society for Mining, Metallurgy and Exploration. All rights reserved. Smart office: A data-driven management tool for mechanized tunneling construction  Construction equipment; Construction industry; Decision making; Digital storage; Engineers; Human resource management; Managers; Project management; Tunneling (excavation); Visualization; Construction manager; Construction period; Construction process; Construction sites; Human interactions; Interactive visualizations; Mechanized tunneling; Tunnel construction; Tunnels",Strategic alignment
1342,"25th ISTE International Conference on Transdisciplinary Engineering, 2018","The proceedings contain 120 papers. The special focus in this conference is on Transdisciplinary Engineering. The topics include: Towards semantic interoperability approach to support the calibration process of electronic engine management system; knowledge management support in the engineering change process; enacted affordance when doing transdisciplinary engineering; design rationale, knowledge based engineering and knowledge management in large supplier company; conceptual and detailed design knowledge management in customized production-industrial perspective; momis dashboard: A powerful data analytics tool for industry 4.0; knowledge management in a dynamic manufacturing context: A case study; the designed product construction information semantic representation in a cad-system; evaluating supply chain risks in a capability-based sustainment environment; investigating into the risks of forming alliance; unstable approach in aviation: Examine air traffic controllers’ involvement from situation awareness and shared-situation awareness; a strategy of providing upgradable product service system for economic and environmental balance; an independent power cell for energy generation from residual frying oil; how can green supply chain management contribute to the product development process; a critical review of industrial symbiosis models; new trends for mitigation of environmental impacts: A literature review; an intelligent patent summary system deploying natural language processing and machining learning; preface; systems evaluation methodology to attend the digital projects requirements for industry 4.0; digital manufacturing and virtual commissioning of intelligent factories and industry 4.0 systems using graph-based design languages; intellectual property protection and licensing of 3d print with blockchain technology; indoor object reconstruction based on acquisition by low-cost devices; project risk management for digital manufacturing. 25th ISTE International Conference on Transdisciplinary Engineering, 2018  ",Risk management
1343,Scientometric analysis of knowledge in the context of project management: Subject area: (Knowledge management and project management),"This research work carried out a meticulous scientometric analysis about the knowledge management in the context of project management, in order to build a detailed state of the art about the matter of study, allowing the identification of main elements investigated on the scientific literature about the knowledge on this context. Firstly; a theoretical framework was build, allowing the identification of concepts about knowledge management and scientometric analysis. Secondly; a methodology was constructed, by integrating analytics and measurement tools, 881 publications related to the knowledge management on projects were identified on the main databases, later, detailed bibliometric analysis were conducted in order to highlight the most investigated topics, main authors (Gemino, Carrillo and Reich) and sources with higher amount of documents and quotes on the scientific literature about the matter of study (International Journal of Project Management, Project Management Journal and Journal of Knowledge Management). Afterwards; the results of the scientometric analysis about the knowledge on projects were documented. And finally; conclusions were established and as future lines of research were identified the impact of knowledge management on project performance, leadership, management, and innovation. © Springer Nature Switzerland AG 2019. Scientometric analysis of knowledge in the context of project management: Subject area: (Knowledge management and project management) Knowledge management; Project management; Scientometric analysis Project management; Bibliometric analysis; International journals; Measurement tools; Project performance; Scientific literature; Scientometric analysis; State of the art; Theoretical framework; Knowledge management",Strategic alignment
1347,Predictive quality performance control in BPM: Proposing a framework for predicting quality anomalies,"Business process management (BPM) literature suggests that more than 60% of quality improvement projects fail due to factors associated with the lack of predictive quality performance control and the failure of continuously searching for quality anomalies in quality performance over time. Quality anomalies are indications of extreme performance deviation from quality expectations and requirements. The findings suggest that quality performance control in BPM is the scientific method for producing quality anomaly knowledge and signalling opportunities for informed, systematic, and continuous performance improvement. A predictive framework is proposed based on the findings. © 2018 The Authors. Published by Elsevier Ltd.. Predictive quality performance control in BPM: Proposing a framework for predicting quality anomalies BPM; BPR; Business process intelligence; Design science research; Outlier and deviation mining; Predictive analytics; Process mining; QFD; Quality control; Quality improvement; Quality performance measurement; SPC; Temporal data mining; Time series; TQM Data mining; Enterprise resource management; Information management; Information systems; Information use; Predictive analytics; Project management; Time series; Total quality management; Business Process Intelligence; Design-science researches; Process mining; Quality improvement; Quality performance; Temporal data mining; Quality control",Value management
1348,Innovative undergraduate degree programs in data science and business analytics,"Department of Data Science and Business Analytics established in January 2018 in the College of Innovation and Technology at Florida Polytechnic University. Two highly innovative undergraduate degree programs are offered: Bachelor of Science in Data Science and Bachelor of Science in Business Analytics. Data Science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from data in various forms, both structured and unstructured. New computational and analytic approaches to a vast array of forms, scales, and sources of data are now critical to research, decision-making, and action. The rigorous curriculum in Data Science program focuses on the fundamentals of applied mathematics, computer science, probability, statistics, optimization, and machine learning while incorporating real-world examples. Business Analytics is analytics expertise with a business focus. It is a cutting-edge program especially designed to prepare students for top jobs in today's technology and data intensive business world. The curriculum provides extensive instruction in the disciplines of optimization, mathematical modeling, probabilistic and statistical analysis, simulation, computer programming, database, data and text mining, and cloud computing. The curriculum also includes classes in subject areas you would expect to find in a more traditional business program, such as economics, accounting, finance, operations management, supply chain management, entrepreneurship, business law, negotiation, project management, and strategy. The curricula of two programs are intertwined by first year common and several common courses and senior yearlong industry project. In this paper we highlight curriculum development along with learning objectives. © IEOM Society International. Innovative undergraduate degree programs in data science and business analytics Business analytics; Data analytics and big data; Data science ",Monitoring and control
1349,"International workshops: ASDENCA, BDA, BIOC, COGNISE, Enterprise Modeling, and FAiSE wokshops are held in conjunction with the 30th International Conference on Advanced Information Systems Engineering, CAiSE 2018","The proceedings contain 22 papers. The special focus in this conference is on Enterprise Modeling. The topics include: Towards a design space for blockchain-based system reengineering; towards collaborative and reproducible scientific experiments on blockchain; The origin and evolution of syntax errors in simple sequence flow models in BPMN; Mining developers’ workflows from IDE usage; designing for information quality in the era of repurposable crowdsourced user-generated content; test first, code later: Educating for test driven development: Teaching case; the “What” facet of the zachman framework – A linked data-driven interpretation; an application design for reference enterprise architecture models; Towards an Agile and ontology-aided modeling environment for DSML adaptation; towards improving adaptability of capability driven development methodology in complex environment; Towards a risk-aware business process modelling tool using the ADOxx platform; a reference framework for advanced flexible information systems; integrating IoT devices into business processes; using open data to support organizational capabilities in dynamic business contexts; capability management in the cloud: Model and environment; Using BPM frameworks for identifying customer feedback about process performance; increasing trust in (Big) data analytics; building payment classification models from rules and crowdsourced labels: A case study; combining artifact-driven monitoring with blockchain: Analysis and solutions; ensuring resource trust and integrity in web browsers using blockchain technology. International workshops: ASDENCA, BDA, BIOC, COGNISE, Enterprise Modeling, and FAiSE wokshops are held in conjunction with the 30th International Conference on Advanced Information Systems Engineering, CAiSE 2018  ",Monitoring and control
1351,"The road from millennium to alma: Two tracks, one destination","This conference report details two integrated library system (ILS) migration projects from Innovative Interfaces, Inc.’s Millennium to ExLibris’ Alma from the perspective of an individual research library, Colorado State University, and a multi-university and college consortial environment, Connecticut State Colleges and Universities. Discussion topics include technical aspects of migrating ILS data and the psychological impact on staff. The authors also explore the benefits such as license record management and the analytics feature for statistical reports, and shortcomings of the migration project such as the overreliance on Basecamp and ExLibris’ support with migrating Innovative Interface’s ILS data. © 2018 Kristin D’Amato and Rachel A. Erb. The road from millennium to alma: Two tracks, one destination Alma; ERM; ILS migration; Millennium; Project management ",Value management
1352,"Predictive modelling: Flight delays and associated factors, Hartsfield-Jackson Atlanta international airport","Nowadays, a downside to traveling is the delays that are constantly being advertised to passengers resulting in a decrease in customer satisfaction and causing costs. Consequently, there is a need to anticipate and mitigate the existence of delays helping airlines and airports improving their performance or even take consumer-oriented measures that can undo or attenuate the effect that these delays have on their passengers. This study has as main objective to predict the occurrence of delays in arrivals at the international airport of Hartsfield-Jackson. A Knowledge Discovery Database (KDD) methodology was followed, and several Data Mining techniques were applied. Historical data of the flight and weather, information of the airplane and propagation of the delay were gathered to train the model. To overcome the problem of unbalanced datasets, we applied different sampling techniques. To predict delays in individual flights we used Decision Trees, Random Forest and Multilayer Perceptron. Finally, each model's performance was evaluated and compared. The best model proved to be the Multilayer Perceptron with 85% of accuracy. © 2018 The Authors. Published by Elsevier Ltd.. Predictive modelling: Flight delays and associated factors, Hartsfield-Jackson Atlanta international airport Atlanta International Airport; Data Mining; Flight Delays; Hartsfield-Jackson International Airport; Predictive Analysis Air traffic control; Air transportation; Airports; Customer satisfaction; Data mining; Decision trees; Information systems; Information use; Multilayers; Predictive analytics; Project management; Flight delays; Historical data; International airport; Knowledge discovery database; Predictive modelling; Random forests; Sampling technique; Unbalanced datasets; Information management",Monitoring and control
1354,SaaS BI for Chinese SMEs: Case study on Zhongli intellectual technology,"SMEs play a very important part in the economy for countries and drive innovation and competition in many economic sectors. To be a strong competitor in the fast growing environment of the economy, smart strategic management decisions are essential for the SMEs. Chinese SMEs have a different character and responsibility compared to others in the developed countries as they make a bigger contribution in Chinese economy. The smart strategic decision and Business Intelligence(BI) solution seem especially import for them. This paper is focusing on analyzing the situation of SMEs, analyzing why they need BI, especial SaaS BI tool and how to implement SaaS BI in SMEs. Copyright © 2018 is held by the owner/author(s). SaaS BI for Chinese SMEs: Case study on Zhongli intellectual technology Business intelligence(BI); SaaS; SMEs; Strategic decision Information analysis; Chinese economy; Developed countries; Economic sectors; Growing environments; SaaS; SMEs; Strategic decisions; Strategic management; Information management",Capacity management
1355,Using a coach to improve team performance when the team uses a kanban process methodology,"Teams are increasing their use of the Kanban process methodology across a range of information system projects, including software development and data science projects. While the use of Kanban is growing, little has been done to explore how to improve team performance for teams that use Kanban. One possibility is to introduce a Kanban Coach (KC). This work reports on exploring the use of a Kanban Coach, with respect to both how the coach could interact with the team as well as how the use of a coach impacts team results. Specifically, this paper reports on an experiment where teams either had, or did not have, a Kanban Coach. A quantitative and qualitative analysis of the data collected during the experiment found that introducing KC led to significant improvement of team performance. Coordination Theory and Shared Mental Models were then employed to provide an explanation as to why a KC leads to better project results. While this experiment was done within a data science project context, the results are likely applicable across a range of information system projects. © 2019, SciKA. Using a coach to improve team performance when the team uses a kanban process methodology Agile; Kanban; Kanban process methodology; Process methodology; Project management; Team performance ",Monitoring and control
1360,Assessment of entrepreneurship competencies through the use of fligby,"Serious games are increasingly being used as innovative tool for developing entrepreneurial skills, such as strategic management, leadership, communication, negotiation or decision making. FLIGBY is a serious game that was initially developed with the objective of simulating the business management process and the application of Flow theory in a business context. However, most recently it has been suggested that serious games in general, and FLIGBY in particular, can be used in other contexts, namely in the development of entrepreneurial skills. In this sense, this study tries to verify if the competences developed and evaluated in FLIGBY on the Master Analytics Profiler (MAP) can be used to evaluate the entrepreneurial capacities of the students in the context of the course of entrepreneurship in a higher education institution. The findings indicate that the performance of these students in FLIGBY is very similar to the benchmark with only small oscillations. However, some asymmetries emerged considering the educational and professional experience, such that the students of the management course showed better business-oriented thinking and IT students revealed greater time-management skills. Finally, professional experience has proved to be relevant for making decisions about stress and for the completion of managerial tasks successfully. © 2019 Research Group Education and Virtual Learning (GREAV). All rights reserved. Assessment of entrepreneurship competencies through the use of fligby Assessment; Entrepreneurship; FLIGBY; Flow; Learning; Serious games ",Strategic alignment
1361,Development of Bulk Material Management System and Research on Material Balance Applications Based on Business Intelligence,"Business intelligence and big data analytics have become increasingly important in both academic and business communities over the past decades. Nuclear power projects involve various bulk materials and the quantities are huge. However, bulk material management in design, procurement and construction phases is disjointed to an extent thus often causes inaccurate purchase quantity of bulk materials, mismatching of upstream and downstream business schedules, poor coordination between different business segments and etc. As a result there are usually large quantities of bulk materials left at the end of the project causing huge waste. Therefore, we developed Bulk Material Management System based on the source data of Design Institute. By calculating quantities of materials as per engineering drawings we can not only provide procurement department with accurate purchase quantities for reference, but also provide construction department with amount of materials in certain installation areas. Then we do research on material balance and add time attribute to the demand materials on field and the received materials after purchasing so as to link the material items with the schedule, cost and other factors. On this basis we develop material balance report of demand quantity, received quantity and inventory quantity in order to make business intelligence analysis for procurement planning, inventory warning, etc. and thus lead to a better EPC coordination for cost and profit efficiency of the company. © 2017 IEEE. Development of Bulk Material Management System and Research on Material Balance Applications Based on Business Intelligence big data; bulk material; business intelligence; EPC coordination; material balance; nuclear power Big data; Competitive intelligence; Cost benefit analysis; Distributed computer systems; Electronic commerce; Information analysis; Materials handling equipment; Nuclear energy; Nuclear fuels; Project management; Research and development management; Sales; Waste management; Bulk materials; Business community; Business segments; Construction phasis; Engineering drawing; EPC coordination; Material balance; Nuclear power project; Information management",Risk management
1362,Customer&#x2019;s Activity Recognition in Smart Retail Environment Using AltBeacon,"SRMS (Smart Retail Management System) is a project based on IoT (Internet of Things), which is an upcoming technology that deserves the attention of the industry. IoT provides unique identifiers to objects and people and transfers data over a network without any interaction of human to human or human to computer, for example tracking your activity level at real-time basis. In this project, we will be empowering retail stores with the power of IoT. In this project, we will be sending tailored offer schemes to the customer of the store whenever he/she is in the radius of a beacon of a particular shelf in store. This process, in turn, will help the customers to get specific offers for happier customer experience and will aid managers of the stores to better analyse the trends of the customers and create appropriate deals to attain higher profits with proper management of inventory. © 2019, Springer Nature Singapore Pte Ltd. Customer&#x2019;s Activity Recognition in Smart Retail Environment Using AltBeacon BLE (Bluetooth Low Energy) devices; Consumer behaviour analytics; Location tracking; Push notification; Smart Retail Management System; Virtual beacons (AltBeacon) Human computer interaction; Internet of things; Project management; Retail stores; Tracking (position); Activity levels; Activity recognition; Bluetooth low energies (BTLE); Customer experience; Management systems; Push notification; Unique identifiers; Virtual beacons (AltBeacon); Sales",Financial management
1363,Designing an online marketing investment evaluation system case study of cosmetics manufacturing company,"At present, online marketing is one of the vital marketing strategies of a normal company such as cosmetics manufacturing company. Conversely, return on ads spending (ROAS) has continued to decrease. This study examines the because of this issues by using 5 Why Is Analysis, conducting and observing the operation and tools of Marketing Department, Project Association Department (PA), Project Management Department (PM) and Managing Director. The result shows that the root because is the marketing department's delay in monitoring the marketing data. Moreover, this study designs the online marketing evaluation system. The dashboard of this system will display the detail of customer journey and the result of measurement of return on marketing investment (ROMI) as well as ROAS which are significant for the case study by real-time form concluding the internal database - customer requirement database system and sale database system and external database - Google Analytics which is the web analytics. From analysing the assumption from historical data by linear regression, the result shows that ROMI can increase up to maximum 6.56%, while ROAS can increase up to maximum 20%. This information shows the effect in using the system will help case study increase opportunity of the purchasing and revenue and will lead to competitive advantage. Additionally, it helps to reduce some of processes for recording the marketing data. © 2018 IEEE. Designing an online marketing investment evaluation system case study of cosmetics manufacturing company google analytics; marketing database system; online marketing; return on ads spending (ROAS); return on marketing investment (ROMI) Competition; Cosmetics; Database systems; Electronic commerce; Investments; Manufacture; Project management; Sales; Social sciences computing; Web services; Competitive advantage; Customer requirements; Google Analytics; Investment evaluation systems; Manufacturing companies; Marketing departments; Online marketing; Re-turn-on; Industrial research",Risk management
1365,Automated performance measurement for 3D building modeling decisions,"Building information modeling (BIM) is instrumental in documenting design, enhancing customer experience, and improving product functionality in capital projects. However, high-quality building models do not happen by accident, but rather because of a managed process that involves several participants from different disciplines and backgrounds. Throughout this process, the different priorities of design modelers often result in conflicts that can negatively impact project outcomes. To prevent such unwanted outcomes from occurring, the modeling process needs to be effectively managed. This effective management requires an ability to closely monitor the modeling process and correctly measure the modelers’ performance. Nevertheless, existing methods of performance monitoring in building design practices lack an objective measurement system to quantify modeling progress. The widespread utilization of BIM tools presents a unique opportunity to retrieve granular design process data and conduct accurate performance measurements. This research improves upon previous efforts by presenting a novel application programming interface (API)-enabled approach to (a) automatically collect detailed model development data directly from BIM software packages in real-time, and (b) efficiently calculate several modeling performance measures during schematic and design development phases of building projects. These indicators can be used to properly arrange modeling teams in the quest for high-quality building models. The specific objectives of this study to examine the feasibility of a proposed automated design performance measurement framework, and to identify optimal modeling team configurations using empirical performance information. A passive data recording approach allows for the real-time capture of comprehensive user interface (UI) interaction and model element modification events. The proposed framework is implemented as an Autodesk Revit plugin. Next, an experiment is conducted to capture data using the developed Revit plugin. Experiment participants’ individual production rates are estimated to establish the validity of the proposed approach to identify the optimal design team configuration. The presented approach uses the earliest due date (EDD) sequencing rule in combination with the critical path method (CPM) to calculate the maximum lateness for different design team arrangements. © 2018 Elsevier B.V. Automated performance measurement for 3D building modeling decisions Application programming interface; BIM; Data analytics; Modeling team; Performance measurement Application programming interfaces (API); Application programs; Product design; Project management; Turnaround time; User interfaces; Accurate performance; Building Information Model - BIM; Data analytics; Empirical performance; Modeling team; Objective measurement; Performance measurements; Performance monitoring; Architectural design",Monitoring and control
1367,Big data—a new technology trend and factors affecting the implementation of big data in australian industries,"Big data is new technology trend and it provides immense advantages. There are too many social networking websites people are using, these websites more than ever before. The data which has been created in the last 5 years is greater than the total size of data accumulated during the preceding. This indicates that people are producing big data knowingly or un-knowingly. In addition to that, every company receives an enormous amount of data in many ways. This data can be transformed into information and information can be converted into knowledge. This knowledge can be very helpful in product marketing. Australian industries can make use of big data. The main objective of this research is to provide more information about big data which is still in its infancy, and find the factors which may affect the Implementation of big data in Australian industries. Big data is still a relatively new field, especially in Australia. It is understandable that big data will become a significant player in Australian industries and that is why it is desirable for these industries to use big data. Big data holds the key to business intelligence and that is why it is important to undertake research on this specific topic. This research study has used a positivist approach and a combination of methodologies. Qualitative and quantitative methodologies have been used and the survey was used as the instrument for gathering data. The survey consists of five sections. The first section is intended to collect the participants’ demographic data. The other four sections are each based on one of the four factors associated with big data: cost, technology, skills and maintenance. The survey has been designed to address these four factors. The Likert scale has been used in this research method. The research design has been explained in detail. The objectives of this research study are: (1) to find factors which determine the Implementation of big data in Australian industries, and (2) to understand the general trend of big data and the technology which can be used to analyze big data. This research provides very important information regarding how big data can be used by different organizations. The participants are employees in the IT department of various industries comprising retail, IT, education, oil-gas and healthcare. These participants’ positions range from entry level to managerial level. The participants’ responses, which constitute the data, were then entered in IBM’s SPSS version 24. The data was entered and analyzed using the factor analysis method. In general, it was found that several factors can affect the Implementation of big data. These factors are: cost, maintenance, skills and technology. The analysis has indicated that statistical skills, IT skills, project management skills and communication skills are important for the people who work with big data. In addition, hardware and software cost, hardware and software maintenance, hardware and software technology also affect the Implementation of big data. © Springer International Publishing AG 2018. Big data—a new technology trend and factors affecting the implementation of big data in australian industries Australian industries; Big data; Factors; Implementation of big data Costs; Maintenance; Project management; Software engineering; Surveys; Websites; Communication skills; Factor analysis method; Hardware and software; Product marketing; Project management skills; Quantitative methodology; research methods; Technology trends; Big data",Value management
1368,Pentasphere predictive analytics for urban environment arrangement and management,"Improvement of the quality of inhabitants' life is the key priority in future urban development. Classical mechanism supporting organizational and management actions on urban environment development is based on two types of resources: man and technical facilities. It is found out that a new reality - digital environment - is mainstreamed along with technosphere development. Interaction criteria for pentasphere environment: noosphere - biosphere - ecosphere - technosphere - digital sphere - have been formulated. It has been proved that urban infrastructure components are not confined to residential and industrial zones so far. The algorithm for resource durability of utilities systems in urban residential estates has been developed using predictive analytics methods. It is established that the technology of augmented and mixed reality expands opportunities for remote control, maintenance and repair of various technical and engineering systems. A long-term forecast method for environment comfort metering with the modeling of variability impact of accentuated components of fie spheres has been proposed. It is proved that the technology of augmented, virtual and mixed reality (AR/VR/MR), which adds information to the environment on the basis of existing data, allows making enhanced solutions and reducing errors in the field of urban infrastructure planning and construction management, accounting and distribution of housing and utilities resources. © Published under licence by IOP Publishing Ltd. Pentasphere predictive analytics for urban environment arrangement and management  Environmental management; Housing; Mixed reality; Predictive analytics; Project management; Remote control; Repair; Safety engineering; Urban growth; Classical mechanism; Construction management; Digital environment; Engineering systems; Long-term forecast; Technical facility; Urban environments; Urban infrastructure; Information management",Monitoring and control
1369,Big Data Platform for Enterprise project management digitization using Machine learning,"In recent years, enterprise related data is becoming increasingly digitized. Due to the exponential volume of enterprise data being generated, there is an increased demand in managing this data for efficient decision making. Data Mining and machine learning aids in achieving valuable insight of a business and its growth through the exploration of data by recognizing business project relationships and dependencies. Enterprise project data is a subset of enterprise data being created online or offline. This paper presents the proposed architectural design and related concept explanation for efficient enterprise project management in current data scenario. Also, an analysis of the current enterprise project techniques has been presented along with challenges that need to be addressed. © 2018 IEEE. Big Data Platform for Enterprise project management digitization using Machine learning big data; business intelligence; data mining; enterprise project Big data; Competitive intelligence; Decision making; Machine learning; Project management; Current data; Data platform; Enterprise data; Enterprise project management; Offline; Project data; Data mining",Financial management
1370,ACM International Conference Proceeding Series,"The proceedings contain 89 papers. The topics discussed include: smart emergency management based on social big data analytics: research trends and future directions; Pathfinding decision-making using proximity sensors, depth camera and active IR marker tracking data fusion for human following companion robot; estimation of vegetation biomass in an alpine marsh using multi-angle hyperspectral data CHRIS; social media platforms: alternative source of fire data in Cebu city using SVM and correlation analysis; data mining of project management data: an analysis of applied research studies; classified support vector regression with particle swarm optimization to forecast Youbike parking lots and number of bikes; extreme anomalous score clustering algorithm; target selection on suppressing IADS for multiple fighters using evolutionary game algorithm; an analysis of DRR suggestions using K-means clustering; MPIjs: an MPI package in JavaScript for browser-based distributed computing environments; sentiment analysis on the impact of K-12 program in the Philippines using naïve Bayes and lexicon approach with code switching; approximate tandem repeats computation; video scene change detection using convolution neural network; shape matching between printed 3d model and digital 3d model based on 2D views and Zernike moments; an improved Chinese chessman recognition method for robot in natural environment; sculpture detection method using the convolution neural network; and a secured and optimized document management tool using advanced encryption standard and NoSQL. ACM International Conference Proceeding Series  ",Strategic alignment
1372,Industrial Practices of Evaluation of Projects in Global Environments: An Empirical study,"One major problem that the software companies faces is the economic evaluation of their software products. Economic evaluation if done inaccurately may prove very atrocious for the firm. In this paper, the outcome of research conducted to analyze the techniques and challenges followed by major software companies for economic evaluation of the software product is given. The questionnaire (Annexure-1) is distributed to two software industry managers, who are involved in undertaking economic evaluation of projects and had wide experience in project management and the data collected is then analyzed and the major challenges faced by these companies are highlighted and the two major research questions are answered based on the data gathered rom the companies. © 2018 The Author(s). Industrial Practices of Evaluation of Projects in Global Environments: An Empirical study data analytics; Economic Evaluation; Software Projects Artificial intelligence; Economic analysis; Project management; Software engineering; Data analytics; Economic evaluations; Empirical studies; Global environment; Industrial practices; Research questions; Software industry; Software project; Industrial economics",Risk management
1373,Smart office; application of a unified analytics center in tunneling construction,"While tunneling customers are becoming increasingly sophisticated, the tunneling construction sector remains severely under-digitized. Mechanized tunneling construction is among the most innovative areas in the construction industry, however, this industry depends on expert skills and experience to deliver the project successfully. Building a unified analytics center to gather, integrate, and analyze data from disparate databases on a construction site (e.g., TBM itself, conveyor system, inventory, schedule, and daily activity reports) and then contextualize this information into a visual, meaningful representation provides a robust tool to enhance instant, and far-off decisionmaking for all levels of site and office personnel. In this paper, the application of a unified analytics center in tunneling construction is introduced. Dugway Storage Tunnel project is considered as a case study. Throughout the construction period of the tunnel, vast quantities of data from TBM sensors is pulled out and streamed directly from TBM’s PLC. Data is then pushed into Power BI, which is an analytics service provided by Microsoft. This system connects the project personnel (e.g., Project manager, Construction manager, Project engineer, and Site Engineers) to a broad range of data via easy-to-use dashboards, interactive reports, and meaningful interactive visualizations that bring data to life. © 2019 Taylor & Francis Group, London. Smart office; application of a unified analytics center in tunneling construction  Construction; Construction industry; Digital storage; Human resource management; Managers; Office buildings; Project management; Visualization; Construction manager; Construction period; Construction sectors; Construction sites; Interactive visualizations; Mechanized tunneling; Project engineers; Project managers; Tunnels",Strategic alignment
1375,Data Visualization of Complex Information Through Mind Mapping in Spain and the European Union,"This chapter is an introduction to the visualization of complex information in the European Union (EU) and Spain using the mind mapping technique. Traditional methods of visualizing complex information, linear text, and web pages, have many problems that are largely solved when mind mapping, a method presented in this chapter, is used. The chapter starts with an introduction to the data science ecosystem in the EU and Spain, with a focus on big data and open data. It explains the problems of classical methods when trying to visualize complex information and describes the mind mapping technique as the best solution to these problems. The chapter also introduces the most common uses and importance of mind mapping in professional applications, the scientific evidence for the advantages of mind mapping, the concept of mind mapping automation, and some examples of applications to big data and open data developed by the author. The chapter concludes with a summary of the possibilities of mind mapping to improve productivity and efficiency in the management of information at the federal government level. © 2018 Elsevier Inc. All rights reserved. Data Visualization of Complex Information Through Mind Mapping in Spain and the European Union Complex information; Information organization; Knowledge management; Management reporting; Mind mapping; Mind maps; Project management; Strategic planning; Visual mapping; Visualization Big data; Flow visualization; Knowledge management; Mapping; Open Data; Partial discharges; Productivity; Project management; Strategic planning; Visualization; Websites; Complex information; Information organization; Mind maps; Mind-mapping; Visual mapping; Data visualization",Strategic alignment
1376,ACM International Conference Proceeding Series,"The proceedings contain 71 papers. The topics discussed include: modeling pause for the synthesis of Kazakh speech; IoT smart homes based on RFID technology: localization systems; DC-DC power converters & charge pump circuits converters for battery operated system; an improved approach protocol for wireless sensor networks based on hierarchical routing protocols; optimization and automation of air traffic control systems: an overview; top-down vs. bottom-up in project management: a practical model; Android forensics tools and security mechanism: survey paper; text-independent speaker recognition based on syllabic pitch contour parameters; generating summaries through selective part of speech tagging; photocatalytic degradation of endocrine disruptor bisphenol A in ZnO/UV system: optimization and modeling using a response surface methodology (RSM); learning analytics: requirements for enhanced learner-content interactions in an online environment; investigating business intelligence in the era of big data: concepts, benefits and challenges; conceptual model for trust and recommendation systems; authentication protocol for securing Internet of Things; effect of temperature on the photocatalytic degradation of an antibiotic residue in urban wastewater; a novel approach for improving the quality of software code using reverse engineering; and towards an elastic implementation of a context broker for green IoT and smart cities. ACM International Conference Proceeding Series  ",Strategic alignment
1377,Visual data and predictive analytics for proactive project controls on construction sites,"This paper presents the theoretical foundation for a project controls system that improves understanding of how construction performance can be captured, communicated, and analyzed in form of a visual production system; predicts and effectively communicates the reliability of the weekly work plan and look-ahead schedules, supports root-because assessment on plan failure at both project and task-levels; facilitates information flows; and decentralizes decision-making. Our model-driven system builds upon novel visual data analytics to map the current state of production in 4D (3D+time), compare to 4D BIM, and expose waste at both project and task-levels. Using predictive analytics and based on actual progress and productivity data, reliability in the future state of production is forecasted to highlight potential issues in a location-driven scheme and support collaborative decision making that eliminates root causes of waste. To evaluate the performance of our system, several case studies are conducted on real-world commercial building projects. It is shown that the developed system provides visual interfaces between people and information on and offsite, enables effective pull flows, decentralizes work tracking, facilitates in-process quality control and hand-overs among contractors, and most importantly transforms retroactive and task-driven workflows in contractor coordination meetings to proactive location-driven practices. © Springer International Publishing AG, part of Springer Nature 2018. Visual data and predictive analytics for proactive project controls on construction sites Lean construction; Predictive data analytics; Visual production management Architectural design; Behavioral research; Contractors; Decision making; Intelligent computing; Office buildings; Project management; Quality control; Collaborative decision making; Commercial building; Construction performance; Construction sites; Data analytics; Lean construction; Production management; Theoretical foundations; Predictive analytics",Strategic alignment
1378,A framework for Business Process Data Management based on Big Data Approach,"A business process (BP) refers to a set of activities carried out by humans to achieve one or more business goals. BPs are ubiquitous and occur in several sectors: marketing, healthcare, financial management and of course business. BPs generate a significant amount of data known as big data. In recent years, the management of business process models and data is very challenging. On one hand, business process must be powerful in terms of modeling. On another hand, big data analytics support to find suitable knowledge to enact business process models. In this paper, we will introduce an overview of our big data process-based approach that places big data and process in the same framework. © 2017 The Authors. Published by Elsevier B.V. A framework for Business Process Data Management based on Big Data Approach big data; big data analytics; BPM; Business Process Information management; Information systems; Management science; Project management; Business goals; Business Process; Business process model; Data analytics; Financial managements; Process-based approach; Big data",Value management
1380,"Managing change in the delivery of complex projects: Configuration management, asset information and 'big data'","As we enter an era of 'big data', asset information is becoming a deliverable of complex projects. Prior research suggests digital technologies enable rapid, flexible forms of project organizing. This research analyses practices of managing change in Airbus, CERN and Crossrail, through desk-based review, interviews, visits and a cross-case workshop. These organizations deliver complex projects, rely on digital technologies to manage large data-sets; and use configuration management, a systems engineering approach with mid-20th century origins, to establish and maintain integrity. In them, configuration management has become more, rather than less, important. Asset information is structured, with change managed through digital systems, using relatively hierarchical, asynchronous and sequential processes. The paper contributes by uncovering limits to flexibility in complex projects where integrity is important. Challenges of managing change are discussed, considering the evolving nature of configuration management; potential use of analytics on complex projects; and implications for research and practice. © 2015 . Managing change in the delivery of complex projects: Configuration management, asset information and 'big data' Asset information; Change; Complex projects; Configuration management Big data; Hierarchical systems; Asset information; Change; Complex projects; Configuration management; Digital technologies; Managing changes; Research analysis; Sequential process; data set; engineering; project management; Information management",Risk management
1381,Proceedings - International Conference on Software Engineering,The proceedings contain 7 papers. The topics discussed include: experiences conducting experiments in industry: the ESEIL FiDiPro project; continuous validation of a modeling tool in an industrial setting; applying data analytics towards optimized issue management: an industrial case study; defect prediction on a legacy industrial software: a case study on software with few defects; plug-in software engineering case studies; case studies in industry: what we have learnt; and evaluating the benefits of systematic project management in large public sector projects. Proceedings - International Conference on Software Engineering  ,Capacity management
1382,Analysis of dimensions influencing the success of BI projects through data mining; [Análise das dimensões influenciadoras do sucesso em projetos de BI através de data mining],"Business Intelligence (BI) is an emergent domain with numerous implementation projects proliferating in the industry. However, several factors may influence the success of such projects. This study addresses the identification of the relevant dimensions that may affect success for leveraging a future empirical analysis of BI projects in Portugal through data mining using the quantitative features hereby identified. © 2017 AISTI. Analysis of dimensions influencing the success of BI projects through data mining; [Análise das dimensões influenciadoras do sucesso them projetos de BI através de data mining] Business intelligence; Data mining; Project management; Success factors; Technology acceptance Competitive intelligence; Information analysis; Information systems; Management science; Project management; Empirical analysis; Implementation projects; Portugal; Quantitative features; Success factors; Technology acceptance; Data mining",Value management
1383,Relationship between learning indicators in the development and result of the building engineering degree final project,"The present work can be included in a much broader investigation related to the content, methodology and success of the Final Degree Project (FDP) in the framework of Building and Construction Management. The aim of our proposal is to study the current FDP in Technical Architecture and Building Engineering degrees using an academic analytics approach. Here, we will focus on the first stage, in order to establish a relationship among the main academic indicators that determine the FDP outcome. The typology of the final projects in engineering degrees requires the use of abilities and technical competences described in several academic plans, in order to prepare the student for joining the job market. This work focuses on and compares the 2004-2007 and 2012-2015 periods (pre and post Bologna), in order to examine the effect of expanding the academic plan from three to four academic years. The results confirm an improvement in the final marks the students attained in the second period, which can be related to the changes in the academic plan. ©2016 ACM.© 2016 ACM. Relationship between learning indicators in the development and result of the building engineering degree final project Academic analytics; Building engineering studies; Educational assessment; Evaluation of the educational system; Learning indicators; Student profile Ecology; Ecosystems; Engineering education; Professional aspects; Project management; Students; Academic analytics; Building engineering; Educational assessment; Educational systems; Student profiles; Education",Risk management
1384,Role models: Mining role transitions data in IT project management,"The notion of roles is crucial in project management across various domains. A role indicates a broad set of tasks, activities, deliverables and responsibilities that the person needs to carry out within a project. Assigning roles to team members clarifies the expectations of work items to be delivered by each and structures the interactions of the team among themselves as well as with external stakeholders. This paper analyzes a sizeable real-life dataset regarding the actual usage of roles in software development and maintenance projects in a large multinational IT organization. The paper introduces and formalizes concepts such as seniority level of a role, career progression and career lines, formulates various business questions related to role-based project management, proposes analytics techniques to answer them and outlines the actual results produced to answer the business questions. The business questions are related to dependencies between roles, patterns in role assignments and durations, predicting role changes, discovering insights useful for meeting career aspirations, interesting role sequences etc. The proposed analytics algorithms are based on Markov models, sequence mining, classification and survival analysis. © 2016 IEEE. Role models: Mining role transitions data in IT project management Classification; Graph Clustering; HR Analytics; Project Management; Role-based Teams; Sequence Mining; Survival Analysis; Workforce Management Bioinformatics; Classification (of information); Data mining; Human resource management; Markov processes; Software design; Graph clustering; HR Analytics; Role-based; Sequence mining; Survival analysis; Workforce management; Project management",Strategic alignment
1385,Analytics for software project management - Where are we and where do we go?,"Software project management is a decision intensive process. Success or failure of the project is highly dependent on these decisions. Analytical techniques and tools can support project managers throughout the software project life cycle by increasing the predictability and chance of success in these projects. In this paper, we report the results of a systematic mapping study within which we investigate the usage of different types of analytics for software project management. We analyze the accessibility of the data as well as the degree of validation reported in the 115 studies selected for final analysis. This resulted in a picture of the status quo (Where are we?) of analytics in software project management. From comparing this status quo with the results of an industrial survey on the industrial needs of different types of analysis, we propose an agenda on future work (Where do we go?). © 2015 IEEE. Analytics for software project management - Where are we and where do we go? Analytical project management; Analytical technique; data analysis; systematic mapping Data reduction; Information management; Life cycle; Mapping; Software engineering; Surveys; Technical presentations; Analytical technique; Industrial surveys; Project managers; Software project; Software project management; Systematic mapping; Systematic mapping studies; Techniques and tools; Project management",Financial management
1386,Control of informational impacts on project management,"In the paper, the impacts on project information space on analytical data basis are analyzed and distinguished. There are scientifically justified the interactions of factors' groups, which because impacts on the project team and the success of the project. Here is presented estimation method of joint action of impacts' groups on the reaction in project management systems; desirable combinations of impacts that provide efficiency of project management and its life cycle are identified. © 2016 IEEE. Control of informational impacts on project management analytics; information impacts; internal impacts; management; model; project Data communication systems; Life cycle; Management; Models; Analytical data; analytics; Estimation methods; information impacts; internal impacts; project; Project informations; Project management system; Project management",Monitoring and control
1387,Bridging Higher Education and Market Dynamics in a Business Intelligence Framework,"The expansion of urbanization and demographic changes defines not only economic demands but makes governments increasingly experience a serious force toward a sustainable development, which causes a need for a qualified and retaining competitive workforce. In view of this trend, governments and businesses start considering technology as vital enabler to solve the rising urbanization issues and improve the living and working environments according to a set of priorities. Simultaneously, competency and skills development are seen as the critical issue for the workforce and the workplaces. Accordingly, the complex interrelationship between strategic management, human capital management, and the overall quality management in an educational as well as enterprise setting move closer to the focus of research. This paper discusses an emerging area of research, which explores the utilization of both, advanced technologies in terms of business intelligence and analytic techniques in a multicomponent architecture and a sustainable competency-based human capital strategy. The novel approach targets at an alignment of one of the most important higher education outputs, namely the human capital, with market needs and thus, maximizing the benefits of higher education in achieving sustainable economic development. © 2015 IEEE. Bridging Higher Education and Market Dynamics in a Business Intelligence Framework Business intelligence; Competency management; Higher education; Human capital management; Human resources Commerce; Competitive intelligence; Economic and social effects; Information analysis; Management science; Personnel; Planning; Quality management; Sustainable development; Competency managements; Demographic changes; Focus of researches; Higher education; Human capitals; Strategic management; Sustainable economic development; Working environment; Education",Financial management
1388,Mining big data is 'sexy' arena,"Construction-sector companies and software firms are embracing data science with vastly improved tools to collect and store enormous amounts of data collected from project stakeholders and the growing need to analyze and interpret it for public-infrastructure project management and planning. The efforts are making a difference for owners. Rocky Kearney, deputy director of New Mexico's Public School Facilities Authority, estimates that the authority's use of a cloud platform, hosted by Plantation, a software firm e-Builder, has saved it $200 million in maintenance and capital costs for the $19.5 billion in assets of its 89 districts. The city of Toronto's transportation services group created a new Big Data innovation team in 2015 to leverage emerging transportation data sets, in particular GPS probe-based data sources. The YOU.S. Dept. of Energy's Los Alamos National Laboratory in New Mexico standardized on a Locus Technologies cloud-based platform to manage environmental compliance and monitoring for multiple stakeholders of a nearly 40-sq-mile site where radioactive and chemical contamination occurred during more than seven decades of nuclear-weapon production and research. Contracting giant Bechtel has created Big Data & Analytics Center of Excellence at its Reston base to access and harness this volume of information, learn from it and use the knowledge gained to transform the way we operate and compete. Design firm Merrick used a LIDAR scan of an existing power substation to build the BIM model and subsequent construction documents for an electrical infrastructure upgrade. Mining big data is 'sexy' arena  Architectural design; Construction industry; Data mining; Environmental technology; Nuclear weapons; Project management; School buildings; Cloud based platforms; Construction documents; Electrical infrastructure; Environmental compliance; Los Alamos National Laboratory; Multiple stakeholders; Public infrastructure project; Transportation services; Big data",Stakeholder management
1389,Work in progress - Design and development of a project management intelligence (PMInt) tool,"The need for project managers to use project management intelligence tools has been extensively argued in the literature. Software project managers also need what can be called 'project intelligence' tools, in the same way that business managers need business intelligence (BI) tools to assist them make intelligent decisions. This research paper builds on the previous research work which proposed a project management intelligence (PMInt) tool which is aimed at assisting project manager take informed decisions. This research paper designs and also develops a prototype of the PMInt tool. The preliminary test results obtained from testing the prototype show that the PMInt tool will provide project managers with insight regarding project team members' concerns and views. © 2016 IEEE. Work in progress - Design and development of a project management intelligence (PMInt) tool business intelligence; design; development; project intelligence; tools Competitive intelligence; Design; Engineering research; Information analysis; Management science; Managers; Project management; Tools; Business managers; Design and Development; development; Informed decision; Intelligence tool; Intelligent decisions; project intelligence; Work in progress; Human resource management",Risk management
1393,Business Intelligence and Analytics in Small and Medium-sized Enterprises: A Systematic Literature Review,"Despite much interest in business intelligence and analytics (BI&A), empirical research shows that small and medium-sized enterprises (SMEs) are still lagging behind in the proliferation of BI&A. However, there are no studies found on literature reviewing research on BI&A in SMEs. This paper collects, categorizes, synthesizes, and analyzes 62 articles related to BI&A in SMEs. The identified research topics being addressed in BI&A include: BI&A components, BI&A solutions, Mobile BI&A, Cloud BI&A, BI&A application, BI&A adoption, BI&A implementation, and BI&A benefits. Further, research gaps and directions for future research are presented to facilitate the progression of BI&A in SMEs research. © 2017 The Authors. Published by Elsevier B.V. Business Intelligence and Analytics in Small and Medium-sized Enterprises: A Systematic Literature Review analytics; BI&A adoption; BI&A benefits; BI&A implementation; BI&A review; BI&A solutions; Business intelligence; SMEs Competitive intelligence; Enterprise resource planning; Information analysis; Information management; Management science; Project management; analytics; Empirical research; Research gaps; Research topics; Small and medium-sized enterprise; SMEs; Systematic literature review; Information systems",Value management
1395,"Challenges of inter-organizational information and middleware system projects: Agility, complexity, success, and failure","There are two main ways to manage projects in business: The traditional and the agile. While the first has been historically the main Project Management methodology for businesses, the latter is gaining more support for projects involving Information Systems (IS) due to its fitting with the unpredictable nature of IS changes. To assess outcomes of projects, businesses have been using rational, narrative, organizational, and/or performative approaches. While each approach has its strengths and weaknesses, the jury is still out on which is more suitable for IS projects. More specifically, there is a lack of targeted research on which type of project management and which approach for project outcome evaluation are most relevant for Inter-Organizational Information Systems (IOIS) and Inter-Organizational Middleware System (IOMS). IOISs are automated ISs spanning across partnering organizations and aiming at synergizing their forces to increase competitiveness and cost efficiency. An Inter-Organizational Middleware System (IOMS) is the component inside IOIS that is responsible for bridging various partners' systems while holding part of the business intelligence. First, this paper collects and reviews existing project management methodologies; it then evaluates present approaches for assessing outcomes of IS projects, before it focuses on IOIS and IOMS projects, highlighting their unique characteristics and challenges. The paper concludes with a highlight of the research gap into IOIS and IOMS project management. © 2015 IEEE. Challenges of inter-organizational information and middleware system projects: Agility, complexity, success, and failure agility; complexity; failure; IOIS; IOMS; IOS; middleware; PM; project management; projects; success Computer system recovery; Information retrieval systems; Information systems; Management science; Middleware; Project management; Promethium; agility; complexity; IOIS; IOMS; projects; success; Information management",Value management
1398,"Proceedings - 2015 International Workshop on Data Mining with Industrial Applications, DMIA 2015: Part of the ETyC 2015","The proceedings contain 12 papers. The topics discussed include: clustering, spectral techniques and inference in large-scale networks; a mining approach to evaluate geoportals usability; an integrated strategy based in processes, requirements, measurement and evaluation, for the formalization of necessities in data warehouse projects; application of business intelligence techniques to analyze IT project management data; data mining applications in entrepreneurship analysis; feature grouping and selection on high-dimensional microarray data; feature selection via approximated Markov blankets using the CFS method; and K-DBSCAN: identifying spatial clusters with differing density levels. Proceedings - 2015 International Workshop on Data Mining with Industrial Applications, DMIA 2015: Part of the ETyC 2015  ",Strategic alignment
1399,Understanding the service desk: Applied forecasting and analytics approach,"In this paper, the study aimed to identify the best forecasting model to represent I-Helpdesk, service desk of an Information Technology (IT) project under the Singapore's Ministry of Defence (MINDEF), in the aspect of Service Requests management. Defence Science and Technology Agency (DSTA) support MINDEF users in the area of technical consultancy and project management areas such as resource allocation management for IT service delivery and excellence. To achieve an overall aim of better service delivery for their system users, we intend to answer questions such as: How should we plan the helpdesk staffing for the next week/month? © 2017 IEEE. Understanding the service desk: Applied forecasting and analytics approach Analytics; Forecasting; IT Service Management; Machine Learning; Service Desk Information management; Machine learning; Project management; Analytic; Analytic approach; Forecasting models; Help Desk; Information technology service management; Information technology services; Machine-learning; Service delivery; Service desk; Service management; Forecasting",Monitoring and control
1402,Developing a big data/analytics project: A case study in the auto industry,"For an organization to develop a sustainable position in today's competitive and volatile market, efforts in increasing its maturity of Industry 4.0 thinking and implementation need to be performed. Inside the context of Industry 4.0, the Internet of Things (IoT) and Big Data/Analytics are concepts that are receiving an increased attention of both academics and industry practitioners. However, their implementation is challenging and requires a high level of competences, both in skilled labor and financial resources related to technology selection and implementation. There is a diversity of scenarios in which those two concepts can be applied, especially in large manufacturing enterprises, where large amounts of data are generated, and need to be interpreted, helping managers make better decisions. Hence, it is necessary to organize and better structure projects that conceive, manage, develop and implement IoT and Big Data/Analytics technologies. The objective of this study is to develop a case study presenting the architecture of a small IoT and Big Data/Analytics project in an automotive company. Results suggest that to promote such projects in a larger scale, organizations need to greatly enhance their infrastructure to develop the necessary integration between IoT components and Big Data/Analytics tools. Developing a big data/analytics project: A case study in the auto industry Big data; Data analytics; Project management Automotive industry; Engineers; Information management; Internet of things; Project management; Automotive companies; Data analytics; Financial resources; Internet of thing (IOT); Large amounts of data; Manufacturing enterprise; Technology selection; Volatile markets; Big data",Financial management
1404,Does pair programming work in a data science context? An initial case study,"While pair programming has been studied extensively for software programmers, very little has been reported with respect to pair programming in a data science project. This paper reports on a case study evaluating the effectiveness of pair programming within a data science / big data context. Our findings show that pair programming can be useful for data science teams. In addition, while the driver role was similar to what has been described for software programmers, we note that the observer role had an expanded set of responsibilities, which we termed researcher activities. Further exploration is required to explore if these expanded roles are specific to data science pair programming. © 2017 IEEE. Does pair programming work in a data science context? An initial case study Big Data; Data Science; Pair Programming; Process Methodology; Project Management Big data; Computer software; Project management; Data contexts; Pair-programming; Science projects; Data Science",Value management
1405,Sentiment analysis for the construction industry: A case study of weibo in China,"Construction industry is a labor-intensive industry. Sentiment or mood of participants in the construction industry is a key issue in this business. To analyze this issue, using traditional ways such as questionnaire survey to collect data is both time- and cost-consuming. Recently, with the rapid development of social media services, data can be collected and extracted for sentiment analysis to provide officials and managers with fresh perspectives on participants in the construction management. In this paper, a sentiment analysis systematic framework is proposed. This system collected user messages from social media sites, establishes and compare different clusters emotion dictionaries by time duration and location. This paper generated valuable information and knowledge in the construction domain. As an initial trial, this study selected social media of Weibo because of its wide usage in China. Four clusters which include construction workers, construction companies, construction unions, and construction media were analyzed. For each user, the crawler is used to collect the Weibo messages from his/her Web page. On average, there are 135 messages collected for each user. This research then analyzed these data in the following aspects to dig out sentiments behind data: hourly, daily, monthly, and locations. Detailed findings, benefits and barriers to incorporating social media data analytics in the construction industry, along with future research, were discussed. This paper benefits the academia by testing an alternative way of studying the construction population, which further will help decision makers better understand the real situations of the construction industry. © 2017 American Society of Civil Engineers. Sentiment analysis for the construction industry: A case study of weibo in China  Construction industry; Data Analytics; Decision making; Project management; Sentiment analysis; Surveys; Websites; Construction companies; Construction management; Construction workers; Decision makers; Questionnaire surveys; Social media datum; Social media services; Systematic framework; Social networking (online)",Value management
1406,Improving distribution circuit performance without circuit rebuilds,"Utilities often address distribution circuit performance by categorizing circuits as 'Worst Performing' and then implementing circuit rebuilds to overcome operational issues. New analysis technologies that use predictive maintenance or conditions-based analytics guide maintenance personnel to specific, pin-point actions minimizing maintenance costs and improving customer service. This paper will review the data analysis and conditions based circuit measurements that avoid the need for circuit rebuilds as a method for circuit performance improvement. Case studies will be presented that review all steps of the predictive maintenance strategy and results of the programs. Specific targets for SAIDI/SAIFI improvement will be evaluated and project outcomes will be discussed. © 2017 IEEE. Improving distribution circuit performance without circuit rebuilds Condition monitoring; Maintenance management; Power system reliability; Predictive maintenance; Radio frequency identification; Sensor systems; Sensor systems and applications Condition monitoring; Maintenance; Predictive analytics; Project management; Radio frequency identification (RFID); Timing circuits; Maintenance management; Power system reliability; Predictive maintenance; Sensor systems; Sensor systems and applications; Electric network analysis",Risk management
1407,Quantitative planning and risk management of agile software development,"The Agile Software Development methodologies has enjoyed a widespread acceptance in the software development industry. While iterative and incremental approach of agile methodologies are the main attractions, at the same time they make estimation and predictability of agile software projects a challenge. Delivering workable software in short cycles helps with collecting more heuristic data as compared to traditional waterfall methodologies. Such data can be used as quantitative metrics for time and effort estimation that in turn can help with risk mitigation and risk avoidance. Although traditional agile formulations and recommendations place emphasis on individuals and interactions over processes and tools, this paper considers processes and tools essential in agile processes of today's complex software systems and distributed teams. Emphasis on processes and tools enables agile software projects to produce project metrics that can be effectively used in predictive analytics and risk management. The system that is introduced here emphasizes on quantitative approach to agile project planning and introduces a risk management model that produces risk metrics that are used to help with risk avoidance and risk mitigation. The risk metrics and the project simulation model are used to adjust project factors such as time, cost and scope during lifespan of project. Such adjustments come from recommender system that proposes changes to a wide range of project parameters for risk mitigation and risk avoidance. © 2017 IEEE. Quantitative planning and risk management of agile software development  Agile manufacturing systems; Heuristic methods; Iterative methods; Predictive analytics; Risk management; Risk perception; Risks; Software design; Agile Methodologies; Agile software development; Complex software systems; Incremental approach; Project parameters; Quantitative approach; Quantitative metrics; Risk management models; Software engineering",Risk management
1408,Assessing business value of Big Data Analytics in European firms,"In the strategic management field, dynamic capabilities (DC) such as organizational agility are considered to be paramount in the search for competitive advantage. Recent research claims that IT business value research needs a more dynamic perspective. In particular, the Big Data Analytics (BDA) value chain remains unexplored. To assess BDA value, a conceptual model is proposed based on a knowledge-based view and DC theories. To empirically test this model, the study addresses a survey to a wide range of 500 European firms and their IT and business executives. Results show that BDA can provide business value to several stages of the value chain. BDA can create organizational agility through knowledge management and its impact on process and competitive advantage. Also, this paper demonstrates that agility can partially mediate the effect between knowledge assets and performance (process level and competitive advantage). The model explains 77.8% of the variation in competitive advantage. The current paper also presents theoretical and practical implications of this study, and the study's limitations. © 2016 Elsevier Inc. Assessing business value of Big Data Analytics in European firms Big Data Analytics (BDA); Competitive advantage; Dynamic capabilities (DC); IT business value; Knowledge Based View (KBV); Organizational agility ",Strategic alignment
1409,Material flow cost accounting needs to collaborate with data science to establish sustainable management,"Material Flow Cost Accounting (MFCA) is a Environmental Management Accounting tool, targeting simultaneously to increasing profit and at the same time reduce environmental impacts. In 2011, the fundamental framework of MFCA was published by the International Organization for Standardization (ISO) within the 14000ff family on Environmental Management [3]. The MFCA analysis shows material loss in production processes by using the instrument of mass balances. Material loss means the loss of input materials that do not end up in good product as output from production process. Usually material loss is qualified as waste and/or emission to be treated in waste management. By reducing material loss a company is able to also reduce the volume of input materials per a unit of product and reduce manufacturing product costs. In order to reduce material loss the improvement or the innovation of a company's production system will be required. When the company targets such improvements or innovations, the company needs an adequate data base for efficient decision making. In the present MFCA projects, the improvement or change of the manufacturing system mostly is based on practical experiences and knowledge of managers and workers. For a sound improvement of material efficiency in production processes a profound data base and data analysis of MFCA data is necessary. This paper explains why MFCA needs computer science and engineering to establish sustainable management in practice. © 2015 IEEE. Material flow cost accounting needs to collaborate with data science to establish sustainable management Business Data Mining; Environmental Management Accounting (EMA); Innovative Production; Material Flow Cost Accounting (MFCA); Sustainable Management Cost accounting; Cost reduction; Costs; Data mining; Decision making; Environmental impact; Environmental management; Human resource management; Manufacture; Project management; Sustainable development; Business data; Computer science and engineerings; Environmental management accountings; International organization for standardizations; Manufacturing products; Material Flow; Practical experience; Sustainable management; Waste management",Value management
1410,"The key Lies in the process, not in the innovation","Big Data, Analytics, Cloud. These three concepts are surrounded by an incredible hype in the last years, but are they really the key to success in the years to come? Using the data from three of the world's largest studies on the priorities, strategies and careers of technology leaders we examine two specific issues: innovation practices and skill profile. Innovation is not an end in itself but a stepping stone to achieve superior business performance. If technologies-such as big data or cloud-are not connected to the core business and objectives of the company they are just information, not knowledge. In order to benefit of these technologies, CIOs should focus their attention on how to strategically align these technologies (and the underlying information they can provide) with the core business, in order to improve the business process. Concerning the second topic-skills profile development-, in the recent years we have witnessed an extremely dynamic setting. In a short span of time we have moved on from the era of mobile technologies to cloud/analytics databases. Consequently, specific skills of IT staff are constantly fluctuating, and professionals are struggling to catch up with the new demands. However, despite new trends demand the development of new skills, it seems that there are some ""transversal"" or ""knowledgeable"" skills that will always be needed and indispensable-e.g., project manager-. The overreaching conclusion is that even the foremost tools/innovations/technologies are ineffective if not properly supported with an appropriate knowledge management process. © The Authors, 2017. The key Lies in the process, not in the innovation Analytics; Big data; Business analyst; Efficiency; Future technology trends; Hype cycle; It objectives; Knowledge management; Project management; Skill profile Efficiency; Knowledge management; Management science; Project management; Analytics; Business analysts; Future technologies; Hype cycle; It objectives; Skill profile; Big data",Financial management
1412,Collabcrew-An intelligent tool for dynamic task allocation within a software development team,"Currently in the IT industry, the people factor has become very critical when determining the quality of a software project. It is highly important that the correct person performs the relevant task and proper human resource allocation happens within the software project team to obtain successful outcome. This often needs critical thinking, regular team meetings and discussions. Typically a software project manager needs to be highly experienced with the team for this purpose and can be really complex and time consuming with the limited project schedules. This research work introduces a task management tool-CollabCrew specially designed for the software development teams which dynamically allocate tasks based on the skills and previous work done by the team members. This uses historical data from its' own repository or from an external source to find useful information of the previous work done by the project team members to automatically allocate them for new tasks. This proposed system will be containing an Extract, Transform and Load (ETL) tool which will extract data from different data sources, a prediction model to predict the aptness of each team member for a given task and a peer review mining and summarization component to provide a viable way to extract features from peer reviews. Then based on the result, the task allocation component will do the allocations in the most optimal and the feasible way for the project. Even though there are several commercially available task management tools, none has an intelligent component to automatically delegate work within the team. The scope of this work extends beyond the IT domain and a similar procedure can be adopted to develop a task allocation framework in other fields as well. © 2017 IEEE. Collabcrew-An intelligent tool for dynamic task allocation within a software development team Business Intelligence; Data Mining; Extract; Genetic Algorithm; Machine Learning; Predictive Modelling; Review Mining; Sentimental Analysis; Software Project Management; Task Allocation; Transform and Load (ETL) Genetic algorithms; Human resource management; Machine learning; Project management; Software design; Business-intelligence; Extract; Machine-learning; Predictive models; Review mining; Sentimental analyse; Software project; Software project management; Task allocation; Transform and load (extract, transform and load); Data mining",Financial management
1413,Business Intelligence Success applied to Healthcare Information Systems,"In this paper, DeLone and McLean's IS Success Model is empirically tested on a Business Intelligence System applied to Healthcare Information Systems at 12 public hospitals in Denmark. The purpose of the study is to investigate which factors contribute to BI Success. A total of 1351 end-users replied to the questionnaire, and the response rate was 32%. Eight relationships in the model were tested, and four relationships were found to be significant. Our results are as follows: System Quality is positively and significantly associated with Use and User Satisfaction. Information Quality is positively and significantly associated with User Satisfaction but not Use, and User Satisfaction is not significantly associated with Use and vice versa. User Satisfaction is positively and significantly associated with Individual Impact, but Use is not significantly associated with Individual Impact. © 2017 The Authors. Published by Elsevier B.V. Business Intelligence Success applied to Healthcare Information Systems Business Intelligence; Evaluation; Healthcare Information Systems; IS success Competitive intelligence; Health care; Hospitals; Human computer interaction; Information analysis; Information management; Management science; Medical computing; Project management; Surveys; Business intelligence success; Business intelligence systems; Evaluation; Health care information system; Information quality; IS success; IS success model; User satisfaction; Information systems",Value management
1414,Solar energy management as an Internet of Things (IoT) application,"Photovoltaic (PV) array analytics and control have become necessary for remote solar farms and for intelligent fault detection and power optimization. The management of a PV array requires auxiliary electronics that are attached to each solar panel. A collaborative industry-university-government project was established to create a smart monitoring device (SMD) and establish associated algorithms and software for fault detection and solar array management. First generation smart monitoring devices (SMDs) were built in Japan. At the same time, Arizona State University initiated research in algorithms and software to monitor and control individual solar panels. Second generation SMDs were developed later and included sensors for monitoring voltage, current, temperature, and irradiance at each individual panel. The latest SMDs include a radio and relays which allow modifying solar array connection topologies. With each panel equipped with such a sophisticated SMD, solar panels in a PV array behave essentially as nodes in an Internet of Things (IoT) type of topology. This solar energy IoT system is currently programmable and can: a) provide mobile analytics, b) enable solar farm control, c) detect and remedy faults, d) optimize power under different shading conditions, and e) reduce inverter transients. A series of federal and industry grants sponsored research on statistical signal analysis, communications, and optimization of this system. A Cyber-Physical project, whose aim is to improve solar array efficiency and robustness using new machine learning and imaging methods, was launched recently. © 2017 IEEE. Solar energy management as an Internet of Things (IoT) application  C (programming language); Fault detection; Internet of things; Project management; Solar concentrators; Solar energy; Solar power generation; Topology; Arizona state university; Fault-detection optimizations; Faults detection; Government projects; Monitoring device; Photovoltaic arrays; Power Optimization; Smart monitoring; Solar arrays; Solar panels; Solar panels",Monitoring and control
1416,Improving construction management of port infrastructures using an advanced computer-based system,"This study presents the design, development, scheme and field validation of an early-alert ocean wave system. It is designed to automate, improve, analyse, design and manage the daily construction activities of any harbour at construction stage. The objective is threefold: a) maximise construction safety, with regards to well-known hazards which occur during construction, especially breakwaters that interact with high-energy sea states, b) optimise the transport, by means of specialised vessels, of the refill material, and c) to minimise the construction delay and disruption on a daily basis, thanks to short-term construction forecasting (96 h). The system, known as PATO, offers short-term sea states characteristics, at any point near harbour structures, and relevant wave-structure interaction parameters at any harbour construction stage. The system is able to assist harbour project managers by providing accurate ocean wave data through a user-friendly interface. © 2017 Elsevier B.V. Improving construction management of port infrastructures using an advanced computer-based system Clustering; Management; Planning; Port construction; Short-term prediction; Wave modelling Construction industry; Management; Ocean currents; Planning; Predictive analytics; Project management; User interfaces; Water waves; Clustering; Construction activities; Construction management; Port constructions; Short term prediction; User friendly interface; Wave modelling; Wave-structure interaction; Ports and harbors",Capacity management
1417,An analysis of international coauthorship networks in the supply chain analytics research area,"This work characterized the research community of supply chain analytics (SCA) with respect to coauthorship, a special kind of collaboration. A characterization of coauthorship in terms of researchers’ countries, institutions and individuals was elaborated, so three different one-mode networks were studied. Besides, the SCA research community is characterized in terms of Supply Chain Management (SCM) research streams. Coauthorship among researchers working on different streams is also analyzed. Metrics that depict the importance of the network nodes were studied such as degree, betweenness and closeness. This study found out an intense collaboration between USA and countries such as China, India, United Kingdom and Canada. Researchers from Canada and Ireland are better situated (central) in the network, although they have not published a considerable amount of papers. The presence of cliques and the small-world effect were also observed in these networks. In terms of research streams, more research on SCA located at the Strategic Management, Technology-focused and Logistics streams was found. The most common links between research streams are on the one side, Technology-focused with both Strategic Management and Logistics and on the other side Strategic Management with both Logistics and Organizational behavior. SCA researchers are rarely working with a focus on Marketing. This study contributes to the SCA literature by identifying the most central actors in this area and by characterizing the area in terms of SCM research streams. This study may contribute to the development of more focused research incentive programs and collaborations. © 2017, Akadémiai Kiadó, Budapest, Hungary. An analysis of international coauthorship networks in the supply chain analytics research area Big data analytics; Business analytics; Coauthorship networks; Social network analysis; Supply chain analytics ",Strategic alignment
1418,"Big Data, Analytic Culture and Analytic-Based Decision Making Evidence from Australia","This study investigates how managerial decision making is influenced by Big Data, analytics and analytic culture. The results of a cross-sectional survey (n = 163) of senior IT managers reveal that Big Data Analytics creates an incentive for managers to base more of their decisions on the analytic insights. However, we also find that the main driver of analytic-based decision making is analytic culture. Considering that culture - in contrast to Big Data Analytics tools and skills - is a resource which cannot be change easily or quickly, we conclude that firms with a highly analytic culture can use this resource as a competitive weapon. Finally, our analysis reveals that managers in smaller organizations are significantly more likely to base their decisions on analytic results than managers in large organizations, which suggests the former use analytics to remain competitive against their larger counterparts. © 2017 The Authors. Published by Elsevier B.V. Big Data, Analytic Culture and Analytic-Based Decision Making Evidence from Australia Analytic Culture; Big Data Analytics; Data Science; Decision Making; Organizational Culture Decision making; Information management; Information systems; Managers; Project management; Australia; Cross-sectional surveys; Data analytics; Data Science; Large organizations; Managerial decision making; Organizational cultures; Big data",Strategic alignment
1419,Utilization of tools during strategic logistic decision-making at SME's in The Netherlands,"In 1993 a study regarding strategic logistic decision-making (SLDM) within Small and Medium sized enterprises (SME) resulted in the limited use of information tools. SLDM shows different phases during the decision-making process, with different type of data gathering and analyses per phase. Within the arena of logistics solutions complex economic tradeoffs may be part of the decision-making. In 1993 and with the software application available in those days it was understandable that decision-making was based on the analyses of limited factors for which tools were not necessary. A new study was performed between November 2015 and April 2016. Two groups of third year bachelor logistics students conducted interviews with a guided questionnaire at a 1993 comparable SME target group. Of all interviews a transcript was made. The pattern analyses determined the decision-making situation at the participating carriers, to which this paper is restricted. After clustering the holistic results these were compared to individual carrier results. The study's outcome only relates to the participating eight carriers and is therefore indicative of nature. Although currently innovative tools are present in the market, it appears that a minority of the responding carriers adopt these tools to utilize them during important, complex logistics decision-making. All carriers see improvements for their SLDM process but due to time constraints are not able to redesign the decision-making process that initiates improvements. Having the right skills creates a decision-making advantage, which was shown by one SME carrier with sophisticated decision-making approach. Based on the outcome of this study the question is raised how education can support strategic, logistic decision-making in the future. Copyright The Authors, 2017. All Rights Reserved. Utilization of tools during strategic logistic decision-making at SME's in The Netherlands Business intelligence; Decision-making; Economic trade-offs; Logistics; Simulation; Strategic management Application programs; Commerce; Competitive intelligence; Economic and social effects; Information use; Logistics; Surveys; Data gathering and analysis; Decision making process; Simulation; Small- and medium-sized enterprise; Software applications; Strategic logistics; Strategic management; Trade off; Decision making",Strategic alignment
1421,The ambiguity of data science team roles and the need for a data science workforce framework,"This paper first reviews the benefits of well-defined roles and then discusses the current lack of standardized roles within the data science community, perhaps due to the newness of the field. Specifically, the paper reports on five case studies exploring five different attempts to define a standard set of roles. These case studies explore the usage of roles from an industry perspective as well as from national standard big data committee efforts. The paper then leverages the results of these case studies to explore the use of data science roles within online job postings. While some roles appeared frequently, such as data scientist and data engineer, no role was consistently used across all five case studies. Hence, the paper concludes by noting the need to create a data science workforce framework that could be used by students, employers, and academic institutions. This framework would enable organizations to staff their data science teams more accurately with the desired skillsets. © 2017 IEEE. The ambiguity of data science team roles and the need for a data science workforce framework big data; data science; data science roles; project management Big data; Personnel; Project management; Academic institutions; Case-studies; Job postings; National standard; Science community; Skill-sets; Team roles; Data Science",Value management
1423,Critical factors for business intelligence success,"Business intelligence (BI) is a strategically important tool for organisations. Numerous studies have attempted to investigate the factors that contribute to BI success. However, an overview of the critical success factors (CSFs) is lacking, as is an understanding of the gaps in the extant research. After examining 444 articles, we integrated the findings of 29 studies. We used the framework of information system success to identify the CSFs and to analyse how researchers identify information system success. We identified 36 variables related to BI success in the extant literature. The distinct CSFs relate to project management skills, management support, user involvement, the external environment and management processes. In the articles in which BI success was operationalised, we found several distinct factors: System quality, information quality, use, service quality, user satisfaction and net benefits. We extended the framework of information system success to include three additional factors: Strategy and vision, organisational form and competency development. We contribute to the extant research by extending the framework of information system success and identifying the gaps in the extant research. We contribute to practice through an enhanced understanding of the CSFs related to BI success. © 2017 Proceedings of the 25th European Conference on Information Systems, ECIS 2017. All rights reserved. Critical factors for business intelligence success BI success; Business intelligence; Critical success factor; Information system success Competitive intelligence; Data mining; Enterprise resource planning; Information systems; Project management; Business intelligence success; Competency development; Critical success factor; External environments; Information quality; Information system success; Management process; Project management skills; Information use",Value management
1425,Construction management scheduling and control: The familiar historical overview,"The paper suggests that 'management by exception' is an historical default control mechanism based on the perception of control as a static process. However, increasingly scholars claim that a dynamic and proactive systems model is a more effective form of project control. These findings are the result of an historical desktop research method that analysed content from a small sample of scheduling methods and control approaches found in online and university library resources. The concept of control has historically influenced both visualization and analytics of different scheduling methods for construction project management. This paper focuses on two control ideals; static and dynamic control mechanisms. The overview begins with the description of early graphical scheduling techniques: Gantt charts and Harmonogram. It continues with examples of contributors to scheduling and control that include: CPM, PERT, LOB, Flowline and Location Based Management. The finding of this simple history suggests that change is the constant element for project control mechanisms. An object-based digital environment such as the data-rich building information modelling (BIM) appears to be continuing the change for new scheduling methods and control mechanisms. © 2016 The Authors. Construction management scheduling and control: The familiar historical overview  Architectural design; Dynamics; Libraries; Project management; Building Information Modelling; Construction management; Construction project management; Digital environment; Dynamic control mechanisms; Scheduling and controls; Scheduling techniques; University libraries; Scheduling",Monitoring and control
1427,Scientific cooperation engineering,"Scientific Cooperation Engineering researches, fosters and supports scientific cooperation on all hierarchical levels and beyond scientific disciplines as a key resource for innovation in the Cluster of Excellence. State-of-the-art research methods-such as structural equation models, success models, or studies on success factors-that are frequently used in IS research are applied to create profound knowledge and insights in the contribution and optimal realization of scientific inter and trans-disciplinary communication and cooperation. A continuous formative evaluation is used to derive and explore insights into interdisciplinary collaboration and innovation processes from a management perspective. In addition, actor-based empirical studies are carried out to explore critical factors for interdisciplinary cooperation and intercultural diversity management. Based on these results, workflows, physical networking events and tailor-made training programs are created and iteratively optimized towards the cluster’s needs. As Scientific Cooperation Engineering aims to gain empirical and data-driven knowledge, a Scientific Cooperation Portal and a prototypic flowchart application are under development to support workflows and project management. Furthermore, data science methods are currently implemented to recognize synergetic patterns based on bibliometric information and topical proximity, which is analyzed via project terminologies. © Springer International Publishing AG 2017. Scientific cooperation engineering  ",Capacity management
1428,An extension of the technology acceptance model for business intelligence systems: Project management maturity perspective,"Business intelligence systems (BISs) refer to a wide range of technologies and applications useful for retrieving and analyzing a large amount of information with the goal to generate knowledge useful for making effective business decision. In order to investigate adoption of BISs in companies, we propose a model based on the technology acceptance model (TAM) that is expanded by variables representing the concept of a project management maturity (PMM). The survey on the sample of USA companies has been conducted with the chief information officer (CIO) as the main informant. A structural equation model has been developed in order to test the research model. Results indicate that TAM expanded with the notion of PMM is useful in increasing understanding of BISs adoption in companies. © 2017, SciKA. An extension of the technology acceptance model for business intelligence systems: Project management maturity perspective Business intelligence systems; Chief information office; Project management maturity; Technology acceptance model ",Financial management
1430,Increasing the business potential of companies by ensuring continuity of the development of their information systems by current information technologies,"This paper deals with applications of information and communication technologies in the management of companies and institutions. It also focuses on Competitive Intelligence and Business Intelligence and the description of their position in business management. The paper presents current trends in information and communication technologies with emphasis on the use of virtualization and Cloud Computing technologies. The author discusses the importance of Cloud Computing to maintain the continuity of information system of enterprises with low financial impact, thereby increasing its stability. Theoretical framework and literature support the assumption that information and communication technologies are essential for the competitiveness of small and mediumsized enterprises. Discussed are factors that affect management and use of information and communication technologies in small and medium-sized enterprises, in particular the use of cloud computing. Based on the results obtained from a questionnaire survey carried out in the Czech Republic, the author proposed methodological recommendations to facilitate the transition to cloud computing. © 2016 Vilnius Gediminas Technical University (VGTU) Press. Increasing the business potential of companies by ensuring continuity of the development of their information systems by current information technologies business intelligence; cloud computing; competitive intelligence; corporate performance management; information systems; small and medium-sized enterprises; strategic management; virtualization ",Financial management
1431,Evaluation of Analytic Projects in the Context of Higher Education,"Higher Education environment is moving to a data-oriented philosophy, as it has happened in general in other contexts, transforming the society in general and popularizing the term analytics. The motivation factor is the paradigm shift that is being experimented in the higher education context, with a high increment of the competition at international level and with the irruption of new kinds of teaching, such as the massive adoption of eLearning and the Massive Online Open Courses (MOOC). Due to this conversion, higher education organizations are investing gargantuan quantities of money and resources to develop and implant analytic systems. Most of these analytic systems are unsuccessful either because they deviate from the initial planning or do not solve the proposed analytic problem. A similar situation happened before in the enterprise context, where the analytical orientation was adopted previously. In this paper we propose to adapt the experience and maturity of the enterprise environment in the specification, development and implantation of analytic systems in order to define a project management roadmap adapted to the university environment. The proposed roadmap take into account lessons learnt in order to provide guides that allow defining, conducting and closing projects to develop and implant analytic systems systematically and increase the potential success of the project. The roadmap gives support to: 1) defining the analytic project, by evaluating whether the university is ready to develop and use an analytic system and whether the analytic problem is real and relevant for the university, 2) conducting the development of the analytic system, by providing a set of critical success factors to take into account and indicators to use to monitor the development activity, and 3) implanting and validating the analytic system, by following strategies for a successful implantation of the system, a successful adoption of the system for its potential users and a validation of its impact and utility in the university. © 2016 IEEE. Evaluation of Analytic Projects in the Context of Higher Education analytical system; enterprise; higher education; project management; roadmap E-learning; Industry; Metadata; Project management; Teaching; Analytical systems; Critical success factor; Development activity; Enterprise environment; Higher education; Motivation factors; Roadmap; University environment; Education",Monitoring and control
1433,Cross-cohort research experience for project management and leadership development,"Project management and leadership skills are essential for career development. However, in typical university settings, undergraduate students take different courses and work on different projects in different teams each semester. As a result, students lack opportunities to work on multi-year projects and develop the skills essential for long-term planning. To remedy this situation, our department has created elective courses that allow students from all years (first-year students to graduate students) to work on research projects under the supervision of faculty members and the mentorship of senior graduate students. These projects provide the opportunities for students to learn many skills essential in workplace, such as (1) understanding how projects are designed and managed; (2) taking responsibilities on different components in the projects; (3) learning computer tools for collaboration and integration; (4) developing leadership skills; (5) cultivating self-learning; and (6) improving communication, both speaking and writing. This paper reports one project that involves undergraduate, masters, and doctoral students. The project, now in its fifth-year, builds computer tools for researchers, educators, and students using cloud computing for large-scale image analysis. The project has received an award in a student competition, and three research grants for international collaboration, entrepreneurship, and big data analytics, and produced more than a dozen research papers. This paper describes the project in detail and shares experiences on many crucial factors necessary for creating a successful cross-cohort research project. Research experience is not required in typical undergraduate curricula; thus, it is essential to recruit well qualified and interested students. From the beginning of this project, there was a clear goal to create software tools that would become available to the research community. The opportunity to serve real users is appealing to many students. In order to build software tools for users, this project has established rigorous procedures common in commercial software development such as version control, testing, documentation, and so on. Leadership development is another key component: if a student continues in this project over multiple semesters, the student may be promoted to lead a subteam or the entire team. In addition to learning technical skills, the team has participated in multiple student competitions and has won the second prize in one competition. This project also encourages entrepreneurship: a group of students plan to start a company after they have interviewed potential customers exploring the feasibility of commercializing the technology and investigating market-product match. Four foreign institutions are collaborators of the project and the students have experience working with these collaborators through video conferencing. © American Society for Engineering Education, 2016. Cross-cohort research experience for project management and leadership development  ",Risk management
1437,"Managerial Decision Modeling: Business Analytics with Spreadsheets, Fourth Edition","This book fills a void for a balanced approach to spreadsheet-based decision modeling. In addition to using spreadsheets as a tool to quickly set up and solve decision models, the authors show how and why the methods work and combine the user's power to logically model and analyze diverse decision-making scenarios with software-based solutions. The book discusses the fundamental concepts, assumptions and limitations behind each decision modeling technique, shows how each decision model works, and illustrates the real-world usefulness of each technique with many applications from both profit and nonprofit organizations. The authors provide an introduction to managerial decision modeling, linear programming models, modeling applications and sensitivity analysis, transportation, assignment and network models, integer, goal, and nonlinear programming models, project management, decision theory, queuing models, simulation modeling, forecasting models and inventory control models. The additional material files Chapter 12 Excel files for each chapter Excel modules for Windows Excel modules for Mac 4th edition errata can be found at https://www.degruyter.com/view/product/486941 Offers complete explanations necessary for understanding concepts and their implementation in spreadsheets Application-oriented and software-based, with a managerial view of effectively applying models to improve the decision-making process. © 2017 Walter de Gruyter Inc., Boston/Berlin. Managerial Decision Modeling: Business Analytics with Spreadsheets, Fourth Edition Decision modeling; Decision science; Excel modeling; Linear programming; Quantitative analysis ",Strategic alignment
1438,Inter-organizational middleware systems: A framework for managing change,"Inter-Organizational Information Systems (IOIS) are automated Information Systems spanning across partnering organizations and aiming at synergizing their forces to increase competitiveness and cost efficiency [1], [2]. An Inter-Organizational Middleware System (IOMS) is the component inside IOIS that is responsible for bridging various partners' systems while holding some business intelligence [3]. Processes, roadmaps, and frameworks managing change in Information Systems are in abundance. However there is a paucity of literature on frameworks for managing change in IOIS and the situation is even direr in the context of IOMS. In this paper we first explore the concept of frameworks and in particular IS and IOIS frameworks. Using these as a foundation, we go on to propose a framework to manage IOMS changes, and we discuss their bases and merits. We then validate it through application in the context of a real-world project in an international firm. The authors of this article have over a decade of experience in IOIS and IOMS systems, and use an interpretative lens to study the implementation. © 2015 IEEE. Inter-organizational middleware systems: A framework for managing change agile; APM; Framework; IADR; IOIS; IOMS; IOS; middleware; project management Information retrieval systems; Information systems; Management science; Project management; agile; Framework; IADR; IOIS; IOMS; Middleware",Value management
1440,Big data analytics using agile model,"This journal introduces the reader the background of Big Data Analytics and how efficiently Agile methodology can be applied to achieve the business goal. The journal focus on giving background of Big Data and how using Agile practices such as iterative, incremental, and evolutionary style of development can be applied for Big Data Analytics. This methodology brings in the advantage of involving business community during development and continuous delivery of working user features. © 2016 IEEE. Big data analytics using agile model Agile; Analytics; Big Data; Data Analyst; Development Methodology; Predictive Tool; Project Management; Software Engineering Iterative methods; Project management; Software engineering; Agile; Analytics; Data analysts; Development methodology; Predictive tools; Big data",Strategic alignment
1441,Data literacy for learning analytics,"This workshop explores how data literacy impacts on learning analytics both for practitioners and for end users. The term data literacy is used to broadly describe the set of abilities around the use of data as part of everyday thinking and reasoning for solving real-world problems. It is a skill required both by learning analytics practitioners to derive actionable insights from data and by the intended end users, such that it affects their ability to accurately interpret and critique presented analysis of data. The latter is particularly important, since learning analytics outcomes can be targeted at a wide range of end users, some of whom will be young students and many of whom are not data specialists. Whilst data literacy is rarely an end goal of learning analytics projects, this workshop aims to find where issues related to data literacy have impacted on project outcomes and where important insights have been gained. This workshop will further encourage the sharing of knowledge and experience through practical activities with datasets and visualisations. This workshop aims to highlight the need for a greater understanding of data literacy as a field of study, especially with regard to communicating around large, complex, data sets. © 2016 Copyright held by the owner/author(s). Data literacy for learning analytics Analysis; Communication; Data literacy; Learning analytics; Visualization Communication; Flow visualization; Human computer interaction; Project management; Visualization; Analysis; Analysis of data; Data literacy; End users; Knowledge and experience; Learning analytics; Project outcomes; Real-world problem; Data visualization",Risk management
1442,Components of big data analytics for strategic management of enterprise architecture,The concept of strategic management is currently being under pressure to adopt big data analytics as a tool for improving efficiency of decision- making and monitoring processes in organizations. This paper aims to provide a systematic approach to map the benefits driven by big data analytics in terms of enterprise architecture focusing on the importance for strategic management. The key components are identified and discussed in the context of TOGAF. The findings can be used as a guide to help developers and designers in reframing their enterprise architecture efforts. Components of big data analytics for strategic management of enterprise architecture Big data analytics; Design science research; Enterprise architecture; Strategic management Big data; Decision making; Information systems; Information use; Strategic planning; Big Data Analytics; Design-science researches; Enterprise Architecture; Improving efficiency; Monitoring process; Reframing; Strategic management; Information management,Strategic alignment
1443,Exploring how different project management methodologies impact data science students,"This paper reports on a controlled experiment comparing different approaches on how to guide students through a semester long data science project. Four different methodologies, ranging from a traditional “just assign some intermediate milestones” to other more agile methodologies, are compared. The results of the experiment shows that the project methodology used in the classroom made a significant difference in student outcomes. Surprisingly, an Agile Kanban approach was found to be much more effective than an Agile Scrum methodology, which was not one of the leading ap-proaches. © 2017 Proceedings of the 25th European Conference on Information Systems, ECIS 2017. All rights reserved. Exploring how different project management methodologies impact data science students Agile development; Big data education; Data science education; Project management Agile manufacturing systems; Big data; Information systems; Information use; Project management; Agile development; Agile Methodologies; Controlled experiment; Project management methodology; Science education; Science projects; Scrum methodologies; Student outcomes; Students",Risk management
1444,The Role of the Chief Data Officer: Managing Expectations,"In this department's continuing series on the evolving role of the chief data officer (CDO), the author breaks down his interview with Ursula Cottone, CDO of Citizens Bank. They discuss a process that began with people rather than data, and examine how projects had to be justified not by direct ROI, but through understanding the process, transparency along the journey, and ongoing expectations management. © 1999-2012 IEEE. The Role of the Chief Data Officer: Managing Expectations data analytics; project management Computer applications; Software engineering; Data analytics; Project management",Risk management
1445,Using data-and network science to reveal iterations and phase-transitions in the design process,"Understanding the role of iterations is a prevalent topic in both design research and design practice. Furthermore, the increasing amount of data produced and stored by companies leaves traces and enables the application of data science to learn from past design processes. In this article, we analyse a documentlog to show the temporal evolution of a real design process of a power plant by using exploratory data analysis and network analysis. We show how the iterative nature of the design process is reflected in archival data and how one might re-construct the design process, involving iterations between many parties, including the client, external consultants, suppliers, and designers. We also show how people use different representations during the design process and how this is associated with a design phasetransition in the process. Finally, we relate our findings with the literature on iterations and discuss implications for research and practice with application to project management and process modelling. Using data-and network science to reveal iterations and phase-transitions in the design process Complexity; Design process; Iterations; Network science; Visualisation Iterative methods; Management science; Plants (botany); Project management; Visualization; Complexity; Design process; Exploratory data analysis; External consultants; Iterations; Network science; Process modelling; Temporal evolution; Design",Governance
1446,Business analytics of enterprises in terms of strategy; [Strategicznie o analityce biznesowej przedsiębiorstw],"The paper is devoted to the problem of strategic business analytics. The aim of the study is to show business analytics supporting the construction and development of the company development strategy. In order to achieve the assumed goal there has been applied the method of critical analysis of the domestic and foreign literature in the field of business analytics of enterprises domestically and worldwide. In the paper, there have been identified the factors for the use of business analytics. Three priority factors which are the arguments for the application of business analytics in management processes and enterprise development strategies are: speed/ease of deployment (68% of indications), ease of use for business users (65% of indications) and self-service and data discovery tools (61% of indications). Moreover, managers of enterprises with the best economic and financial results, achieving return on assets (ROA) of more than 10%, almost agreeably (90% of indications) claim that analytical tools are necessary for the proper implementation of the strategy and the achievement of increasingly higher economic and financial results. The value of the paper consists in showing a new trend in the development of business analytics, which is to support the process of strategic management and analytical competition. © 2017, Czestochowa University of Technology. All rights reserved. Business analytics of enterprises in terms of strategy; [Strategicznie o analityce biznesowej przedsiębiorstw] Analytical competition; Business analytics; Business intelligence; Decision support; Strategic management ",Strategic alignment
1448,"Proceedings - 2015 30th IEEE/ACM International Conference on Automated Software Engineering Workshops, ASEW 2015","The proceedings contain 22 papers. The topics discussed include: extending manual GUI testing beyond defects by building mental models of software behavior; software development analytics: experiences and the way forward; a conceptual framework for the comparison of fully automated GUI testing techniques; testing approach for mobile applications through reverse engineering of UI patterns; data mining methods and cost estimation models; empirical analysis on parallel tasks in crowdsourcing software development; analytics for software project management - where are we and where do we go?; a method to evaluate estimates produced by the capture-recapture model; using collective intelligence to support multi-objective decisions: collaborative and online preferences; RepMine: a system for transferrable analyses of collaboration activities in software engineering; an automated contextual collaboration approach for distributed agile delivery; and comparing model coverage and code coverage in model driven testing: an exploratory study. Proceedings - 2015 30th IEEE/ACM International Conference on Automated Software Engineering Workshops, ASEW 2015  ",Strategic alignment
1449,First and Second Strings: Using Entity Analytics to Contrast Core and Supporting Teams,"Purpose: This research demonstrates the importance of the concept of the core team (Humphrey et al. in Journal of Applied Psychology 94:48–61, 2009) in contrast to supporting or peripheral members within the context of larger nominal teams. It asks the question, “In a team-oriented organization, should we assume that all nominal team members are, in fact, relatively equal contributors?” Design: Utilizing individual batter, fielder, and pitcher data from the National League from 2005 to 2014, we operationalize the concept of the core group, which is a subset of high-performing members. Findings: After testing for aggregation, we found that for core team members, their collective versions of batting average, runs batted in (RBI), and pitching earned run average (ERA) significantly predicted team win-loss percentages with an R2 of.62, p < .0001. The prediction of team attendance based on the performances of the core team was also significant with an R2 of.40, p < .0001. The same three independent variables based solely on supporting players were neither aggregable nor significant. Similarly, results for the nominal teams composed of all players on the roster were also not aggregable. Implications: These findings highlight the importance of identifying and managing core members (Fonti et al. Strategic Management Journal 37:1765–1786, 2016). Implications for team settings in management and business settings are discussed. Originality: WABA provides a unique multi-level, visual analytical tool for testing the existence of entities for these baseball data. Its application shows very different data configurations for nominal, core, and support subteams and dissimilar patterns of relationships among the key performance variables for each type of team. © 2017, Springer Science+Business Media New York. First and Second Strings: Using Entity Analytics to Contrast Core and Supporting Teams Baseball team performance; Core group; Core team; MLB offense; Multi-level WABA; Team aggregation ",Monitoring and control
1450,Building effective SMA systems taking advantage of information technology,"Strategic management accounting (SMA) is a set of valuable tools to manage a company at its best. However, it is expensive and complex to implement. After a first part of theoretical introduction about the SMA framework, this paper will attempt to assess the possible contribution from the information technologies (IT) in this field both theoretically and empirically. The second part describes a medium-size company and its experiences in implementing a SMA system. We will describe the Nespoli Group, which comprehends 45 medium-size firms localized all-over Europe and the issues linked to the management of such a differentiated multinational ensemble of entities. In the third part, we will propose a new approach to the use of information technology in SMA systems. For this purpose we will evaluate in depth a particular indicator, the service level agreement (SLA), to understand how IT can effectively improve SMA systems. Several final considerations conclude the paper. © Springer International Publishing AG 2017. Building effective SMA systems taking advantage of information technology Accounting information systems; Business intelligence; ERP systems; Modelling; Simulation; Strategic management accounting ",Monitoring and control
1451,Industrial Business Process Management Using Adonis Towards a Modular Business Process Modelling Method for Zero-Defect-Manufacturing,"This paper introduces the idea of industrial business process management in the context of the European Commission's project GO0D MAN. It presents showcases on how the industry-domain independent, Business Process Management Toolkit ADONIS can be extended through composition and injection mechanisms to support the modelling of industrial business processes for zero-defect manufacturing and can be further enhanced with conceptual analytics techniques as a knowledge management infrastructure in the domain of advanced manufacturing. © 2017 IEEE. Industrial Business Process Management Using Adonis Towards a Modular Business Process Modelling Method for Zero-Defect-Manufacturing factories for the future; industrial business process management; knowledge management; meta-modelling; modeling Administrative data processing; Defects; Enterprise resource management; Industrial management; Knowledge management; Management science; Manufacture; Models; Systems engineering; Advanced manufacturing; Business process management; Business process modelling; Domain independents; European Commission; Injection mechanisms; Knowledge management infrastructure; Meta-modelling; Project management",Monitoring and control
1453,Big data dreams: A framework for corporate strategy,"The phenomenon of big data—large, diverse, complex, and/or longitudinal data sets—is having a stark influence on organizational strategy making. An increase in levels of data and technological capabilities is redefining innovation, competition, and productivity. This article contributes to both practical strategic application and academic research in the strategic management domain by presenting a framework that identifies how big data improves functional capabilities within organizations, shapes entirely new industries, and is a key component of innovative and disruptive strategies used by learning organizations to diversify and break down barriers of traditionally defined industries. This framework provides an appropriate basis for internal corporate strategy discussions that surround big data investments by explaining how firms create value through various approaches. In addition, we offer guidance for how firms might derive their own big data approach through the merits of aligning data strategy aspirations with data strategy authenticity. © 2017 Kelley School of Business, Indiana University Big data dreams: A framework for corporate strategy Big data; Big data strategy; Business intelligence; Corporate innovation; Data collection ",Strategic alignment
1454,Perhaps a shift in direction in engineering management education: A discussion and work in progress of adding data modeling topics to the foundation of an EM curriculum,"Traditionally, the Engineering Management body of knowledge contains topics such as project management, financial resource planning, and the management of technology, etc. But are these traditional tools enough to prepare an Engineering Management student for the ever more technologically complex and data driven corporate world of the 2020's? More recently systems engineering concepts have been added to the Engineering Management Body of Knowledge. Is this now enough? Stevens Institute of Technology thinks not. Over the past few semesters, Stevens incorporated more concepts on informatics and other data analytics, modeling and visualization topics beyond Operations Research into its' curriculum. Recent alumni, and cooperative education students alike, have been impressed with the quality and quantity of employment prospects. Should other Engineering Management programs consider the addition of such topics to the foundation of their curriculums? This article will discuss the pros and the pushbacks to making what others call an 'area of emphasis', core to a traditional Engineering Management curriculum. © American Society for Engineering Education, 2016. Perhaps a shift in direction in engineering management education: A discussion and work in progress of adding data modeling topics to the foundation of an THEM curriculum  ",Capacity management
1456,From VFX project management to predictive forecasting,"VFX production companies are currently challenged by the increasing complexity of visual effects shots combined with constant schedule demands. The ability to execute in an efficient and cost-effective manner requires extensive coordination between different sites, different departments, and different artists. This coordination demands data-intensive analysis of VFX workflows beyond standard project management practices and existing tools. In this paper, we propose a novel solution centered around a general evaluation data model and APIs that convert production data (job/scene/shot/schedule/task) to business intelligence insights enabling performance analytics and generation of data summarization for process controlling. These analytics provide an impact measuring framework for analyzing performance over time, with the introduction of new production technologies, and across separate jobs. Finally, we show how the historical production data can be used to create predictive analytics for the accurate forecasting of future VFX production process performance. © Copyright 2017 Authors. From VFX project management to predictive forecasting Business Intelligence; Data Analytics; Machine Learning; VFX Workflows Competitive intelligence; Computer graphics; Cost effectiveness; Information analysis; Interactive computer graphics; Learning systems; Management science; Predictive analytics; Project management; Data analytics; Data-intensive analysis; Historical production; New production technologies; Process controlling; Production companies; Project management practices; Work-flows; Data handling",Monitoring and control
1457,Application of business intelligence techniques to analyze IT project management data,"The IT management project must face many challenges, including the identification of the main criteria that lead to success or failure. Project managers generate a lot of data, which is stored in different formats, but in most organizations its use is not systematized with the aim of «learning from data» and generating reusable knowledge. The objective of this article is to present a framework based on Business Intelligence techniques that contributes improving the management of IT projects. © 2015 IEEE. Application of business intelligence techniques to analyze IT project management data Business Intelligence; CMMI; Data Mining; Data Warehousing; PMI; Project Management Competitive intelligence; Computer software reusability; Data mining; Data warehouses; Information analysis; Project management; CMMI; IT management; IT project; It project managements; Project managers; Reusable knowledge; Management science",Risk management
1458,Integration of business intelligence with corporate strategic management,"Integration of business intelligence and corporate strategic management has a direct impact on modern and flexible organizations. This integration helps decision makers to implement their corporate strategies, adapt easily to changes in the environment, and gain competitive advantages. This paper extends the studies in this domain, and clarifies the relationships between business intelligence and strategic management. It highlights also the role of business intelligence in corporate performance management and strategic intelligence. This paper proposes a BSC-BI framework that facilitates the integration of business intelligence with a balanced scorecard methodology. The BSC-BI framework implementation is demonstrated using a case study on the telecom field. Integration of business intelligence with corporate strategic management Balanced scorecard; Business intelligence; Competitive intelligence; Corporate performance management; Corporate strategic management; Strategic intelligence ",Strategic alignment
1459,Managing the Uncertainty of Bias-Variance Tradeoff in Software Predictive Analytics,"The importance of providing accurate estimations of software cost in management life cycle has led to an overabundant pool of prediction candidates exhibiting certain advantages and limitations. Thus, there is an imperative need for well-established principles that will aid the right decision-making regarding the selection of the best candidate. Unfortunately, the choice of the most appropriate estimation technique is not a trivial task, due to the multi-faceted nature of error. Accuracy, bias and variance are notions describing different aspects of predictive power that someone has to take into consideration during the validation process. The main objective of this paper is the utilization of visual analytics for the evaluation of two fundamental ingredients of prediction accuracy: the bias and the variance. Through a bootstrap-based resampling algorithm, we provide an easy-to-interpret way in order to acquire significant knowledge about the quality of a prediction candidate and manage the uncertainty of the estimation process. Ensemble techniques utilizing the advantages of both simple and complex solo methods are possible balancing solutions to the problem of the bias-variance tradeoff. © 2016 IEEE. Managing the Uncertainty of Bias-Variance Tradeoff in Software Predictive Analytics Bias; Prediction models; Project management; Software cost estimation; Variance; Visualization tools Cost estimating; Decision making; Forecasting; Life cycle; Predictive analytics; Project management; Software engineering; Visualization; Bias; Prediction model; Software cost estimations; Variance; Visualization tools; Application programs",Risk management
1460,Towards strategic management accounting: The nespoli group case,"The process to implement a Strategic Management Accounting (SMA) system is quite long and complex. SMA is not a separate item: it should be linked and closely integrated with the rest of the Accounting Information System (AIS). Firstly, the core of the AIS, the ERP system, should work properly, acting as the main source for the SMA system. Secondly, it is necessary a great empowerment of the management, which is supposed to operate directly the SMA system. The first part of the paper describes the SMA theoretical framework, discussing the more recent theories. The second part describes a medium-size company and its experiences in implementing a SMA system. In particular, we will describe the Nespoli Group, which comprehends 45 medium-size firms localized all-over Europe and the issues linked to the management of such a differentiated multinational ensemble of entities. The third part highlights the specific characteristics of the Nespoli Group SMA system. Several final considerations conclude the paper. © Springer International Publishing Switzerland 2016. Towards strategic management accounting: The nespoli group case Accounting information systems; Business intelligence; ERP systems; Strategic management accounting ",Strategic alignment
1461,"5th International Conference on Context-Aware Systems and Applications, ICCASA 2016","The proceedings contain 20 papers. The special focus in this conference is on Context-Aware Systems and Applications. The topics include: Modelling and reasoning about context-aware agents over heterogeneous knowledge sources; context-based project management; organisational knowledge sharing using social networking sites; context adaptive business networks; context-aware hand pose classifying algorithm based on combination of Viola-Jones method, wavelet transform, PCA and neural networks; a load balancing game approach for VM provision cloud computing based on ant colony optimization; optimizing the algorithm localization mobile robot using triangulation map; enhanced human activity recognition on Smartphone by using linear discrimination analysis recursive feature elimination algorithm; LCD-based on probability in content centric networking; multivariate cube for representing multivariable data in visual analytics; an approach to analyzing execution preservation in java program refactoring; a new method to analyze graphical user interfaces of android applications; an efficient method for time series join on subsequence correlation using longest common substring algorithm; an ORM based context model for context-aware computing; a conceptual framework for is project success; notes on recognizing echinocyte by the top-hat transform; personalized email user action prediction based on spamassassin; enhance performance of action evaluation functions with stochastic optimization algorithms and a method for mobility management in cellular networks using data mining. 5th International Conference on Context-Aware Systems and Applications, ICCASA 2016  ",Strategic alignment
1462,Implementation and assessment of a predictive analytics model for development project management,"In order to strengthen their competitive position, companies in high wage countries strive towards shortened innovation cycles while decreasing development costs. To achieve this, development projects need to be managed in a lean and efficient way. Existing approaches targeting the development project management mainly focus the target dimensions time, cost and quality on the superior project level. Corrective steering measures however need to be implemented on an activity level. Thus, a concept has been developed that applies predictive analytics techniques to predict deviations in the activities of development projects based on deviation indicators. In the presented paper, a methodology for the evaluation of suitable input parameters is presented. A predictive analytics model based on this concept is then implemented and validated. Therefore, a data set was acquired, which is used to train a neural network. To validate the applicability of the model, the accuracy of the predicted deviations is assessed against the actual deviations. © 2017 IEEE. Implementation and assessment of a predictive analytics model for development project management artificial neural networks; development project management; predictive analytics Neural networks; Predictive analytics; Activity levels; Analytic modeling; Competitive position; Development costs; Development programmes; Development project management; Innovation cycles; Project levels; Target dimensions; Time cost; Project management",Monitoring and control
1464,Supervised item response models for informative prediction,"Supporting human decision-making is a major goal of data mining. The more decision-making is critical, the more interpretability is required in the predictive model. This paper proposes a new framework to build a fully interpretable predictive model for questionnaire data, while maintaining a reasonable prediction accuracy with regard to the final outcome. Such a model has applications in project risk assessment, in healthcare, in social studies, and, presumably, in any real-world application that relies on questionnaire data for informative and accurate prediction. Our framework is inspired by models in item response theory (IRT), which were originally developed in psychometrics with applications to standardized academic tests. We extend these models, which are essentially unsupervised, to the supervised setting. For model estimation, we introduce a new iterative algorithm by combining Gauss–Hermite quadrature with an expectation–maximization algorithm. The learned probabilistic model is linked to the metric learning framework for informative and accurate prediction. The model is validated by three real-world data sets: Two are from information technology project failure prediction and the other is an international social survey about people’s happiness. To the best of our knowledge, this is the first work that leverages the IRT framework to provide informative and accurate prediction on ordinal questionnaire data. © 2016, Springer-Verlag London. Supervised item response models for informative prediction Item response theory; Metric learning; Questionnaire data Behavioral research; Data mining; Decision making; Forecasting; Iterative methods; Risk assessment; Surveys; Human decision making; Information technology projects; Item response theory; Maximization algorithm; Metric learning; Probabilistic modeling; Project risk assessment; Questionnaire data; Predictive analytics",Strategic alignment
1466,Integration of business intelligence with corporate strategic management,"The traditional model of competitive intelligence and its operationalization in most organizations appears to be inadequate to address the intelligence challenges arising from the speed of change in the environment, increasing data complexity, and growth of international activities. To address this challenge, this article borrows concepts from open innovation, applying them to all CI activities. We are suggesting going beyond the traditional model of an in-house CI unit with activities largely conducted by the units personnel and moving towards a cross pollination approach whereby others in the firm contribute to all CI activities including, for example, the selection of key intelligence topics and being involved in analysis and eventually towards a full open intelligence model in which key stakeholders and external experts also assist the organization in all aspects of competitive intelligence activity. In proposing a more open approach for intelligence, the authors recognize the concern that CI professionals will have regarding sharing intelligence and intelligence activities outside the CI unit and outside the organization. However, as pointed out in this article, organizations around the world have been moving quickly towards an open innovation model generally concluding that the benefits associated with opening up all elements of the innovation process, including ARE & D, outweigh the risks of intellectual property loss. Integration of business intelligence with corporate strategic management Analytics; Big data; Competitive intelligence; Open innovation ",Strategic alignment
1467,Integrated process and data model for applying scenario-technique in requirements engineering,"Originating from strategic management, scenario-technique yields potentials for requirements engineering. In this paper an integrated process model for such an application of scenario-technique is proposed. Flanked by an Integrated Scenario Data Model (ISDM), efficient prognosis of changes in complex requirements models is facilitated. The ISDM support this process by interlinking scenario data, requirements management data and additional data sources such as PDM/PLM systems. Scenarios are interpreted as results of interrelations among requirements. Combined with consistency assessment, the anticipation of potential future changes in requirements is facilitated, reducing potential risks for the product development process. Alongside the product development process, developers can develop reaction strategies for changes of requirements. The ISDM reduces the required effort for scenario derivation significantly by integrating data analytics and semantic modelling. In addition, the combination of process and data model allows efficient adaptations of scenarios to depict the dynamics of requirements. Intuitivism of derived scenarios is enhanced by the proposed approach. Integrated process and data model for applying scenario-technique in requirements engineering Design practice; Information management; Requirements; Scenario technique; Uncertainty Planning; Product development; Requirements engineering; Risk assessment; Semantics; Design practice; Integrated process model; Product development process; Requirements; Requirements management; Scenario technique; Strategic management; Uncertainty; Information management",Strategic alignment
1468,From financial merchandise planning to supply chain design and execution,"For years agents believed that technology could have bridged the gap in fashion companies. This has not come true. Significant investments aimed at implementing complex systems have often failed. Indeed, they have not been able, in a simple, flexible and comprehensive way, to integrate all the processes that, by definition, are changeable and not only influenced by deterministic factors. Therefore, it proves necessary, in complex organizations, to promote those best practices and habits that support and enhance personal freedom, judgments and hypotheses. This is the process by which the retailer seeks to provide the right amount and quality of the right merchandise in the right store at the right time, while also seeking to meet the financial goals of the company. This project, developed by the collaboration between the Department of Industrial Engineering (University of Bologna) and K.Group, aims to show how financial planning of Merchandise Planning may be implemented in a major Italian Fashion Retail Company, presenting the preliminary plan to integrate it with the specific processes of Supply Chain Planning and Execution, hence highlighting achievements, methodology and technological resources in terms of: data management (normal- ization and load data), business intelligence (score carding, dashboards, reporting, analysis), predictive analytics (clustering, simulation), performance management (budgeting, planning and forecasting, profitability), workflow management and data integration. © Springer International Publishing AG 2017. From financial merchandise planning to supply chain design and execution Business intelligence; Data integration; Merchandise Planning (MP); Performance management; Predictive analytics Budget control; Competitive intelligence; Data integration; Finance; Information analysis; Predictive analytics; Project management; Supply chains; Textile industry; Work simplification; Fashion companies; Financial planning; Merchandise Planning (MP); Performance management; Supply chain design; Supply chain planning; Technological resources; Workflow managements; Merchandise planning (MP); Information management",Strategic alignment
1469,A framework for describing big data projects,"With the ability to collect, store and analyze an ever-growing diversity of data generated with ever-increasing frequency, Big Data is a rapidly growing field. While tremendous strides have been made in the algorithms and technologies that are used to perform the analytics, much less has been done to determine how the team should work together to do a Big Data project. Our research reports on a set of case studies, where researchers were embedded within Big Data teams. Since project methodologies will likely depend on the attributes of a Big Data effort, we focus our analysis on defining a framework to describe a Big Data project. We then use this framework to describe the organizations we studied and some of the socio-technical challenges linked to these newly defined project characteristics. © Springer International Publishing AG 2017. A framework for describing big data projects Big data; Data science; Process methodology; Project management Project management; Case-studies; Data science; Project characteristics; Research reports; Sociotechnical; Big data",Value management
1470,Process-driven data analytics supported by a data warehouse model,"Business process management and business intelligence initiatives are commonly seen as separated organisational projects, suffering from lack of coordination, leading to a poor alignment between strategic management and operational business processes execution. Information systems researchers andprofessionals have recognised that business processes are the key for identifying the user needs for developing the software that supports those requirements. This paper presents a process based approach for identifying an analytical data model using as input a set of interrelated business processes, modelled with business process model and notation (BPMN), and the corresponding persistent operational data model. This process-based approach extends the BPMN language allowing the integration of behavioural aspects and processes performance measures in the persistent operational data model. The proposed approach ensures the identification of an analytical data model for a data warehouse, integrating dimensions, facts, relationships and measures, providing useful data analytics perspectives of the data under analysis. Copyright © 2017 Inderscience Enterprises Ltd. Process-driven data analytics supported by a data warehouse model Analytical data model; BPMN; Business intelligence; Business process model and notation; Operational data model; PPIs; Process performance indicators ",Strategic alignment
1472,Data Visualization of Complex Information Through Mind Mapping in Spain and the European Union,"This chapter is an introduction to the visualization of complex information in the European Union (EU) and Spain using the mind mapping technique. Traditional methods of visualizing complex information, linear text, and web pages, have many problems that are largely solved when mind mapping, a method presented in this chapter, is used. The chapter starts with an introduction to the data science ecosystem in the EU and Spain, with a focus on big data and open data. It explains the problems of classical methods when trying to visualize complex information and describes the mind mapping technique as the best solution to these problems. The chapter also introduces the most common uses and importance of mind mapping in professional applications, the scientific evidence for the advantages of mind mapping, the concept of mind mapping automation, and some examples of applications to big data and open data developed by the author. The chapter concludes with a summary of the possibilities of mind mapping to improve productivity and efficiency in the management of information at the federal government level. © 2018 Elsevier Inc. All rights reserved. Data Visualization of Complex Information Through Mind Mapping in Spain and the European Union Complex information; Information organization; Knowledge management; Management reporting; Mind mapping; Mind maps; Project management; Strategic planning; Visual mapping; Visualization ",Strategic alignment
1474,Centralized biobanks: a basis for medical research; [Zentralisierte Biobanken als Grundlage für die medizinische Forschung],"Biobanks are the basis for a substantial part of biomedical research. The development, establishment and operation of biobanks are connected to a broad range of aspects, mainly concerning the preparation, storage, usage and dissemination of samples and associated data, in addition to the social and public involvement of these processes. These complex requirements can often only be managed in large centralized biobanks. In recent years, centralized clinical biobanks have been established in several university clinics in Germany. Similar activities take place in other European countries and worldwide. This article highlights the requirements and main tasks of centralized clinical biobanks: high-quality pre-analytics and sample storage, the creation of professional IT structures, data protection, ethical issues, in addition to quality and project management. © 2016, Springer-Verlag Berlin Heidelberg. Centralized biobanks: a basis for medical research; [Zentralisierte Biobanken als Grundlage für die medizinische Forschung] Centralized biobanks in clinical environment; High quality sample generation and storage; Professional IT management; Project management; Quality management Biological Specimen Banks; Biomedical Research; Germany; Humans; Interinstitutional Relations; Internationality; Models, Organizational; Quality Assurance, Health Care; Systems Integration; Tissue and Organ Procurement; Tissue Donors; Germany; human; medical research; storage; university; biobank; donor; health care quality; international cooperation; medical research; nonbiological model; organization and management; public relations; system analysis; transplantation",Value management
1475,Comparing data science project management methodologies via a controlled experiment,"Data Science is an emerging field with a significant research focus on improving the techniques available to analyze data. However, there has been much less focus on how people should work together on a data science project. In this paper, we report on the results of an experiment comparing four different methodologies to manage and coordinate a data science project. We first introduce a model to compare different project management methodologies and then report on the results of our experiment. The results from our experiment demonstrate that there are significant differences based on the methodology used, with an Agile Kanban methodology being the most effective and surprisingly, an Agile Scrum methodology being the least effective. © 2017 Proceedings of the Annual Hawaii International Conference on System Sciences. All rights reserved. Comparing data science project management methodologies via a controlled experiment  Agile manufacturing systems; Project management; Controlled experiment; Project management methodology; Research focus; Science projects; Scrum methodologies; Data Science",Risk management
1477,Educational data mining perspectives within university big data environment,"All organizations are working nowadays in a very dynamic and strongly competitive environment. In order to survive and remain competitive, they need to take timely, adequate and informed decisions that are based not only on intuition and past experience. The main challenges for data analysis are related with the specific characteristics of 'big data' and the availability of suitable analytical tools for knowledge extraction that would support the processes of taking strategic management decisions. While 'big data' are already widely available and used in business, there are only rare cases of utilizing 'big data' in the educational sector. The main purpose of this paper is to focus on the challenges related to the analytical processing of 'big data' generated and stored at higher education institutions. The paper discusses the unique opportunities that Big Data analysis could give for the educational sector development and the improvements that could scale from a single school, to governmental directions and satisfaction of the labor market. However, big data analytics confronts universities with great challenges as well, related to finding appropriate methods and tools for extracting knowledge and patterns from extremely rich and complex data sets, and integrating the insights into a coherent vision for strategic management decisions. © 2017 IEEE. Educational data mining perspectives within university big data environment Big Data; Educational Data Mining; Educational Industry Advanced Analytics; Big data; Data Analytics; Data handling; Decision making; Strategic planning; Analytical tool; Competitive environment; Data environment; Educational data mining; Educational industry; Educational sectors; Informed decision; Knowledge extraction; Management decisions; Strategic management; Data mining",Strategic alignment
1479,Development of Information and Communication Systems within the Building of Project-Oriented Manufacturing Organization,"Project management (PM) has become a managerial discipline which is an inevitable prerequisite of managing modern business organizations, especially manufacturing ones. When applied properly it helps organizations to cope with permanently changing environment predominantly represented by customers, suppliers, competitors, and public authorities. Development of the project management methodology leads to increasingly effective implementation of strategic changes which is fundamental for being competitive on the marketplace. This PM development is in the center of building project orientation of an organization. But, developing just PM without development of overall organizational culture leads to low effectiveness of projects implemented in the organization. That is why development of projects and project management should be supported by development systems and areas of activities such as communications, knowledge, training and development of employees, and development of organizational standards and norms. The article focuses on the area of communication by using Business Intelligence systems. © IFIP International Federation for Information Processing 2014. Development of Information and Communication Systems within the Building of Project-Oriented Manufacturing Organization Business Intelligence; communications; culture; knowledge; Project management; project management office; project-oriented organization Cell culture; Communication; Competitive intelligence; Human resource management; Industrial management; Knowledge based systems; Management science; Manufacture; Personnel training; Project management; Business intelligence systems; Information and communication systems; knowledge; Manufacturing organizations; Organizational standards; Project management methodology; Project management offices; Project-oriented organizations; Knowledge management",Financial management
1480,Cost-sensitive and ensemble-based prediction model for outsourced software project risk prediction,"Nowadays software is mainly developed through outsourcing and it has become one of the most important business practice strategies for the software industry. However, outsourcing projects are often affiliated with high failure rate. Therefore to ensure success in outsourcing projects, past research has aimed to develop intelligent risk prediction models to evaluate the success rate and cost-effectiveness of software projects. In this study, we first summarized related work over the past 20 years and observed that all existing prediction models assume equal misclassification costs, neglecting actual situations in the management of software projects. In fact, overlooking project failure is far more serious than the misclassification of a success-prone project as a failure. Moreover, ensemble learning, a technique well-recognized to improve prediction performance in other fields, has not yet been comprehensively studied in software project risk prediction. This study aims to close the research gaps by exploring cost-sensitive analysis and classifier ensemble methods. Comparative analysis with T-test on 60 different risk prediction models using 327 outsourced software project samples suggests that the ideal model is a homogeneous ensemble model of decision trees (DT) based on bagging. Interestingly, DT underperformed Support Vector Machine (SVM) in accuracy (i.e.; assuming equal misclassification cost), but outperformed in cost-sensitive analysis under the proposed framework. In conclusion, this study proposes the first cost-sensitive and ensemble-based hybrid modeling framework (COSENS) for software project risk prediction. In addition, it establishes a new rigorous evaluation standard for assessing software risk prediction models by considering misclassification costs. © 2015 Elsevier B.V. Cost-sensitive and ensemble-based prediction model for outsourced software project risk prediction COSENS; Cost-sensitive; Ensemble; Outsourced software project; Risk management; Risk prediction Cost effectiveness; Decision trees; Failure analysis; Forecasting; Outsourcing; Predictive analytics; Risk assessment; Risk management; Software testing; Support vector machines; COSENS; Cost-sensitive; Ensemble; Risk predictions; Software project; Cost benefit analysis",Risk management
1481,Improving software project outcomes through predictive analytics: Part 2,"This paper deals with the systems mindset in addressing failure to introduce a software-specific predictive analytics model that accurately predicts software project outcomes of failure or success and identifies opportunities for incorporation in the federal and commercial space. The results of the model would be used during acquisition, prior to project initiation, and throughout the software development lifecycle. It is a decision analysis tool to assist decision makers in making the crucial decisions early in the lifecycle to cancel a project predicted of failure or to identify and implement mitigation strategies to improve project outcome. © 1973-2011 IEEE. Improving software project outcomes through predictive analytics: Part 2 predictive analytics; project management; software engineering; software failure; systems engineering Decision making; Life cycle; Project management; Safety engineering; Software design; Software engineering; Systems engineering; Decision analysis tool; Decision makers; Mitigation strategy; Project initiations; Project outcomes; Software development life cycle; Software failure; Software project; Predictive analytics",Risk management
1483,A data warehouse model for business processes data analytics,"Business Process Management and Business Intelligence initiatives are commonly seen as separated organizational projects, suffering from lack of coordination, leading to a poor alignment between strategic management and operational business processes execution. Researchers and professionals of information systems have recognized that business processes are the key for identifying the user needs for developing the software that supports those needs. In this case, a process-driven approach could be used to obtain a Data Warehouse model for the Business Intelligence supporting software. This paper presents a process-based approach for identifying an analytical data model using as input a set of interrelated business processes, modeled with Business Process Model and Notation version 2.0, and the corresponding operational data model. The proposed approach ensures the identification of an analytical data model for a Data Warehouse repository, integrating dimensions, facts, relationships and measures, providing useful data analytics perspectives of the data under analysis. © Springer International Publishing Switzerland 2016. A data warehouse model for business processes data analytics Analytical data model; Business intelligence; Business process management; Business process model and notation; Data warehousing; Operational data model Competitive intelligence; Data Analytics; Data warehouses; Enterprise resource management; Information analysis; Analytical data; Business Process; Business process management; Business process model; Operational business; Operational data; Process-based approach; Strategic management; Analytical models",Strategic alignment
1484,Towards model-based strategic sourcing,"Strategic sourcing recognizes that procurement is not just a cost function, but supports the firm’s effort to achieve its long-term objectives. Strategic sourcing has become a critical area of strategic management that is centered on decision-making regarding an organization’s procurement activities such as spend analysis, capability sourcing, supplier selection and evaluation, contract management and relationship management. Many companies face challenges in obtaining the benefits associated with effective strategic sourcing. From an organizational perspective, procurement data management is a core organizational challenge for chief procurement officers (CPOs) for fact-based strategic sourcing decision-making. To address this challenge, we define research objectives to design a holistic view on strategic sourcing orientations and to develop a conceptual basis for enabling centralization of procurement data and enabling the systemic exploration of sourcing alternatives. From a service ecosystem perspective as a holistic view on strategic sourcing, we define a model driven approach to explore sourcing alternatives based on a common language (C.A.R.S) that enables companies to achieve procurement data management and analytics competencies for fact-based decision-making. © Springer International Publishing Switzerland 2015. Towards model-based strategic sourcing Fact-based decision-making; Model based strategic sourcing; Procurement analytics; Procurement data management; Service-dominant conceptual modeling; Strategic sourcing and procurement; Strategic sourcing decision-making Cost functions; Information management; Conceptual model; Model driven approach; Organizational perspectives; Procurement analytics; Relationship management; Research objectives; Strategic management; Strategic sourcing; Decision making",Strategic alignment
1485,Visualization of a product's life cycles in the common information space on the basis of project management methods,"In this article a set of visual models developed by the authors and used in making the concept of industrial enterprises automation is considered. The problems of the integrated information support of the product lifecycle are solved by using the visual analytical methods. The visual modeling technology aimed at the use of flat and 3D-scenes in the formation of design decisions for the development of management and process automation, as well as mathematical modeling and decision support that is built by using the concept of a single information space of an industrial enterprise. By using the methods of visual analytics based on the project-oriented approach to management a set of graph-analytical models used in making the configuration of the integrated information environment of modern industrial enterprise is revealed. In the article, they consider visual models and the possibility of their use in the data, functional and structural modeling, planning and project management of creating integrated information systems. The basis of informational integration of lifecycle stages is a product tree which is produced and most intensively used in the stage of design and technological preparation of production. In this paper presented the experience of application and development of visual analytics models in the process of design and creation of integrated automated systems of enterprise management. Visualization of a product's life cycles in the common information space on the basis of project management methods Common information space; Design management; Product's life-cycle; Project; Project management; Visual analysis Automation; Decision support systems; Information management; Product design; Project management; Visualization; Common information spaces; Design management; Industrial enterprise; Integrated information system; Integrated informations; Project; Project management method; Visual analysis; Life cycle",Strategic alignment
1487,"16th International Conference on Computational Science and Its Applications, ICCSA 2016","The proceedings contain 47 papers. The special focus in this conference is on Information Systems and Technologies. The topics include: Discovering popular events on twitter; user-friendly ontology structure maintenance mechanism targeting Sri Lankan agriculture domain; a software project management problem solved by firefly algorithm; a baseline domain specific language proposal for model-driven web engineering code generation; solving manufacturing cell design problems by using a dolphin echolocation algorithm; cryptanalysis and improvement user authentication scheme for multi-server environment; an operational semantics for android applications; a type system for android applications; a weed colonization inspired algorithm for the weighted set cover problem; software architecture and software quality; multi-hop localization method based on tribes algorithm; developing tasty calorie restricted diets using a differential evolution algorithm; using classification methods to reinforce the impact of social factors on software success; cat swarm optimization with different transfer functions for solving set covering problems; data warehouse model for business processes data analytics; a novel voting mathematical rule classification for image recognition; towards a common data framework for analytical environments; representation and reasoning of topological relations between enclave and exclave regions; geographical communities and virtual communities; analysis spreading patterns generated by model; state of art survey on; a felder and silverman learning styles model based personalization approach to recommend learning objects; modeling software security requirements through functionality rank diagrams; finding divergent executions in asynchronous programs and clustering of wikipedia texts based on keywords. 16th International Conference on Computational Science and Its Applications, ICCSA 2016  ",Value management
1488,Service Science: The Foundations of Service Engineering and Management,"Features coverage of the service systems lifecycle, including service marketing, engineering, delivery, quality control, management, and sustainment Featuring an innovative and holistic approach, Service Science: The Foundations of Service Engineering and Management provides a new perspective of service research and practice. The book presents a practical approach to the service systems lifecycle framework, which aids in understanding and capturing market trends; analyzing the design and engineering of service products and delivery networks; executing service operations; and controlling and managing the service lifecycles for competitive advantage. Utilizing a combined theoretical and practical approach to discuss service science, Service Science: The Foundations of Service Engineering and Management also features: • Case studies to illustrate how the presented theories and design principles are applied in practice to the definitions of fundamental service laws, including service interaction and socio-technical natures • Computational thinking and system modeling such as abstraction, digitalization, holistic perspectives, and analytics • Plentiful examples of service organizations such as automobile after-sale services, global project management networks, and express delivery services • An interdisciplinary emphasis that includes integrated approaches from the fields of mathematics, engineering, industrial engineering, business, operations research, and management science • A detailed analysis of the key concepts and body of knowledge for readers to master the foundations of service management Service Science: The Foundations of Service Engineering and Management is an ideal reference for practitioners in the contemporary service engineering and management field as well as researchers in applied mathematics, statistics, business/management science, operations research, industrial engineering, and economics. The book is also appropriate as a text for upper-undergraduate and graduate-level courses in industrial engineering, operations research, and management science as well as MBA students studying service management. © 2014 John Wiley & Sons, Inc. All rights reserved. Service Science: The Foundations of Service Engineering and Management  Commerce; Competition; Computation theory; Engineering education; Foundations; Industrial management; Industrial research; Life cycle; Operations research; Product design; Quality control; Students; Applied mathematics; Competitive advantage; Computational thinkings; Global project managements; Graduate level course; Holistic perspectives; Service interaction; Service organizations; Project management",Strategic alignment
1489,Snakes or Ladders? Evaluating a LibGuides Pilot at UCD Library,"Online subject guides are commonly used by libraries to provide information support to students. LibGuides (a cloud-based commercial product launched in 2007) represents one of the latest incarnations of the traditional subject guide or portal, and are widely used across American academic libraries. In Ireland however, library subject guides of entirely local design and hosted on a local web server still dominate.This paper outlines the project management process involved in implementing a LibGuides pilot at University College Dublin Library, including the planning, design and implementation of a new range of subject-related guides. The pilot nature of the project necessitated a strong focus on evaluation, particularly in assessing the effectiveness and suitability of LibGuides as a platform for delivering information literacy support, both from an administrative and end-user perspective. A two-stranded approach was used in this review process, incorporating quantitative web statistics and analytics alongside qualitative feedback from students, academic staff and Library staff.Feedback that was gathered suggested that the LibGuides subject guides were generally viewed very positively by both staff and students. Notwithstanding this, awareness (as indicated through usage statistics) remained moderate during the pilot, pointing to the importance of the visibility, positioning and promotion of guides. © 2014 Elsevier Inc.. Snakes or Ladders? Evaluating a LibGuides Pilot at UCD Library Information literacy; LibGuides; Subject guides ",Governance
1490,Factors affecting team motivation: A survey of Finnish software engineers,"Motivation in software engineering is a complex topic. Cultural background is reported to be one of the factors moderating software engineers' motivation and project outcome. The authors conducted a survey with 36 software engineers from Finland to explore 1) the relationship between team motivation and project outcome, 2) factors that motivate Finnish engineers, and 3) how these motivational factors are related. The authors compare Finnish motivational factors with those identified in prior research. In addition they build a prediction model to identify the best indicators of team motivation for Finnish software engineers. Their results show that teamwork is the only culturally independent motivational factor. Having 1) a project manager with a clear vision (project manager vision) and 2) a project manager given full authority to manage the project (project manager authority) are also significant motivational factors among Finnish engineers. There are significant associations between some factors, e.g., customer involvement and staff appreciation. While these factors partially explain motivation in software engineering, cultural differences also play an active role in explaining team motivation. Their questionnaire needs to be updated to enable measurement of motivation for modern development practices such as agile development. Copyright © 2015, IGI Global. Factors affecting team motivation: A survey of Finnish software engineers Culture; Motivational Factors; Software Development Team Motivation; Software Project Outcome; Survey Cell culture; Engineers; Managers; Motivation; Predictive analytics; Project management; Software design; Surveying; Surveys; Agile development; Cultural backgrounds; Cultural difference; Modern development; Motivational Factors; Prediction model; Software development teams; Software project; Professional aspects",Risk management
1491,Data assessment model for strategic management,"A data-oriented culture enables new ways of building competitive advantage, but also requires new type of managerial skills. Despite of fast-growing attention to data assets, there are very few managerial tools for the purpose. In this paper, we propose a new data assessment model for strategic management. The model makes the organization's analytical capabilities and development needs transparent. The model consists of three phases and uses an easily communicated four-field model for mapping existing data and discovering new possibilities. The model helps to identify what datasets the organization could use in analytics and to assess them according to their strategic importance. Therefore, organization's development resources can be targeted effectively. The relevancy of the model was discovered during empirical studies with regional organizations on business analytics maturity. Data assessment model for strategic management Business analytics; Business intelligence; Strategic management; Structured data; Unstructured data Competition; Competitive intelligence; Management science; Managers; Strategic planning; Business analytics; Competitive advantage; Development needs; Development resources; Managerial skills; Strategic management; Structured data; Unstructured data; Information management",Monitoring and control
1493,Process analytics approach for R&D project selection,"R&D project selection plays an important role in government funding agencies, as allocation of billions of dollars among the proposals deemed highly influential and contributive solely depend on it. Efficacious assignment of reviewers is one of the most critical processes that controls the quality of the entire project selection and also has a serious implication on business profit. Current methods that focus on workflow automation are more efficient than manual assignment; however, they are not effective, as they fail to consider the real insight of core tasks. Other decision models that analyze core tasks are effective but inefficient when handling large amounts of submissions, and they suffer from irrelevant assignment. Furthermore, they largely ignore real deep insight of back-end data such as quality of the reviewers (e.g., quality and citation impact of their produced research) and the effect of social relationships in project selection processes that are essential for identifying reviewers for interdisciplinary proposal evaluation. In light of these deficiencies, this research proposes a novel hybrid process analytics approach to decompose the complex reviewer assignment process into manageable subprocesses and applies data-driven decision models cum process analytics systematically from a triangular perspective via the research analytics framework to achieve high operational efficiencies and high-quality assignment. It also analyzes big data from scientific databases and generates visualized decision-ready information to support effective decision making. The proposed approach has been implemented to aid the project selection process of the largest funding agency in China and has been tested. The test results show that the proposed approach has the potential to add great benefits, including cost saving, improved effectiveness, and increased business value. © 2014 ACM. Process analytics approach for R&D project selection Big data; Process analytics; Research analytics; Reviewer assignment Big data; Decision making; Quality control; Data driven decision; Government funding; Operational efficiencies; Process analytics; Project selection; Reviewer assignment; Scientific database; Social relationships; Advanced Analytics",Strategic alignment
1495,ICEIS 2016 - Proceedings of the 18th International Conference on Enterprise Information Systems,"The proceedings contain 138 papers. The topics discussed include: an allegory on the role of the action researcher to enable user engagement and change management in the early phases of information systems implementation; the data-driven factory - leveraging big industrial data for agile, learning and human-centric manufacturing; efficient self-similarity range wide-joins fostering near-duplicate image detection in emergency scenarios; assessment of factors influencing business process harmonization - a case study in an industrial company; semantic integration between context-awareness and domain data to bring personalized queries to legacy relational databases; on the support of a similarity-enabled relational database management system in civilian crisis situations; the concept of project management platform using BI and big data technology; a multiagent-based simulation of the infection of the macrophage by trypanosoma cruzi in the acute phase of Chagas' disease: influence of the initial inoculum and protozoan escape factor; decision guidance approach to power network analysis and optimization; continuous improvement of proactive event-driven decision making through sensor-enabled feedback (SEF); factors affecting university instructors' continuance intention to use learning management systems: the blackboard system case; and sales forecasting as a service - a cloud based pluggable e-commerce data analytics service. ICEIS 2016 - Proceedings of the 18th International Conference on Enterprise Information Systems  ",Financial management
1496,The gamification as a resourceful tool to improve work performance,"Gamification is the use of game and behavioral analytics, game mechanics, interactive media, and social networking to improve work performance and transform a business by engaging and training users to solve problems. Gaming techniques and strategies have been used in areas such as employee training programs, financial services websites, customer relationship management, project management, business intelligence, market research, online shopping, and education. The level of sophistication involved in the technology applied to these needs varies greatly. This chapter will focus on the more technologically sophisticated methods applied to implementing gaming solutions in work situations. The use of gamification technologies that extends the video gamer lifestyle and skill set to engage and build loyalty with customers and employees will be discussed. Cases of practices in applying gamification to provide solutions for businesses will be examined. This chapter will conclude with imitations, implications, managerial caveats, and assessment of gamification. © Springer International Publishing Switzerland 2015. The gamification as a resourceful tool to improve work performance Engagement; Gamification; Motivation ",Strategic alignment
1497,A framework for model-driven acquisition and analytics of visual data using UAVs for automated construction progress monitoring,"Automated assessment of work-in-progress using large collections of site images and 4D BIM has potential to significantly improve the efficiency of construction project controls. Nevertheless, today's manual procedures for taking site photos do not support the desired frequency or completeness for automated progress monitoring. While the usage of Unmanned Aerial Vehicles for acquisition of site images has gained popularity, their application for addressing issues associated with image-based progress monitoring and particularly leveraging 4D BIM for steering the data collection process has not been investigated before. By presenting examples from two case studies conducted on real-world construction projects, this paper suggests a framework for model-driven acquisition and analytics of progress images. In particular, the potential of spatial (geometry, appearance, and interconnectivity) and temporal information in 4D BIM for autonomous data acquisition and analytics that guarantees completeness and accuracy for both as-built modeling and monitoring work-in-progress at the schedule task-level is discussed. © 2015 ASCE. A framework for model-driven acquisition and analytics of visual data using UAVs for automated construction progress monitoring  Antennas; Architectural design; Automation; Data acquisition; Project management; Automated assessment; Automated construction; Case-studies; Construction projects; Data collection process; Interconnectivity; Temporal information; Work in progress; Image enhancement",Strategic alignment
1500,Decision support with big data: A case study in the hospitality industry,"The term 'big data' is used to describe data that are beyond the capabilities of an organization to store, analyze and use for accurate and timely decision making. They have alternately been described in terms of characteristics of volume, velocity, variety, veracity. We propose another characteristic of volatility for big data that should be considered in their use for decision making. We utilize a case study approach with a mid-sized company in the hospitality industry to elucidate challenges that an organization faces in developing a big data strategy and highlight research needed in this domain. Challenges identified were technical (inconsistent and unstandardized data, implementation and use of new analytics platforms, obtaining a global view of data, visualization of data, integrating mobile data), organizational (finding people with the right skills, users' desire for customization), and strategic management (finding return on investment in big data, alignment of business and analytics strategies, leadership of analytics initiatives and thought). Decision questions to be addressed in using big data for a marketing decision are illustrated. We demonstrate the use of the decision framework and illustrate challenges identified in the case study with sentiment analysis of Twitter data. © 2014 The authors and IOS Press. All rights reserved. Decision support with big data: A case study in the hospitality industry Analytics; Big data; Decision support; Sentiment analysis; Social media; Twitter Data mining; Data visualization; Decision making; Decision support systems; Research; Social networking (online); Behavioral research; Big data; Data mining; Data visualization; Decision making; Decision support systems; Social networking (online); Analytics; Decision supports; Sentiment analysis; Social media; Twitter; Analytic; Case study approach; Case-studies; Decisions makings; Hospitality industry; Volume velocities; Big data; Sentiment analysis",Strategic alignment
1503,A Dashboard to Support Management of Business Analytics Capabilities,"Business analytics (BA) systems create value and provide competitive advantage for organisations. They involve technology and data infrastructure, BA capabilities and business processes that provide useful insights and support decision-making. To provide value and competitive advantage, BA capabilities should be valuable, rare and inimitable, and have organisational support (VRIO). In this paper, we develop and evaluate a prototype dashboard for the VRIO assessment of BA capabilities. The dashboard is intended to support the strategic management of BA capabilities. We discuss implications of the prototype dashboard for researchers and practitioners and suggest directions for future research. © 2015 Taylor & Francis. A Dashboard to Support Management of Business Analytics Capabilities business analytics; design science; information dashboard design; resource-based view Competition; Decision making; Business analytics; Business Process; Competitive advantage; Data infrastructure; Design science; Organisational; Resource-based view; Strategic management; Advanced Analytics",Strategic alignment
1504,Towards defence strategic data planning,"Defence generates and receives vast quantities of data in relation to the specification, development and intended operation of its capabilities and related decision processes. As an organisation, Defence has made good progress in the collection and availability of these types of data; however, the growing pace of technological advancement and the rapid evolution of the spectrum of potential adversaries mean that Defence must become even more effective and efficient in its data analytics to ensure agile decision making at all levels of the organisation. Achieving this objective requires focused strategic data planning, rigorous data design, speedy and accurate data collection, and effective data validation, processing, and management. Current Defence data management practices in Australia do not, for example, provide a unified asset identification number, nor a unified asset naming convention, leading to confusion and ambiguity in the provision of timely and accurate advice to decision makers. This is in part due to the lack of rigorous definitions, data structures and effective data management mechanisms and policies, making universal data integration difficult. This also increases the risk of inconsistencies between Defence's many data repositories. Based on reviewing data management practices of Defence organisation internationally, the authors argue that the Australian Defence Organisation needs an improved strategic data planning mechanism in order to develop a more holistic and effective approach to managing its capability-related data. We propose a framework based on review of the literature and best practices internationally, as well as our experience in analytical support of Force Design and capability-related decision processes, albeit being mindful of the evolving structure of Defence as the implementation of First Principles Review is taking shape. The value proposition of the framework is four-fold: (1) it proposes an iterative strategic data planning process that helps to improve the effectiveness and efficiency of data management in support of effective decision making, (2) it provides a set of guidelines for developing a strategic data plan, helping to facilitate and streamline data management across the organisation, (3) it provides guidelines for data design, collection, and management, as well as resource planning, and (4) it helps develop consistent capture of 'quality' information and other critical data required for Defence capability planning. Our framework is intended to be useful to strategic data planners and data managers working at different levels of Defence, and will also better support robust analytics for capability planning. This can, in turn, help reduce costs in data management but, more importantly, enhance Defence's capability development and trade-off decision processes. © 2020 Proceedings - 21st International Congress on Modelling and Simulation, MODSIM 2015. All rights reserved. Towards defence strategic data planning Capability management; Data planning; Information management; Strategic management Data acquisition; Data Analytics; Data integration; Decision making; Economic and social effects; Operations research; Planning; Agile decision makings; Asset identification; Capability development; Effective approaches; Effectiveness and efficiencies; Management mechanisms; Strategic management; Technological advancement; Information management",Strategic alignment
1508,Influence of domain and technology upon scope creep in software projects,Project management strategies play an influencing role in accomplishment of project success. A successful project alone can attain total customer satisfaction and withholding the industry in the market. Scope creep is deemed to be one of the several factors which influence the success of the project. Our research has proven that there are various factors which make an impact on scope creep. This paper put forth a case study carried out in leading software industries in order to realize the impact of domain and technology on scope creep. The investigation is carried out on various projects which are developed either in retail or pharmaceuticals domain. Retail projects are developed using Java and J2EE and the pharmaceutical projects are developed in Data Warehousing and Business Intelligence technology. Knowledge of impact of domain and technology upon scope creep certainly ensures one baseline their requirement specifications such that it reduces the scope creep associated overheads. It further accelerates quality of projects and productivity of the company. © 2014 IEEE. Influence of domain and technology upon scope creep in software projects Project Management Process; Scope Creep; Software Development Life Cycle; Software Engineering; Software Quality Computer software selection and evaluation; Creep; Customer satisfaction; Data warehouses; Life cycle; Sales; Software design; Software engineering; Management strategies; Pharmaceutical projects; Project management process; Requirement specification; Software development life cycle; Software industry; Software Quality; Total customer satisfactions; Project management,Value management
1509,The Secret of my Success: An exploratory study of Business Intelligence management in the Norwegian Industry,"The purpose of this paper is the exploration of the management of Business Intelligence (BI) solutions in the Norwegian industry. BI aims to improve data analysis and enhance business performance. Nonetheless, academics and practitioners note that BI is complex and difficult to manage. We interviewed BI managers in 5 companies who had been awarded the Norwegian Computer Society's BI prize, based on the following research question: What can we learn from the successful BI implementations and management in award-winning Norwegian companies? We surmise two findings. First, building upon concepts derived from the Digital Infrastructure Theory we find that while the Norwegian industry still has a traditional, complex BI architecture, it is scalable in the sense that they can add or remove elements, or even scrap the whole BI solution. The companies demonstrate innovation and adoption through their use of dashboards and real-time data. In light of these findings we propose a future research agenda for BI. Second, we offer three lessons for managers of BI in organisations. © 2015 The Authors. Published by Elsevier B.V. The Secret of my Success: An exploratory study of Business Intelligence management in the Norwegian Industry Business Intelligence; Digital Infrastructure; exploratory case study; Norwegian Industry Competitive intelligence; Information analysis; Information systems; Management science; Managers; Project management; Business performance; Digital infrastructures; Exploratory case studies; Exploratory studies; Real-time data; Research agenda; Research questions; Information management",Governance
1510,The use of data mining for strategic management: A case study on mining association rules in student information system; [Upotreba rudarenja podataka u strateškom menadžmentu: Analiza slučaja upotrebe pravila pridruživanja rudarenja podataka u informacijskom sustavu podataka o studentima],"In today’s competitive conditions changes in business environment and business structures make strategic management an effective form of management for business and organizations. Strategic management is a current management strategy that requires setting of the appropriate strategies, plans and applications and putting them into action in order to reach the aims and goals of organizations. The process of strategic management involves setting the company’s vision, mission and objectives, determining the competitive position, and the evaluation of results obtained by strategy selection, development and application. In the application of activities related to the strategic management of business processes, the discipline of data mining, which can be defined as the process of extracting useful and meaningful patterns from large volumes of data, emerges as a viable method. In this study, strategic management and data mining disciplines and their basic concepts and applications are introduced. Apart from that, data mining methods in the context of strategic management are taken into consideration. In addition, a sample case study about the use of association rule mining algorithms in student information systems data will be presented. © 2016, FACTEACHEREDUCATION. All rights reserved. The use of data mining for strategic management: A case study on mining association rules in student information system; [Upotreba rudarenja podataka you strateškom menadžmentu: Analiza slučaja upotrebe pravila pridruživanja rudarenja podataka you informacijskom sustavu podataka o studentima] Business intelligence; Data mining; Educational data; Knowledge discovery ",Strategic alignment
1511,"Improving health analytic process through project, communication and knowledge management","A health analytics process model is necessary to carryout projects in a manageable and a repeatable way without having to rely on the individual skills and experiences. This study is one of the first steps in developing a comprehensive process model for health analytics. Literature suggests that successful IT systems require project management, communication management and knowledge management dimensions. This paper explores the inter-relationship among these supporting dimensions on health analytic project performance based on apriori determinability of project requirements as per their difficulty and clarity. The data were collected by one author working as an intern in a health analytic department of a hospital. The findings from this case study will be useful in developing and evaluating a well-defined process model as well as a theoretical model to represent health analytics process model usability. Improving health analytic process through project, communication and knowledge management Communication; Health Analytics; Knowledge management; Process model; Project management Communication; Health; Information systems; Information use; Project management; Apriori; Communication management; Individual skills; Inter-relationships; IT system; Process Modeling; Project performance; Theoretical modeling; Knowledge management",Strategic alignment
1512,Predicting business opportunities and/or threats – Business intelligence in the service of corporate security (Empirical analysis of the usage in the economy of republic of Croatia); [Predikcija poslovnih prilika i/ili opasnosti – Business intelligence u funkciji korporativne sigurnosti (Empirijska analiza primjene u gospodarstvu u republici Hrvatskoj)],"Predicting business opportunity sand risks is based on existing knowledge about them. In practice, this knowledge comes from collecting business information from the business environment, within the framework of something that is known as business intelligence (BI). Prediction of opportunities and risks is inherent in business of successful company. Corporate security as a framework for ensuring the safety of business is based on timely and accurate information that becomes foreknowledge of threats, for which prediction companies are using different tools, and business intelligence (BI) is a proven tool. The aim of using BI in company is well-timed detection of opportunities and threats in business environment that management could react in time. Implementation of BI activities in Croatia significantly differs from the world average and in applying of BI activities companies are primarily focused on business dimension of business environment, while ignoring the political and security dimensions from which threats are generated, as shown by the survey results conducted from October 2010 till April 2011 with online survey method on a sample of 1,000 largest Croatian companies by revenue. © 2014, Croatian Anthropological Society. All rights reserved. Predicting business opportunities and/or threats – Business intelligence in the service of corporate security (Empirical analysis of the usage in the economy of republic of Croatia); [Predikcija poslovnih prilika i/ili opasnosti – Business intelligence you funkciji korporativne sigurnosti (Empirijska analiza primjene you gospodarstvu you republici Hrvatskoj)] Business intelligence; Corporate security; Crisis management; Croatian economy; Strategic management ",Governance
1514,"8th International Symposium on Project Approaches in Engineering Education, PAEE 2016 and 14th Active Learning in Engineering Education Workshop, ALE 2016","The proceedings contain 78 papers. The special focus in this conference is on Project Approaches in Engineering Education and Active Learning in Engineering Education. The topics include: A text analytics evaluation of a first-year engineering project-based unit; the representation of engineering education as a social media topic on twitter; project based learning: an approach to one house automation design; simulating a business competitive environment in a discipline of the chemical engineering course; characterising the Australian engineering workforce and engineering graduate occupational outcomes using national census data; using the flipped classroom approach in engineering courses to improve student motivation and learning outcomes; applying problem-based learning in laboratory education; collaborative learning experience of students in distance education; engendering diversity in engineering education and other stem areas; project management in engineering education; active learning with the use of MOOCS at chalmers university of technology; innovative experiences and proposals in engineering education for sustainability; the construction of an analog vocoder as a hands-on introductory course in electrical engineering; team based serious games and useful mathematics in engineering education; sustainability in three-cycle engineering education based on CDIO syllabus; a practical approach with project-based learning; framework for implementing a sustainability curriculum in an engineering technology program and implementing a vehicle dynamics curriculum with significant active learning components. 8th International Symposium on Project Approaches in Engineering Education, PAEE 2016 and 14th Active Learning in Engineering Education Workshop, ALE 2016  ",Monitoring and control
1516,A perspective of federal procurement certification in small business intelligence maturity,"Regardless of size, improving performance and productivity by measuring decision-making attributes are priority for business success. The groups of disadvantaged small businesses are no exception. This paper aims to draw attention to the certain characteristics of these businesses to contribute some knowledge to the BI field of research. For example, the federal certification process offers a rich place to explore intellectual capital disclosure effects on initial BI development. This perspective can be used by organizations to validate their ideas about federal certification, BI maturity, and advancement to desired maturity vantage points. The two hypothesis-generating questions asked are: Does federal certification offer BI strategies for disadvantaged businesses? Can the intellectual capital requirements be leveraged? The study was based primarily on a critical analysis of the parallelism of federal procurement digitization and certification; supporting literature; and the ongoing dialog within the Minnesota construction sector. The paper is organized with an introduction into broadening the BI maturity research to be more inclusive. Next, a historical perspective of procurement certification and business intelligence maturity are described and linked to the value of increasing BI maturity participation. Subsequently, domain support, sample state scenario and discussion points are provided to further illustrate the value proposition. © 2014, International Academy of Business and Economics. All rights reserved. A perspective of federal procurement certification in small business intelligence maturity Business analytics; Business intelligence; Certification; Data mining; Intellectual capital; Knowledge discovery; Maturity models; Strategic management ",Strategic alignment
1518,The JIRA repository dataset: Understanding social aspects of software development,"Issue tracking systems store valuable data for testing hypotheses concerning maintenance, building statistical prediction models and recently investigating developers ""affectiveness"". In particular, the Jira Issue Tracking System is a proprietary tracking system that has gained a tremendous popularity in the last years and offers unique features like the project management system and the Jira agile kanban board. This paper presents a dataset extracted from the Jira ITS of four popular open source ecosystems (as well as the tools and infrastructure used for extraction) the Apache Software Foundation, Spring, JBoss and CodeHaus communities. Our dataset hosts more than 1K projects, containing more than 700K issue reports and more than 2 million issue comments. Using this data, we have been able to deeply study the communication process among developers, and how this aspect affects the development process. Further-more, comments posted by developers contain not only technical information, but also valuable information about sentiments and emotions. Since sentiment analysis and human aspects in software engineering are gaining more and more importance in the last years, with this repository we would like to encourage further studies in this direction. © 2015 ACM. The JIRA repository dataset: Understanding social aspects of software development Affective Analysis; Issue Report; Mining software repository Open source software; Predictive analytics; Project management; Sentiment analysis; Social aspects; Software design; Tracking (position); Affective Analysis; Apache software foundations; Communication process; Issue Report; Mining software repositories; Project management system; Statistical prediction model; Technical information; Open systems",Risk management
1519,Business intelligent system to make management decision in project management,"This paper discusses the concept and components of business intelligence, its developmental trend, and the role of business intelligence system in managing a project. Provide a structure model of business intelligence system to help the decision-maker in making a decision. Extract information to face the competition and to anticipate the changing in business environment also in making decision for strategic business. Business intelligence will continuously develop along with the development of information technology. The future development or trend will mostly be influenced by the development of social network, cloud technology, and mobile devices. © 2015 International Information Institute. Business intelligent system to make management decision in project management Business intelligence; Decision making; Information technology; Project ",Value management
1520,Project-based learning: Methodology and assessment learning technologies and assessment criteria,"This paper uses a project-based learning methodology in higher education to analyse its relation to a theoretical framework of competency. Based on this analysis, we propose a set of technological tools to support the development of competency at the university level as well as a set of indicators to systematize the assessment process. Finally, indicators are related to data that can be obtained from these technological tools. This is the basis for additional work on learning analytics that is used to support the assessment of a project-based learning approach. © Springer International Publishing Switzerland 2015. Project-based learning: Methodology and assessment learning technologies and assessment criteria Cloud technology; Competence-based learning; Competency assessment; Learning technology; Project management; Project-based learning Project management; Cloud technologies; Competence-based learning; Competency assessment; Learning technology; Project based learning; Engineering education",Monitoring and control
1521,The Impact of Business Intelligence on the Quality of Decision Making - A Mediation Model,"Business Intelligence (BI) systems have been a top priority of CIOs for a decade, but little is known about how to successfully manage those systems beyond the implementation phase. This paper investigates the direct and indirect effects of BI management quality on the quality of managerial decision making using PLS analysis of survey responses of senior IT managers in Australia. The results confirm this overall relationship (total effect), but also reveal mediating effects of data/information quality and BI solution scope. The study contributes to both academia and industry by providing first time evidence of direct and indirect determinants of managerial decision support improvements related to BI solutions scope and active management of BI. © 2015 The Authors. Published by Elsevier B.V. The Impact of Business Intelligence on the Quality of Decision Making - A Mediation Model BI benefits; BI management; Business Intelligence; information quality; quality of decision making Competitive intelligence; Decision support systems; Information analysis; Information management; Information systems; Management science; Managers; Project management; Active management; Business intelligence systems; Indirect effects; Information quality; Management quality; Managerial decision; Managerial decision making; Mediating effect; Decision making",Value management
1522,"Implementation of a web based universal exchange and inference language for medicine: Sparse data, probabilities and inference in data mining of clinical data repositories","We extend Q-UEL, our universal exchange language for interoperability and inference in healthcare and biomedicine, to the more traditional fields of public health surveys. These are the type associated with screening, epidemiological and cross-sectional studies, and cohort studies in some cases similar to clinical trials. There is the challenge that there is some degree of split between frequentist notions of probability as (a) classical measures based only on the idea of counting and proportion and on classical biostatistics as used in the above conservative disciplines, and (b) more subjectivist notions of uncertainty, belief, reliability, or confidence often used in automated inference and decision support systems. Samples in the above kind of public health survey are typically small compared with our earlier ""Big Data"" mining efforts. An issue addressed here is how much impact on decisions should sparse data have. We describe a new Q-UEL compatible toolkit including a data analytics application DiracMiner that also delivers more standard biostatistical results, DiracBuilder that uses its output to build Hyperbolic Dirac Nets (HDN) for decision support, and HDNcoherer that ensures that probabilities are mutually consistent. Use is exemplified by participating in a real word health-screening project, and also by deployment in a industrial platform called the BioIngine, a cognitive computing platform for health management. © 2015 Elsevier Ltd. Implementation of a web based universal exchange and inference language for medicine: Sparse data, probabilities and inference in data mining of clinical data repositories Bayes; Cognitive computing; Dirac notation; Electronic health record; Probability theory; Public health reporting; Universal exchange language; Watson; Zeta function Algorithms; Bayes Theorem; Data Mining; Databases, Factual; Decision Support Systems, Clinical; Electronic Health Records; Female; Humans; Internet; Male; Medical Informatics; Probability; Public Health; Software; Artificial intelligence; Computation theory; Computational linguistics; Data mining; Decision support systems; Diagnosis; Electronic document exchange; Health; Medicine; Probability; Project management; Public health; Social networking (online); Surveys; Bayes; Cognitive Computing; Dirac notation; Electronic health record; Probability theory; Universal exchange language; Watson; Zeta function; Article; association constant; biostatistics; clinical data repository; computer program; data mining; decision support system; health care management; human; language; mass screening; maximum likelihood method; medical informatics; medical information; partition coefficient; priority journal; probability; quantum mechanics; reliability; web browser; algorithm; Bayes theorem; clinical decision support system; data mining; electronic health record; factual database; female; Internet; male; procedures; public health; software; Big data",Financial management
1524,Using GIS analytics and social preference data to evaluate utility-scale solar power site suitability,"Determining socially acceptable and economically viable locations for utility-scale solar projects is a costly process that depends on many technical, economic, environmental and social factors. This paper presents a GIS-based multi-criteria solar project siting study conducted in the southwestern United States with a unique social preference component. Proximity raster layers were derived from features including roads, power lines, and rivers then overlain with 10×10m raster terrain datasets including slope and potential irradiance to produce a high resolution map showing solar energy potential from ""poor"" to ""excellent"" for high potential counties across the southwestern United States. Similar maps were produced by adding social acceptance data collected from a series of surveys showing the potential public resistance to development that can be expected in areas of high solar energy suitability. Applying social preferences to the model significantly reduced the amount of suitable area in each of the selected study areas. The methods demonstrated are expected to help reduce time, money, and resources currently allocated toward finding and assessing areas of high solar power suitability. © 2015 Elsevier Ltd. Using GIS analytics and social preference data to evaluate utility-scale solar power site suitability GIS; Photovoltaic electricity; Public attitudes; Site suitability; Solar energy United States; Geographic information systems; Solar energy; Economically viable; High potential; High resolution; Photovoltaic electricities; Public attitudes; Site suitability; Social acceptance; Social preference; cost analysis; data acquisition; data set; GIS; irradiance; mapping method; preference behavior; project management; raster; solar power; Solar power generation",Value management
1525,Human-Computer Interaction in Electronic Medical Records: From the Perspectives of Physicians and Data Scientists,This study investigated the most common challenges of human computer interaction (HCI) while using electronic medical records (EMR) based on the experience of a large Russian medical research center. Inadequate HCI may have a dramatic effect on the quality of data stored in the electronic medical system. We identified the most common classes of mistakes that emerge because of poor HCI design in EMR. Possible consequences of such mistakes are discussed from clinical and data science perspectives. Integration of specially designed clinical decision support system (CDSS) is considered as a possible way to improve HCI with subsequent increase of the EMR quality. This study is a part of a larger project to develop complex CDSS on cardiovascular disorders for medical research centers. Human-Computer Interaction in Electronic Medical Records: From the Perspectives of Physicians and Data Scientists clinical disicion support systems; electronic health records; healthcare quality; human-coumputer interaction; medical data analysis Artificial intelligence; Decision support systems; Health; Information management; Information systems; Medical computing; Project management; Electronic health record; Healthcare quality; human-coumputer interaction; Medical data analysis; Support systems; Human computer interaction,Risk management
1526,Business Intelligence Implementations: A Multi-Project Classification,"Recently, businesses are investing vast sums of money in Business Intelligence (BI) implementation efforts. However, BI implementation failures are rampant. Although many BI projects are implemented along with source system projects in multi-project environments, few studies have looked at BI implementation projects from a multi-project perspective. This study proposes a multi-project classification framework for how BI implementation in a multi-project environment can impact implementation complexity and outcome. It makes an important contribution to both practice and theory in offering a new classification system for BI implementations as well as in delineating factors that impact BI multi-project complexity and BI implementation outcomes. Business Intelligence Implementations: A Multi-Project Classification BI; Business Intelligence; Business Intelligence Implementation Classification; Implementation; IS Projects; Multi-project; Multi-Project Complexity; Outcome; Source System; Success Bismuth; Competitive intelligence; Information systems; Implementation; IS Projects; Multi-projects; Outcome; Source systems; Success; Project management",Value management
1527,Big data team process methodologies: A literature review and the identification of key factors for a project's success,"This paper reports on our review of published research relating to how teams work together to execute Big Data projects. Our findings suggest that there is no agreed upon standard for executing these projects but that there is a growing research focus in this area and that an improved process methodology would be useful. In addition, our synthesis also provides useful suggestions to help practitioners execute their projects, specifically our identified list of 33 important success factors for executing Big Data efforts, which are grouped by our six identified characteristics of a mature Big Data organization. © 2016 IEEE. Big data team process methodologies: A literature review and the identification of key factors for a project's success Analytics Process; Big Data; Data Science; Process Methodology; Project Management Project management; Data organization; Data Science; Improved process; Literature reviews; Research focus; Success factors; Team process; Big data",Value management
1529,Reducing data dimensions for systems engineering and risk management of transportation corridors,"The agencies responsible for transportation corridors tend to hold large volumes of data that can be relevant for both operations and planning. Meanwhile, it is a challenge for these agencies to prioritize their investments addressing risk, benefits, and costs. The agencies recognize an opportunity to improve project selection and programming with a centralized database of performance measures that will aid a consistent application of evaluation metrics. To support the identification, planning, and selection of highway transportation projects, this research has used a data structure known as dynamic segmentation for cross-referencing heterogeneous data sources including projects, traffic, safety, bridge and pavement conditions, etc. The result is a multiscale method for agencies to utilize big-data analytics and multicriteria decision analysis to assemble evidence for project selection and prioritization. The paper describes an approach to (1) visualize multiple attributes along linearized road corridors to identify future projects, minimize conflicts with current projects, and identify potential project synergies and (2) prioritize projects to the Top-20 with multiple performance factors. The methods are demonstrated at several geographic scales. The effort supports a fast, repeatable, and evidence-driven prioritization of projects and provides a complement to the use of electronic map data that has been overwhelming the available computing resources. © 2014 IEEE. Reducing data dimensions for systems engineering and risk management of transportation corridors Big data; Business process models; Corridor trace analysis; Decision analysis; Project selection; Risk management; Systems engineering; Transportation planning Big data; Cybernetics; Decision making; Decision theory; Highway administration; Risk assessment; Risk management; Systems engineering; Trace analysis; Transportation; Business process model; Centralized data-base; Heterogeneous data sources; Multi-criteria decision analysis; Project selection; Transportation corridors; Transportation planning; Transportation projects; Information management",Strategic alignment
1530,Visualization of large data sets for project planning and prioritization on transportation corridors,"Transportation planners address mobility, accessibility, safety, environment, and economic development on thousands of miles of multimodal corridors. Improvement projects are periodically implemented with input from diverse public stakeholders. Various performance metrics and data visualization are used to identify and prioritize locations of the projects. An agency has needs for business processes to identify opportunities and threats, potential cost savings, and programming conflicts across hundreds of acquisition projects in transportation systems. This paper describes supporting data, analytics, and prioritization to aid the planning of improvement projects. The approach includes design of an algorithm to extract relevant data from multiple databases to a single file that can be imported to a data visualization software application, Tableau. Tableau is used to provide an interface for visualization of a six-year improvement program of several billion dollars. The results help transportation planners at several levels of the agency to make data-driven decisions in project planning. Customizing Tableau data interfaces enables the visualization and prioritization of projects considering metrics such as project status, bridge condition, pavement condition, crashes per vehicle-miles traveled, etc. Prior to this work, the agency relied on a program in which users query the status of individual road segments. The lessons learned include how to approach data security while providing flexibility and utility to a variety of users, how to ensure that the interfaces are robust to missing or inaccurate data, and how to account for potential cognitive bias of analysts and decision makers. © 2015 IEEE. Visualization of large data sets for project planning and prioritization on transportation corridors Corridor trace analysis; Project prioritization; Risk management; Systems analysis; Transportation planning Accidents; Application programs; Bridges; Decision making; Planning; Risk assessment; Risk management; Security of data; Systems analysis; Visualization; Data driven decision; Project prioritization; Transportation corridors; Transportation planners; Transportation planning; Transportation system; Vehicle-miles traveled; Visualization software application; Data visualization",Risk management
1532,Detection of Road Accident Accumulation Zones with a Visual Analytics Approach,"Nowadays, road accidents are a major public health problem, which increase is forecasted if road safety is not treated properly, dying about 1.2 million people every year around the globe. In 2012, Portugal recorded 573 fatalities in road accidents, on site, revealing the largest decreasing of the European Union for 2011, along with Denmark. Beyond the impact caused by fatalities, it was calculated that the economic and social costs of road accidents weighted about 1.17% of the Portuguese gross domestic product in 2010.Visual Analytics allows the combination of data analysis techniques with interactive visualizations, which facilitates the process of knowledge discovery in sets of large and complex data, while the Geovisual Analytics facilitates the exploration of space-time data through maps with different variables and parameters that are under analysis. In Portugal, the identification of road accident accumulation zones, in this work named black spots, has been restricted to annual fixed windows. In this work, it is presented a dynamic approach based on Visual Analytics techniques that is able to identify the displacement of black spots on sliding windows of 12 months. Moreover, with the use of different parameterizations in the formula usually used to detect black spots, it is possible to identify zones that are almost becoming black spots. Through the proposed visualizations, the study and identification of countermeasures to this social and economic problem can gain new grounds and thus the decision-making process is supported and improved. © 2015 Published by Elsevier B.V. Detection of Road Accident Accumulation Zones with a Visual Analytics Approach black spots; Road traffic accidents; visual analytics Accidents; Behavioral research; Decision making; Economic analysis; Economic and social effects; Health; Highway administration; Information management; Information systems; Motor transportation; Project management; Roads and streets; Transportation; Visualization; Black spot; Data analysis techniques; Decision making process; Geovisual analytics; Gross domestic products; Interactive visualizations; Road traffic accidents; Visual analytics; Highway accidents",Value management
1533,ERP Simulator: Examining Competitive Supply Chain Team Dynamics,"This chapter leverages theory from the strategic management and organizational behavior literatures to examine issues related to supply chain competition. The information-processing view is also considered as information acquisition and dissemination are crucial to the performance of supply chains. In so doing, we develop a SAP enterprise resource planning (ERP) software simulation classroom exercise that enables SCM scholars to study dynamic team behavior. In addition, a case example illustrates how a supply chain team used SAP information to complete a Plan for Every Part (PFEP) to lower prices and lead times, thereby enabling market share growth in response to rival actions. Both the SAM exercise and case example illustrate the importance of business intelligence in team decision-making performance. © Oxford University Press 2015. All rights reserved. ERP Simulator: Examining Competitive Supply Chain Team Dynamics Competition; Competitive advantage; Erp; Sap; Simulation; Supply chain; Team behavior ",Financial management
1534,Application of specialized ICT tools for measuring strategic performance: Empirical evidence of contingency factors,"This study examines the association between using specialized ICT tools - such as Business Intelligence - strategic management accounting techniques and contingency factors, e.g. company size, market characteristics and the company's perception of itself and others. Empirical data collected from Czech and Slovak companies was analyzed using Spearman's rank correlation coefficient. The analysis revealed that the bigger a company is, the more likely it is to use specialized ICT tools for measuring strategic performance. Market characteristics do not seem to play such an important role in this regard. Using specialized ICT tools for measuring strategic performance is positively correlated with using integrated financial and non-financial metrics. ICT tools also seem to be typically used for addressing a wider range of decision-making needs, rather than for addressing individual and unrelated tasks. Application of specialized ICT tools for measuring strategic performance: Empirical evidence of contingency factors Business Intelligence; ICT; Information systems; Management accounting; Strategic performance measurement Commerce; Competitive intelligence; Decision making; Economic and social effects; Information analysis; Information systems; Management science; Contingency factors; Empirical data; Financial metrics; Management accounting; Market characteristics; Performance measurements; Spearman's rank correlation coefficients; Strategic management accountings; Information management",Strategic alignment
1536,Effective strategy execution: Improving performance with business intelligence: Second edition,"This book demonstrates how an improved strategic management approach, leveraging established management concepts in conjunction with the innovative technology solutions offered by business intelligence, can lead to better performance. It presents the three main barriers to effective strategy execution and explains how they can be overcome. Creating a shared understanding of the strategy at all levels of the organization using a Value ScorecardTM and following the Strategic Alignment ProcessTM allow organizations to measure and monitor performance. Strategic Alignment Remote ControlTM is presented as the ultimate tool for managers to remain in control of their business. Seven case studies from different industries across the globe provide examples of how the organizational performance can be improved. They include companies like Daimler, Tetra-Pak, Würth, Germany's Federal Employment Agency, the city of Aix-Les-Bains, and Giesecke & Devrient. Additional examples from organizations like Disney, Marriott, Volkswagen, Avis, FedEx, and Harrahs help to demonstrate how applying the concepts introduced adds unique value. The second edition of this book has been updated and improved. Additionally it includes a separate section on decision-making under uncertainty and the results of a survey on the adoption of business intelligence. © Springer-Verlag Berlin Heidelberg 2016. All rights reserved. Effective strategy execution: Improving performance with business intelligence: Second edition  ",Strategic alignment
1537,Quantitative model for predicting the referential intention of construction management services,"Many companies pay a great deal of attention to existing customers' referral intentions when they attempt to attract new customers. However, little is known about the necessary level of satisfaction of existing customers to understand when existing customers are likely to complete a referral and by what mechanism. This study assumed referral routes and established a model for predicting referral intentions based on the satisfaction level as described by the disconfirmation of expectation theory and the net promoter score theory. Then, the routes were verified by surveying 103 construction management (CM) clients using structural equation modeling, and the prediction model was tested by applying it to 194 CM clients using multinomial logistic regression. The results indicated that the accuracy rate of the prediction model was 79.3%. This model can be used effectively to attract new clients, particularly in fields where long-term services are provided, such as CM, because it allows service providers to predict customers' referral intentions depending on their satisfaction levels. © 2014 American Society of Civil Engineers. Quantitative model for predicting the referential intention of construction management services Construction management; Long-term service; Prediction; Referral intention Customer satisfaction; Forecasting; Logistic regression; Project management; Sales; Construction management; Disconfirmation of expectations; Level of satisfaction; Long term service; Multinomial logistic regression; Quantitative modeling; Referral intention; Structural equation modeling; Predictive analytics",Strategic alignment
1541,Business intelligence as a key information and knowledge tool for strategic business performance management,"The indicators, methods and models applied in performance management in the past were largely based on financial indicators and financial management methods. Of course, we do not claim that financial indicators are not currently necessary or relevant, but as the business results showed, the management based only on the financial statements is no longer enough. The paper focuses on the presentation of selected research results related to strategic business performance management. With the currently presented issue, we enter an area of research which is not as clearly defined as it was in the case of a traditional financially oriented approach to measuring and managing corporate performance which prevailed in the past. The aim of the paper is to analyse and synthetize findings regarding the chosen, mainly not traditional methods and models, which have started to be used for strategic business performance management. The results of our empirical scientific study provide interesting and valuable findings that the overall performance of industrial enterprises in Slovakia is to be looked at comprehensively strategically and not just in financial terms. Why are some industrial enterprises more efficient than others? What methods and procedures are applied by more efficient companies? The answers to these questions can be found in our paper. We recommend industry enterprises to apply selected methods and models of strategic business performance management. The key tool in increasing the overall performance of the enterprise in the selected Slovak industries seems to be employing a system of strategic performance management, supported by a knowledge-based Business Intelligence Information System. © 2016, Technical University of Liberec. All rights reserved. Business intelligence as a key information and knowledge tool for strategic business performance management Business intelligence; Business performance management; Information support for business and economics; Strategic information systems; Strategic management ",Strategic alignment
1542,A cross-project evaluation of text-based fault-prone module prediction,"In the software development, defects affect quality and cost in an adverse way. Therefore, various studies have been proposed defect prediction techniques. Most of current defect prediction approaches use past project data for building prediction models. That is, these approaches are difficult to apply new development projects without past data. In this study, we focus on the cross project prediction that can predict faults of target projects by using other projects. We use 28 versions of 8 projects to conduct experiments of the cross project prediction and intra-project prediction using the fault-prone filtering technique. Fault-prone filtering is a method that predicts faults using tokens from source code modules. Additionally, we try to find an appropriate prediction model in the fault-prone filtering, since there are several ways to calculate probabilities. From the results of experiments, we show that using tokens extracted from all parts of modules is the best way to predict faults and using tokens extracted from code part of modules shows better precision. We also show that the results of the cross project predictions have better recall than the results of the intra-project predictions. © 2014 IEEE. A cross-project evaluation of text-based fault-prone module prediction  Codes (symbols); Defects; Predictive analytics; Project management; Software design; Defect prediction; Development project; Fault-prone modules; Filtering technique; Prediction model; Project data; Project evaluation; Source codes; Forecasting",Risk management
1543,"International Symposium on Intelligent Systems Technologies and Applications, ISTA 2015 co-located with 4th International Conference on Advances in Computing, Communications and Informatics, ICACCI 2015","The proceedings contain 39 papers. The special focus in this conference is on Ad-hoc, Wireless Sensor Networks, Intelligent Distributed Computing, Business Intelligence and Big Data Analytics. The topics include: Analysis of communication delay and packet loss during localization among mobile robots; an encryption technique to thwart android binder exploits; android smudge attack prevention techniques; a variant of SEP for WSN; network monitoring and internet traffic surveillance system; issues and challenges in India; an empirical study of OSER evaluation with SNR for two-way relaying scheme; an optimization to routing approach under WBAN architectural constraints; wind farm potential assessment using GIS; real time water utility model using GIS; a case study in Coimbatore district; distributed air indexing scheme for full-text search on multiple wireless channel; fuzzy differential evolution based gateway placements in WMN for cost optimization; power efficient routing by load balancing in mobile ad hoc networks; network optimization using femtocell deployment at macrocell edge in cognitive environment; path planning of a mobile robot in outdoor terrain; forensic framework for Skype communication; structuring reliable distributed storages; ensemble prefetching through classification using support vector machine; intelligent distributed economic dispatch in smart grids; classification of software project risk factors using machine learning approach; data integration of heterogeneous data sources using QR decomposition and towards development of national health data warehouse for knowledge discovery. International Symposium on Intelligent Systems Technologies and Applications, ISTA 2015 co-located with 4th International Conference on Advances in Computing, Communications and Informatics, ICACCI 2015  ",Monitoring and control
1544,Designing a knowledge base for OSS project recommender system: A big data analytics approach,"Online software engineering repositories like GitHub are great resources of socio-technical data about software development process. GitHub as a large-scale social coding environment contains various types of open source projects. Selecting a suitable project from a developer's perspective is difficult and time-consuming task. In this paper, general Big Data approaches and machine learning techniques are used to analyse GitHub data. Variety of socio-technical metrics and factors are extracted from online repositories for data analysis. We find that data pre-processing plays an important role in the proposed approach for GitHub Mining. Design science research method is applied on the pre-processed data on open source software (OSS) projects to design recommendation system for project selection. Content-Based recommendation techniques are proposed with evaluation mechanism. Designing a knowledge base for OSS project recommender system: A big data analytics approach Big data; Design science; OSS repositories; Project selection; Recommender systems Artificial intelligence; Data handling; Design; Information systems; Knowledge based systems; Learning systems; Open source software; Open systems; Recommender systems; Software design; Software engineering; Content-based recommendation; Design science; Design-science researches; Machine learning techniques; Open source software projects; OSS repositories; Project selection; Software development process; Big data",Strategic alignment
1545,Reshaping strategic management accounting systems,"Two technologies are currently reshaping Accounting Information Systems: In-Memory Technology and Mobile Technology. It is not only a matter of generic increase of performances, as it usually occurs in the IT field, but a dramatic improvement that is affecting hardware and software architecture and even the organization of the actual systems. We are facing a new generation of BI tools that will greatly improve the capabilities of Strategic Management Accounting systems. In fact, the improved tools available will finally deliver capabilities that enable end users to directly support their decision, as long-promised. © 2014 The authors and IOS Press. All rights reserved. Reshaping strategic management accounting systems Accounting Information System; Business Intelligence; Decision Support Systems; In-memory Technology; Management; Mobile Technology; OLAP; OLTP; Real-time applications; Strategic Management Accounting Artificial intelligence; Competitive intelligence; Decision support systems; Information systems; Management; Strategic planning; Telecommunication equipment; Artificial intelligence; Information management; Information systems; Information use; Real time systems; Strategic planning; Telecommunication equipment; Accounting Information Systems; Mobile Technology; OLAP; OLTP; Real-time application; Strategic management accountings; Business-intelligence; In-memory technology; Management accounting systems; Memory technology; Management science; Decision support systems",Risk management
1547,Technology Acceptance Model for Business Intelligence Systems: Preliminary Research,"Business intelligence systems refer to the set of software platforms that support organizations in retrieving valuable information and knowledge, with the purpose to increase efficiency of decision making. Goal of the paper is to discuss a framework for investigating the adoption of business intelligence systems in companies, from the technology acceptance model perspective. We propose a research framework based on the technology acceptance model that is expanded using the concepts of technology driven strategy, information quality and project management in companies. Based on the framework, we propose a model with research propositions and discuss scientific contributions. © 2016 The Authors. Technology Acceptance Model for Business Intelligence Systems: Preliminary Research business intelligence systems; information quality; project management; technology acceptance model; technology driven strategy Decision making; Decision support systems; Information analysis; Information systems; Management science; Project management; Business intelligence systems; Information quality; Research frameworks; Research propositions; Scientific contributions; Software platforms; Technology acceptance model; Information management",Strategic alignment
1548,Making strategy process intelligent with business intelligence: An empirical investigation,"With the advancements in information systems, 'business intelligence (BI)' has gained considerable ground in business and strategic decision making. A number of studies show the relation between BI and business value, and provide ample evidence of uses of BI. The present study contributes to the growing body of literature on BI and strategic management by attempting to find out the relation between BI applications and tightness/looseness of strategy process structuring with the help of binary logistic regression. It is found from the empirical investigation that more use of BI systems increases the odds of tightly structured strategy process in a significant way, i.e., increased control systems, higher focus on goals and strategies, more objectivity in performance measurement, etc. Also some of the BI measures, like spreadsheet analytics, dashboards, etc. dominate the information systems but a need for improved systems is felt by businesses. Copyright © 2014 Inderscience Enterprises Ltd. Making strategy process intelligent with business intelligence: An empirical investigation Automotive component manufacturers; Automotive manufacturers; Binary logistic regression; Business intelligence; Cluster analysis; SPSS analysis; Strategic management; Strategy process ",Monitoring and control
1550,The concept of project management platform using BI and big data technology,"In current world, organizations need to adapt to the changing business environment. They decide to conduct projects that result with new business processes, new products or services. Very often the goal of the project is to streamline specific area of company or a whole business. The projects become a very complex set of activities that require a sophisticated IT tools to support the efficiency of all the actions. Probably none single software application is able to handle every aspect of the project. That is why the authors decided to identify the kinds of software necessary for supporting the project and choose the applications that from their perspective can aid specific project activities. We have to remember that projects can generate a significant amount of data. If we are able to transform data into relevant information, we can maximize the possibility of success (both project and organization). In this paper authors propose the foundation of a complex platform supporting project management and execution with an emphasis on the analytical and reporting part by usage of Business Intelligence and Big Data technologies. Evaluation of such a platform is the subject for the future work. Copyright © 2016 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved. The concept of project management platform using BI and big data technology Big Data; Business Intelligence; Project management; Project management software; Workflow Application programs; Big data; Competitive intelligence; Information analysis; Information systems; Information use; Business Process; Changing business environment; Data technologies; Management platforms; Project activities; Project management software; Software applications; Workflow; Project management",Governance
1551,Towards of a Business Intelligence Platform to Portuguese Misericórdias,"In the healthcare industry it is imperative the need to increase the efficiency of resource management and services. The increasing of Business Intelligence (BI) use in organizations and the demonstrated effectiveness of this type of solution, arises the desire to use BI in healthcare as in Misericórdias. So, in this work some concepts associated to the use of BI in Misericórdias were addressed and a BI architecture was designed. Furthermore, a survey was made in order to understand what are the tools used by Misericórdias every day and which ones have the BI components. Finally, a BI architecture was developed based on the organization's mission and their stakeholders. Through this work it was possible to identify the critical processes and designing the Entity Relationship Diagram as well as a set of indicators to meet the needs of a sustainable decision-making in Portuguese Misericórdias. © 2016 The Authors. Towards of a Business Intelligence Platform to Portuguese Misericórdias Business Intelligence; Data Warehouse; HealthCare; Misericórdia Competitive intelligence; Data warehouses; Decision making; Health care; Information analysis; Information systems; Management science; Project management; Business Intelligence platform; Entity relationship diagrams; Healthcare industry; Resource management; Sustainable decision makings; Information management",Governance
1553,"Big data investment, skills, and firm value","This paper analyzes how labor market factors have shaped early returns on big data investment using a new data source-the LinkedIn skills database. The data source enables firm-level measurement of the employment of workers with technical skills such as Hadoop, MapReduce, and Apache Pig. From 2006 to 2011, Hadoop investments were associated with 3% faster productivity growth, but only for firms (a) with significant data assets and (b) in labor markets where similar investments by other firms helped to facilitate the development of a cadre of workers with complementary technical skills. The benefits of labor market concentration decline for investments in mature data technologies, such as Structured Query Language-based databases, for which the complementary skills can be acquired by workers through universities or other channels. These findings underscore the importance of geography, corporate investment, and skill acquisition channels for explaining productivity growth differences during the spread of new information technology innovations. © 2014 INFORMS. Big data investment, skills, and firm value Big data; Business analytics; Economics of is; Hadoop; Information systems; It policy and management; It productivity; It value; It workforce; Management of it human resources; Strategic management of it; Technical skills Commerce; Economics; Employment; Industry; Information systems; Information technology; Investments; Productivity; Query languages; Business analytics; Economics of is; Hadoop; It policy and managements; IT productivity; It value; IT Workforce; Management of IT human resources; Strategic management; Technical skills; Big data",Financial management
1555,Deploying mobile construction inspection forms as a case study for technology adoption,"Within pipeline construction field inspection data collection relies mostly on archaic systems and processes. For almost all projects, paper-based or at best word processor and spreadsheet reports are manually collected, reviewed, aggregated and archived. The effort and error in this typical process is reduced using a mobile inspection form system that simplifies the field data collection workflow, increases data accuracy and quality, and can be used to generate dynamic project management dashboards. An evaluation of two case study projects provides insight to overcoming technology adoption for pipeline construction as well as performance, quality and forecasting benefits witnessed during these projects. The use of a mobile inspection form system creates the ability for improved analytics such as detailed construction tracking, dynamic forecasting and spatial overlays of construction progress. Improved data standardization and data integrity from the use of tablet forms produces detailed and functional key performance indicators (KPIs) delivered on-demand through a project dashboard. When both field data quality is improved and project managers are provided timely KPIs, projects have the opportunity to be delivered safer, faster and with higher quality, which is a win for the entire pipeline industry. © Copyright 2016 by ASME. Deploying mobile construction inspection forms as a case study for technology adoption  Benchmarking; Data acquisition; Information management; Inspection; Pipeline laying; Pipelines; Construction progress; Data standardization; Dynamic forecasting; Field data collection; Key performance indicators; Mobile construction; Pipeline construction; Technology adoption; Project management",Capacity management
1556,ICEIS 2016 - Proceedings of the 18th International Conference on Enterprise Information Systems,"The proceedings contain 138 papers. The topics discussed include: an allegory on the role of the action researcher to enable user engagement and change management in the early phases of information systems implementation; the data-driven factory - leveraging big industrial data for agile, learning and human-centric manufacturing; efficient self-similarity range wide-joins fostering near-duplicate image detection in emergency scenarios; assessment of factors influencing business process harmonization - a case study in an industrial company; semantic integration between context-awareness and domain data to bring personalized queries to legacy relational databases; on the support of a similarity-enabled relational database management system in civilian crisis situations; the concept of project management platform using BI and big data technology; a multiagent-based simulation of the infection of the macrophage by trypanosoma cruzi in the acute phase of Chagas' disease: influence of the initial inoculum and protozoan escape factor; decision guidance approach to power network analysis and optimization; continuous improvement of proactive event-driven decision making through sensor-enabled feedback (SEF); factors affecting university instructors' continuance intention to use learning management systems: the blackboard system case; and sales forecasting as a service - a cloud based pluggable e-commerce data analytics service. ICEIS 2016 - Proceedings of the 18th International Conference on Enterprise Information Systems  ",Financial management
1557,Business intelligence for sustainable competitive advantage,"The strategic management literature emphasizes the concept of business intelligence (BI) as an essential competitive tool. Yet the sustainability of the firms' competitive advantage provided by BI capability is not well researched. To fill this gap, this study attempts to develop a model for successful BI deployment and empirically examines the association between BI deployment and sustainable competitive advantage. Taking the telecommunications industry in Malaysia as a case example, the research particularly focuses on the influencing perceptions held by telecommunications decision makers and executives on factors that impact successful BI deployment. The research further investigates the relationship between successful BI deployment and sustainable competitive advantage of the telecommunications organizations. Another important aim of this study is to determine the effect of moderating factors such as organization culture, business strategy, and use of BI tools on BI deployment and the sustainability of firm's competitive advantage. This research uses combination of resource-based theory and diffusion of innovation (DOI) theory to examine BI success and its relationship with firm's sustainability. The research adopts the positivist paradigm and a two-phase sequential mixed method consisting of qualitative and quantitative approaches are employed. A tentative research model is developed first based on extensive literature review. The chapter presents a qualitative field study to fine tune the initial research model. Findings from the qualitative method are also used to develop measures and instruments for the next phase of quantitative method. The study includes a survey study with sample of business analysts and decision makers in telecommunications firms and is analyzed by partial least square-based structural equation modeling. The findings reveal that some internal resources of the organizations such as BI governance and the perceptions of BI's characteristics influence the successful deployment of BI. Organizations that practice good BI governance with strong moral and financial support from upper management have an opportunity to realize the dream of having successful BI initiatives in place. The scope of BI governance includes providing sufficient support and commitment in BI funding and implementation, laying out proper BI infrastructure and staffing and establishing a corporate-wide policy and procedures regarding BI. The perceptions about the characteristics of BI such as its relative advantage, complexity, compatibility, and observability are also significant in ensuring BI success. The most important results of this study indicated that with BI successfully deployed, executives would use the knowledge provided for their necessary actions in sustaining the organizations' competitive advantage in terms of economics, social, and environmental issues. This study contributes significantly to the existing literature that will assist future BI researchers especially in achieving sustainable competitive advantage. In particular, the model will help practitioners to consider the resources that they are likely to consider when deploying BI. Finally, the applications of this study can be extended through further adaptation in other industries and various geographic contexts. © 2015 by Emerald Group Publishing Limited. Business intelligence for sustainable competitive advantage Business Intelligence; Deployment; Mix Methods; Qualitative method; Quantitative method; Resource based theory; Sustainable Competitive Advantage ",Strategic alignment
1558,Using Learning Analytics to Assess Capstone Project Teams,Machine-learning-based analytics are an effective tool to help assess student teamwork skills and predict learning outcomes in software engineering courses. © 2016 IEEE. Using Learning Analytics to Assess Capstone Project Teams assessment; Computing Education; machine learning; project management; project-based learning; software engineering; team-based learning Artificial intelligence; Human resource management; Learning systems; Project management; Software engineering; Students; assessment; Capstone projects; Computing education; Learning outcome; Project based learning; Software engineering course; Team-based learning; Teamwork skills; Engineering education,Risk management
1562,A decision-making framework for project portfolio planning at Intel Corporation,"The work we describe addresses the problem of deciding between project-funding opportunities under budget and headcount constraints. Although the projects lead to products that yield revenue in the market, complex interactions between the projects and products make the selection of a portfolio difficult. Furthermore, the senior managers in the company have a wealth of business intuition that can inform the required decisions. We combine modeling, simulation, and optimization techniques to provide a set of the best portfolios possible from the proposed projects and resulting products. We also provide a rich set of analysis and visualization tools for the decision makers to use in exploring the suggested portfolios and applying their intuition to make the final selection. The resulting interplay between analytics and intuition produces better business solutions through a more focused and effective debate in a shorter time than previously achieved. © 2015 INFORMS. A decision-making framework for project portfolio planning at Intel Corporation Analytics; Binary integer linear program; Decision support; Elimination by aspects; Intuition; Portfolio management; Practice of OR; Simulation ",Strategic alignment
1563,Development and implementation of an asset management framework for wastewater collection networks,"This paper presents a framework to develop, implement and communicate a multi-perspective asset management plan for wastewater collection networks. The framework takes into account four strategic perspectives - socio-political, financial, operational/technical, and regulatory - and devises four strategic themes for sustainable wastewater collection systems. The asset management strategic themes, perspectives, and strategic objectives were developed from the proceedings of collaborative working sessions held at the first Canadian National Asset Managers workshop in 2007 in Hamilton, Ontario. The themes and strategic objectives are illustrated in a strategy map and detailed in the modified balanced scorecard model. A case study based on real data presents the use of business intelligence tools to implement, monitor, and report various components of the proposed framework. The proposed framework provides a unified gateway for efficient and effective management of wastewater collection systems. It can be adapted to devise and implement strategic asset management plans in comparable organizations, and to comply with the new legislative requirements that demand increasing accountability to meet stakeholders' expectations, protection of public health and environment, efficient allocation of funds, and greater disclosure. © 2012 Elsevier Ltd. Development and implementation of an asset management framework for wastewater collection networks Asset management; Balanced scorecard; Business intelligence; Strategic management; Strategy maps; Wastewater collection systems Canada; Hamilton [Ontario]; Ontario [Canada]; Competitive intelligence; Gateways (computer networks); Management science; Public health; Sanitary sewers; Balanced scorecards; Health and environment; Legislative requirements; Modified balanced scorecards; Strategic asset managements; Strategic management; Strategy map; Wastewater collection systems; accountability; environmental assessment; numerical model; strategic approach; waste management; wastewater; Asset management",Strategic alignment
1564,An integrated framework of business intelligence and analytic with performance management system: A conceptual framework,"The power of data had induced the implementation of Business Intelligence and Analytic (BIA) in most organizations. They need to uncover hidden information to accelerate organizational performance. However, the gap between IT and management had arisen in its implementation. Therefore, this study reviews broad perspective of BIA and Performance Management System (PMS) that seeks to build an integrated framework for the implementation. The proposed framework has distinguished four main layers which contribute to the key elements of the implementation. © 2015 IEEE. An integrated framework of business intelligence and analytic with performance management system: A conceptual framework Business Analytic; Business Information and Analytics; Business Intelligence; Decision Support System; Performance Management; Performance Measurement; Strategic Management; Strategic Planning Artificial intelligence; Competitive intelligence; Decision support systems; Information analysis; Management science; Strategic planning; Business information; Conceptual frameworks; Integrated frameworks; Organizational performance; Performance management; Performance management systems; Performance measurements; Strategic management; Information management",Strategic alignment
1565,Bringing agility into linked data development: An industrial use case in logistics domain,"Logistics is a complex industry where many different types of companies collaborate in order to transport containers to the last point. One of the most important problem in logistics domain is observation and monitoring of container life cycle where each step of the container transportation may be performed by different company. Thus, observing and monitoring of the container's life cycle in real time become a challenging engineering task. In this research, Linked Data development infrastructure has been used to implement dynamic container observation and monitoring system for ARKAS company which is the leading logistics company in Turkey. During the development of the system, it has been observed that agile practices like feature/story oriented development, test first development and usage of Agile Architecture approach improves the product and project management quality. So, a new methodology has been proposed based on these practices for Linked Data development. Bringing agility into linked data development: An industrial use case in logistics domain Agile analytics; Agile architecture; Linked data development methodology Agile manufacturing systems; Containers; Life cycle; Project management; Social networking (online); Transfer cases (vehicles); World Wide Web; Agile analytics; Agile architectures; Complex industries; Container transportation; Industrial use case; Linked datum; Logistics company; Test-first development; Data handling",Strategic alignment
1566,Understanding the hidden value of business intelligence and analytics (BI&A),"Although some studies have examined specific business intelligence & analytics (BI&A) technologies and how they might contribute to create value and improve firm performance, to our knowledge there is no clearly articulated and theoretically grounded model in the literature that holistically explains BI&A technologies. Only limited research has examined the path to value through the impact of knowledgebased dynamics capabilities on the hidden competitive value. Grounded on strategic management theories (dynamic capabilities and knowledge-based view), we present a theoretical model for understanding how and why BI&A applications can create competitive value and improve firm performance through specific knowledge-based dynamic capabilities. Based on this model a number of propositions are postulated. This research contributes to the IT Value literature. We also expect that the continuing progress on research will produce insights for practitioners and researchers regarding the drivers that lead to BI&A value concerning organizational level. Understanding the hidden value of business intelligence and analytics (BI&A) Business intelligence and analytics (BI&A); Competitive advantage; Dynamic capabilities; IT value; Knowledge-based view Competition; Enterprise resource management; Industrial management; Knowledge based systems; Strategic planning; Competitive advantage; Dynamic capabilities; Dynamics capability; IT value; Knowledge-based views; Organizational levels; Strategic management theories; Theoretical modeling; Management science",Strategic alignment
1569,A dashboard to support management of business analytics capabilities,"Business analytics (BA) systems create value and provide competitive advantage for organisations. They involve technology and data infrastructure, BA capabilities, and business processes that provide useful insights and support decision-making. To provide value and competitive advantage, BA capabilities should be valuable, rare, inimitable and have organisational support (VRIO). In this paper, we develop and evaluate a prototype dashboard for the VRIO assessment of BA capabilities. The dashboard is intended to support the strategic management of BA capabilities. We discuss implications of the prototype dashboard for researchers and practitioners and suggest directions for future research. © 2014 The authors and IOS Press. All rights reserved. A dashboard to support management of business analytics capabilities Business analytics; Design science; Information dashboard design; Resource based view Decision making; Design; Competition; Information management; Business analytics; Business Process; Competitive advantage; Data infrastructure; Design science; Organisational; Resource-based view; Strategic management; Analytics systems; Decisions makings; Information dashboard design; Technology infrastructure; Competition; Decision making",Strategic alignment
1570,HR and analytics: why HR is set to fail the big data challenge,"The HR world is abuzz with talk of big data and the transformative potential of HR analytics. This article takes issue with optimistic accounts, which hail HR analytics as a ‘must have’ capability that will ensure HR's future as a strategic management function while transforming organisational performance for the better. It argues that unless the HR profession wises up to both the potential and drawbacks of this emerging field and engages operationally and strategically to develop better methods and approaches, it is unlikely that existing practices of HR analytics will deliver transformational change. Indeed, it is possible that current trends will seal the exclusion of HR from strategic, board-level influence while doing little to benefit organisations and actively damaging the interests of employees. © 2016 John Wiley & Sons Ltd HR and analytics: why HR is set to fail the big data challenge big data; HR analytics; human resource information systems ",Strategic alignment
1573,Use of Symbolic Regression for lean six sigma projects,"Lean Six Sigma projects and the quality engineering profession have to deal with an extensive selection of tools most of them requiring specialized training. The increased availability of standard statistical software motivates the use of advanced data science techniques to identify relationships between potential causes and project metrics. In these circumstances, Symbolic Regression has received increased attention from researchers and practitioners to uncover the intrinsic relationships hidden within complex data without requiring specialized training for its implementation. The objective of this paper is to evaluate the advantages and drawbacks of using computer assisted Symbolic Regression within the Analyze phase of a Lean Six Sigma project. An application of this approach in a service industry project is also presented. Use of Symbolic Regression for lean six sigma projects Data science; Lean six sigma; Symbolic Regression ",Monitoring and control
1574,From big data to big projects: A step-by-step roadmap,"While technologies to build and run big data projects have started to mature and proliferate over the last couple of years, exploiting all potentials of big data is still at a relatively early stage. In fact, building effective big data projects inside organizations is hindered by the lack of a clear data-driven and analytical roadmap to move businesses and organizations from an opinion-operated era where humans skills are a necessity to a data-driven and smart era where big data analytics plays a major role in discovering unexpected insights in the oceans of data routinely generated or collected. This paper provides a solid and well-founded methodology for organizations to build big data projects and reap the most rewards out of their data. It covers all aspects of big data project implementation, from data collection to final project evaluation. In each stage of the process, we introduce different sets of platforms and tools in order to assist IT professionals and managers in gaining a comprehensive understanding of the methods and technologies involved and in making the best use of them. We also complete the picture by illustrating the process through different real-world big data projects implementations. © 2014 IEEE. From big data to big projects: A step-by-step roadmap advanced analytics; big data; big data project; big data technologies Big data; Data Analytics; Project management; Data collection; Data driven; Data technologies; IT professional; Project evaluation; Project implementation; Real-world; Roadmap; Advanced Analytics",Value management
1577,Mining Level of Control in Medical Organizations,"In literature of strategic management, there are three layers of control defined in organizational structures. These layers are strategic, tactical and operational, in which resides senior, medium level and low level managers respectively. In strategic level, institutional strategies are determined according to senior managers' perceived state of organization. In tactical level, this strategy is processed into methods and activities of a business management plan. Operational level embodies actions and functions to sustain specified business management plan. An acknowledged lead organization in Turkish medical area is examined using case study and data mining method in the scope of this paper. The level of decisions regarded in managerial purposes evaluated through chosen organization's business intelligence event logs report. Hence specification of management level importance of medical organizations is made. Case study, data mining and descriptive statistical method of taken case's reports present that positions of 'Chief Executive Officer', 'Outpatient Center Manager', 'General Manager', monitored and analyzed functions of operational level management more frequently than strategic and tactical level. Absence of strategic management decision level research in medical area distinguishes this paper and consequently substantiates its significant contribution. © 2014 European Federation for Medical Informatics and IOS Press. Mining Level of Control in Medical Organizations business intelligence; data mining; hospital; management pyramid; medical organizations; operational; strategic; Strategic Management; tactical Computer Security; Data Mining; Efficiency, Organizational; Electronic Health Records; Hospital Administration; Hospital Information Systems; Models, Organizational; Organizational Objectives; Turkey; Competitive intelligence; Data mining; Hospitals; Managers; Strategic planning; Business management; Chief executive officer; Institutional strategy; operational; Organizational structures; strategic; Strategic management; tactical; data mining; human; intelligence; manager; mining; outpatient; statistical analysis; computer security; electronic medical record; hospital information system; hospital management; nonbiological model; organization; organization and management; procedures; Turkey; Medical computing",Strategic alignment
1578,Worldwide workflow is growing as an industry project norm,"Global work-share techniques are reaching deeper into project functions, especially during the construction and supply-chain management phases. John MacDonald, Bechtel senior vice president and global manager of EPC functions, said that the contractors have to be able to access quality suppliers around the world to be competitive. On a $520-million powerplant project in Perm, Russia, completed in 2011 and on which Bechtel was lead engineer, procurement, engineering and supplier teams took advantage of the time-zone differences between participant locations in the YOU.S., Turkey, India, Belgium, Germany and Russia to maintain project execution around the clock. Jim Scotti, Fluor Corp. senior vice president and chief procurement officer, says the firm now has 2,400 staff assigned to global procurement management. The industry consortium has several technology initiatives under way now to apply advanced data analytics to give EPCs better tools to predict trouble in the project workflow and address it early. Worldwide workflow is growing as an industry project norm  Civil engineering; Construction; Around the clock; Bechtel; Belgium; Data analytics; Industry project; Procurement management; Project execution; Vice president; Project management",Risk management
1579,Research analytics for reviewer recommendation,"Peer review plays an important role in research project selection at funding agencies. Given the practical challenge that even the most experienced researcher may be unable to point out the whole deficiencies in a complex body of research work, peer review addresses this problem by introducing independent experts to critically analyze and assess the quality of research proposals. Recommending appropriate reviewers for proposals presents a great challenge for funding agency especially when the number of proposals and reviewers are large. Reviewer recommendation involves several issues which need to be considered: avoiding the conflict of interests between authors and reviewers; whether and to what extent the reviewer has expertise in corresponding areas of proposals. This research investigates how research analytics can be used for reviewer recommendation by integrating three dimensions: connectivity, relevance and quality. © 2012 IEEE. Research analytics for reviewer recommendation peer review; research analytics; reviewer recommendation Management science; Complex bodies; Conflict of interest; Funding agencies; Peer review; Project selection; Research proposals; reviewer recommendation; Three dimensions; Engineering research",Strategic alignment
1580,Paradise by the dashboard light: Designing governance metrics in turbulent environments,"One of the emerging sweet-spots in business intelligence (BI) and business performance management (BPM) projects is the design of metrics (dashboards) in such a way that they balance the need for ""inward focused"" operational management with the need for ""outward-focused"" strategic management. Unfortunately, hitting this sweet spot often proves to be a difficult process, particularly in more turbulent environments. This article aims to understand the forces at work in the design process of these metrics using two starkly contrasting case studies involving the design of dashboards for IT departments, specifically for IT governance which concerns the alignment between business and IT. The first case explores metrics design for an organization with low external and high internal turbulence (a German insurance company), the second contrasting case focuses on a similar metrics design process but now for an organization facing low internal turbulence and high external turbulence (an African national government). Our findings suggest how the source of project turbulence impacts the success of the metrics design process, the bias towards more inward-focused or outward-focused metrics, and the overall outcomes of the project. © 2012 IEEE. Paradise by the dashboard light: Designing governance metrics in turbulent environments  Insurance; Management science; Turbulence; Business Performance Management; Design process; External turbulence; Insurance companies; National governments; Operational management; Strategic management; Turbulent environments; Design",Governance
1582,Managing data source quality for data warehouse in manufacturing services,"Data quality and Data Source Management is one of the key success factors for data warehouse project. Many data warehouse projects fail due to poor quality of the data. It is believed that the problems can be fixed later and because of that, a lot of time will be spent to fix the error. If low quality data fed into the data warehouse system, the result will be not accurate if these data are used in the decision making. Many data warehouse and business intelligence projects failure are due to wrong or low quality data. Therefore this paper will underpins several aspects such as Total Data Quality Management (TDQM), ISO 9001:2008 and Quality Management System (QMS) in order to address data quality problems in the early stage and find out the best procedure to manage the data sources. To find a standard procedure in managing data source base on ISO 9001:2008 standard, process in managing data source is identified and a compared to the ISO 9001:2008 Quality Management System (QMS) requirements. As a result, this process is viewed as a kind of production process and relate to the concepts of quality management known from the manufacturing and service domain. More precisely, a high quality management system in managing data source is proposed. This system is based on ISO 9001:2008 standard and hopes it can help organizations in implementing and operating quality management system. By using ISO 9001:2008 framework to the process of managing data source, this approach will be similar to the manufacturing concept that has an added advantage when compared to traditional approaches in managing data source. © 2011 IEEE. Managing data source quality for data warehouse in manufacturing services Data Quality; Data Source; Data Warehouse; ISO 9001:2008; Quality Management System Data reduction; Data warehouses; Decision making; Electrical engineering; Information science; Project management; Quality control; Quality management; Standards; Business Intelligence projects; Data quality; Data quality management; Data source; Data warehouse systems; High quality; ISO 9001; Key success factors; Low qualities; Manufacturing concepts; Manufacturing service; Production process; Projects fail; Quality Management System; Quality management systems; Service domain; Standard procedures; Information management",Value management
1583,Towards an Intelligent Project Based Organisation (IPBO) business model,"Global economy is undergoing a recession phase that had made competition tougher and imposed new business framework. Developing countries, as less impacted by the international economic crisis, should prepare their businesses to take advantages from the raising development opportunities. They have to shift from the classical management approaches to an Intelligent Project Based Organization IPBO model that provides flexibility and agility. IPBO model is intended to reinforce the proven advantages of PBO by the use of suitable Enterprise Intelligence (EI) Systems. © 2013 IEEE. Towards an Intelligent Project Based Organisation (IPBO) business model Business Intelligence (BI); Competitive Intelligence (CI); Enterprise Intelligence (EI); Intelligent Project Based Organization (IPBO); Knowledge Management (KM); Project Based Organization (PBO); Project Management (PM) Competitive intelligence; Developing countries; Information technology; Intelligent networks; Knowledge management; Management science; Project management; Business framework; Business modeling; Enterprise intelligence; Global economies; International economics; Project-based organisations; Project-based organizations; Competition",Governance
1585,A social network-empowered research analytics framework for project selection,"Traditional approaches for research project selection by government funding agencies mainly focus on the matching of research relevance by keywords or disciplines. Other research relevant information such as social connections (e.g., collaboration and co-authorship) and productivity (e.g., quality, quantity, and citations of published journal articles) of researchers is largely ignored. To overcome these limitations, this paper proposes a social network-empowered research analytics framework (RAF) for research project selections. Scholarmate.com, a professional research social network with easy access to research relevant information, serves as a platform to build researcher profiles from three dimensions, i.e., relevance, productivity and connectivity. Building upon profiles of both proposals and researchers, we develop a unique matching algorithm to assist decision makers (e.g. panel chairs or division managers) in optimizing the assignment of reviewers to research project proposals. The proposed framework is implemented and tested by the largest government funding agency in China to aid the grant proposal evaluation process. The new system generated significant economic benefits including great cost savings and quality improvement in the proposal evaluation process. © 2013 Elsevier B.V. A social network-empowered research analytics framework for project selection Research analytics; Research project selection; Research social networks Productivity; Quality control; Social networking (online); Access to research; Economic benefits; Government funding; Matching algorithm; Project selection; Quality improvement; Social connection; Traditional approaches; Research",Strategic alignment
1589,Agile business process management in research projects of life sciences,"In life science laboratories the sub-process automation of methods with semi- or full automated, isolated solutions called islands of automation and several IT sys tems dominate. There are deficits in networking of these sub-processes. The R&D processes in the life sciences research are complex, flexible, unstructured, knowledge-intensive, distributed, and parallel; they use heterogeneous resources, and com bi ne automated, semi-automated, and manual activities in high-variable process chains with a high number of control structures. This characteristic of LSA-processes makes high demands on an integrated process management that contains an interdisciplinary collaborative process control and the documentation of the global process from for example purchasing, sample storage, method development to analytics and interpretation of results as well as the extraction of knowledge. Using methods, techniques, and tools of business process management (BPM) in the life science automation offers the potential to improve the automation level, the networking and the quality of the life science applications. The authors investigate the suitability of the standard based methods and techniques of BPM for the introduction of a flexible, integrated, and automated process management in the heterogeneous and hybrid systems in life sciences. This article will be focusing on the advances made in distributed workflow automation in highly variable system and application environments of research projects by use of BPM methods and tools. © 2011 Springer-Verlag. Agile business process management in research projects of life sciences Agile R&D Processes; BPM; Life Sciences; Process Automation; Workflow Management Agile manufacturing systems; Automation; Distributed computer systems; Enterprise resource management; Hybrid systems; Information science; Project management; Research; Work simplification; Application environment; Automated process management; Automation levels; BPM; Business process management; Collaborative process; Control structure; Distributed workflow; Heterogeneous resources; High demand; Integrated process management; Life sciences research; Life-sciences; Method development; Process automation; Process chain; Sample storage; Semi-automated; Variable systems; Workflow managements; Process control",Strategic alignment
1590,Design of a construction management data visualization environment: A bottom-up approach,"Described is designing a construction management (CM) data visualization environment with emphasis on exploring a bottom-up design methodology as part of the environment development process. Investigated is how such a methodology combined with design guidelines and a top-down approach can aid the creation of ready-to-use visualizations that help CM practitioners answer specific CM questions and support general CM analytics useful for a range of CM functions/tasks. Three visualization development cases for the time performance control application are used to demonstrate the bottom-up design process. The visualizations implemented are tested to show their usefulness and to identify opportunities for incorporating new visualization features that are common to other visualizations supported in the environment. The key lesson learned from the bottom-up development process is that by attending to individual image details, the developed visualizations: 1) are responsiveness to answering specific CM questions, 2) apply state-of-the-art data visualization techniques, 3) fit within an environment architecture, and 4) allow for the enrichment of design guidelines and lists of common visualization features. © 2013 Elsevier B.V. Design of a construction management data visualization environment: A bottom-up approach Construction management analytics; Construction management data visualization environment; Construction project data; Construction time performance management Data visualization; Design; Project management; Visualization; Construction management; Construction projects; Development process; Environment architectures; Performance control; Performance management; Visualization environment; Visualization technique; Environmental management",Monitoring and control
1592,Kiechel's history of corporate strategy,"Purpose: In this paper aims to interview Walter Kiechel III about his book, The Lords of Strategy: The Secret Intellectual History of the New Corporate World, and the lessons it offers for today's managers. Design/methodology/approach - In this interview, Strategy & Leadership asked Walter Kiechel III about his book, The Lords of Strategy: The Secret Intellectual History of the New Corporate World (Harvard Business Press. 2010), and the lessons it offered for today's managers. First as a Fortune writer, then as its editor and finally as editorial director of Harvard Business Publishing, Kiechel has interviewed originators of the core ideas behind strategy and, to a lesser extent, strategic management and executives at the large companies where it was first practiced. Findings - Kiechel chronicles the rise and stumbles of a number of leading consultancies - primarily Boston Consulting Group, Bain and McKinsey - as they, Professor Michael Porter and a few others ""invent"" the concept of strategy over the course of about six decades. Practical implications - Kiechel highlights the lasting accomplishments of the pioneering consultants he calls the Lords of Strategy and the tools they developed like the experience curve and the BCG matrix. He concludes that Greater Taylorism, the application of analytics to virtually every aspect of what a company does, is as important a product of the strategy revolution as strategy itself. Originality/value - Senior managers will find his combination intellectual and business history engrossing and they should learn many lessons from it. For example: the development of strategic thinking has caused a genuine revolution in the way business is done; strategy is now the dominant framework by which companies understand what they are doing and want to do; and the intellectual models of innovative consulting firms have played a key role in figuring out competitive advantage © Emerald Group Publishing Limited. Kiechel's history of corporate strategy Business history; Corporate strategy; Leadership; Management strategy; Process planning ",Strategic alignment
1593,Competitive intelligence as a source of competitive advantage: An exploratory study of the Portuguese biotechnology industry,"Regarded as a strategic information management process, competitive intelligence (CI) is defined as the conversion of the data and information, gathered by an organization from its external and internal environment, into intelligence that supports the organizational decision-making process. It is widely accepted that CI provides management with valuableitems of information that improve the quality of decisions and has a positive effect on a company's competitiveness. Considering the economic importance of the biotechnology industry for Portugal, it is appropriate to study the level of awareness of CI and to identify and describe CI best practices in this sector. An exploratory study was carried out to examine how organizations in the Portuguese biotechnology industry use CI in order to obtain sustainable competitive advantage. Additionally, this research was designed to create hypotheses that could be tested in further studies. Case studies were carried out in two companies and a literature review was conducted to help develop a theoretical framework that would support further analysis. Research shows that the terminology and concept of CI were not well-known inside the two companies, probably because of the prevailing scientific-technical background of their personnel, as distinct from one of management; they therefore lacked the specific, dedicated infrastructure and personnel to perform CI. Yet, CI is carriedout in an informal way, mostly by the decision makers themselves. Additionally, the findings suggest that organizations tend to focus on developing information management processes primarily oriented to internal information assets, such as Business Intelligence or Knowledge Management. The resources provided for these processes tend to reinforce and support the intelligence cycle, thereby improving the overall awareness of CI. Differences in the level of informality of CI activities were also found between the two organizations; the larger and older organization tended to be more formal in itsapproach to CI activities. Interestingly, recent approaches featured in the strategic management literature (e.g., dynamic capabilities) indicate that an informal and tacit CI process is more likely to generate sustainable competitive advantage than a completely explicit one, since the former is more difficult to imitate or substitute. Competitive intelligence as a source of competitive advantage: An exploratory study of the Portuguese biotechnology industry Biotechnology industry; Competitive advantage; Competitive intelligence; SME Biotechnology; Competition; Decision making; Human resource management; Industry; Knowledge management; Management science; Research; Societies and institutions; Sustainable development; Biotechnology industry; Competitive advantage; Data and information; Decision makers; Dynamic capabilities; Economic importance; Exploratory studies; Information assets; Literature reviews; Organizational decision making; Portugal; SME; Strategic information management; Strategic management; Sustainable competitive advantages; Theoretical framework; Competitive intelligence",Strategic alignment
1597,Efficient knowledge management with an SBA: How innovative technology can revive older ones,"The development of new vehicle is becoming increasingly complex: proliferation of on-board electronics, development of hybrid advanced powertrain technologies, globalization of design and production, to name a few. At the same time, companies are straining constantly to better meet consumer desires while reducing time to market and production costs. Definitely a tough challenge! The ever growing amount of data (both in variety and in number of sources) that the enterprise generates is central to this situation. Compiling and processing this data efficiently is easier said than done. And failure to do so can lead to critical business opportunities being lost! A solution exists. It comes from the web where an innovative approach was indeed mandatory to cope with the billions of documents to search. It is called search technology and is engrained in the DNA of EXALEAD. Search Based Applications that can be quickly deployed and configured, have been developed to provide business owners with an intuitive and efficient access to data (PLM, ERP, DMS, and so on). Enhanced by semantic processing and quantitative analytics, these SBAs simplify access to and integration of automotive design and project management data. This paper will first sketch the problem at hand then describe what a SBA is in relation to it. It will then discuss how these search-based applications can be put to great use in order to improve knowledge management and automotive enterprise's efficiency at various stages of a product development. To finish with, some real life usages currently being deployed will be quickly depicted. Copyright © 2013 SAE International. Efficient knowledge management with an SBA: How innovative technology can revive older ones  Data handling; Project management; Semantics; Automotive designs; Critical business; Innovative approaches; Innovative technology; Number of sources; On-board electronics; Search technology; Semantic processing; Knowledge management",Financial management
1599,Information support of corporate governance and strategic management using analytical software,"In the paper an information aspect of corporate governance and strategic management is considered. Relying on analysis of merits and limitations of current developments the concept of performance management information support system and the appropriate modeling approach are proposed. Finally, the possibilities of use of analytical information systems for corporate governance and strategic management are discussed. ©2010 IEEE. Information support of corporate governance and strategic management using analytical software Analytical applications; Business intelligence; Corporate governance; Performance management systems; Strategic management Industrial management; Intelligent systems; Management science; Strategic planning; Analytical applications; Business intelligence; Corporate governance; Performance management systems; Strategic management; Intelligent computing",Governance
1600,The operations plan management system of metallurgical mining enterprise based on Business Intelligence,"This paper, guided by market demand, puts forward the mine enterprise operations plan management system for the purpose of enhancing the market competitiveness of the enterprise. The system shows the connection among the market forecasting, the plan management and the performance management with Business Intelligence technologies, such as data warehouse, data mining and on-line analysis processing, etc.. The system ensures that the flowing of information in the value chain and the information chain among the financial department, the sales department, the production department and the purchasing department is unblocked and efficient. The system continuously improves the level of the business operations management, meets the demands of the enterprise strategic management and supports enterprises to make decisions on the business strategy. It helps achieve the purpose of optimizing the operations plan management process and of improving the performance of business operations. © 2010 IEEE. The operations plan management system of metallurgical mining enterprise based on Business Intelligence Business intelligence; Metallurgical mining enterprise; Operations plan management Commerce; Competition; Data warehouses; Industry; Information science; Management science; Metallurgy; Purchasing; Strategic planning; Business intelligence; Business operation; Business strategy; Management process; Management systems; Market demand; Market forecasting; Metallurgical mining enterprise; Online analysis processing; Operations plan management; Performance management; Purchasing department; Sales department; Strategic management; Value chains; Information management",Capacity management
1601,Agile data warehousing project management: Business intelligence systems using scrum,"You have to make sense of enormous amounts of data, and while the notion of ""agile data warehousing"" might sound tricky, it can yield as much as a 3-to-1 speed advantage while cutting project costs in half. Bring this highly effective technique to your organization with the wisdom of agile data warehousing expert Ralph Hughes. © 2013 Elsevier Inc. All rights reserved. Agile data warehousing project management: Business intelligence systems using scrum  ",Financial management
1602,Notice of Retraction: Constructing an enterprise business intelligence maturity model (EBI2M): Applying Delphi method for consensus (prelimary result),"This paper proposes an enterprise business intelligence maturity model (EBI2M) which can assist the enterprise on BI implementation. EBI2M consists of thirteen key process areas namely change management, culture, strategic management, people, performance measurement, balanced scorecard, information quality, data warehousing, metadata management, master data management, analytical, infrastructure and knowledge management. The Stage 1 Delphi study is used to narrow down the scope of this research. Thirteen Delphi panels were asked to map these key process area to suitable the maturity levels. Result of round 1 Delphi study indicates that none of the key process areas achieve consensus among Delphi panels. In the future, the subsequent round will be conducted to ensure the consensus among the Delphi panels. © 2011 IEEE. Notice of Retraction: Constructing an enterprise business intelligence maturity model (EBI2M): Applying Delphi method for consensus (prelimary result) Business Intelligence; maturity model Civil defense; Competitive intelligence; Data warehouses; Disasters; Information analysis; Knowledge management; Metadata; Risk management; Balanced scorecards; Change management; Information quality; Master data management; Maturity model; Metadata management; Performance measurements; Strategic management; Strategic planning",Risk management
1604,IT instruments of complex projects management,"A complicated and unstructured data flow frequently appears in a complex project management. Data flow management, systematization and control could be organized by the project team using newly implemented tools or standard tools developed on a major software solutions base. Microsoft Project and other MS Office products, Business Intelligence systems, teamwork software, online project management applications could serve as such tools. The best results, assuming three classic project management limitations - time, budget, project scope - could be obtained by interaction synergy of applicable instruments including software. One of the possible complex project information management solutions was perfected during projects management in 2009-2013 by authors and their project teams. IT instruments of complex projects management BI; CAD; Dashboards; EPM; Megaprojects; Mind mapping; PDM; Project management; Toolkit; WBS Application programs; Bismuth; Computer aided design; Data transfer; Information management; Project management; Pulse width modulation; Tools; Dashboards; EPM; Megaprojects; Mind-mapping; Toolkit; WBS; Human resource management",Financial management
1605,A novel regression prediction model for structural engineering applications,"Recently, artificial intelligence tools are most used for structural engineering and mechanics. In order to predict reserve prices and prices of awards, this study proposed a novel regression prediction model by the intelligent Kalman filtering method. An artificial intelligent multiple regression model was established using categorized data and then a prediction model using intelligent Kalman filtering. The rather precise construction bid price model was selected for the purpose of increasing the probability to win bids in the simulation example. A novel regression prediction model for structural engineering applications Construction project and management; Intelligent fuzzy regression; Kalman filtering; Prediction model Artificial intelligence; Costs; Forecasting; Kalman filters; Project management; Regression analysis; Structural design; Artificial intelligence tools; Construction projects; Engineering applications; Fuzzy regressions; Kalman filtering method; Kalman-filtering; Multiple regression model; Prediction model; Predictive analytics",Monitoring and control
1608,Distribution and use of digital learning materials: An evaluation of a virtual maths website and surrounding activities,"The next generation of construction professionals are IT literate; they surf the web, access the latest news from multiple sources and use social networking websites to connect with their peers around the globe. The skills of the construction knowledge worker are changing and while IT skills for many are second nature, the standard of maths has been questioned. Many learning providers are using websites to deliver information and improve various skills, yet there is little knowledge on how to use web applications to help ensure potentially interested parties are exposed to the material. The construction industry is mathematically demanding and construction workers need relevant skills. In an attempt to tackle this, construction based maths resources were created in an interactive web environment. However, while the web resource exists, it does not mean that it will be found nor used. Research was undertaken to determine how the website applications and surrounding activities impacted on the use of the website and ultimately provided exposure to the maths tools. The use of the Virtualmaths website has been monitored with Google Analytics. The results have proved interesting in terms of the activities undertaken to promote the site and comparisons with the website hits. While the maths website ranks well in search engine results, much of the traffic to the site is by direct access, through social media and networking sites such as Twitter and YouTube. However, face to face workshops that were used to showcase the web resources were found to produce a significant impact with regard to direct access and user registrations. It is clear that the quality of the materials is important as is the promotion of open access web materials if they are to be used. Distribution and use of digital learning materials: An evaluation of a virtual maths website and surrounding activities Functional skills; Google Analytics; Learning resources; Mathematics; Open access; Teaching and learning Construction industry; Mathematical techniques; Project management; Research; Search engines; Web services; Functional skills; Google Analytics; Learning resource; Open Access; Teaching and learning; Websites",Value management
1609,Developing business intelligence model for scientific research project management,"The subject of this paper is aimed at defining the concept of business intelligence in order to manage a scientific research project. An overview of the process of formulating business intelligence and methodology necessary to build such a system is provided. It refers to activities of request definition, object-oriented analysis, object-oriented design and implementation, as well as the tasks carried out within the framework of these activities. Business intelligence system adapted to managing a scientific research project will provide high quality monitoring of the activities of researchers during the project implementation, monitoring of scientific research institutions, development of the scientific field for the related project and resources used in the project. Developing business intelligence model for scientific research project management Business intelligence; E-business; Project management; Scientific research Competitive intelligence; Management science; Project management; Business intelligence systems; eBusiness; High-quality monitoring; Object-oriented analysis; Object-oriented design; Project implementation; Scientific fields; Scientific researches; Research",Strategic alignment
1610,A study of strategic intelligence as a strategic management tool in the long-term insurance industry in South Africa,"Purpose: The purpose of this paper is to explore the extent to which strategic intelligence is utilised within the South African long-term insurance industry and whether it could be used to identify opportunities or threats within the global environment to remain competitive, create greater innovation, and corporate advantage. Design/methodology/approach: The approach of this paper is to obtain the qualitative views and opinions of strategic decision makers, on an executive managerial level within the South African long-term insurance industry, on their organizations' use of strategic intelligence. Findings: There are marked differences in the conformity and usage of strategic intelligence and its components between the organizations surveyed, with a measurable difference between large and small organizations, however, it is generally viewed that the use of a strategic intelligence framework could greatly enhance decision making. Research limitations/implications: Data collection was limited to the 82 long-term insurance companies which were registered with the South African Financial Services Board, with a focus on the organizations listed on the Johannesburg Securities Exchange within the Life Assurance Sector, within which a final response rate of 36.1 per cent was achieved, including the 100 per cent response rate from the six listed organizations. Practical implications: The paper identifies the extent to which strategic intelligence is utilised in the South African long-term insurance industry, and identifies the benefits or problems that are experienced by implementing and using strategic intelligence as an input to the strategic management process and what value strategic intelligence adds in the decision-making process. Originality/value: The identification and utilisation of the most important factors of a strategic intelligence framework will greatly enhance global corporate decision making and result in competitive advantage and constant innovation within the South African business environment. © Emerald Group Publishing Limited. A study of strategic intelligence as a strategic management tool in the long-term insurance industry in South Africa Business intelligence; Competitive intelligence; Corporate strategy; Insurance companies; Knowledge management; South Africa; Strategic decision making; Strategic intelligence; Strategic management; Strategic management tool ",Strategic alignment
1612,"6th International Workshop on Enabling Real-Time Business Intelligence, BIRTE 2012, Held at the 38th International Conference on Very Large Databases, VLDB 2012","The proceedings contain 10 papers. The special focus in this conference is on Enabling Real-Time Business Intelligence. The topics include: Real-time business intelligence in the MIRABEL smart grid system; a case study on SAPs in-memory computing engine; the vivification problem in real-time business intelligence; an on-demand ELT architecture for real-time BI; query processing of pre-partitioned data using sandwich operators; a visual interface for analyzing text conversations; live analytics service platform; strategic management for real-time business intelligence and pricing approaches for data markets. 6th International Workshop on Enabling Real-Time Business Intelligence, BIRTE 2012, Held at the 38th International Conference on Very Large Databases, VLDB 2012  ",Monitoring and control
1615,CEUR Workshop Proceedings,The proceedings contain 4 papers. The topics discussed include: assisted migration of enterprise applications to the cloud - a hybrid cloud approach; and towards an economic foundation for the decision between agile and plan-driven project management in a business intelligence context. CEUR Workshop Proceedings  ,Risk management
1618,Business intelligence modeling in action: A hospital case study,"Business Intelligence (BI) projects are long and painful endeavors that employ a variety of design methodologies, inspired mostly by software engineering and project management lifecycle models. In recent BI research, new design methodologies are emerging founded on conceptual business models that capture business objectives, strategies, and more. Their claim is that they facilitate the description of the problem-at-hand, its analysis towards a solution, and the implementation of that solution. The key question explored in this work is:Are such models actually useful to BI design practitioners? To answer this question, we conducted an in situ empirical evaluation based on an on-going BI project for a Toronto hospital. The lessons learned from the study include: confirmation that the BI implementation is well-supported by models founded on business concepts; evidence that these models enhance communication within the project team and business stakeholders; and, evidence that there is a need for business modeling to capture BI requirements and, from those, derive and implement BI designs. © 2012 Springer-Verlag Berlin Heidelberg. Business intelligence modeling in action: A hospital case study Balanced Scorecards; Business Intelligence; Business Modeling; Key Performance Indicators; Performance Management Benchmarking; Competitive intelligence; Design; Hospitals; Information systems; Project management; Software engineering; Systems engineering; Balanced scorecards; Business modeling; Business models; Business objectives; Design Methodology; Design practitioners; Empirical evaluations; In-situ; Key performance indicators; Life cycle model; New design; Performance management; Project team; Toronto; Management science",Monitoring and control
1622,Uncovering research opportunities in the medical informatics field: A quantitative content analysis,"With rapid improvements in technology, the ever-pressing need to reduce healthcare costs, and continuing legislation emphasizing medical reforms, the demand for research within the healthcare/information systems interface is growing. In this study, we ascertain the prevalent themes from the extant medical informatics literature in an effort to motivate research beyond the traditional domain of health information technology research so information systems scholars can better understand where their expertise might contribute to advancements in healthcare. We used a quantitative content analysis method to systematically explore 2,188 article texts from journals in medical informatics, medicine, and MIS published over a ten-year period. Texts were analyzed using centering resonance analysis and factor analysis and the following themes emerged from the literature: Analytics; Healthcare Operations and Standards (with sub-themes: Operations, Project Management, and Information Assurance); Knowledge Transfer/Communication (with sub-themes: Extending beyond the Organization, Internal to the Organization, and Patient-Provider); Perceptions and Managing Expectations of Information Technology; Advancements in Research; and Software as a Service. In this article, we discuss these themes in greater detail and offer directions for future research. © 2013 by the Association for Information Systems. Uncovering research opportunities in the medical informatics field: A quantitative content analysis Centering resonance analysis; Content analysis; Medical informatics Health care; Knowledge management; Medical computing; Medical information systems; Project management; Software as a service (SaaS); Centering resonance analysis; Content analysis; Health information technologies; Health-care operations; Information assurance; Quantitative content analysis; Research opportunities; Systems interfaces; Medical informatics",Capacity management
1625,Performance indicators in software project monitoring: Balanced scorecard approach,"Balanced scorecard is one of most used methodologies for performance measurement in enterprises and non-profit organizations. It presents a general framework that has been applied more precisely by many scientists and practitioners. It has been adapted to particular performance measurement systems. These monitoring systems are based on collecting data and analysis of these data. This paper presents overview of implementing balanced scorecard in IT project management. Framework for software project success monitoring, based on PRINCE2 methodology and balanced scorecard is proposed. Key performance indicators as measures to be performed upon data collected during software project implementation are defined. This way, analytics to be performed upon data according to specified key performance indicators, needed for decision support is enabled. © 2012 IEEE. Performance indicators in software project monitoring: Balanced scorecard approach  Decision support systems; Information science; Intelligent systems; Project management; Supply chain management; Balanced scorecard approach; Balanced scorecards; Decision supports; Key performance indicators; Monitoring system; Non profit organizations; Performance indicators; Performance measurement system; Performance measurements; PRINCE2; Software project; Benchmarking",Monitoring and control
1626,Effectiveness of a business intelligence solution to manage the antiretroviral therapy programme,"The Human Immunodeficiency Virus and Acquired Immune Deficiency Syndrome (HIV/AIDS) has caused the death of millions of people worldwide. To combat the effect of HIV/AIDS, the South African government started with the provisioning of Antiretroviral Therapy (ART) in the public health sector. Monitoring and evaluating the effectiveness of this ART programme is of the utmost importance. A business intelligence approach was followed that first of all integrated several independent operational sources into one data warehouse and then delivered strategic management information with easy to use business intelligence tools that was developed and deployed for the users. The business intelligence solution was then evaluated by the users of the system and the results indicated that the users deemed the solution to be an effective way to obtain strategic information on the rollout of the ARV treatment programme. Effectiveness of a business intelligence solution to manage the antiretroviral therapy programme Antiretroviral therapy; Business intelligence; Data warehousing; HIV/AIDS; OLAP Data warehouses; Diseases; Health; Information science; Viruses; Acquired immune deficiency syndrome; Antiretrovirals; HIV/AIDS; Human immunodeficiency virus; OLAP; Public health; South African government; Strategic management; Management science",Governance
1632,Paradigm shift in supply chain management (SCM),"In today's world, the way we do business is not the only changing phenomenon but also the pace of change is changing. The business is global, highly competitive, technology-intense, prices are market driven with shorter product life cycles, integrated processes starving to collaboration. Within this context, supply chain management (SCM) has earned its right place in the strategic management arena. Technology became the enabler for SCM development, making visibility, transparency and integrity possible among the SC partners. This leads the transformation of SC's from dyadic, material management oriented relationships into complex, collaborative, networked, web-enabled, extended architectures. With multiple, global actors and complex dependencies, enterprise borders vanish and enterprise-centric strategy development becomes insufficient. What defines success in such an environment is a network-centric, collaborative and holistic approach characterized by network level strategy development and decision making based on business intelligence with the partners. Integration of multi-function and multi-agent systems will be the main key performance index (KPI) for the SCM development. This is a radical paradigm shift in which collaborative long-term partnerships reign, managerial borders extend, and network-level results dominate the enterprise-centric results. Thus, this study provides a comprehensive discussion of this paradigm shift in SCM from organizational, technological, and managerial perspectives. Following the ""leagile"" philosophy, capabilities and advantages of lean and agile SCM can be achieved. In this paradigm, network level performance and risk management is the key for chain-level managerial control and trust among partners is the vital binding glue keeping the partnerships alive. Paradigm shift in supply chain management (SCM) Paradigm; Strategic management; Supply chain management Industry; Life cycle; Management science; Managers; Multi agent systems; Risk management; Strategic planning; Supply chain management; Key performance index; Long-term partnerships; Network level performance; Paradigm; Product life cycles; Strategic management; Strategy development; Supply chain managements (SCM); Complex networks",Risk management
1633,Agile way of BI implementation,"The rapidly changing IT economy has influenced the Business Intelligence (BI) systems to look at innovative ways to be equally fast and flexible. There is a need to be more intuitive and quick in implementation so as to adapt to the changing environment. One of the ways by which organizations can achieve these goals is by using Agile based BI development models. There are many components in a successful BI solution which include data integration, analytics, data quality, metadata management, enterprise data warehouse, dashboards and so on. Each of these components are critical for an organization, and stakeholders are ready to invest in these. The only issue is how quickly we can provide these solutions and how flexible these solutions are with the changing demands. Traditionally, we have been using the waterfall SDLC model for BI implementations which encourages getting requirements clarity in the initial phases of the projects and having distinct deliverables for each phase. With time the approach has been customized and enhanced to 'iterative waterfall approach' where a chunk of requirements is implemented in one SDLC cycle. Though this approach has been successful in the past, the BI practitioners recognize that business requirements are not static and we must be able to effectively mould the deliverables based on changing requirements. Hence, we cannot continue with the Waterfall (or Iterative waterfall) project management approach that is neither fast nor flexible. Applying the concepts of agile development to BI is the intuitive way forward. The aim of this paper is to provide a background on agile project management & development techniques, and suggest some guidelines and best practices which can help in successful Agile BI implementations. © 2011 IEEE. Agile way of BI implementation Agile; Buisness Intelligence; Development methodlogy; Scrum Innovation; Management science; Metadata; Agile; Agile development; Agile project management; Buisness Intelligence; Business intelligence systems; Business requirement; Changing environment; Data integration; Data quality; Development methodlogy; Development model; Development technique; Enterprise data warehouse; Initial phasis; Metadata management; Scrum; Project management",Governance
1639,Applying analytics: A practical introduction,"Newcomers to quantitative analysis need practical guidance on how to analyze data in the real world yet most introductory books focus on lengthy derivations and justifications instead of practical techniques. Covering the technical and professional skills needed by analysts in the academic, private, and public sectors, Applying Analytics: A Practical Introduction systematically teaches novices how to apply algorithms to real data and how to recognize potential pitfalls. It offers one of the first textbooks for the emerging first course in analytics. The text concentrates on the interpretation, strengths, and weaknesses of analytical techniques, along with challenges encountered by analysts in their daily work. The author shares various lessons learned from applying analytics in the real world. He supplements the technical material with coverage of professional skills traditionally learned through experience, such as project management, analytic communication, and using analysis to inform decisions. Example data sets used in the text are available for download online so that readers can test their own analytic routines. Suitable for beginning analysts in the sciences, business, engineering, and government, this book provides an accessible, example-driven introduction to the emerging field of analytics. It shows how to interpret data and identify trends across a range of fields. © 2014 by E.S. Levine. All rights reserved. Applying analytics: A practical introduction  ",Strategic alignment
1643,Design of a construction management data visualization environment: A top-down approach,"Explored in this paper is the topic of designing a construction management (CM) data visualization environment with emphasis on its use for supporting the time management function during the planning and execution phases of construction projects which are characterized by sizeable volumes of data of different types. A brief overview of recent construction data visualization work is first provided. Then, as part of a top-down design approach, we introduce concepts and useful terminology related to a structured way of thinking about analytical reasoning and visual analytics, and their relationship with construction management functions. The focus of the latter then shifts to how a construction data visualization environment can support project participant analytical reasoning needs for the management of time, specifically planning/predicting and monitoring/diagnosing/controlling construction conditions and time performance. A case study of aspects of an actual project examined using the construction data visualization environment developed to date is then presented. Purposes served include demonstrating the breadth of support that can be offered for reasoning by such an environment, and providing a test case for demonstrating the kind of evaluation process one should engage in to assess how well an environment conforms to the requirements set out for it. Time management functions treated for this case study include assessing quality of a baseline schedule, assessing actual vs. planned construction conditions and time performance, and assessing reasons for deviations. An evaluation of the current environment is then made to assess conformance/non-conformance with the requirements established for it and to identify worthwhile extensions to it. The paper concludes with a discussion of lessons learned from work performed to date, and their application to create a more comprehensive visualization environment that supports multiple CM functions. © 2010 Elsevier B.V. All rights reserved. Design of a construction management data visualization environment: A top-down approach Construction management; Construction management data visualization; Construction management data visualization environment; Construction performance monitoring; Construction project data; Design requirements; Schedule quality; Visual construction management analytics Construction industry; Data visualization; Design; Environmental management; Function evaluation; Monitoring; Planning; Visualization; Construction management; Construction management data visualization environment; Construction performance monitoring; Construction project data; Design requirements; Schedule quality; Visual construction management analytics; Project management",Risk management
1644,Strategic management in world class universities,"This chapter focuses on strategic management in World Class Universities while considering Information and Communication Technologies. In order to reach an elite status on the basis of international recognition, different methodologies are being researched and implemented worldwide. The intent of this chapter is to describe how Universidade Estadual de Campinas developed its strategic planning using the Balanced Scorecard management system and to present a new research-based methodology for World Class Universities that considers process improvement, process automation and massive use of Information and Communication Technologies. The text fosters a discussion on the many possibilities of business analytics software to support all types of decisions on the perspective of automating the strategy management processes with business intelligence software. © 2013 Nova Science Publishers, Inc. All rights reserved. Strategic management in world class universities  ",Strategic alignment
1645,"Applicability of asset analytics, CBM and life cycle management to the smart grid: Balancing short-term and long-term risks","One of the key issues facing those deploying smart grid technologies is: how to determine return on investments while also justifying increased rates to regulators and customers? Ideally, the return on investment should benefit utility and consumer alike, deliver in both the short- and long-term, and work with or without a smart grid. Asset management is one such area where this might be achieved. The challenge is, while a large part of a company’s budget goes into maintaining high-value assets, most companies still struggle with gaining greater asset awareness. Incentives, in the way of smart grid dollars, have given the Asset Manager an opportunity for tackling new approaches to asset management and condition-based maintenance (CBM), but with the exception of integration projects, and tests of new devices, protocols and communications—not much has been done to reimagine the role and value substations and their asset’s play in the smart grid. If we were to reimagine how asset management evolves so that it better meets the needs of the organization today and deliver value for smart grid investments tomorrow, what would it look like? A good place to begin evolving our view of asset management is in our perspective. To date, asset management systems have been anchored in tabular views of data, making it difficult to glean a snapshot of overall asset risk from a consolidated view. The day-to-day, short-term view of the asset tends to be static and analytics are limited. If a system supports analytics, few have the time to implement them. When it comes to the long-term view, most companies still manage their capital replacement program in a separate department with manually generated spread sheets—always wondering if the right decisions have been made and how to flex in the face of new operational contingencies. From the ground up, overall asset health scores and risks should feed short-term intervention and long-term capital replacement, but today, very little application and analytic infrastructure may be shared between differing views of the same asset. Analytics and “Big Data” are the industries new “Holy Grail.” And it is true; data collection is on the rise and we need a way to intelligently filter through all that data to make informative decisions. Analytics will help with this process, but the initial results may not be as informative as we thought. Determining an asset’s condition and risks using a set of analytics is a multi-layered process that is part science and part art. The art is tricky. The authors will discuss how a multi-layered, common set of analytics searches for the low frequency events that have a high-cost to the business while also tracking against the long-term view of assets on “Watch” due to age, manufacturer, health, etc. Beyond health scoring, assets are further evaluated for Criticality, Risk, Recommended Action and Time to Action, %Capacity, and Revenue Impact. The authors will also forward ideas on how asset management might complement smart grid strategies while simultaneously enabling companies to better manage short- and long-term risks. © 2013 CIGRE. All Rights Reserved. Applicability of asset analytics, CBM and life cycle management to the smart grid: Balancing short-term and long-term risks Analytics; Asset Risk Management; Condition-based Maintenance; Criticality Impact; Economic Value; Online Monitoring; Revenue Impact; Smart Grid; Standards; Strategic Management of Assets Asset management; Big data; Budget control; Criticality (nuclear fission); Economics; Electric power transmission networks; Health; Health risks; Investments; Life cycle; Risk management; Smart power grids; Standards; Analytics; Asset risk management; Condition based maintenance; Economic values; Online monitoring; Smart grid; Strategic management; Information management",Strategic alignment
1646,Strategic enterprise management in the taps and fittings sector: Application of the balanced scorecard methodology to business intelligence systems,"Global competition is intensifying and so companies need to optimize their operational business processes. But this is no longer enough because they must also be able to react quickly on a strategic level to new developments, considering alternatives and taking the correct decisions. In the latest few years, it has become obvious that only efficient enterprise management processes ensure the consistent realization of strategy and its continuing translation into the day-today activities, one of the most important success factors in enterprise management today. In the light of the above said considerations, and through the analysis of the taps and fittings sector of north-eastern Piedmont, with this work we want to display: the main perspectives of analysis companies should adopt for their business performance; the main key performance indicators (KPI) for each perspective; the organization of KPI in a business intelligence (BI) system to optimize strategic management. © Springer-Verlag Berlin Heidelberg 2013. Strategic enterprise management in the taps and fittings sector: Application of the balanced scorecard methodology to business intelligence systems Business intelligence; Key performance indicator; Strategic enterprise Management ",Strategic alignment
1648,Issues and topics to consider for information management research in eMedia industries,"The digital media industry produces vast data along the content value chain. Any interaction with digital media yields data, often in real-time, both on production and consumption side. This data can be turned to immediately available business information on the operational level, e.g. editors continuously tweak their workflows upon social media feedback. This questions the traditional distinction between organization levels, changing the role of strategic management in the media business and the kind of information it is acting upon. An integration of business information and content management systems throughout the whole value chain holds great potential for future business intelligence applications in the media sector. Issues and topics to consider for information management research in eMedia industries EMedia; Information management; Information systems; Media technology; Multimedia Automation; Digital storage; Information systems; Semantics; Business intelligence applications; Content management system; Content value chains; EMedia; Media technology; Multimedia; Production and consumption; Traditional distinctions; Information management",Governance
1649,IT-Related Service: A Multidisciplinary Perspective,"Information technology (IT)-related service is the strategic management of the creation and delivery of service in which information and communication technology (referred to as IT here) plays a substantial role. IT can serve as a facilitator (e.g., facilitates access to customer information and customer communication) or enabler (e.g., enables value cocreation), serves as the context (e.g., mobile phone market or e-commerce), or is itself service (e.g., social networking sites or information goods). There are three essential characteristics of IT-related service-it is information-intensive, customer-centric, and multidisciplinary. IT-related service is information-intensive. The ability to communicate (firm-to-customer, firm-to-firm, and customer-to-customer) anytime, anywhere, and to anyone, is significantly facilitated by the recent technology trends of big data, cloud computing, and mobile and networking platforms. IT-related service is customer-centric. The use of IT, both by firms and by customers, alleviates the common observation that there is a trade-off between customer satisfaction and productivity improvement for service. Customers are able to talk back to firms with their new communication power and firms are better able to cost-efficiently satisfy their customers. The study of IT-related service is thus inherently multidisciplinary involving such fields as marketing, strategic management, computer science/information systems, and operations management/organizational research. The results of the IT-service transformation are that IT blurs the distinction between goods and service; service is becoming more goods-like and goods are acquiring the characteristics of service. The Journal of Service Research special issue on IT-related service brings together all of these elements, and provides rich strategic implications for managers. © The Author(s) 2013. IT-Related Service: A Multidisciplinary Perspective big data; customer analytics; customer-centric; environmental scanning; information systems; information technology; service; service marketing; service operations; service science; service transformation ",Stakeholder management
1653,"16th Americas Conference on Information Systems, AMCIS 2010, Volume 4","The proceedings contain 76 papers. The special focus in this conference is on business intelligence, business process modelling, project management, IT organizational learning and healthcare information technology. The topics include: IT investment decision making; virtual team collaboration; a taxonomy of digital music services; knowledge sharing in globally distributed IT outsourcing arrangements; IT innovation budgets in turbulent times; overcoming challenges for managing IT innovations in Non-IT companies; service contract automation; cloud services from a consumer perspective; a framework for the design of service maps; ITSM and education; user acceptance of multiple password systems; corporatizing open source software innovation in the Plone Community; a new approach for collaborative knowledge management; organizational green IT; master data compliance; citation analysis and trends in knowledge management; IT outsourcing's role in China's economic growth; a survey of doctoral programs in IS in the USA and Canada; the role of cognitive conflict in open-content collaboration; web service standards taxonomy; mobile technology in governmental organizations; ERP implementation as participatory design; understanding the performance impact of enterprise architecture management; information systems evolution; an ERP-centric master data management approach; mobile technology in governmental organizations; the effect of pricing online content on perceived information quality in online sites and application of a data mining process model. 16th Americas Conference on Information Systems, AMCIS 2010, Volume 4  ",Governance
1654,Drivers and Barriers to Business intelligence adoption: A case of Pakistan,"This research project deals with the Drivers and Barriers to Business Intelligence Adoption. The Drivers and Barrier Methodology (DBM) is a useful tool for measuring how a particular technology such as Business Intelligence (BI) brings tangible benefits to a business enterprise such as a Consumer Bank or Telecommunications Company. This research helps us determine the key factors which force a company to adopt a certain technology and at the same time points out likely impediments and road blocks on the path to successful project completion. A previous body of work both on DBM and on the vast amount of research done in the domain of Business Intelligence and Data warehousing is used as foundation for this research project. A case of Pakistan is used apply the aforementioned methodolgy. The research concludes that depending on a secific user type such as a Power user, IT User or Business user a 'peculiar preference' in drivers and unique aversion to barriers exist. The challenges and problems faced at each step of the adoption cycle are also highlighted.The findings provide valuable recommendations to application designers, IT Vendors, business analysts, marketers and solution providers. Drivers and Barriers to Business intelligence adoption: A case of Pakistan Business analytics; Business applications; Business intelligence; CIO interviews; Customer insight; Data mining; Data warehousing; Data-driven decision making; Drivers and barriers; Information technology adoption cycle; Information technology strategy; IT case study; IT solution selling; Marketing Competitive intelligence; Data mining; Data warehouses; Industry; Information management; Information systems; Marketing; Project management; Business analytics; Business applications; CIO interviews; Customer insights; Data-driven; Information technology adoption cycle; Information technology strategy; IT case study; IT solution; Research",Strategic alignment
1657,"Decision-making in virtual teams - Developing business intelligence across time, culture and space using serious games","The goal of the research is to determine the performance of multicultural teams in a managed simulation exercise. We will build on Wong and Burtons [1] premise, examining the affects of cross-national team composition on overall team performance using several measures, e.g., frequency, mode and intensity of communication as well as cultural proximity of the participants in the classroom environment. We have conducted cross national, internationally composed business simulation instances designed to reflect real market conditions, i.e. virtual business simulations, over a three year period and results suggest that the intensity of communication increases creativity, strategy and motivation. Furthermore, we investigate the cultural proximity of the students as a component of overall team effectiveness. Our research is based on two simultaneously occurring MBA courses, one at the University of Kassel, Germany and the other at Gannon University, USA. The multicultural personality questionnaire (MPQ): a multidimensional instrument of measuring multicultural effectiveness [2]. The questionnaire has scales for Cultural Empathy, Openmindedness, Emotional Stability, Orientation to Action, Adventurousness/Curiosity, Flexibility, and Extraversion. Supported the predictive value of the instrument of multicultural activity and its incremental value above the Big Five in predicting international orientation and aspiration of an international career, the MPQ may be used as a diagnostic tool for assessing further training needs in international acuity. Using the latest technology and international students (this is not limited to German and YOU.S. citizens) on both sides of the Atlantic, we are testing the learning potential using these virtual environments while using the mediating factors of cultural proximity (MPQ) and modes of communication (skype, facebook, chat, etc.). An additional likert scale evaluation of attitudes towards the importance of specific decision making and business outcome criteria is administered at the beginning of the class and again at the end to track student learning outcomes based on attitudes towards these chosen criteria. The paper proposes an agenda for future teaching in strategic management. Decision-making in virtual teams - Developing business intelligence across time, culture and space using serious games Business simulation; Collaborative and problem-based learning; Crosscultural learning; E-learning; Educational software; Virtual team building Behavioral research; Computer software; Decision making; Education; Management science; Serious games; Students; Surveys; Teaching; Virtual reality; Business simulation; Cross-cultural learning; Educational software; Problem based learning; Virtual team buildings; E-learning",Value management
1661,Adoption of SCRUM for software development projects: An exploratory case study from the ICT industry,"Agile process models are aimed to improve the track record of software development projects b you t in literature and in daily project work there is a broad debate of the usage of agile project management methods. Despite initial positive results in practice, the conhtions, possibilities and effects of agile process models are mscussed controversial. In ths paper, the adoption of agile project management models has been investigated in terms of an exploratoIy case study for a business intelligence development project in the ICT industry. In ths case study the adoption of SCRUM as agile project management method was analyzed and evaluated. The results and experience gained from the method adoption are discussed. General and SCRUM-specific implementation proposals for the practical application were derived. These recommendations seme as a suppod in the introduction of agile process models in other similar projects situations. Adoption of SCRUM for software development projects: An exploratory case study from the ICT industry Agile project management; ExploratoIy case study; ICT IndustIy; SCRUM Information systems; Management science; Research; Software design; Agile process model; Agile project management; Development project; ExploratoIy case study; ICT IndustIy; ICT industries; Project work; SCRUM; Software development projects; Track record; Project management",Strategic alignment
1662,Identification and representation of information resources for construction firms,"Large amounts of information are being accumulated nowadays due to the use of information systems in construction firms. Among them, the reusable legacy information can be referred to as 'information resources' and two approaches have been adopted to utilize them, i.e. to utilize them by using general-purpose software such as Microsoft Office, or by using specialized software such as BI (Business Intelligence) software. Identifying and representing the information resources is the essential step in both approaches. This research identified decision activities and information resource items through literature reviews and two expert workshops: an evaluation workshop and a verification workshop. The reusability of information resource items was evaluated in the evaluation workshop and validated in the verification workshop. A way to represent information resources by using IFC was formulated and an extension of IFC was developed and validated based on the analysis of information resources. This research will benefit construction firms in utilizing the information resources more effectively and efficiently to support decision-making in their project and enterprise management. © 2011 Elsevier Ltd. All rights reserved. Identification and representation of information resources for construction firms Construction firms; Expert workshop; IFC; Information resources; Management; Object-oriented Computer software reusability; Construction industry; Decision making; Information use; Management; Project management; Reusability; Construction firms; Expert workshop; IFC; Information resource; Object oriented; Information management",Risk management
1663,Key success factors in Business Intelligence,"Business Intelligence can bring critical capabilities to an organization, but the implementation of such capabilities is often plagued with problems. Why is it that certain projects fail, while others succeed? The aim of this article is to identify the factors that are present in successful Business Intelligence projects and to organize them into a framework of critical success factors. A survey was conducted during the spring of 2011 to collect primary data on Business Intelligence projects. Findings confirm that Business Intelligence projects are wrestling with both technological and non-technological problems, but the non-technological problems are found to be harder to solve as well as more time consuming than their counterparts. The study also shows that critical success factors for Business Intelligence projects are different from success factors for Information Systems projects in general. Business Intelligences projects have critical success factors that are unique to the subject matter. Major differences can be found primarily among non-technological factors, such as the presence of a specific business need and a clear vision to guide the project. Success depends on types of project funding, the business value provided by each iteration in the project and the alignment of the project to a strategic vision for Business Intelligence at large. Furthermore, the study provides a framework for critical success factors that, explains sixty-one percent of variability of success for projects. Areas which should be given special attention include making sure that the Business Intelligence solution is built with the end users in mind, that the Business Intelligence solution is closely tied to the company's strategic vision and that the project is properly scoped and prioritized to concentrate on the best opportunities first. Key success factors in Business Intelligence Business intelligence; Critical success factors; Data warehouse; Enterprise data warehouse; Project risk management; Success factors framework ",Capacity management
1664,Launch of Roulette – a premium brandy in India by JDPL,"Subject area: Launch strategies, marketing techniques and data analytics procedure adopted by a firm before launching a new product. Study level/applicability: Academic students and management trainees who want to learn the methodology adopted by firms with respect to strategic management and marketing for launching a new product in Indian market. Case overview: Launch plan for Roulette, a premium segment brandy manufactured by John Distilleries Private Limited, has to be designed for Karnataka, Pondicherry and Andhra Pradesh markets in India by the Brand Manager Mr Pundlik Kalburgi. Competitors and target market share needs to be identified for all the three markets. Potential outlets, target outlets, channel-wise sales contribution, depot-wise sales contribution and size of the packs to be produced need to be identified for Karnataka market. These identifications need to be submitted to the chairman of the company and other department heads to implement the launch. Expected learning outcomes: Pareto rule (80/20 rule) application for cost-efficient launch strategy; segmentation and identification of competitors; procedure to identify potential of the launch product and market share that can be targeted; and understanding the complete functioning of alcoholic beverage industry in Indian markets (with special reference to Karnataka) and analysing the market data to build an entire launch plan; 4.1 Identifying channel-wise potential and target outlets for the launch product; 4.2 Identifying potential and target depots and number of outlets under each of the depots; 4.3 How pack size of launching product to be manufactured is decided upon. Supplementary materials: Teaching notes are available for educators only. Please contact your library to gain login details or email support@emeraldinsight.com to request teaching notes. © 2013, Emerald Group Publishing Limited. Launch of Roulette – a premium brandy in India by JDPL Alcoholic beverage industry; Competitor analysis; Market potential; Pareto rule; Product launch ",Strategic alignment
1665,Will the heart of the smart grid please stand up?,"Metering has long been touted as the center of the Smart Grid but a question the power industry has not yet asked, but is ripe to do so now, is, ""Do investments in smart grid technologies make sense for areas where the price per kWh is low and of plenty of capacity exists?"" Until price inelasticity increases, or overall network stability requires otherwise, the answer should be, no. And if no, under what circumstances should investment occur? When carefully evaluated, value first lies in the strategic management of assets at the substation. This does not preclude the value metering offers, but companies need a broader approach for their Smart Grid Strategies. This paper presents an example of meter integration with substations as the heart of the Smart Grid, provides a discussion around the Substation of the Future, and offers concepts for rethinking a substation strategy as central to a Smart Grid program. © 2011 IEEE. Will the heart of the smart grid please stand up? AMI; analytics; asset management; automated metering infrastructure; demand management; EISA; metrics; Smart Grid; smart meters; substations Asset management; Electric load management; Electric substations; Grid computing; Investments; AMI; analytics; automated metering infrastructure; EISA; metrics; Smart grid; Smart meters; Smart power grids",Strategic alignment
1667,Strategic management for real-time business intelligence,"Even though much research has been devoted on real-time data warehousing, most of it ignores business concerns that underlie all uses of such data. The complete Business Intelligence (BI) problem begins with modeling and analysis of business objectives and specifications, followed by a systematic derivation of real-time BI queries on warehouse data. In this position paper, we motivate the need for the development of a complete Real Time BI stack able to continuously evaluate and reason about strategic objectives. We argue that an integrated system, able to receive formal specifications of the organization’s strategic objectives and to transform them into a set of queries that are continuously evaluated against the warehouse, offers significant benefits. In this context, we propose the development of a set of real-time query answering mechanisms able to identify warehouse segments with temporal patterns of special interest, as well as novel techniques for mining warehouse regions that represent expected, or unexpected threats and opportunities. With such a vision in mind, we propose an architecture for such a framework, and discuss relevant challenges and research directions. © Springer-Verlag Berlin Heidelberg 2013. Strategic management for real-time business intelligence  Data warehouses; Formal specification; Information analysis; Specifications; Warehouses; Business objectives; Integrated systems; Model and analysis; Novel techniques; Real time business intelligence; Strategic management; Strategic objectives; Temporal pattern; Management science",Strategic alignment
1668,"Strategy, strategic management accounting and performance: A configurational analysis","Purpose - The purpose of this paper is to investigate the effectiveness of different configurational archetypes of strategy and strategic management accounting and to appraise how management accounting's horizontal and vertical alignment with strategy can facilitate performance. Design/methodology/approach - The study deploys a holistic configurational approach to examine the relationship between strategy, strategic management accounting, and performance. Configurations are derived empirically, using an inductive approach, from a sample of 109 manufacturing companies. Findings - The observed configurations (i.e. ""analytics"", ""blue-chips"", ""first movers"", ""domestic protectors"", ""laggards and socialism relics"") constitute varying levels of performance and varying degrees of fit. Support is provided for the equifinality proposition that different strategic and structural alternatives are associated with similar performance levels. Equivocal support is provided for the configurational proposition that internally consistent configurations are associated with higher performance. Research limitations/implications - The variables examined do not fully capture the complexity of pertinent configurations. Limitations revolve around application of the cluster analytical technique and its reliance on researcher judgement. Practical implications - The study's most important message concerns the manner in which it highlights the fallibility of assuming a singular relationship between strategic choices and management accounting system design. While prior research has tended to offer fragmented and unidirectional management accounting prescriptions, the authors raise the notion of how key variables can interact to create an effective organization. Originality/value - The paper breaks new ground by showing that multiple designs of strategy and strategic management accounting may be equally effective in a particular context. This finding challenges much traditional contingency-based modelling in management accounting. © 2012 Emerald Group Publishing Limited. All rights reserved. Strategy, strategic management accounting and performance: A configurational analysis Configurations; Equifinality; Management accounting; Manufacturing industries; Organizational performance; Slovenia; Strategic management; Strategic management accounting Manufacture; Research; Strategic planning; Systems analysis; Configurations; Equifinality; Management accounting; Manufacturing industries; Organizational performance; Slovenia; Strategic management; Management",Strategic alignment
1669,"Concept, implementation and evaluation of a web-based software cockpit","Software cockpits (software project control centers) provide systematic support for monitoring and controlling the activities in a software development project. Important aspects are to track progress, to visualize team performance, and to provide feedback about the quality of delivered results. Therefore, software cockpits integrate and visualize data from various data sources such as project plans, requirement management, version control, as well as test results. Each of these data sources represents a different perspective on the software project. The integrated view provided by a software cockpit produces a complete and persistent picture of the project status. This paper describes the architecture and functionality of a Web-based software cockpit and its implementation with open source software from the Business Intelligence area. Furthermore, the results and lessons learned from evaluating the practical benefits of the software cockpit in context of a large software development organization are presented. © 2010 IEEE. Concept, implementation and evaluation of a web-based software cockpit Business intelligence; Dashboard; Software cockpit Computer software selection and evaluation; Open systems; Project management; World Wide Web; Business intelligence; Dashboard; Data source; Monitoring and controlling; Open Source Software; Project plans; Requirement management; Software cockpit; Software development organizations; Software development projects; Software project; Software Project Control Center; Team performance; Test results; Track progress; Version control; Web-based softwares; Software design",Governance
1670,Intelligent decision support system for construction project monitoring,"Business Monitoring is a complex task and it has been noted that most of the reporting and analysis time is being spent on collecting data from the various systems. Over the past decade, a lot of research has been reported on Decision Support Systems (DSS) used in many fields. To improve the decision-making ability of an enterprise in construction management, information technology is being applied in each step of construction management. The problem is to organize and analyze the data in construction management to obtain quick analysis and decision support results. Various Data mining techniques have been used for clustering of data by using case examples. In this research we have applied Learning Vector Quantization (LVQ) to classify projects in one of the given categories and conducted a comparative analysis by using standard algorithm. A number of case examples have been used to verify the results and to obtain a comparison between various methodologies. © 2012 IEEE. Intelligent decision support system for construction project monitoring Business Intelligence; Decision Support Systems; Learning Vector Quantization Artificial intelligence; Competitive intelligence; Decision support systems; Information technology; Project management; Comparative analysis; Construction management; Construction projects; Decision support system (dss); Decision supports; Intelligent decision support systems; Learning Vector Quantization; Standard algorithms; Vector quantization",Financial management
1672,User-centered requirements elicitation for Business Intelligence solutions,"The implementation of Business Intelligence solutions, which support the planning, con-trolling and coordination processes of a company have been a high priority in the project portfolio of many IT departments in the last years. However, many of these projects could not fulfill the expectations due to an insufficient match with the actual business require-ments. The research project described in this paper aims at closing this gap by develop-ing an integrated, domain-specific method and tool support to enable user-centered re-quirements elicitation for BI solutions. User-centered requirements elicitation for Business Intelligence solutions  Research; Coordination process; Domain specific; Project portfolio; Requirements elicitation; Tool support; User-centered; Management science",Governance
1673,The electronic scientific portfolio assistant: Integrating scientific knowledge databases to support program impact assessment,"The US National Institutes of Health (NIH) supports basic and applied biomedical research by funding grants and contracts. To measure the outcomes and impact of their programs, NIH staff conduct program evaluations and undertake targeted investigations of research portfolios. Recently, the NIH launched the electronic scientific portfolio assistant (eSPA), a web-based analytics system based on linked scientific databases that provides quantitative information for program officers and planning and evaluation officials managing research portfolios. This system has improved the ability to create and collaboratively refine research portfolios, has reduced the time needed to collect and link outcomes data such as publications and patents, and is providing information used to support research management decisions. After describing the eSPA system, we provide examples of three eSPA evaluation projects that illustrate the impact of this system on NIH evaluation objectives. © The Author 2012. Published by Oxford University Press. All rights reserved. The electronic scientific portfolio assistant: Integrating scientific knowledge databases to support program impact assessment Automated system; Funding program evaluation; Research impact; Research portfolio database; knowledge; project management; publishing; research work",Risk management
1676,"55th Annual Conference of the Operational Research Society 2013, OR55","The proceedings contain 19 papers. The special focus in this conference is on Operational Research. The topics include: Learning from distributed project management; approximation schemes for quadratic Boolean programming problems and their scheduling applications; hybrid approach for solving the irregular shape bin packing problem with guillotine constraints; simulation-based optimisation using simulated annealing for crew allocation in the precast industry; a simulation model of dynamic resource allocation of different priorities packing lanes; modelling influential factor relationships using system dynamics methodology (fibre cement buildings as a case study); a simulation-based model for portfolio cost management; management of container terminal operations using Monte Carlo simulation; the mangle of O.R. Practice; Simulation modelling of through-life engineering services; towards cooperative simulation-aided decision making in the digital age and analytics for enabling strategy in sport. 55th Annual Conference of the Operational Research Society 2013, OR55  ",Monitoring and control
1679,Automation federation joins security project,"ISA's new Automation Federation has joined end-user companies and automation vendors to participate in a study being conducted by Wurldtech Analytics towards forming a Control Systems Security Foundation. Wurldtech will evaluate the feasibility of creating specifications and processes for the security testing and certification of critical control systems products. The new initiative aims to help define methods that suppliers can use to validate their products enable necessary levels of secure operation. Control system suppliers would be able to offer products that are proven to meet a standard set of minimum security requirements with this program. A proposal for setting up the organization will be completed by September, 2006 and it could be launched in early 2007. The certification organization that arises will work closely with existing standards groups, giving them documents that can be formulated as standards. Automation federation joins security project  Accident prevention; Automation; Control systems; Project management; Societies and institutions; Standards; Control system suppliers; Security requirements; Security testing; Wurldtech (CO); Security systems",Capacity management
1680,The steel making plant business intelligence project; [Projet d'aide à la décision stratégique à l'aciérie],"The Business Intelligence (BI) aims at providing the operational, tactic and strategic levels with information in order to support managers and specialists through the practice of analysis and decision making. The results of the project pertaining to the steel making plant are reported. The steel making plant business intelligence project; [Projet d'aide à la décision stratégique à l'aciérie]  Competitive intelligence; Decision making; Industrial plants; Project management; Strategic planning; Pertaining; Tactics; Steelmaking",Strategic alignment
1682,Aligning Process Automation and Business Intelligence to Support Corporate Performance Management,"In recent years, companies have tried to realize efficiency gains of business process orientation by implementing business process automation (BPA) systems coordinating the interaction of existing function-oriented enterprise applications according to the logic of specific business processes. Additionally, business intelligence (BI) applications have been installed to support management in measuring the company’s performance and deriving appropriate decisions. BI and BPA initiatives are usually organized as separate IS projects that are not properly coordinated, leading in turn to an unsatisfactory alignment of strategic management and operational business process execution. The proposed article is to depict how new trends in the IS area can lead to a convergence of BPA and BI and therefore deliver appropriate support for an integrated corporate performance management (CPM). The utilization of IS in an integrated CPM solution is illustrated by an example from the telecommunications sector. © 2004, Association for Information Systems. All rights reserved. Aligning Process Automation and Business Intelligence to Support Corporate Performance Management Business Intelligence; Corporate Performance Management; Data Warehousing; Enterprise Application Integration; Process Management Competitive intelligence; Information analysis; Process control; Business Process; Business process automation; Business-intelligence; Corporate performance; Corporate performance management; Efficiency gain; Enterprise applications integration; Performance management; Process automation; Process management; Data warehouses",Strategic alignment
1683,Insights and surprises from usage patterns: Some benefits of data mining in academic online systems,"With the rise of cyber-infrastructure in higher education research and teaching, new challenges surface when it comes to understanding users and usage. How, where, and when user activity gets captured and analyzed in academic online systems is particularly critical in internet-based systems. The flexibility that these open systems allow for in promoting easy integration of different technologies (e.g., applications layer, presentation layer, middleware, and data sources) has repercussions for usage analysis: round the clock access, unseen users, distributed logs, and huge volumes of cryptic data. This paper demonstrates how knowledge discovery solutions - particularly web usage mining methods - have been taken up to address these challenges in one higher education setting involving the Sakai collaboration and learning environment. The goals of this paper include: 1) providing some definitions and explications by example of specific data mining processes as they are actually being used; 2) describing the issues and challenges that motivate the use of data mining and 3) showing how data mining integrates with established project management best practices. Copyright 2008 ACM. Insights and surprises from usage patterns: Some benefits of data mining in academic online systems Associations; Clustering; Data mining; Log analytics; Web usage mining Data mining; Knowledge management; Middleware; Mining; Online systems; Open systems; Project management; Teaching; Associations; Best practices; Clustering; Cyber infrastructures; Data mining process; Data sources; Higher educations; Internet-based systems; Issues and challenges; Knowledge discoveries; Learning environments; Log analytics; Usage analysis; Usage patterns; User activities; Web usage mining; Cluster analysis",Strategic alignment
1684,Multidimensional knowledge spaces for strategic management - Experiences at a leading manufacturer of construction and mining equipment,"This paper provides a case study from the Business Intelligence Department (BID) of a leading manufacturer of construction and mining equipment. BID is a corporate department that provides crucial information for strategic management activities. In order to support these activities, concepts of knowledge management (KM) can be of substantial importance. Using a conceptual framework, we identified the deficits in managing knowledge assets at the BID and developed a prototype of a KM information system. In particular, we address the problem of an explicit specification of document classification schemas. Thus, we developed a methodological framework for modelling multidimensional knowledge spaces. Multidimensional knowledge spaces for strategic management - Experiences at a leading manufacturer of construction and mining equipment  Competition; Construction equipment; Customer satisfaction; Knowledge based systems; Mining equipment; Societies and institutions; Business Intelligence Department (BID); Competitive analysis (CR); Knowledge management (KM); Strategic management; Strategic planning",Strategic alignment
1686,Applying multi-agent system technique to production planning in order to automate decisions,"Coordinate and deliver information is vital for the financial and operational success of a company. The information is used for understanding and evaluating performance of a manufacturing company and making decisions based on incoming information. Information about orders but also parts to be purchased, assembled for the final product to be delivered, can streamline the production line to provide good quality products in the right time and to right costs at highest profit. For profit, costs are cut by reducing storage and searching for lowest price from established suppliers and providers on web as well as handling production planning automatically. To increase profit, we apply a multi-agent technique to production planning, which can automate business decision-making for the production line. The agents handle incoming orders, the production line, and search for information about the products at the intranet and the extranet. The outcome is decisions about the production line. The multi-agent solution becomes a complement to the production planning brought about by the company's enterprise resource planning system. © 2009 Springer Berlin Heidelberg. Applying multi-agent system technique to production planning in order to automate decisions Business Intelligence; ERP; Intelligent agents; Logistics; Meta-agents; Multi-agent systems; Supply chain Decision making; Enterprise resource planning; Extranets; Intelligent agents; Planning; Production control; Production engineering; Profitability; Project management; Resource allocation; Supply chain management; Supply chains; Business decisions; Business Intelligence; Enterprise resource planning systems; ERP; Lowest price; Making decision; Manufacturing companies; Meta-agents; Multi-Agent; Multi-agent techniques; Production line; Production Planning; Quality product; Multi agent systems",Value management
1687,Neural network based framework for optimization of enterprise resource planning,"Enterprise resource planning (ERP) systems are comprehensive information technology packages which aim towards managing and integrating all the business functions within a company. ERP is built on a totally integrated unique data warehouse. Business intelligence may be created based on the information stored in the data warehouse, to enable efficient management and operations monitoring of all areas of the organization. This paper introduces a new approach that employs soft computing (SC) techniques towards automatic optimization of the management decision making process which allows for an efficient strategic management of the company. Neural network based framework for optimization of enterprise resource planning Enterprise resource planning; Neural networks Data warehouses; Enterprise resource planning; Information management; Information technology; Optimization; Soft computing; Automatic optimization; Business functions; Information technology packages; Neural networks",Financial management
1688,"For ERP payback, look to winning add-on modules","The role of customer relationship management (CRM) or supplier relationship management (SRM) in getting better returns on investments (ROI) on the enterprise resource planning (ERP) sector is discussed. The integration of CRM or SRM with company's ERP helps in improving customer services and optimizing processes, and thereby reduces cost to the company. The other add-ons services provided by these management schemes, such as e-procurement and business intelligence, are also discussed. For ERP payback, look to winning add-on modules  Customer satisfaction; Industrial relations; Inventory control; Investments; Production control; Project management; Customer relationship management (CRM); Enterprise resource planning",Monitoring and control
1689,Setting the IT agenda for your business in 2007,"The views of senior manufacturers on various Information Technology (IT) agendas for 2007 for complex manufacturing companies for a successful business operation, cost cutting in business projects, and efficient and flexible production, are discussed. The manufacturers made observations to find success in harnessing IT and improve business processes, profitability, and business outlook. Top 10 recommendations are aimed at particular manufacturing business functions for efficient business products, services, and IT approaches. These recommendations include: synchronizing IT with the business, business intelligence, mobile systems, security, demand-driven manufacturing, Enterprise Resource Planning (ERP) and legacy business drivers, forecasting, manufacturing execution systems, Advanced Planning and Scheduling (APS) systems, and consolidation. These agendas lead to IT and network infrastructure consolidation, collaborative product development, and efficient distributed communications. Setting the IT agenda for your business in 2007  Computer supported cooperative work; Enterprise resource planning; Information technology; Product development; Project management; Scheduling; Advanced Planning and Scheduling (APS) systems; Business products; Network infrastructure; Professional aspects",Financial management
1690,PUZZLE: A concept and prototype for linking business intelligence to business strategy,"Business intelligence (BI) is a strategic approach for systematically targeting, tracking, communicating and transforming relevant weak signs 1 into actionable information on which strategic decision-making is based. Despite the increasing importance of BI, there is little underlying theoretical work, which directly can guide the interpretation of ambiguous weak signs. This paper gives an insight into the issue through a new strategic business intelligence system called PUZZLE. We describe this system and validate it by designing a prototype, test the system using in-depth interviews, and hold learning sessions in order to further knowledge about BI. The main results from tests show that: interpreting weak signs is potentially important for senior managers, consultants, and researchers; interpretation can be achieved gradually by bringing the weak signs together using a tracking form based upon the concept of actor/theme/weak signs/enrichment /links; interpreting weak signs is a complex process of establishing links between the weak signs. Final results show that the individual cognitive process appears heuristic when interpreting weak signs. Implications for strategic management practice and research are addressed. © 2002 Elsevier Science B.V. PUZZLE: A concept and prototype for linking business intelligence to business strategy Business intelligence; Creativity; Exploratory research; Fast response management; Ill-structured problem; Interpreting weak signs; Prototyping; Research engineering; Strategic business intelligence system; Strategic information system; Weak signs ",Strategic alignment
1694,Text analytics for homeland security technology development,"Homeland Security Technology (HST) encompasses many, often disparate fields ranging in complexity, focus, and maturity. From a managerial perspective it is challenging todetermine what fields are relevant to solving a HST problem, and to target investment in research which will meet emerging solution requirements. The challenge from the standpoint of a scientific researcher, already familiar with state-of-the-art in hisfield, is to determine what new approaches, other disciplines, and research partnerships might contribute to scientific innovation. Investigation of the scientific literature is the first step in identifying approaches relevant to a technology solution for each of these potential project team members. Ultimately, text analytics can be used both as a business intelligence tool by project managers and as a knowledge discovery tool by scientific researchers. Its objective is to provide a more dynamic representation of the state-of-the-art anddetermine the state-of-the-possible. © 2009 IEEE. Text analytics for homeland security technology development Casebased reasoning; Research and development management; Technological innovation; Technology assessment Computer crime; Engineering research; Innovation; Management science; Network security; Project management; Security systems; Technological forecasting; Technology; Business intelligence; Dynamic representation; Homeland security technology; Knowledge Discovery; New approaches; Project managers; Project team; Scientific literature; Technological innovation; Technology solutions; Research and development management",Strategic alignment
1697,Using Set of Experience in the Process of Transforming Information into Knowledge,"Some of the most complicated issues about knowledge are its acquisition and its conversion into explicit knowledge. Nevertheless, among all knowledge forms, storing formal decision events in a knowledge-explicit way becomes an important advance. The set of experience knowledge structure can help in achieving this purpose. Explicit knowledge of formal decision events emerges to help managers in decision-making because, usually, they use previous similar or equal decisions to help themselves in new decision-making processes. The Knowledge Supply Chain System (KSCS) is a platform proposed to administer formal decision events in a knowledge-explicit way. It is supported by Sets of Experience Knowledge Structure in the effective process of transforming information into knowledge. The purpose of this paper is to show how the set of experience knowledge structure is implemented into the KSCS. Fully developed, KSCS certainly would improve the quality of decision-making and could advance the notion of administering knowledge in the current decision-making environment. © 2006, IGI Global. All rights reserved. Using Set of Experience in the Process of Transforming Information into Knowledge business intelligence; conceptual modelling for knowledge management; DSS and expert systems; explicit knowledge; intelligent support systems; knowledge acquisition; knowledge and information management; knowledge representation; knowledge-based systems engineering; modelling concepts and information integration tools; strategic management information systems; XML ",Value management
1698,Knowledge creation in marketing based on data mining,"Survival in a knowledge-based economy is derived from the ability to convert information to knowledge. To do so, researchers and managers are increasingly relying on the field of data mining. Data mining identifies and confirms relationships between explanatory and criterion variables. Data mining are increasingly popular because of the substantial contributions they can make in converting information to knowledge. Marketing is among the most frequent applications of the techniques, and whether you think about product development, advertising, distribution and retailing, or marketing research and business intelligence, data mining increasingly are being applied. The purpose of this paper is to provide an overview of data mining, summarize how it impact on knowledge creation in marketing, and suggest future developments in marketing and data mining for both organizations and researchers. © 2008 IEEE. Knowledge creation in marketing based on data mining Data mining; Knowledge creation Data mining; Information management; Knowledge based systems; Marketing; Product development; Project management; Business intelligences; Future developments; IT impacts; Knowledge creation; Knowledge-based; Marketing researches; Knowledge management",Strategic alignment
1699,Business intelligence for strategic management in a technology-oriented company,"Business Intelligence (BI) is a tool that supports proactive strategic management and decision making by producing actionable information that enables the identification of emerging changes. A human-source intelligence network consisting of an organisation’s frontline employees is a valuable intelligence asset as the employees possess early and interpreted information about the business environment. The aim of the paper is to consider how this information can be effectively utilised in the BI process and further, to analyse how BI can be smoothly integrated in the process of strategic management. To achieve these objectives, literature research and case study approaches are applied. The case study examines a technology-oriented, global telecommunications company to obtain a pragmatic and insightful view and to improve the understanding of the research issue so sparsely covered in the literature. As a managerial implication, the authors suggest the establishment of an intelligence process and a network in the case company. © 2006 Inderscience Enterprises Ltd. Business intelligence for strategic management in a technology-oriented company BI; Business Intelligence; case study; decision making; human-source intelligence network; strategic management; telecommunications industry ",Financial management
1701,"Proceedings of the 36th Annual Hawaii International Conference on System Sciences, HICSS 2003","The proceedings contain 516 papers. The topics discussed include: an overview of systems enabling computer supported collaborative learning requiring immersive presence (CSCLIP); facilitating collaborative knowledge construction; facilitator's invisible expertise and supra-situational activities in a telelearning environment; community building and virtual teamwork in an online learning environment; revising the intellectual bandwidth model and exploring its use by a corporate management team; increase of potential intellectual bandwidth in a scientific community through implementation of an end-user information system; towards a cultural ontology for interorganizational knowledge processes; managing concurrent engineering across company borders: a case study; business intelligence explorer: a knowledge map framework for discovering business intelligence on the web; supporting the design of an inland container terminal through visualization, simulation and gaming; a collaborative project management architecture; and knowledge management in non-collocated environments: a look at centralized vs. distributed design approaches. Proceedings of the 36th Annual Hawaii International Conference on System Sciences, HICSS 2003  ",Governance
1702,A method to elaborate a qualifications framework for innovative design - Application to a master degree in innovation,"We describe the construction of a skill standard for a master degree education in innovation and products engineering. Our final goal is to check and assess pedagogy result for accreditation, to explain the purpose of each training unit and to facilitate the employability of trainees by the definition of new skills for these emergent jobs. Our method is inspired from the one used to elaborate occupational standard, a framework to evaluate qualifications in vocational training. From the occupational analysis, an occupational profile describes the missions, activities and skills. An activity is observable and requires multiple skills with various levels. Each skill is assessed by clustering a set of capabilities. Two adaptations of the initial method have been done; they concern the acquisition of information for understanding the activity, and a definition of skill levels that relates to roles taken in design teams (from a simple participation to the ability to manage a design session). The application is part of the curriculum of the second year of Master Innov-acteur in UTBM. It contains five core units (innovation strategic management, business intelligence, innovative design, emotional perception, and marketing). The method is applied to ""innovative design"" which aims to search and to validate solutions. This activity uses methodological tools like functional analysis, brainstorming, Triz .. but differs from classical problem solving in many aspects: problem and solution co-evolution, use of representations, reasoning modes for creativity, a process constantly refined by designers, its collective character, and the use of reflexive practice necessary to conduct it. Four core skills are identified and linked to sixty one capabilities. This study shows the possibility to apply this method to high level cognitive activities. Another result is the necessary combination of communication capabilities to technical ones in order to assess technical skills. A method to elaborate a qualifications framework for innovative design - Application to a master degree in innovation Accreditation; Design; Innovation; Qualifications framework for lifelong learning; Reflective Practice Accreditation; Behavioral research; Curricula; Design; Innovation; Management science; Personnel training; Problem solving; Cognitive activities; Communication capabilities; Life long learning; Methodological tools; Occupational standards; Reflective practices; Strategic management; Vocational training; Engineering education",Strategic alignment
1704,"Creation, management, and assessment of library screencasts: The regis libraries animated tutorials project","The Distance Learning Department of the Regis University Library is charged with providing library instruction for undergraduate and graduate students enrolled in accelerated courses offered at six distance campus locations and online. The department has created a series of animated online tutorials (i.e., screencasts) accessible via the library Web site. These interactive tutorials cover basic library services and resources and represent an effort to provide asynchronous bibliographic instruction to remote library users. This paper describes the use of screencasting software to create library tutorials and related issues including software options, production tips and techniques, and project management. In addition, the author describes the use of Google Analytics to record usage statistics and perform assessments. © 2008 Central Michigan University. Creation, management, and assessment of library screencasts: The regis libraries animated tutorials project Asynchronous instruction; Distance learning; Library instruction; Online tutorials; Screencasts ",Financial management
1705,Decision making system of petrochemical engineering budget based on Data Warehouse,"The application of data warehouse and data mining technology in petrochemical industry and the framework and implementation of the DW analysis system of budget work in petrochemical construction projects were illustrated and a mathematical model of data warehouse was proposed. Combining advanced technology of the information management and the experience of the project management, an intelligent solution was obtained about a decision making system of the petrochemical engineering budget based on models of analysis and optimization which were set up on the foundation of data mining algorithms. Decision making system of petrochemical engineering budget based on Data Warehouse Budget; Business intelligence; Construction; Data warehouse; OLAP ",Value management
1706,Making marketing happen: How great companies make strategic marketing planning work for them,"'Making Marketing Happen' is prompted by needs of practising managers who have found the traditional marketing planning texts to be ""fine in theory, but hard to apply to my special market"". In short, it holds that marketing planning fails for most companies because it either does not fit their organizational culture, their market conditions or both. Successful companies do not plan. They use a hybrid strategy making process including vision, incrementalism and planning. The ratio of these three things is critical and the right ratio is unique to every company. The author develops this argument and explains how companies can construct the right hybrid strategy making process for their situation. The book has been designed for those practising managers who need more than the planning text book. It will tell you: * Why attempts at planning are foiled by the market, the company culture or both. * How effective strategists do not plan, but use organizationally tailored strategy making processes * How to design the right process for your company and your market * How to know if the strategy you make is strong before you implement it. ""An incredibly practical and hands-on book concerned with the realities of doing strategic marketing planning to enhance customer and shareholder value. It is packed with new ideas and practical tools and should be on every marketing manager's desk."" Professor Nigel F Piercy, Professor of Marketing, Warwick Business School ""This book starts where most others finish - making the theory work in the real world. Having done an MBA and held several Senior Marketing positions, I recommend it both to practising marketers who already have a thorough understanding of marketing theory and also to MBA students who are eager to apply their knowledge within their own organisation's framework."" Mathias Aeberhardt, Director Business Intelligence Europe, Zimmer GmbH ""As a strategic marketing professionals working in the fast-moving and complex world of UK retail banking and personal financial services, inevitable time pressures demand that we must be highly selective in the material we choose to read. Within this context, I would recommend 'Make Marketing Happen' as an invaluable investment of scarce managerial time. The text is full of practical guidance and exciting insights into the world of strategic market planning and is presented in an informative and highly accessible format - well worth the read."" Dr Jansen Ryder, Product Manager, Halifax Bank of Scotland ""Making Marketing Happen is one of those rare marketing books that brings the right information to the table at precisely the right time. As a senior marketer in the highly competitive automotive industry, I have struggled with what the appropriate measures of marketing value are now, and what they should be in the future in order to maintain a competitive edge. Brian Smith has provided a detailed, pragmatic approach to marketing strategy with not only interesting examples but also with clear advise to make marketing really happen. The text is concise and clearly presented. Most of all, Making Marketing Happen is very readable and enjoyable to read."" Willem Verschuur, GM Product Marketing Management, Mitsubishi Motors Europe B.V. ""In the past writing on Marketing Planning seems to have been dominated by mechanistic planning models. This book brings a welcome insight into other approaches and their application and challenges managers to think about what works for them."" Gerry Johnson, Professor of Strategic Management, University of Strathclyde Graduate School of Business. © 2005, Dr Brian Smith. Published by Elsevier Ltd. All rights reserved. Making marketing happen: How great companies make strategic marketing planning work for them  ",Strategic alignment
1708,Analytics in Shanghai,"ABB located an Analytical Systems Integration Unit (SIU) in Shanghai, P.R. China, which started its operation in January, 2003, to better serve the needs of the Asian market, especially China. The Shanghai SIU, which served the rapidly expanding market in China, especially in the chemicals sector, now exports systems to other countries in the Asia Pacific region, and also servicing the European and middle Eastern markets. This SIU assumes total responsibility for the projects from purchase order to client handover, and ongoing service support is also available through ABB's strong service network. ABB's Shanghai SIU's fully integrated operation with a multi-discipline staff has the experience and track record to handle major global analyzer system projects. It has successfully delivered over 47 analyzer system projects since its start in the year 2003 and until the end of 2006. Analytics in Shanghai  Automation; Chemical industry; Marketing; Process engineering; Project management; Exports systems; Service network; Systems Integration Unit (SIU); Integrated control",Financial management
1710,A framework for enhancing competitive intelligence capabilities using decision support system based on web mining techniques,"Nowadays Competitive Intelligence (CI) represents one of the most important pieces in strategic management of organizations in order to sustain and enhance competitive advantage over competitors. There are some studies that claim that a successful strategic management is influenced by the accuracy of external environment's evaluation and, in the same time, in order to have correct and complete business strategies it is necessary to be sustained by competitive advantage. But till at the beginning of '80 the things were totally different. This paper will present the evolution and the objectives of CI, the results of using CI in organizations and how can be improved the CI process using tools and techniques provided by business intelligence (BI). The study will propose a framework of a decision support system based on web mining techniques in order to enhance capabilities of organization's competitive intelligence. Copyright © 2006-2009 by CCC Publications. A framework for enhancing competitive intelligence capabilities using decision support system based on web mining techniques Competitive intelligence; Decision making process; Information system; Web mining ",Strategic alignment
1711,Customer analytics projects: Addressing existing problems with a process that leads to success,"This article explicitly outlines an approach designed to allow optimal utilisation of Analytics in the industry setting. The paper focuses on the key stages of the Analytics process that have not been identified in previous Analytics methodologies and draws on industry, consulting and research experience to show that correct design of the project trajectory can allow the industry to fully realise the benefits that Analytics has to offer. As the case studies provided demonstrate, it is often the skipping of key stages, especially the preliminary analysis stage, that are currently responsible for preventing success of an Analytics project. It has been shown how, using the outlined approach, project can achieve maximum effectiveness and business buy-in. © 2007, Australian Computer Society, Inc. Customer analytics projects: Addressing existing problems with a process that leads to success Analytics industry case studies; Analytics projects management; Customer analytics; Data mining effectiveness; Industry analytics Data mining; Industry; Research; Analytics industry case studies; Customer analytics; Existing problems; Preliminary analysis; Research experience; Project management",Capacity management
1715,The NASA program management tool: A new vision in business intelligence,"This paper describes a novel approach to business intelligence and program management for large technology enterprises like the YOU.S. National Aeronautics and Space Administration (NASA). Two key distinctions of the approach are that 1) standard business documents are the user interface, and 2) a ""schema-less"" XML database enables flexible integration of technology information for use by both humans and machines in a highly dynamic environment. The implementation utilizes patent-pending NASA software called the NASA Program Management Tool (PMT) and its underlying ""schema-less"" XML database called Netmark. Initial benefits of PMT include elimination of discrepancies between business documents that use the same information and ""paperwork reduction"" for program and project management in the form of reducing the effort required to understand standard reporting requirements and to comply with those reporting requirements. We project that the underlying approach to business intelligence will enable significant benefits in the timeliness, integrity and depth of business information available to decision makers on all organizational levels. © 2006 IEEE. The NASA program management tool: A new vision in business intelligence  Computer software; Database systems; Decision making; Information retrieval; Societies and institutions; XML; Business documents; Program management tools (PMT); Schema; Competitive intelligence",Financial management
1718,Diagnosis of initial data to design a project model enabling the implementation of a business intelligence process for design and construction engineering companies in Cuba,"Business Intelligence Process during recent years have provided a growing number of companies all over the world with the ability to control internal and external environments and to create appropriate know-how for decision making processes; to provide a new Strategic Management orientation; to reduce the potential risk margin and to increase competitiveness. Cuba represents a leading sector of Design and Construction Engineering Companies for the implementation of this new Cuban Management and Business Administration System, seeking to diversify services in the national and international construction market. The purpose of this paper is to demonstrate the use of different tools, such as the expertise calculation index procedure, orientation and information interviews, surveys, category correlation and focus groups, which allowed the diagnosis of initial data situation on the project approach for the implementation of such a process. The most important outcomes were the definition of organization stages to be implemented, by means of a Business Intelligence Project, those elements that limit and facilitate the process. From these elements, an on-going scientific investigation design was created, counting with the cosponsorship from several universities and companies. Concrete results are expected to design a Business Intelligence Process for such companies and a Manual to guide researchers and executive staff involved in the improvement of similar entities of management. Diagnosis of initial data to design a project model enabling the implementation of a business intelligence process for design and construction engineering companies in Cuba Construcción; Dirección de Proyectos; Inteligencia empresarial ",Governance
1722,Visual representation of construction management data,"Construction projects are associated with voluminous and often unstructured data sets, generated in support of construction management functions. Project managers face the challenge of making meaningful deductions from this data. A central contribution of this paper is that visual analytics can provide a means of analyzing data from various dimensions of a project to extract information in aid of decision making and helping to explain reasons for performance to date. Questions posed relate to the role of visual analytics in the execution and post-construction phases of a project, data representations and transformations of specific interest, and the kinds of visual representations and interactions that provide useful insights to management personnel, help explain performance, or assist with communication. Principles of designing effective visual analytics solutions for various construction management functions and applicable to the associated analytic reasoning tasks, data representations and transformations, and visual representations including relevant interaction features are described. Emphasis is placed on the choice of visual representations along with discussion of approaches for validating the usefulness of the visual analytics solutions proposed. The notion of context dimensions and performance dimensions for representing construction projects is introduced as part of the formulation of visual representation designs. To demonstrate the application of the concepts presented, data sets from two different projects were used to produce visual representations helpful for analytical reasoning about change order management data. A detailed assessment is given of several of the images presented in terms of strengths and weaknesses, and interaction features desired. The findings of the paper in terms of principles, concepts and lessons learned should prove helpful to those wishing to apply visual analytics to a broad range of construction management functions. © 2009 Elsevier B.V. All rights reserved. Visual representation of construction management data Analytics design principles; Change order data; Construction project data; Data representation and transformations; Performance insights; Visual analytics Construction industry; Set theory; Analytics design principles; Change order data; Construction project data; Data representation and transformations; Performance insights; Visual analytics; Project management",Strategic alignment
1724,Data warehousing infusion and organizational effectiveness,"Data warehousing (DW) has emerged as one of the most powerful technology innovations in recent years to support organization-wide decision making and has become a key component in the information technology (IT) infrastructure. Proponents of DW claim that its infusion can dramatically enhance the ability of businesses to improve the access, distribution, and sharing of information and provide managerial decision support for complex business questions. DW is also an enabling technology for data mining, customer-relationship management, and other business-intelligence applications. Although data warehouses have been around for quite some time, they have been plagued by high failure rates and limited spread or use. Drawing upon past research on the adoption and diffusion of innovations and on the implementation of information systems (IS), we examine the key organizational and innovation factors that influence the infusion (diffusion) of DW within organizations and also examine if more extensive infusion leads to improved organizational outcomes. In this paper, we conducted a field study, where two senior managers (one from IS and the other from a line function) from 117 companies participated, and developed a structural model to test the research hypotheses. The results indicate that four of the seven variables examined in this paper-organizational support, quality of the project management process, compatibility, and complexity-significantly influence the degree of infusion of DW and that the infusion, in turn, significantly influences organization-level benefits and stakeholder satisfaction. The findings of this paper have interesting implications for both research and practice in IT and DW infusion, as well as in the organization-level impact of the infusion of enterprise-wide infrastructural and decision support technologies such as DW. © 2008 IEEE. Data warehousing infusion and organizational effectiveness Data warehousing (DW); Field study; Information technology (IT) infrastructure; Infusion; Innovation diffusion; Organizational decision support; Organizational outcomes Administrative data processing; Communication; Data warehouses; Decision making; Decision support systems; Dielectric waveguides; Industry; Information management; Knowledge management; Management; Model structures; Planning; Problem solving; Project management; Public key cryptography; Quality management; Research; Search engines; Semiconductor doping; Societies and institutions; Strategic planning; Technology; (algorithmic) complexity; (OTDR) technology; Adoption and diffusion; Business Intelligence (BI); Customer relation ship management (CRM); Decision supports; Failure rates; Field studies; Information systems (IS); Information technology (IT); Line function; Managerial decisions; Organizational effectiveness; Organizational supports; Project management process (PNP); Senior managers; Structural model (SM); technology innovations; Innovation",Value management
1725,Business intelligence - A successful approach to creating performance management system. Case study: Application and prospect of business intelligence in metallurgical enterprises in Romania,"Business intelligence refers to in-depth analysis of company data for better decision-making. The technology and processes that make this analysis possible take unwieldy collections of information and translate them into organized, readily-accessible, human-readable compilations of data. With an effective BI tool, companies can easily track their own operations, their customers' activity patterns, and industry trends. These fact-based assessments help companies work toward specific goals with confidence. The research looks upon the challenges of implementing BI technologies in matallurgical companies and analyzes Romania's situation. Conclusions refer to the way BI helps the business community in general nowadays emphasizing the main trends of BI sector in Romania. Main trends of economic developments of the Romanian industries are also considered when forecasting the dynamics of BI sector. Business intelligence - A successful approach to creating performance management system. Case study: Application and prospect of business intelligence in metallurgical enterprises in Romania Business intelligence; Information technologies; Strategic management ",Financial management
1727,Support of e-business by business intelligence tools and data quality improvement,"The aim of this paper is to evaluate what electronic commercial and business opportunities firms and organizations in this modern world have and which opportunities are best suited for them. The paper categorizes and describes currently available e-business tools in the electronic business model proposed by Timmers. The next part discusses the tools and activities for creating successful implementations of e-business within companies and organizations, in particular, business intelligent tools, strategic management tools, electronic forms, and competitive intelligence. The paper then offers general recommendations and a summary of which tools used for which purposes depending on the size of business and organizations. © 2010 IEEE. Support of e-business by business intelligence tools and data quality improvement  Competition; Competitive intelligence; Business intelligent; Business opportunities; Data quality; E-business tools; eBusiness; Electronic business model; Electronic forms; Strategic management tools; Electronic commerce",Strategic alignment
1728,The ethics of business intelligence,"A review of the strategic management, policy, information management, and the marketing literature reveals that many large and medium sized companies now collect and use business intelligence. The number of firms engaging in these activities is increasing rapidly. While the ""why is"" and ""how is"" of this practice have been discussed in the academic and professional literature, the ethics of intelligence gathering have not been adequately discussed in a public forum. This paper is intended to generate discussion by advancing criteria which could be used as the basis for judging actions of those involved in business intelligence and for creating reasonable policies in this sensitive area of practice. © 1994 Kluwer Academic Publishers. The ethics of business intelligence  ",Governance
1729,A conceptual model for technology intelligence,"Companies are finding it increasingly difficult to keep abreast of the latest technology developments and trends. Technology intelligence provides an organisation with the capability to capture and deliver information in order to develop an awareness of technology threats and opportunities. A conceptual model has been developed to support the establishment and operation of technology intelligence systems. The model consists of three tiers: (a) a framework level, (b) a system level, and (c) a process level. The ‘framework’ level maps the information requirements and knowledge gaps of the decision-makers to the business intelligence activities of an organisation. The ‘system’ level provides a mechanism to both tailor and configure a system architecture and its operational modes (mine, trawl, target, scan) to the actual intelligence needs. The ‘process’ level consists of an operating cycle for running a technology intelligence system. The cycle is composed of six phases, namely: coordinate, search, filter, analyse, document and disseminate. © 2006 Inderscience Enterprises Ltd. A conceptual model for technology intelligence competitive intelligence; strategic management; Technology Intelligence (T-Intel); technology planning; technology scanning ",Capacity management
1731,The role of business intelligence in strategic management,[No abstract available] The role of business intelligence in strategic management Business intelligence; Decision making; Querying tools; Reporting tools ,Value management
1732,The art of outsourcing,"Outsourcing aspects of the advertizing development function which enables the companies to focus more effectively on their core competencies, are discussed. The solution provider can employ targeting strategies to understand their client's prospects and fine-tune messages to most effectively reach the audiences. The experts can utilize consumer and media research as well as database marketing and analytics to help their clients best reach their targeted audiences and improve campaign performance. It is suggested that in outsourcing production needs, companies can take advantages of the information processing and targeting services for which they do not have resources to do in-house. The art of outsourcing  Decision Making; Graphic Methods; Marketing; Outsourcing; Packaging; Printing; Resources; Decision making; Graphic methods; Marketing; Packaging; Printing; Project management; Resource allocation; Strategic planning; Ace Hardware (CO); Advertising development; Image production; Vertis (CO); Outsourcing",Strategic alignment
1733,Active process-reuse model for collaboration,"We discuss the reuse of process knowledge through conceptualized workflow patterns. Whilst knowledge reuse in the lifecycle of projects has been widely studied, most of the known solutions have been focusing on the reuse of communicated information - the process results - isolated from the processes themselves. Consequently, the focus on the immediate reuse of communicated project information (i.e. architectural and structural solutions, drawings, technical specifications and so on) intensified the research in the field of information retrieval and contextualization of the retrieved information. Actual processes, actors and tools that have led to the results of the work usually remain unrecorded, the latter was considered technically almost impossible. In contrast to the intuitive ad-hoc reuse of parts of existing results in a given context, ad-hoc reuse of parts of previously executed processes - together with processes' meta-descriptors - is not so straightforward. In this paper we identify the methods and media in which processes are modeled and executed as a major barrier for process reuse. Based on the analysis of process modeling techniques we suggest a novel methodological approach and a conceptual solution for a collaboration system supported by active process models - utilizing the idea of conceptualized reusable workflow patterns. We present an architecture supporting process reuse, and two early prototypes. At the end, we also outline issues for further research. Active process-reuse model for collaboration Active process modeling; Business intelligence; Collaboration; Process reuse; Workflow patterns Architecture; Information retrieval; Information theory; Process control; Active process modeling; Process reuse; Workflow patterns; Project management",Strategic alignment
1734,Customer Relationship Management systems (CRM): Legal issues,"Customer Relationship Management, known as CRM, is currently a very important topic due to its economic impact on organizations. It is undergoing notable progress from the technological point of view. An effective CRM environment is highly complex as it combines different types of systems: data warehouse, business intelligence, data mining, electronic commerce, etc. However, the legal aspects of CRM, the laws affecting it among others, cannot be forgotten. In this work we review the Spanish and European Community norms that should be taken into account when implanting a CRM system. Customer Relationship Management systems (CRM): Legal issues Audit; Customer Relationship Management; Datawarehouse; Security Data mining; Electronic commerce; Legacy systems; Project management; Servers; Customer relationship management (CRM); Datawarehouses; Software engineering",Risk management
1735,Management online: Mining the full potential of the intranet,"Attention is given to the benefits that the intranet can confer in the areas of accounting and management. Three intranet-based technology initiatives at the consulting engineering and information management firm Psomas are described. It is noted that the systems developed have aided daily operations by serving as information management, business intelligence, and accounting tools. The first provides up-to-the-minute Web-based reporting of the financial status of all projects throughout the company. Customized project portfolios, similar to stock portfolios, allow a group of projects to be consolidated so that the project manager can review them quickly and track particular types of information. The second system calls attention to collection problems before they become serious, and the third reduces the number of hours employees spend on data entry and data management by making it easier than ever to fill out, submit, and approve time sheets. Management online: Mining the full potential of the intranet  Automation; Competitive intelligence; Costs; Data acquisition; Information management; Information technology; Intranets; Project management; Accounting tools; Stock portfolios; Time sheets; Work flow automation; Online systems",Financial management
1737,An approach to integrating bridge and other asset management analyses,"Bridge management systems have proved to be of great value for developing preservation strategies, understanding life-cycle costs, and analyzing the benefits and costs at the alternatives at the project and program levels. However, a critical issue in using bridge management systems to address broader asset management needs is that an agency must integrate results generated from bridge management systems with results from other asset management systems in making tradeoff decisions. National Cooperative Highway Research Program (NCHRP) Project 20-57 was undertaken to provide new analytical tools to support asset management. The project's research objectives emphasize the need for tools that help agencies to make the difficult tradeoff decisions for resource allocation while considering both asset preservation concerns as well as the broader set of policy objectives (e.g., safety and economic development) that must be taken into account when making investments in transportation assets. This paper describes a new analytical tool developed as part of the project, AssetManager NT. The system works with 10- to 20-year simulation results from existing asset management systems and allows users to explore the consequences of different levels of investment within and across asset classes. AssetManager NT is analogous to a data warehouse for simulation results. It provides an interpolation engine, together with a set of business intelligence data exploration capabilities that operate on these simulation results so that program managers can quickly and easily understand the relationship between expenditure levels and performance across multiple dimensions. Dimensions in AssetManager NT are: time (up to 20 single or multi-year periods), asset classes (e.g. pavement and bridges), geographic areas (e.g. districts or regions), and network subsets (e.g. based on functional class or ownership). For example, the tool can be used to understand how bridge and pavement condition on trunkline routes in the urbanized portion of a state might be expected to change over the next 10 years given different allocations of the budget across these two asset classes. Some currently available infrastructure management systems provide built-in capabilities to explore simulation results, and off-the-shelf commercial asset management solutions are available that provide integrated analysis capabilities for multiple asset classes. AssetManager NT addresses the needs of agencies wishing to stay with existing tool sets and/or leave the door open for future implementation of best-of-breed tools that have an appropriate level of sophistication for specific asset classes. AssetManager NT does not attempt to implement a consistent approach to deterioration modeling, treatment selection or optimization across asset classes. Rather, the tool serves a single function - investment vs. performance what-if analysis. This is in keeping with a modular, service-oriented approach to software architecture. It also recognizes that there are typically distinct user communities for asset management tools - one concerned in more detailed technical aspects of treatment selection and project prioritization; the other concerned with higher-level resource allocation tradeoffs. Figure 1 presents the key organizing concepts of AssetManager NT. Using AssetManager NT involves a three step process: running simulations in external tools and creating input files, using (Figure Presented) the AssetManager NT preprocessor to create a scenario file, and finally, conducting what-if analysis for a given AssetManager NT scenario. Companion ""robot"" tools also were developed to produce inputs needed by AssetManager NT from systems such as the American Association of State Highway and Transportation Officials (AASHTO) Pontis Bridge Management System. AssetManager NT was tested at New York and Montana state DOTs. After reviewing the system, staff at both agencies observed that AssetManager NT was a useful tool that could be used both in the context of periodic long-range infrastructure needs analysis (for long range plan updates), as well as for the annual program development cycle. Both states have embraced performance-based planning and programming, and were therefore very receptive to tools that could support their established business processes for making budget decisions based on performance tradeoffs. Both of these agencies were already analyzing infrastructure investment tradeoffs, but AssetManager NT makes this process easier. The testing process highlighted some limitations of AssetManager NT. First, for some types of assets - notably bridges, expenditures tend to be ""lumpy"", i.e. there are a relatively small number of high cost treatments that must be done on an all-or-nothing basis. In this context, performance predictions obtained from piecewise linear interpolation may not be realistic in certain circumstances. A second limitation is that the tool works only with aggregated performance results, and users can not drill down to specific projects or assets that underlie these results. A new project being launched by the American Association of State Highway and Transportation Officials (AASHTO) AASHTOW are program will implement the current version of AssetManager NT in ten state DOTs and then develop a plan for future enhancement of the tool. Enhancements will be determined based on input from the participating agencies. Some of these enhancements may address the limitations discussed above that were identified in the initial testing process. It is likely that changes to the tool's software architecture and platform will be made as well - the tool is currently a stand-alone desktop application. The AASHTOW are project will also encompass a second tool developed in NCHRP Project 20-57 - AssetManager PT, which works with a list of proposed projects and addresses shorter range tradeoff questions. Integration of the NT and PT tools will be explored as one way to address the desire for additional drill-down capabilities. © 2006 Taylor & Francis Group. An approach to integrating bridge and other asset management analyses  Application programs; Asset management; Budget control; Data warehouses; Deterioration; Drills; Economics; Highway administration; Highway planning; Infill drilling; Information services; Interpolation; Investments; Life cycle; Maintenance; Network architecture; Pavements; Piecewise linear techniques; Resource allocation; Service oriented architecture (SOA); Software architecture; American Association of State Highway and Transportation Officials; Asset management systems; Bridge management system; Infrastructure investment; Infrastructure management system; National Cooperative Highway Research Program; Piecewise linear interpolations; Service-oriented approaches; Costs",Strategic alignment
1738,Business intelligence and data warehouse - Technological support for decisional management in geographical information systems,"In a world which is in a continuous motion, in which hardware and software products almost fail to be worn-out as they are already replaced by new ones, because their generation has already become «out-of-date», in a world in which «information» is considered the main resource within a company, the need for executive information systems which lie at the basis of strategic management's decision-making is quite obvious. The viability and success of modern enterprises are subject to the increasing dynamic of the economic environment, so they need to adjust rapidly their policies and strategies in order to respond to sophistication of competitors, customers and suppliers, globalization of business, international competition. Perhaps the most critical component for success of the modern enterprise is its ability to take advantage of all available information - both internal and external. The IT solutions designed to address these challenges have been developed in two different approaches: structured data management (BI - Business Intelligence) and unstructured content management (KM - Knowledge Management). Integrating Business Intelligence and Knowledge Management in new software applications designated not only to store highly structured data and exploit it in real time but also to interpret the results and communicate them to decision factors provides real technological support for Strategic Management. Integrating Business Intelligence and Knowledge Management in order to respond to the challenges the modern enterprise has to deal with represents not only a ""new trend"" in IT, but a necessity in the emerging knowledge based economy. These hybrid technologies are already widely known in both scientific and practice communities as Competitive Intelligence. But, behind an executive information system there is actually a data warehouse. Business intelligence and data warehouse - Technological support for decisional management in geographical information systems Business intelligence; Competitive intelligence; Data warehouse; Geographical information system; Knowledge contribution Competition; Competitive intelligence; Data warehouses; Decision making; Geographic information systems; Industry; International trade; Knowledge based systems; Knowledge engineering; Knowledge management; Management information systems; Management science; Strategic planning; Business intelligence; Content management; Continuous motions; Critical component; Decision factors; Economic environment; Enterprise IS; Executive information systems; Geographical Information System; Hardware and software; Hybrid technology; International competitions; IT solution; Knowledge based economy; Knowledge contribution; Real time; Software applications; Strategic management; Structured data; Technological supports; Information technology",Financial management
1739,The balanced scorecard and its practical applications in oracle balanced scorecard,"In today's rapidly changing world, strategy management is a vital element of management. In connection with this, there exists a necessity to translate strategy into definite goals and steps. This facilitates communication of corporate strategy to staff at a company, and execution is simpler for an employee of the firm. Furthermore, it is imperative to maintain feedback on information about the state of performance objectives. The methodology of the Balanced Scorecard proves most appropriate for this task. Implementing the Balanced Scorecard, however, cannot be successful without the support of a high-quality business information system as, especially within a larger business, this concept will not function if not working with large amounts of data and information. This paper deals with applying Balanced Scorecard methodology in business information systems, focusing on practical applications in Oracle Balanced Scorecard. Beginning with a brief overview of current knowledge about the Balanced Scorecard, it includes a discourse on the essential logic of this methodology and a presentation of its individual components. Attention is also paid to the role of the Balanced Scorecard in strategic management. Following this, focus switches to the support of the Balanced Scorecard in information systems. In this section of the paper, concentration is placed on Business Intelligence and Corporate Performance Management and their relation to the above-mentioned methodology. In the main body of this study, practical solutions are addressed that support this methodology within the information system of Oracle. This part is divided into two chapters. First, a solution is dealt with from the perspective of Viewers (end users). The major benefits for employees of the company are highlighted, as is the influence on the effectiveness of their work. The other portion is devoted to the duties of designers - people transposing the Balanced Scorecard into an information system. Therefore, it constitutes a way of setting up and customizing the Balanced Scorecard within an information system. Finally, this paper summarizes the main benefits of integrating the Balanced Scorecard and an information system, such as aligning corporate goals with those of employees, improved communication and better feedback. The balanced scorecard and its practical applications in oracle balanced scorecard Balanced scorecard; Corporate performance management; Information system Information systems; Information use; Management information systems; Balanced scorecards; Business information systems; Corporate performance; Corporate performance management; Corporate strategies; Management IS; Performance management; Performance objective; Simple++; Strategy management; Information management",Governance
1740,Building successful technology commercialization teams: Pilot empirical support for the theory of cascading commitment,"Improving the process of commercializing a technology from a public lab requires a deep understanding of which factors actually contribute to successful commercialization. Such factors are complex, including the quality of the technology itself, choice of transfer mechanism, quality of business intelligence, project evaluation and selection techniques, team building processes, organizational structures, reward and penalty structures, financial support, human resources support, and project management tools, among others. This article focuses on the improvement of team-building processes. It describes a pilot empirical test of the theory of cascading commitment, using data from a study of 34 technology transfer cases from 5 different Canadian federal labs. Of the 34 cases, 20 were successful, 11 were unsuccessful, and 3 were uncertain. In this paper, ""success"" was defined as the private sector manufacturer's perception of how well the project attained the profit objective associated with sales of a new product which embedded the new technology. The pilot results provide directional support for the key propositions of the theory: 1) a complete team of both public and private organizations is necessary; 2) a complete team of key individuals (""linchpins"") from within each organization is necessary; 3) there is an optimal sequence for recruiting the organizational partners; 4) there is an optimal stage for recruiting each organizational partner; 5) high commitment from every linchpin is necessary; 6) linchpin commitment to the team is determined by a variety of factors; and 7) the probability of success is improved if the team of linchpins remains intact until the commercial launch is achieved. Some provisional implications for management practice are provided, as well as suggestions for future research. © 2000 Kluwer Academic Publishers. Building successful technology commercialization teams: Pilot empirical support for the theory of cascading commitment  Management science; Project management; Stress corrosion cracking; Technology transfer; Transfer cases (vehicles); Management practices; Organizational structures; Private organizations; Probability of success; Project management tools; Selection techniques; Team-building process; Technology commercializations; Human resource management",Strategic alignment
1741,Illinois workers want lagging public-works projects bolstered,"The shortage of jobs for the workers in the construction industry in lllinois is discussed. According to McGrew-Hill construction analytics, the housing grew 14% in the first six months in 2003. Workers are supporting Democratic Sen.John Kerry in the presidential elections for his union objectives of voting in the election. People have different viewpoints for supporting and standing in against of the democratic and republican party candidates for presidential elections. Illinois workers want lagging public-works projects bolstered  Buildings; Economic and social effects; Housing; Job analysis; Personnel; Project management; Public policy; Endorsement; Presidential elections; Union objectives; Voting; Construction industry",Capacity management
1742,Implementing six sigma: Are you getting results fast enough?,"Six Sigma focuses on high return projects that will maximize customer satisfaction. Is every part of your Six Sigma effort providing a maximum return, including your software? Do your graphics come alive, helping you make key discoveries with the click of your mouse? Can you easily find relationships among numerous variables without sorting through pages of pvalues? Do your designed experiments provide the maximum information using the minimal resources? The presentation covers concepts you typically do not learn in analytics training and presents techniques that shorten your journey from question to answer. Implementing six sigma: Are you getting results fast enough?  Computer software; Costs; Data reduction; Project management; Societies and institutions; Sorting; Customer expectations; Data distribution; Professionalism; Customer satisfaction",Risk management
1743,Business intelligence in large organizations: Integrating which data?,"This paper describes a novel approach to business intelligence and program management for large technology enterprises like the YOU.S. National Aeronautics and Space Administration (NASA). Two key distinctions of the approach are that 1) standard business documents are the user interface, and 2) a ""schema-less"" XML1 database enables flexible integration of technology information for use by both humans and machines in a highly dynamic environment. The implementation utilizes patent-pending NASA software called the NASA Program Management Tool (PMT) and its underlying ""schema-less"" XML database called Netmark. Initial benefits of PMT include elimination of discrepancies between business documents that use the same information and ""paperwork reduction"" for program and project management in the form of reducing the effort required to understand standard reporting requirements and to comply with those reporting requirements. We project that the underlying approach to business intelligence will enable significant benefits in the timeliness, integrity and depth of business information available to decision makers on all organizational levels. © Springer-Verlag Berlin Heidelberg 2006. Business intelligence in large organizations: Integrating which data?  Data reduction; Database systems; Decision making; Information theory; Intelligent agents; Societies and institutions; User interfaces; Patent pending; Program management; Standard business documents; Technology information; Competitive intelligence",Financial management
1744,Business intelligence - A helping hand for the strategic management,"The Information Age has shaped the business world and has obliged the companies to adapt and design their strategies accordingly. In order to develop and execute a strategy, the company's management must make clear decisions on the activities that it will prioritize in allocating resources. This paper focuses on the main phases of strategic management and how these have been changed by the information technology boom that started at the end of the last century. The authors have also looked into the role that the Chief Information Officers have today, analyzing the job concept since its appearance. The final part of this research focuses on the way the business intelligence could be implemented in the strategic management process while the conclusions summarize main findings on the impact that the information technology has had upon business processes in general and strategic management in particular. Business intelligence - A helping hand for the strategic management Business intelligence; Chief information officers; Information technologies; Strategic management Competitive intelligence; Information technology; Knowledge management; Strategic planning; Business Process; Chief information officer; Information age; Research focus; Strategic management; Management science",Governance
1746,Contributors to construction delays,"Projects can be delayed for a large number of reasons. The third phase of an investigation into such factors focused on the causes of construction delays in Hong Kong. A questionnaire was based on 83 factors identified in previous phases of the investigation. Analysis of the responses reveals differences in perceptions of the relative significance of factors between clients, consultants and contractors. There was general agreement about the relative importance of delay factors such as unforeseen ground conditions. Improving productivity is a useful approach to controlling delays. Important factors affecting productivity itself are thus examined in more depth, with a view to enhancing productivity and reducing delays. The conclusions of this phase of the investigation include a ranking of factors and factor categories that are perceived by different groups of project participants to contribute to delays. For example, higher-ranking factors, such as unforeseen ground conditions, and factor categories such as project-related factors, are found to merit special management attention in countering construction delays. The high degree of disagreement as discerned between the groups of clients, consultants and contractors is indicative of their experiences, possible prejudices and lack of effec-tive communication. It is also confirmed that productivity and other non-scope factors such as effective communications should supplement the project scope factors incorporated into the construction time prediction models that were proposed in the previous phases of this investigation. Contributors to construction delays Delay; Hong Kong; Productivity; Project management; Time Contractors; Predictive analytics; Productivity; Construction delays; Delay; Effective communication; Factors affecting productivity; Ground conditions; Hong-kong; Project participants; Time; Project management",Risk management
1747,Case study: Visual analytics in software product assessments,"We present how a combination of static source code analysis, repository analysis, and visualization techniques has been used to effectively get and communicate insight in the development and project management problems of a large industrial code base. This study is an example of how visual analytics can be effectively applied to answer maintenance questions and support decision making in the software industry. We comment on the relevant findings during the study both in terms of used technique and applied methodology and outline the favorable factors that were essential in making this type of assessment successful within tight time and budget constraints. ©2009 IEEE. Case study: Visual analytics in software product assessments  Computer software; Computer software maintenance; Visualization; Budget constraint; Industrial codes; Software industry; Software products; Static sources; Visual analytics; Visualization technique; Project management",Strategic alignment
1748,Using business intelligence solutions for achieving organization's strategy: Arab international university case study,"Business Intelligence (BI) is becoming an important IT framework that can help organizations managing, developing and communicating their intangible assets such as information and knowledge. Thus it can be considered as an imperative framework in the current knowledge-based economy arena. In this paper, we will explain the role BI is playing in providing organizations with a way to plan and achieve their business strategy. We will experiment this role using a case study in the field of high education, especially helping one of the new private university in Syria (Arab International University)planning and achieving their business strategy. © 2009 IIJ. Using business intelligence solutions for achieving organization's strategy: Arab international university case study Business intelligence; Data mining; Strategic management ",Governance
1749,Regional intelligence: Distributed localised information systems for innovation and development,"Technological information is recognised as an important factor shaping regional systems of innovation and innovative regions. However, little has been written on how regions set up and manage this vital resource. This paper focuses on regional intelligence: distributed information systems localised over a region allowing continuous update and learning on technologies, competitors, markets, and the environment. We start by defining regional intelligence with respect to the concepts of business intelligence, organisational, and collective intelligence. We look at a number of case studies and experiences gained in the context of EU regional innovation and regional economic strategies, which highlight early forms of regional intelligence. We examine the fundamental information modules making up regional intelligence, including, R&D dissemination, technology and market watch, company benchmarking and competition analysis, regional foresight, and regional performance. We discuss the integration of distributed information systems and solutions which may be given to consolidate public content applications with the internal information systems of companies, and the role of information integration in the continuous making and remaking of innovative regions. Regional intelligence: Distributed localised information systems for innovation and development Collective intelligence; Distributed intelligence; Information integration; Innovative regions; Organisational intelligence; Regional benchmarking; Regional foresight; Regional innovation systems; Regional intelligence; Regional observatories Benchmarking; Competition; Project management; Research; Research and development management; Societies and institutions; Technology transfer; Collective intelligence; Distributed intelligence; Information integration; Innovative regions; Organizational intelligence; Regional benchmarking; Regional foresight; Regional innovation systems; Regional intelligence; Regional observatories; Information management",Governance
1750,Data mining for business applications,"Data Mining for Business Applications presents state-of-the-art data mining research and development related to methodologies, techniques, approaches and successful applications. The contributions of this book mark a paradigm shift from ""data-centered pattern mining"" to ""domain-driven actionable knowledge discovery (AKD)"" for next-generation KDD research and applications. The contents identify how KDD techniques can better contribute to critical domain problems in practice, and strengthen business intelligence in complex enterprise applications. The volume also explores challenges and directions for future data mining research and development in the dialogue between academia and business. Part I centers on developing workable AKD methodologies, including: domain-driven data mining post-processing rules for actions domain-driven customer analytics the role of human intelligence in AKD maximal pattern-based cluster ontology mining Part II focuses on novel KDD domains and the corresponding techniques, exloring the mining of emergent areas and domains such as: social security data community security data gene sequences mental health information traditional Chinese medicine data cancer related data blog data sentiment information web data procedures moving object trajectories land use mapping higher education data flight scheduling algorithmic asset management Researchers, practitioners and university students in the areas of data mining and knowledge discovery, knowledge engineering, human-computer interaction, artificial intelligence, intelligent information processing, decision support systems, knowledge management, and KDD project management are sure to find this a practical and effective means of enhancing their understanding of and using data mining in their own projects. © 2009 Springer Science+Business Media, LLC All rights reserved. Data mining for business applications  ",Strategic alignment
1751,What every technical communicator should know about metadata,"Technical Communicators who begin working with content management systems, knowledge bases, portals, data warehouses, or information retrieval systems discover they are expected to know how to work with metadata. Metadata is ""data about data."" It can describe data or content (databases, data modeling, data access and reporting, data movement, data stewardship, data quality); organizations (business rules, process stewardship, data users, project management); content management and information retrieval (document properties, revision and change control, reference and navigation, document standards); and business intelligence (decision support, competitive intelligence). Metadata management can positively impact productivity and the quality of web and documentation projects. What every technical communicator should know about metadata  Information management; Information retrieval systems; Information technology; Knowledge based systems; Project management; Data movement; Data stewardship; Metadata",Value management
1753,Processing online analytics with classification and association rule mining,"Business performance measurements, decision support systems (DSS) and online analytical processing (OLAP) have a common goal i.e., to assist decision-makers during the decision-making process. Integrating DSS and OLAP into existing business performance measurements hopes to improve the accuracy of analysis and provide in-depth, multi-angle view of data. This paper describes a decision support system containing our methodology, Weighted and Layered workflow evaluation (WaLwFA), extended to incorporate business intelligence using C4.5 and association rule algorithms. C4.5 produces more comprehensible decision trees by showing only important attributes. Furthermore, C4.5 can be transformed into IF-THEN rules. However, association rules are preferred as data can be described in rules of multiple granularities. Sorting rules based on rules' complexities permits OLAP to navigate through layers of complexities to extract rules of relevant sizes and to view data from multidimensional perspectives in each layer. Experimental results on an airline domain are presented. © 2010 Elsevier B.V. All rights reserved. Processing online analytics with classification and association rule mining Decision support systems; Development of methodology for business models; Performance measurement and metrics; Strategic management Artificial intelligence; Association rules; Associative processing; Data mining; Decision trees; Management science; Online systems; Strategic planning; Accuracy of analysis; Association rule algorithm; Association rule mining; Business intelligence; Business models; Business performance; Decision makers; Decision making process; Decision supports; If-then rules; Multi-angle; On-line analytical processing; Performance measurements; Rules based; Strategic management; Decision support systems",Strategic alignment
1756,Agile Development of Machine Learning (ML) for Conventional Artificial Lift Systems in the Middle-East,"Most of Oman's southern fields are produced by beam-pumps which are installed in approximately 2,000 wells. Globally beam pumps remain an extremely popular choice for secondary lift. Identification and diagnosis of beam pumps performance using dynamometer cards (dynocards) is an expensive human visual interpretation process that requires both significant labor time and deep expertise in the production technology domain. The team tasked with developing an improved diagnostic method had three goals: 1) use open-source analytics, 2) develop a machine learning application (ML) to solve business challenges and finally 3) foster solutions with significant value investment ratio (VIR). In this case, a proof-of-concept application was developed to automatically screen beam pump dynocards and identify abnormalities undetected by conventional monitoring systems such as electrical related failures causing improper operation of the well, leading to deferment, undetected by conventional monitoring systems, and/or mechanical damage. An analytics minimum viable product (MVP) was developed for pattern recognition that significantly assisted in automating (analysis of a 100 hundred wells with real-time data in less than 1 second) the visual interpretation process, increasing efficiency, and reducing maintenance activities due to missed early diagnosis. With the system, an analysis of 100 wells with real-time data was performed in less than a minute. The system detects current and future abnormal conditions that because improper operation of the artificial system to deferment and potentially to mechanical damage. This new system identifies and highlights these wells so that operations and maintenance staff can focus their attention where it is really needed, improving their workflows and decision making. This paper outlines how applying ML along with the scaled agility methodology enabled the operator to develop an MVP and diagnose abnormalities on daily basis not raised by any other system. Of the 100 wells in the selected field approximately 10% were identified with clear failures. This translated to an approximate ~5 % improvement in lead indicator (prior to issues) detection projecting ~2.5 million USD in efficiencies and deferment reduction. There was no CAPEX cost as the team developed this entirely on open-source platforms that were license free and independent without needing third-party application or resources. Copyright © 2022, Society of Petroleum Engineers. Agile Development of Machine Learning (ML) for Conventional Artificial Lift Systems in the Middle-East  Chemical detection; Computer aided instruction; Decision making; Efficiency; Gases; Machine learning; Monitoring; Pattern recognition; Pumps; Agile development; Artificial lift systems; Conventional monitoring; Dynamometer card; Machine-learning; Mechanical damages; Middle East; Monitoring system; Real-time data; Visual interpretation; Diagnosis",Monitoring and control
1761,The Effect of Big Data Analytics in Enhancing Agility in Cybersecurity Incident Response,"The ongoing automation of business operations is putting enterprises at risk of cyber attacks more than ever before. Incident response teams are employed by the enterprises for the identification, management, and elimination of cybersecurity attacks along with for the recovery of business operations timely and effectively. In this paper, we argue that to effectively react to the cybersecurity attacks enterprises should build agility in their incident response method and big data analytics performs an effective role in developing agility in incident response. Grounded on twenty-one in depth expert interviews, we develop a framework that explains the salient features and effect of big data analytics in the incident response method at three stages, i.e., manual analysis, basic analysis, and advanced analysis. The agile properties of flexibility, innovation and swiftness are instilled in the incident response method by practicing big data analytics at higher stages of analysis. The results informed that the key features of big data analytics can be firstly utilize to estimate the existing analytical capability and secondly as an assisting tool to enhance incident response method capability.  © 2022 IEEE. The Effect of Big Data Analytics in Enhancing Agility in Cybersecurity Incident Response agility; analytical capability; Big data analytics; cybersecurity; incident response Big data; Crime; Data Analytics; Human resource management; Network security; Agility; Analytical capability; Big data analytic; Business operation; Cyber security; Cyber-attacks; Data analytics; Incident response; Manual analysis; Salient features; Cybersecurity",Risk management
1762,"IFIP WG 5.7 International Conference on Advances in Production Management Systems, APMS 2023","The proceedings contain 58 papers. The special focus in this conference is on IFIP WG 5.7 International Conference on Advances in Production Management Systems. The topics include: A Systematic Literature Review on Combinations of Industry 4.0 and Lean Production; lean and Digital Strategy Role in Achieving a Successful Digital Transformation; tying Digitalization to the Lean Mindset: A Strategic Digitalization Perspective; characterization of Digitally-Advanced Methods in Lean Production Systems 4.0; synergies Between Industry 4.0 and Lean on Triple Bottom Line Performance; Driving Sustainability Through a VSM-Indicator-Based Framework: A Case in Pharma SME; Design and Application of a Development Map for Aligning Strategy and Automation Decisions in Manufacturing SMEs; using the Lean Approach for Improving Eco-Efficiency Performance: A Case Study for Plastic Reduction; work Pattern Analysis with and without Site-Specific Information in a Manufacturing Line; lean and Digitalization Status in Manufacturing Companies Located in Norway; digital Transformation Towards Industry 5.0: A Systematic Literature Review; industry 5.0 and Manufacturing Paradigms: Craft Manufacturing - A Case from Boat Manufacturing; industry 4.0 Readiness Assessment of Enterprises in Kazakhstan; critical Factors for Selecting and Integrating Digital Technologies to Enable Smart Production: A Data Value Chain Perspective; business Process Reengineering in Agile Manufacturing – A Mixed Method Research; service-Oriented Architecture for Driving Digital Transformation: Insights from a Case Study; application of Digital Tools, Data Analytics and Machine Learning in Internal Audit; Consumer Engagement in the Design of PLM Systems: A Review of Best Practices; a Distributed Ledger Technology Solution for Connecting E-mobility Partners; human in Command in Manufacturing. IFIP WG 5.7 International Conference on Advances in Production Management Systems, APMS 2023  ",Strategic alignment
1766,Benefits and Challenges of Making Data More Agile: A Review of Recent Key Approaches in Agriculture,"Having reliable and timely or ongoing field data from development projects or supply chains is a perennial challenge for decision makers. This is especially true for those operating in rural areas where traditional data gathering and analysis approaches are costly and difficult to operate while typically requiring so much time that their findings are useful mostly as learning after the fact. A series of innovations that we refer to as Agile Data are opening new frontiers of timeliness, cost, and accuracy. They are leveraging a range of technological advances to do so. This paper explores the differences between traditional and agile approaches and offers insights into costs and benefits by drawing on recent field research in agriculture conducted by diverse institutions such as the World Bank (WB), World Food Program (WFP), United States Agency for International Development (USAID), and the Committee on Sustainability Assessment (COSA). The evidence collected in this paper about agile approaches—including those relying on internet and mobile-based data collection—contributes to define a contemporary dimension of data and analytics that can contribute to more optimal decision-making. Providing a theoretical, applied, and empirical foundation for the collection and use of Agile Data can offer a means to improve the management of development initiatives and deliver new value, as participants or beneficiaries are better informed and can better respond to a fast-changing world. © 2022 by the authors. Benefits and Challenges of Making Data More Agile: A Review of Recent Key Approaches in Agriculture data innovation; data quality; household surveys United States; data quality; decision making; household survey; innovation; supply chain management",Value management
1767,Value creation from analytics with limited data: a case study on the retailing of durable consumer goods,"Companies are pinning high hopes on competitive advantages through data analytics. So far, value gains through analytics have been demonstrated for IT-heavy and data-rich business areas. Yet, research has paid little attention to value creation through data analytics in the plethora of companies with limited data (i.e. having transactions in the hundreds and attributes in the tens). Building on the literature of big data value creation and the resource-based view, we carried out an in-depth analytics case study with a retailer of renewable energy systems. Firms in this business area operate with expensive but few sales, so their available data are notoriously limited. Our findings demonstrate that data analytics capabilities and value creation mechanisms (democratise, contextualise, experiment with data, and execute data insights) are also effective in situations with limited data. Practice and research should therefore put not only emphasis on the volume and the variety of data but also on contextual factors related to managers (e.g. clear strategy, vision, leadership) and all employees (e.g. openness for agile working mode, data awareness). © 2022 Informa UK Limited, trading as Taylor & Francis Group. Value creation from analytics with limited data: a case study on the retailing of durable consumer goods data analytics; information systems (IS) value creation; machine learning(ML); renewable energy systems (RES); resource theory; retail; value creation mechanisms Competition; Human resource management; Machine learning; Renewable energy resources; Sales; Data analytics; Energy systems; Information system  value creation; Information systems values; Machine-learning; Renewable energies; Renewable energy system; Resource theory; Retail; Value creation; Value creation mechanisms; Data Analytics",Strategic alignment
1768,Mastering Snowflake Solutions: Supporting Analytics and Data Sharing,"Design for large-scale, high-performance queries using Snowflake’s query processing engine to empower data consumers with timely, comprehensive, and secure access to data. This book also helps you protect your most valuable data assets using built-in security features such as end-to-end encryption for data at rest and in transit. It demonstrates key features in Snowflake and shows how to exploit those features to deliver a personalized experience to your customers. It also shows how to ingest the high volumes of both structured and unstructured data that are needed for game-changing business intelligence analysis. Mastering Snowflake Solutions starts with a refresher on Snowflake’s unique architecture before getting into the advanced concepts that make Snowflake the market-leading product it is today. Progressing through each chapter, you will learn how to leverage storage, query processing, cloning, data sharing, and continuous data protection features. This approach allows for greater operational agility in responding to the needs of modern enterprises, for example in supporting agile development techniques via database cloning. The practical examples and in-depth background on theory in this book help you unleash the power of Snowflake in building a high-performance system with little to no administrative overhead. Your result from reading will be a deep understanding of Snowflake that enables taking full advantage of Snowflake’s architecture to deliver value analytics insight to your business. What You Will Learn Optimize performance and costs associated with your use of the Snowflake data platform Enable data security to help in complying with consumer privacy regulations such as CCPA and GDPR Share data securely both inside your organization and with external partners Gain visibility to each interaction with your customers using continuous data feeds from Snowpipe Break down data silos to gain complete visibility your business-critical processes Transform customer experience and product quality through real-time analytics Who This Book Is for Data engineers, scientists, and architects who have had some exposure to the Snowflake data platform or bring some experience from working with another relational database. This book is for those beginning to struggle with new challenges as their Snowflake environment begins to mature, becoming more complex with ever increasing amounts of data, users, and requirements. New problems require a new approach and this book aims to arm you with the practical knowledge required to take advantage of Snowflake’s unique architecture to get the results you need. © 2022 by Adam Morton. Mastering Snowflake Solutions: Supporting Analytics and Data Sharing Cloud Data Warehousing; Data Engineering; Data Lake; Data Sharing; Data Streaming; Data Warehouse; Jumpstart Snowflake; Modern Cloud Analytics; Snowflake; Snowflake Cookbook; Snowflake Data Platform; Snowpark; Snowpipe ",Governance
1769,Digital twin-based intelligent fish farming with Artificial Intelligence Internet of Things (AIoT),"This paper focuses on designing a Digital Twin infrastructure that supports an agile-based Artificial Intelligence Internet of Things (AIoT) system for intelligent fish farming in aquaculture. Our infrastructure includes the Internet of Things, cloud technology, and Artificial Intelligence (AI) as its building blocks. Our physical entity is equipped with smart devices such as sensors and actuators embedded in smart machines (fish feeding and sorting machines) that collect and transmits big data to the cloud using wireless communication networks for real-time and remote monitoring. We have four major digital twin services: fish feeding to automate the feeding process, metric estimation (fish count, size, and weight), environmental monitoring (water condition, net hole, and green algae), and health monitoring (vitality, mortality, and diseases). Each digital twin service is equipped with multiple AI services (or the digital twin objects) capable of performing complex and other functions such as optimizations, predictions, and analyses for intelligent decision-making to optimize farm profits and production. We integrated a prototype that represents the virtual entity accessible using the web and mobile devices where users can perform fish farm monitoring using the various digital twin services and their related AI services. © 2023 The Author(s) Digital twin-based intelligent fish farming with Artificial Intelligence Internet of Things (AIoT) AIoT system; Big data; Big data analytics; Digital twins; Smart aquaculture ",Stakeholder management
1771,Advanced analytics for mining industry,"Digital transformation (DT) quickly and fundamentally modifies corporate entities or organizations because of digitalization development. This development requires a progressive assessment of the technology used for strategy modification, value chain, management, and business models with important repercussions for customers, business, employees, and the public. As a result, companies launch DT initiatives to examine customer needs and create operational models that exploit new competitive opportunities. In this context, customer value proposals and the operation model's reconfiguration from the main solutions to manage changes in the digital age. In this regard, companies have been launching their DT initiatives to upgrade their operations in industries in which the product is primarily a commodity, such as the mining industry. In addition, they modify their operating model to reflect users' preferences and expectations in every activity within the value chain. This approach requires integrating business activities and optimizing the management and monitoring of data associated with each value chain's essential activity. Although there are numerous possibilities for future development, the present level of digital mining transformation is low. Then, a question arises: How should DT initiatives to enhance mining companies' operational models be launched and implemented effectively? This chapter discusses the mining industry's importance, focusing on the sector's main issues to answer this question. Then, through research, the DT initiative's key aspects are presented to improve the operational model of the large, diversified mining company; challenges and success factors in a given context are identified and classified. Moreover, the research efforts and obtained results that focus on the role and importance of the DT phenomenon in mining to use digital technologies more widely and efficiently are discussed. Mining companies shift their strategy to include new technologies and adopt new business and operating models; they have done so more quickly and globally than ever before. The combination of market volatility, changing global demand, radically different input economies, the expansion of mining operations for locating additional reserves, and commitment to operational excellence contribute to a seismic shift in the industry. Decades of reducing costs and an aging workforce have led to reduced resource adaptation within mining companies. DT, a rapidly changing set of new technologies, opens new opportunities to improve business efficiency, build accurate and agile planning, increase provider awareness, and cooperate with business partners throughout the value chain. DT can lead to significant differentiation and competitive advantage in the mining industry. This industry has been disrupted by mining automation, new analysis capabilities, a digital workforce, and remote and autonomous operations. All of these factors must be closely examined to boost growth and efficiency. DT and its associated opportunities and risks are crucial to mining businesses. DT has been succeeded by a more interconnected and information-based operation of human interactions. The next wave of industry differentiation is created by the possibilities of new operating models and new optimization levels. This chapter explains the varying levels of acceptance of DT in the mining industry. © Springer Nature Switzerland AG 2022. All rights reserved. Advanced analytics for mining industry Advanced analytics; Decision-making; Digital transformation; Mining; Optimization; Prediction ",Strategic alignment
1773,"Third Element of Strategy—Team, Processes, and Governance: Establishing Building Blocks, Including an Agile Team, for Success","The structure of data and analytics organization needs to be different from the other IT functions of an enterprise. There are inherent ambiguities in most of the high business value data and analytics initiatives, because of which the way a data and analytics project needs to be implemented is different from the way an ERP or a CRM project is implemented, hence the need for a different organization structure. Before defining data and analytics organization and its processes, it is important to choose the right organization model (decentralized, centralized, or federated), that would work for an enterprise. Each of these models have their pros and cons, which must be weighed carefully before choosing the best-suited one. Having chosen the optimum model, the next step is to define data and analytics organization structure and processes that are agile, business friendly, and would deliver high business value to the enterprise. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. Third Element of Strategy—Team, Processes, and Governance: Establishing Building Blocks, Including an Agile Team, for Success  ",Governance
1775,"The importance of data mining, user information behaviour and interaction audit for information literacy","Purpose: This study aims to promote the need for advanced skills acquisition within the LIS and academic libraries. This study focuses on the importance of library management systems and the need for the graduates to be equipped with analytics skills. Combined with basic data, text mining and analytics, knowledge classification and information audit skills would benefit libraries and improve resource allocation. Agile institutional libraries in this big data era success hinge on the ability to perform depth analytics of both data and text to generate useful insight for information literacy training and information governance. Design/methodology/approach: This paper adopted a living-lab methodology to use existing technology to conduct system analysis and LMS audit of an academic library of one of the highly ranked universities in the world. One of the benefits of this approach is the ability to apply technological innovation and tools to carry out research that is relevant to the context of LIS or other research fields such as management, education, humanities and social sciences. The techniques allow us to gain access to publicly available information because of system audits that were performed. The level of responsiveness of the online library was accessed, and basic information audits were conducted. Findings: This study indicated skill gaps in the LIS training and the academic libraries in response to the fourth industrial technologies. This study argued that the role of skill acquisition and how it can foster data-driven library management operations. Hence, data mining, text mining and analytics are needed to probe into such massive, big data housed in the various libraries’ repositories. This study, however, indicated that without retraining of librarians or including this analytics programming in the LIS curriculum, the libraries would not be able to reap the benefits these techniques provided. Research limitations/implications: This paper covered research within the general and academic libraries and the broader LIS fields. The same principle and concept is very important for both public and private libraries with substantial usage and patrons. Practical implications: This paper indicated that librarianship training must fill the gaps within the LIS training. This can be done by including data mining, data analytics, text mining and processing in the curriculum. This skill will enable the news graduates to have skills to assist the library managers in making informed decisions based on user-generated content (UGC), LMS system audits and information audits. Thus, this paper provided practical insights and suggested solutions for academic libraries to improve the agility of information services. Social implications: The academic librarian can improve institutional and LMS management through insights that are generated from the user. This study indicated that libraries' UGC could serve as robust insights into library management. Originality/value: This paper argued that the librarian expertise transcends information literacy and knowledge classification and debated the interwoven of LMS and data analytics, text mining and analysis as a solution to improve efficient resources and training. © 2022, Patrick Ajibade and Ndakasharwa Muchaonyerwa. The importance of data mining, user information behaviour and interaction audit for information literacy Data analytics; Fourth Industrial Revolution; Information literacy; Library management; Library services; Text mining ",Strategic alignment
1778,Role of chemical reaction engineering for sustainable growth: One industrial perspective from India,"Chemical reaction engineering (CRE) is vital to solve many of the pressing societal challenges—energy and energy transition, materials, food, mobility, and so forth, to meet the aspirational goals of developing country population in the face of climate change, changing demographics, and geopolitical challenges. Application of the core principles of CRE to the emerging societal challenges is creating new technologies and cost-effective solutions by integrating the widely varied CRE activities into broad, powerful, systems descriptions with the help of interdisciplinary teams with broad expertise including chemistry, catalysis, chemical kinetics, transport phenomena, biology, applied mathematics and modeling, emerging data science technologies to design and optimize chemical/biochemical reactors. Such developments will be critical for CRE to play an important role in the emerging fourth industrial revolution—amalgamation of physical, digital, and biological worlds, where the velocity of disruption and acceleration of innovation are hard to comprehend or anticipate and such broadening of CRE discipline will be critical for the field to remain agile and relevant. This article describes some latest technical advances in Reliance Industries Ltd. using this philosophy to help achieve sustainable growth and Net Zero business targets. We will broadly discuss renewable hydrogen from novel biomass catalytic gasification, multizone catalytic cracking process to convert crude oil and low value hydrocarbon streams to petrochemical building blocks, an adsorption/desorption process for CO2 concentration and monetization from industrial flue gases. Furthermore, biotechnology advances in leveraging photosynthesis kinetics, synthetic biology, and genetic modifications for converting solar energy and carbon dioxide through algae production will be discussed to produce proteins, biomaterials, renewable biocrude, and so forth. We will also discuss new catalytic technologies to convert mixed plastic waste to stable oil and organic waste such as agri and municipal solid waste, and so forth, to biocrude for circular economy, and biodegradable plastics production to manage plastics pollution. © 2022 American Institute of Chemical Engineers. Role of chemical reaction engineering for sustainable growth: One industrial perspective from India  Biodegradable polymers; Catalysts; Climate change; Developing countries; Elastomers; Fluid catalytic cracking; Metals; Municipal solid waste; Oils and fats; Proteins; Solar energy; Sustainable development; Chemical reaction engineering; Cost-effective solutions; Energy; Energy transitions; Engineering activities; Powerful systems; Pressung; Sustainable growth; System description; Transition materials; Carbon dioxide",Value management
1779,Organisational enablers of advanced analytics adoption for supply chain flexibility and agility,"Flexible and agile supply chains (SCs) add value by facilitating cost optimisation. Exceptional opportunities are being offered by smartening the SCs. The key is to further enhance the intelligence and intellect of the SCs by using analytics. Use of advanced analytics warrants proficient SC management that primes SC performance and helps to attain competitive advantage. It is worth noting that while this topic, in general, has attracted attention, there is a dearth of related work especially in the context of emerging markets. Many questions are yet unanswered and many aspects also remain unexplored, such as, what are the enablers for the adoption of advanced analytics in SC and the effects of the implementation of advanced analytics on SC performance indicators like flexibility and agility. Through this research work, with the help of detailed and structured literature review, prudent and viable organisational enablers for the adoption of advanced analytics in SC have been proposed. This research work other than adding to the existing knowledge base is intended to aid in acquiring a comprehensive understanding of the topic by the policy and decision makers. Copyright © 2022 Inderscience Enterprises Ltd. Organisational enablers of advanced analytics adoption for supply chain flexibility and agility advanced analytics; agility; flexibility; organisational enablers; supply chain performance ",Strategic alignment
1781,"Proceedings of the 22nd International Conference on Business Informatics Research, BIR 2023","The proceedings contain 24 papers. The special focus in this conference is on Business Informatics Research. The topics include: An Approach for Knowledge Graphs-Based User Stories in Agile Methodologies; A Flexible, Extendable and Adaptable Model to Support AI Coaching; Improving IT Governance, Security and Privacy Using Fractal Enterprise Modeling: A Case of a Highly Regulated Company; model-Based Digital Business Ecosystems: A Method Design; managing Variability of Large Public Administration Event Log Collections: Dealing with Concept Drift; digital Store Window: A Promising Approach for Stationary Retailer in Germany?; enriching Enterprise Architecture Stakeholder Analysis with Relationships; developing Digital Competencies in Small and Medium-Sized Enterprises Through Microlearning Applications: A Research Agenda; exploring Smart Meters: What We Know and What We Need to Know; value Creation from Data Science Applications - A Literature Review; open Government Data in Educational Programs Curriculum: Current State and Prospects; Towards a Framework for Intelligent Cyber-Physical System (iCPS) Design; a Model-Based Approach to Decision Support for Designing Inclusive Services; predicting Patterns of Firms’ Vulnerability to Economic Crises Using Open Data, Synthetic Minority Oversampling Technique and Machine Learning; user Interaction Mining: Discovering the Gap Between the Conceptual Model of a Geospatial Search Engine and Its Corresponding User Mental Model; extending Conceptual Model with Object Life Cycles; towards Healthcare Digital Twin Architecture; Morphological Box for AI Solutions: Evaluation and Refinement with a Taxonomy Development Method; preface; towards Agile Requirements Engineering in Maritime Freight Transportation; knowledge Visualization Towards Digital Literacy Development: Critical Success Factors; Exploring Techno-Invasion and Work-Life Balance on Digital Platforms: A Preliminary Study with Amazon MTurk’s Gig Workers. Proceedings of the 22nd International Conference on Business Informatics Research, BIR 2023  ",Strategic alignment
1782,Artifact Traceability in DevOps: An Industrial Experience Report,"In DevOps, the traceability of software artifacts is critical to the successful development and operation of project delivery to stakeholders. Before the introduction of end-to-end traceability in DevOps at a Data Analytics team at bp (BP plc), an international integrated energy company, the tracing of artifacts throughout a project life cycle was manual and time-consuming. This changed when traceability become more automated with end-to-end traceability capability as an offering on the platform. This paper reports on the ways of working and the experience of developers implementing DevOps for developing and putting in production a Javascript React web application, with a focus on traceability management of artifacts produced throughout the life cycle. This report highlights key opportunities and challenges in traceability management from the development stage to production.  © 2023 Owner/Author. Artifact Traceability in DevOps: An Industrial Experience Report Agile; DevOps; Industry; Software traceability; Web application Application programs; Data Analytics; Agile; Development and operations; End to end; Experience report; Industrial experience; Software artefacts; Software traceability; Traceability managements; WEB application; Web applications; Life cycle",Capacity management
1785,Research trends analysis using text mining in construction management: 2000–2020,"Purpose: This study aims to identify the trends that have changed in the field of construction management over the last 20 years. Design/methodology/approach: In this study, 3,335 journal articles published in the years 2000–2020 were collected from the Web of Science database in construction management. The authors applied bibliometric analysis first and then detected topics with the latent Dirichlet allocation (LDA) topic detection method. Findings: In this context, 20 clusters from cluster analysis were found and the topics were extracted in clusters with the LDA topic detection method. The results show “building information modeling” and “information management” are the most studied subjects, even though they have emerged in the last 15 years “building information modeling,” “information management,” “scheduling and cost optimization,” “lean construction,” “agile approach” and “megaprojects” are the trend topics in the construction management literature. Research limitations/implications: This study uses bibliometric analysis. The authors accept that the co-citation and co-authorship relationship in the data is ethical. They accept that honorary authorship, self-citation or honorary citation do not change the pattern of the construction management research domain. Originality/value: There has been no study conducted in the last 20 years to examine research trends in construction management. Although bibliometric analysis, systematic literature reviews and text mining methods are used separately as a methodology for extracting research trends, no study has used enhanced bibliometric analysis and the LDA topic detection text mining method. © 2021, Emerald Publishing Limited. Research trends analysis using text mining in construction management: 2000–2020 Bibliometric; Construction management; Research trends; Text analytics Architectural design; Cluster analysis; Information management; Information theory; Scheduling; Statistics; Bibliometric analysis; Building Information Model - BIM; Construction management; Construction management research; Cost optimization; Design/methodology/approach; Latent dirichlet allocations; Systematic literature review; Text mining",Strategic alignment
1790,Role of big data in agile business management: A conceptual framework,"This study aims to establish a link between big data and agile management. Agility is a need of the present era to be competitive in the market and big data provide an opportunity for organization to move from traditional methods to agile management. In this chapter, based on literature, a relation between the different characteristics of big data and agile practices was established. The flexibility, timeliness, and quick decision-making which is needed for agile management are derived from big data. Businesses can create value through agile practices; securing this value from data. 5 Vs of big data enabled the flexibility in organization, authentic and quick information, and quick decisions of organizations to be competitive in an organization. The relation between data and agility is not directional, it can go in both ways, both agility and big data are interlinked. Big data help the organization to be agile and agility is needed to entrench data analytics at every level of the firm. © 2023 by Apple Academic Press, Inc. All rights reserved. Role of big data in agile business management: A conceptual framework Agile management; Big data; Data analytics; Digitalization; Industry 4.0 ",Strategic alignment
1791,Science Autonomy and Planetary Missions: ML and Data Science Applied to the ExoMars Mission,"Future planetary science instruments will be capable of producing far more data than can be transmitted back to Earth, potentially leaving valuable scientific data on a planet's surface. Instruments will need to carefully identify the subset of total data to be prioritized for return, as transmission of the full data volume, even after compression, will not be feasible. The concept of science autonomy, where instruments collect measurement data, perform selected science data analyses onboard, and then autonomously act upon those analyses through self-adjustment and tuning of instrument parameters, can be used to identify and produce an optimal and compact data set for return, maximizing the value of each bit returned to Earth. Furthermore, the selection of the next operation(s) to be run following preliminary measurements, without requiring ground-in-the-loop communication, increases mission efficiency and enables successful yet shorter duration missions to hazardous planetary environments. This capability allows missions to prioritize the most compelling or time-critical data, yielding a more efficient and productive scientific investigation overall. In this paper, we present our implementation using different machine learning (ML) techniques (i.e., clustering, classification) for analyzing science data from the Mars Organic Molecule Analyzer (MOMA) instrument onboard the ExoMars rover Rosalind Franklin. MOMA is a dual-source (laser desorption and gas chromatography) mass spectrometer that will search for past or present life on the Martian surface and subsurface through analysis of soil samples. We use data collected from the MOMA flight-like engineering model to develop mass-spectrometry-focused ML techniques. This effort, in preparation for operating on Mars, is aimed at 1) helping the ExoMars science and operations team quickly analyze new data and support them in their decision-making process regarding subsequent operations and, 2) getting a better understanding of the challenges to enable science autonomy in future missions. We also present two significant challenges we faced in this development that are particular to space missions and will be common to most, if not all, robotic planetary missions. First, the lack of sufficient data volume from these unique and highly optimized instruments to train neural networks, and second the lack of sufficient results from the system to fully trust its output. To tackle the first challenge, we analyze the performance of ML algorithms after adding augmented data. We discuss adopting transfer learning techniques to fine-tune a NN trained on large amounts of commercial instrument data so that it can operate on our limited MOMA dataset. For the 'trust' challenge-as it is not always clear what we are looking for in planetary science-we must consider agile ML applications and demonstrate that these will not filter out potentially critical data. We will discuss our concept of a Trust Readiness Level for science autonomy akin to the NASA Technology Readiness Level. This initial project for advanced autonomy illustrates some key first steps of a longer-term objective to enable the spacecraft and instruments themselves to make real-time adjustments during operations as direct human oversight will not be possible for missions going further away in our solar system and beyond. © 2023 IEEE. Science Autonomy and Planetary Missions: ML and Data Science Applied to the ExoMars Mission  Decision making; Gas chromatography; Learning systems; Machine learning; Mass spectrometry; Rovers; Critical data; Data volume; ExoMars mission; Machine data; Machine learning techniques; Machine-learning; Organic molecules; Planetary mission; Planetary science instruments; Scientific data; Earth (planet)",Strategic alignment
1793,"HiVE, an agile microsatellite constellation for thermal infrared Earth observation enabling “more crop per drop”","HiVE (High-resolution VEgetation monitoring mission) is going to be the world's first microsatellite constellation for thermal infrared (TIR) land surface temperature monitoring. The mission is developed by ConstellR GmbH, together with OHB System, NanoAvionics and Fraunhofer EMI, using a blend of standard techniques and newspace approaches. The primary goal is to provide global land surface temperature imagery optimised for high-precision agriculture, water management, temperature-derived crop health management, yield forecasting and sustainable resource management. Sub-field crop monitoring calls for high resolution imagery, day-to-day planning requires high revisit frequencies and operational use demand low latencies for data and analytics delivery. Due to its commercial nature, the constellation aims to be cost-efficient via the use of commercial-off-the-shelves components, while providing all key capabilities. The HiVE constellation mission architecture comprises multiple operational concepts including virtual calibration for payload miniaturization and dynamic tasking/targeting, serving more users. By introducing novel operational concepts, the required payload mass and volume can be minimised, thereby reducing space segment costs to a fraction of current systems whilst ensuring high radiometric precision. Each satellite is equipped with a multispectral TIR cryocooled sensor, delivering the main mission data, and a multispectral VNIR to enable precise geolocation, atmospheric correction and cloud detection. The payload is developed by OHB and can record, with high radiometric accuracy, four spectral bands using a push-frame scanning scheme. The payload brain is an instrument control and data processing unit, developed at Fraunhofer EMI, based on a single COTS heterogenous multiprocessor system-on-a-chip and is capable of onboard data processing such as image correction and data compression. The satellite platform is based on the NanoAvionics versatile MP42 microsatellite bus, whose performance is optimized for remote sensing, high data throughput, high agility and high attitude stability. The first generation of the satellites will target the delivery of 1-day global revisit, <50m ground resolution in TIR and <15m in VNIR and the derivation of L-2 land surface temperature (LST) data at 1K precision. To achieve the targeted revisit time as well as being flexible and highly responsive to customers' requests, each satellite will be able to target preselected areas of interest. The satellites are designed such that it can operate either in mapping mode, where continuous stripes are recorded, or in targeting mode, where specific targets are pre-selected by the mission planning within the field-of-regard of the satellite. For this, the microsatellite needs to be agile and highly stable during imaging: particular attention is being put in the mechanical design, the AOCS design, the micro vibration environment, and in the concept of operations. Copyright © 2022 by the International Astronautical Federation (IAF). All rights reserved. HiVE, an agile microsatellite constellation for thermal infrared Earth observation enabling “more crop per drop” constellation tasking; cross-calibration; high-agility; land-surface-temperature; newspace; thermal-infrared Atmospheric temperature; Calibration; Data handling; DNA sequences; Infrared radiation; Land surface temperature; Radiometry; Remote sensing; Satellite imagery; Surface measurement; Surface properties; System-on-chip; Temperature control; Constellation tasking; Cross calibration; Fraunhofer; High agility; Land surface temperature; Microsatellites; Multi-spectral; Newspace; Operational concepts; Thermal-infrared; Crops",Monitoring and control
1796,Bringing Huge Core Analysis Legacy Data Into Life Using Machine Learning,"Advances in the fields of information technology, computation, and predictive analytics have permeated the energy industry and are reshaping methods for exploration, development, and production. These technologies can be applied to subsurface data to reliably predict a host of properties where only few are available. Among the numerous sources of subsurface data, rock and fluid analysis stand out as the means of directly measuring subsurface properties. The challenge in this work is to maximize information gain from legacy pdf reports and unstructured data tables that represented over 70 years of laboratory work and investment. The implication of modeling this data into an organized data store means better assessment of economic viability and producibility in frontier basins and the capability to identify bypassed pay in old wells that may not have rock material. This paper presents innovative and agile technologies that integrate data management, data quality assessment, and predictive machine learning to maximize the company asset value using underutilized legacy core data. The developed machine learning algorithms identify potential outliers, benchmark the valuable data against current industry standards, increase the confidence in data quality and avoid amplifying error in predicting reservoir properties. The workflow presented in the paper is expected to reduce uncertainties in subsurface studies caused by limited core data, improper analog selection, high cost, limited time for acquiring new cores, and long delivery times of core analysis data. The workflow reduces the requirement for subsurface formation evaluation rework as new data becomes available at later project stages resulting in optimized field development. The workflow enhanced by machine learning also improves the prediction and propagation of reservoir properties to uncored borehole sections. In conclusion, managing legacy core data and transforming it to generate new subsurface insights are critical step to establish a reliable database in support of business excellence and the digitalization journey. Innovative machine learning tools continue to unlock new values from legacy core data that significantly impact the entire reservoir life cycle including reserves booking, production forecasting, well placement, and completion design. Copyright © 2022, Offshore Technology Conference. Bringing Huge Core Analysis Legacy Data Into Life Using Machine Learning  Benchmarking; Data Analytics; Information management; Investments; Learning algorithms; Life cycle; Machine learning; Metadata; Offshore oil well production; Uncertainty analysis; Core datum; Energy industry; Fluid analysis; Legacy data; Machine-learning; Property; Reservoir property; Rock analysis; Subsurface data; Work-flows; Predictive analytics",Strategic alignment
1797,Impact of artificial intelligence-driven big data analytics culture on agility and resilience in humanitarian supply chain: A practice-based view,"This study attempts to understand the role of artificial intelligence-driven big data analytics capability in humanitarian relief operations. These disasters play an important role in mobilizing several organizations to counteract them, but the organizations often find it hard to strike a fine balance between agility and resilience. Operations Management Scholars’ opinion remains divided between responsiveness and efficiency. However, to manage unexpected events like disasters, organizations need to be agile and resilient. In previous studies, scholars have adopted the resource-based view or dynamic capability view to explain the combination of resources and capabilities (i.e., technology, agility, and resilience) to explain their performance. However, following some recent scholarly debates, we argue that organizational theories like the resource-based view or dynamic capability view are not suitable enough to explain humanitarian supply chain performance. As the underlying assumptions of the commercial supply chain do not hold true in the case of the humanitarian supply chain. We note this as a potential research gap in the existing literature. Moreover, humanitarian organizations remain sceptical regarding the adoption of artificial intelligence-driven big data analytics capability (AI-BDAC) in the decision-making process. To address these potential gaps, we grounded our theoretical model in the practice-based view which is proposed as an appropriate lens to examine the role of practices that are not rare and are easy to imitate in performance. We used Partial Least Squares (PLS) to test our theoretical model and research hypotheses, using 171 useable responses gathered through a web survey of international non-governmental organizations (NGOs). The findings of our study suggest that AI-BDAC is a significant determinant of agility, resilience, and performance of the humanitarian supply chain. Furthermore, the reduction of the level of information complexity (IC) on the paths joining agility, resilience, and performance in the humanitarian supply chain. These results offer some useful theoretical contributions to the contingent view of the practice-based view. In a way, we have tried to establish empirically that the humanitarian supply chain designs are quite different from their commercial counterparts. Hence, the use of a resource-based view or dynamic capability view as theoretical lenses may not help capture true perspectives. Thus, the use of a practice-based view as an alternative theoretical lens provides a better understanding of humanitarian supply chains. We have further outlined the limitations and the future research directions of the study. © 2022 Impact of artificial intelligence-driven big data analytics culture on agility and resilience in humanitarian supply chain: A practice-based view Artificial intelligence; Big data analytics; Culture; Humanitarian operations management; Humanitarian supply chain; PLS-SEM; Practice-based view; Supply chain agility; Supply chain resilience Artificial intelligence; Big data; Computational complexity; Decision making; Disasters; Least squares approximations; Supply chains; Big data analytic; Culture; Data analytics; Humanitarian operation management; Humanitarian operations; Humanitarian supply chain; Operation management; Partial least square-SEM; Partial least-squares; Practice-based view; Supply chain agility; Supply chain resiliences; Data Analytics",Governance
1798,Towards MLOps in Mobile Development with a Plug-in Architecture for Data Analytics,"Smartphones are increasingly used as universal IoT gateways collecting data from connected sensors in a wide range of industrial applications. With the increasing computing capabilities, they are used not just for simple data aggregation and transferring, but have now become capable of performing advanced data analytics. As AI has become a key element in enterprise software systems, many software development teams rely on dedicated Machine Learning (ML) engineers who often follow agile development practices in their work. However, in the context of mobile app development, there is still limited tooling support for MLOps, mainly due to unsuitability of native programming languages such as Java and Kotlin to support ML-related programming tasks. This paper aims to address this gap and describes a plug-in architecture for developing, deploying and running ML modules for data analytics on the Android platform. The proposed approach advocates for modularity, extensibility, customisation, and separation of concerns, allowing ML engineers to develop their components independently from the main application in an agile and incremental manner.  © 2022 IEEE. Towards MLOps in Mobile Development with a Plug-in Architecture for Data Analytics Android; Fitbit; Internet of Things; Machine Learning; MLOps; Mobile Development; Plug-in Architecture Agile manufacturing systems; Android (operating system); Enterprise resource planning; Enterprise software; Internet of things; Machine learning; Object oriented programming; Software design; Android; Computing capability; Data analytics; Fitbit; Machine-learning; MLOp; Mobile development; Plugin architecture; Simple++; Smart phones; Data Analytics",Strategic alignment
1799,An empirical investigation of effect of sustainable and smart supply practices on improving the supply chain organizational performance in SMEs in India,"Implementing sustainable and smart supply chain practices have a great impact on the performance of an organization. In today’s globalized and highly industrialized world, sustainability is recognized as one of the highest priorities of all organizations. Evolution of internet-based technologies, digital platforms and big data analytics have paved the way for redesigning supply chains to be smart, agile, and resilient. Therefore, the implementation of practices related to these two concepts is found to improve the supply chain related organizational performance. This research aims to investigate empirically the impact of these two practices on improving the supply chain organizational performance in the Small and Medium Enterprises (SMEs) of India. This research considered the dimensions and the variables related to sustainable supply chain and smart supply chain practices in SMEs in India which were not considered in research contributions prior to this. Therefore, this research becomes a unique contribution to the existing body of knowledge. Empirical analysis was carried out on data from 92 SMEs from Telangana State in India, collected using a questionnaire. The directory of SMEs of Government of Telanagana, India was used to select the cluster sample of SMEs as respondents, based on a criterion using exploratory research methodology. SPSS software was used to test the model. Regression and ANOVA were used for this purpose. Findings of this research reveal significant influence of sustainable and smart supply chain (SC) practices on improving SC organizational performance. Additionally, individually each of these practices also have a direct influence on the performance of SMEs. Obtaining responses from the representatives of SMEs was a challenge and limitation of this research while expanding the scope of this research to different geographical regions and clusters will be a topic for further research. The outcomes and results of this research provide significant contribution to the existing body of knowledge by filling the gaps and value-adding to the researchers, academicians, students, policy makers and industry practitioners. © 2023 Growing Science Ltd. All rights reserved. and by the authors; licensee Growing Science, Canada. An empirical investigation of effect of sustainable and smart supply practices on improving the supply chain organizational performance in SMEs in India Block Chain; Digital Technologies; Internet of Things; Organizational Performance; Smart Supply Chain; Sustainable Supply Chain ",Value management
1800,Cancer Clinic Redesign: Opportunities for Resource Optimization,"Ambulatory cancer centers face a fluctuating patient demand and deploy specialized per-sonnel who have variable availability. This undermines operational stability through the misalign-ment of resources to patient needs, resulting in overscheduled clinics, budget deficits, and wait times exceeding provincial targets. We describe the deployment of a Learning Health System frame-work for operational improvements within the entire ambulatory center. Known methods of value stream mapping, operations research and statistical process control were applied to achieve organ-izational high performance that is data-informed, agile and adaptive. We transitioned from a fixed template model by an individual physician to a caseload management by disease site model that is realigned quarterly. We adapted a block schedule model for the ambulatory oncology clinic to align the regional demand for specialized services with optimized human and physical resources. We demonstrated an improved utilization of clinical space, increased weekly consistency and improved distribution of activity across the workweek. The increased value, represented as the ratio of monthly encounters per nursing worked hours, and the increased percentage of services delivered by full-time nurses were benefits realized in our cancer system. The creation of a data-informed demand capacity model enables the application of predictive analytics and business intelligence tools that will further enhance clinical responsiveness. © 2022 by the authors. Licensee MDPI, Basel, Switzerland. Cancer Clinic Redesign: Opportunities for Resource Optimization ambulatory clinic; block schedule; cancer operations; disease site teams; interdisci-plinary care; learning health system; oncology value stream Ambulatory Care Facilities; Humans; Neoplasms; Article; cancer center; electronic medical record; health care; health care cost; health care delivery; health care organization; health care planning; health care system; hospital design; hospital management; human; intelligence; learning; learning health system; medical practice; oncologist; pandemic; physician; predictive model; retrospective study; system analysis; total quality management; value stream mapping; working time; workload; neoplasm; outpatient department",Strategic alignment
1801,Data-Driven Transformation: The Role of Ambidexterity and Analytics Capability in Building Dynamic and Sustainable Supply Chains,"Data-driven supply chain analytics skills are seen as the next frontier of the supply chain transformation. The potential of data analytics-enabled dynamic capability for improving organizational performance and agility has been investigated in past research. However, there has not been sufficient research on the potential benefits of the data analytics capability and supply chain ambidexterity paradox to develop a sustainable and agile supply chain that can integrate and reorganize all of its resources in order to respond to rapidly changing business circumstances. This study aimed to empirically validate how an organization’s SC ambidexterity affects its sustainability and dynamic capability, and the mediating role of supply chain analytics capability (SCAC) in their relationship. The research’s theoretical framework is founded on dynamic capability theory. A pretested questionnaire was used to collect responses from 427 supply chain specialists who worked in diverse product-based industries across Pakistan, Bangladesh, and India. Using partial least squares structural equation modeling (PLS-SEM), a total of six hypotheses were evaluated, and the results show that supply chain ambidexterity has a positive effect on dynamic capability and sustainability, and SCAC plays a complementary, partially mediating role in their interaction. The findings of the research reveal the expected results of investing in the analytics capability of the supply chain and provide firms with some recommendations for improving their dynamic capabilities. This study will facilitate in creating an agile and sustainable supply chain, enabling it to adapt to both short- and long-term changes in the market while simultaneously considering the social, economic, and environmental vitality. © 2023 by the authors. Data-Driven Transformation: The Role of Ambidexterity and Analytics Capability in Building Dynamic and Sustainable Supply Chains ambidexterity; dynamic capability; supply chain analytics capability; sustainability Bangladesh; India; Pakistan; analytical method; business development; industrial performance; market system; questionnaire survey; supply chain management; sustainability; theoretical study",Strategic alignment
1803,Bean counter to value-adding business partner: the changing role of the accountant and situated rationality in a multinational firm,"Purpose: This paper aims to explore the changing role of the accountant amid multiple drivers, responses of accountants and situated rationality in a multinational firm, Max-choice Lanka. Design/methodology/approach: It adopts the single-site case study approach under the qualitative methodology and leans on institutional theory, specifically Ter Bogt and Scapens (2019) framework. Findings: The case study findings reveal that the role of the accountant has undergone change amid local and broader institutions linked to organizational culture/norms, the influence of the parent company, global trends and technological advancements. Based on evolving situated rationalities, the contemporary accountant performs an agile role as a value-adding business partner; data scientist; strategic decision-maker; and a cross-functional team member. Practical implications: At the practice level, identifying drivers influencing the changing role of accountants enables organizations to shape their accounting functions attuned to evolving needs by implementing appropriate strategies and recruiting competent personnel. In the realm of education, it calls for incorporating areas such as big data analytics, artificial intelligence, reporting nonfinancial information and integrated accounting software to the accounting curricular and upskill students based on industry expectations catering to changing roles. Originality/value: This paper adds to the ongoing debate on the contemporary role of the accountant. Capitalizing on case study data, this research illuminates the influence of multiplicity of institutions, different forms and situated rationality within this changing role and extends the Ter Bogt and Scapens (2019) framework. © 2022, Emerald Publishing Limited. Bean counter to value-adding business partner: the changing role of the accountant and situated rationality in a multinational firm Accountant; Case study; Changing role; Forms of rationality; Multiplicity of institutions; Situated rationality ",Strategic alignment
1804,Towards a Democratization of Data in the Context of Industry 4.0,"Data-driven transparency in end-to-end operations in real-time is seen as a key benefit of the fourth industrial revolution. In the context of a factory, it enables fast and precise diagnoses and corrections of deviations and, thus, contributes to the idea of an agile enterprise. Since a factory is a complex socio-technical system, multiple technical, organizational and cultural capabilities need to be established and aligned. In recent studies, the underlying broad accessibility of data and corresponding analytics tools are called “data democratization”. In this study, we examine the status quo of the relevant capabilities for data democratization in the manufacturing industry. (1) and outline the way forward. (2) The insights are based on 259 studies on the digital maturity of factories from multiple industries and regions of the world using the acatech Industrie 4.0 Maturity Index as a framework. For this work, a subset of the data was selected. (3) As a result, the examined factories show a lack of capabilities across all dimensions of the framework (IT systems, resources, organizational structure, culture). (4) Thus, we conclude that the outlined implementation approach needs to comprise the technical backbone for a data pipeline as well as capability building and an organizational transformation. © 2022 by the authors. Towards a Democratization of Data in the Context of Industry 4.0 data democratization; fourth industrial revolution; Industrie 4.0 ",Strategic alignment
1807,Skills Measurement Strategic Leadership Based on Knowledge Analytics Management through the Design of an Instrument for Business Managers of Chilean Companies,"The growth of business intelligence and analytics (BI&A) and technological advancement is having an impact on business dynamics, implying that executives need to adjust their management skills to create value and a sustainable competitive advantage in agile environments. In this research study, a model (LDM–BI&A) consisting of knowledge and skills domains for BI&A leadership has been proposed to measure strategic leadership skills for BI&A business managers in Chilean SMEs. The skills were studied according to their demand (role requirements) and supply (university curricula in Chile). Factorial validity was determined using Confirmatory Factor Analysis, showing that the model satisfactorily fit the data. The instrument proves reliable, showing solid psychometric properties, and contributes to the literature as it has the design and empirical validation of an instrument that measures skills linked to business managers in Chilean SMEs. This will provide support to Human Resources managers on how BI&A leadership skills can be diagnosed, maintained, and developed. © 2022 by the authors. Skills Measurement Strategic Leadership Based on Knowledge Analytics Management through the Design of an Instrument for Business Managers of Chilean Companies business analytics; business intelligence; capability; entrepreneurial competences; knowledge management; skill leadership Chile; business; entrepreneur; growth response; industrial enterprise; knowledge; leadership; model validation; strategic approach; sustainability; technological development",Strategic alignment
1808,Information Resilience: the nexus of responsible and agile approaches to information use,"The appetite for effective use of information assets has been steadily rising in both public and private sector organisations. However, whether the information is used for social good or commercial gain, there is a growing recognition of the complex socio-technical challenges associated with balancing the diverse demands of regulatory compliance and data privacy, social expectations and ethical use, business process agility and value creation, and scarcity of data science talent. In this vision paper, we present a series of case studies that highlight these interconnected challenges, across a range of application areas. We use the insights from the case studies to introduce Information Resilience, as a scaffold within which the competing requirements of responsible and agile approaches to information use can be positioned. The aim of this paper is to develop and present a manifesto for Information Resilience that can serve as a reference for future research and development in relevant areas of responsible data management. © 2022, The Author(s). Information Resilience: the nexus of responsible and agile approaches to information use Data quality; Effective information use; Information Resilience; Responsible data science; Value creation Data privacy; Data Science; Information management; Information systems; Regulatory compliance; Scaffolds; Agile approaches; Case-studies; Data quality; Effective information use; Information assets; Information resilience; Public and private sector; Public sector organization; Responsible data science; Value creation; Information use",Value management
1809,A Robust Statistical Methodology for Measuring Enterprise Agility,"In an era characterized by rapid technological advancements, economic fluctuations, and global competition, adaptability and resilience have become critical success factors for businesses navigating uncertainty and complexity. This article explores the role of enterprise agility in today’s business landscape at Latam branch of Tata Consultancy Services, where organizations face complex and diverse operations. We aim to examine how companies can become more agile in the face of emerging challenges and seize opportunities swiftly to drive growth and deliver value. Since 2014, the division has embarked on an agile transformation journey to drive growth, deliver value, foster innovation, and build resilience in an increasingly dynamic environment. We scrutinize an approach to measuring and enhancing enterprise agility, employing statistical analysis and continuous improvement methodologies to tackle real-world challenges while offering valuable insights and recommendations for organizations aiming to implement similar systems. The results of an agile transformation in a certain company’s Latam branch serve as a compelling case study, demonstrating how the implementation of targeted measures and continuous improvement can significantly bolster enterprise agility. Methodologically, our work applies a novel sequence of parametric statistical tests which, to the best of our knowledge, have not been used in the industry to validate the results of business agility metrics. In future work, we aim to create a new workflow considering non-parametric tests to address data with other statistical distributions. We conclude our work by proposing a sequence of steps for organizations to implement business agility metrics. © 2023 by the authors. A Robust Statistical Methodology for Measuring Enterprise Agility business agility; data analytics; enterprise agility; metrics; organizational transformation; statistics ",Governance
1810,Taming the HiPPO (Highest Paid Person's Opinion) with agile metrics and value management,"If the organisation's ecosystem has an excess of HiPPOs, data, measured analytics, and other realtime information that minimises uncertainty can help them mitigate the consequences. They disregard the wisdom of the crowd, overlook the front-line staff's abilities, and risk disengaging the workforce. This research advocates agile metrics and value management to tame the HiPPO. The authors posit to reign in opinions with metrics. The corporate enlightenment brought about a revolutionary idea over three centuries ago: to elevate science and knowledge above magical thought and mysticism. When the authors convert this into modern terms, they are referring to data-driven management, analytics, and hypothesis validation. In fact, the idea of applying science in the form of true evidence, confirmed data, etc. to production processes underlies much of the industrial revolution. © 2023, IGI Global. All rights reserved. Taming the HiPPO (Highest Paid Person's Opinion) with agile metrics and value management  ",Risk management
1811,"IFIP WG 5.7 International Conference on Advances in Production Management Systems, APMS 2023","The proceedings contain 58 papers. The special focus in this conference is on IFIP WG 5.7 International Conference on Advances in Production Management Systems. The topics include: A Systematic Literature Review on Combinations of Industry 4.0 and Lean Production; lean and Digital Strategy Role in Achieving a Successful Digital Transformation; tying Digitalization to the Lean Mindset: A Strategic Digitalization Perspective; characterization of Digitally-Advanced Methods in Lean Production Systems 4.0; synergies Between Industry 4.0 and Lean on Triple Bottom Line Performance; Driving Sustainability Through a VSM-Indicator-Based Framework: A Case in Pharma SME; Design and Application of a Development Map for Aligning Strategy and Automation Decisions in Manufacturing SMEs; using the Lean Approach for Improving Eco-Efficiency Performance: A Case Study for Plastic Reduction; work Pattern Analysis with and without Site-Specific Information in a Manufacturing Line; lean and Digitalization Status in Manufacturing Companies Located in Norway; digital Transformation Towards Industry 5.0: A Systematic Literature Review; industry 5.0 and Manufacturing Paradigms: Craft Manufacturing - A Case from Boat Manufacturing; industry 4.0 Readiness Assessment of Enterprises in Kazakhstan; critical Factors for Selecting and Integrating Digital Technologies to Enable Smart Production: A Data Value Chain Perspective; business Process Reengineering in Agile Manufacturing – A Mixed Method Research; service-Oriented Architecture for Driving Digital Transformation: Insights from a Case Study; application of Digital Tools, Data Analytics and Machine Learning in Internal Audit; Consumer Engagement in the Design of PLM Systems: A Review of Best Practices; a Distributed Ledger Technology Solution for Connecting E-mobility Partners; human in Command in Manufacturing. IFIP WG 5.7 International Conference on Advances in Production Management Systems, APMS 2023  ",Strategic alignment
1812,Big Data Analytics Capability Ecosystem Model for SMEs,"The unprecedented COVID-19 pandemic, together with globalization and advanced technologies, has drastically changed the business environment and forced companies to become more innovative and agile in the way they run their business and respond to the needs and wants of customers. Survival highly depends on the adaptability of SMEs to this ever-changing complex dynamic environment by taking steps in implementing Big Data Analytics as the next frontier for innovation, competition, productivity, and value creation. Based on the grounded theory, this study employed a qualitative method via focus group discussion. Focus groups were conducted with 14 government agencies, SMEs associations, business owners, Chief Operating Officers (CEOs), academic and industrial experts and directors of SMEs in Malaysia. The study revealed the challenges of Malaysian SMEs in adopting Big Data Analytics Capability, presents the criticality of Big Data Analytics Capability to overcome the identified challenges, and develops a BDA Capability Ecosystem Model that integrates the internal enablers, external barriers and support to explain the adoption of BDA Capability for value creation and support the decision-making process. This paper is followed by some policy suggestions for companies’ owners, policymakers, government agencies, universities, and SMEs. This study directly impacts Malaysia’s economy as a whole by addressing Malaysia’s Shared Prosperity Vision 2030. This research contributes to industries that are still in the low value added category with low adoption of technology. Furthermore, it will ultimately lead to the realization of SMEs as ‘game changers’ to transition the economy to a high-income nation. This study proposes a model that could help SMEs improve their value creation performance, directly influencing the country’s GDP and employability. © 2022 by the authors. Big Data Analytics Capability Ecosystem Model for SMEs Big Data Analytics; business model; capability; competencies; SMEs; value creation Malaysia; business; COVID-19; decision making; decision support system; ecosystem modeling; policy approach; small and medium-sized enterprise; technology adoption",Strategic alignment
1814,Digital conflicts in procurement,"The increasing global competition, worldwide economic and political uncertainities, and continuously changing dynamics in business environment require companies act differently and differentiate via smart strategies in order to have sustainable operations, growth, and profitability. Therefore, firms should be more agile, creative, and adaptive in planning and strategizing their mid- to long-term business objectives. In that regard, for the last decade globally many firms across all industries seek opportunities to utilize benefits of digitalization. Lately, COVID-19 has also accelerated companies' efforts and investments in digital platforms. Today, supply chain and procurement functions are expected to have a strategic role for organizations contributing to management decisions. The digital transformation in procurement is promising to enhance and lean the total workflow of operations. Data analytics, artificial intelligence, robotics, and other emerging digital technologies are all highly powerful tools supporting strategic supply and supplier management, providing predictability for demand planning as well as value-based negotiation power to buyers. On the other hand, there are still challenges and conflicts throughout this transformation process. Level of technological maturity, infrastructure and investment decisions, expertise and competency of procurement professionals, cultural adaptation, and compliance of related stakeholders are some of the key barriers that are addressed with a unique model in this chapter. Digital era offers a lot of advantages to firms to improve their procurement facilities and practices while it may still take time both for the technologies to fully evolve and also for companies to adapt and embrace digitalization on their benefit. © 2022 by Emerald Publishing Limited. All rights reserved. Digital conflicts in procurement Change management; Conflict modeling; Digitalization; Lean purchasing; Predictivity; Strategic procurement ",Capacity management
1815,Selection of Industry 4.0 Technology to Support Lean Manufacturing from the Perspective of Enterprise Interoperability,"Faced with paradigm shifts in the global manufacturing context promoted by the Fourth Industrial Revolution, many organizations are seeking to meet customer needs through the integration of Lean Manufacturing (LM) philosophy principles with Industry 4.0 (I4.0) technologies. When there is the integration of technological enablers from I4.0 and deep advances in efficiency and productivity with LM, these systems tend to offer enhanced and more assertive results, since they are complementary concepts. The main goal of this paper is the selection of I4.0 technologies to support the LM system, considering the perspectives and barriers of Enterprise Interoperability (EI) and using multicriteria methods (MCDM) to support decision-making. Using the DEMATEL multicriteria method, it was possible to develop a diagnostic evaluation, analyze the existing influences between the elements of the LM, and support the elicitation of weights in the decisional evaluation, with the FITradeoff method. In this way, the decisional evaluation indicated as the I4.0 technology that must be implemented as a priority to raise the level of organizational maturity in LM is Big Data Analytics. Big Data integrated with Business Analytics (BA) can offer several advantages, such as assertiveness in decision-making; Keeping the company updated about the market; Indicating risks and improving data security; Promotes alignment between marketing and sales, among others. © 2024, The Author(s), under exclusive license to Springer Nature Switzerland AG. Selection of Industry 4.0 Technology to Support Lean Manufacturing from the Perspective of Enterprise Interoperability Enterprise Interoperability; Industry 4.0; Lean Manufacturing; MCDM methods Agile manufacturing systems; Big data; Commerce; Data Analytics; Decision making; Industry 4.0; Interoperability; Customer need; Decisions makings; DEMATEL; Enterprise interoperability; Global manufacturing; Industrial revolutions; Lean manufacturing systems; MCDM method; Multi-criteria method; Paradigm shifts; Lean production",Strategic alignment
1818,Multi-access Edge Computing video analytics of ITU-T P.1203 Quality of Experience for streaming monitoring in dense client cells,"5G promises unseen network rates and capacity. Furthermore, 5G ambitions agile networking for specific service traffic catalysing the application and network symbiosis. Nowadays, the video streaming services consume lots of networking assets and produce high dynamics caused by players mobility meaning a challenging traffic for network management. The Quality of Experience (QoE) metric defined by ITU-T P.1203 formulates the playback issues related to widely employed Dynamic Adaptive Streaming over HTTP (DASH) technologies based on a set of parameters measured at the video player. Monitoring the individual QoE is essential to dynamically provide the best experience to each user in a cell, while video players compete to enhance their individual QoE and because high network performance dynamics. The edge systems have a perfect position to bring live coordination to dense and dynamic environments, but they are not aware of QoE experienced by each video player. This work proposes a mechanism to assess QoE scores from network dynamics at the cell and manifests of DASH streams without an explicit out of band messaging from video players to edge systems. Hence, this paper implements an edge proxy, independent from video servers and players, to monitor and estimate QoE providing the required information to later decide streaming qualities in a coordinated manner in a dense client cell. Its lightweight computation design provides real-time and distributed processing of local sessions. To check its validity, a WiFi setup has been exercised where the accuracy of the system at the edge is checked by assessing the ITU-T P.1203 QoE of individual players. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature. Multi-access Edge Computing video analytics of ITU-T P.1203 Quality of Experience for streaming monitoring in dense client cells 5G; MEC; MOS; MPEG-DASH; QoE 5G mobile communication systems; Cells; Cytology; Dynamics; Edge computing; Motion Picture Experts Group standards; Video streaming; Dynamic Adaptive Streaming over HTTP; Edge computing; MEC; MOS; MPEG-dynamic adaptive streaming over HTTP; Multiaccess; Service traffic; Video analytics; Video players; Video streaming services; Quality of service",Financial management
1820,Implications of using Industry 4.0 base technologies for lean and agile supply chains and performance,"The adoption of Industry 4.0 (I4.0) technologies in recent years has generated conditions for substantial changes in supply chain management. However, research is still ongoing on how I4.0 technologies can be integrated into current supply chain models to improve supply chain capabilities and performance. This work aims to contribute to understanding the relationships between Industry 4.0 technologies and lean and agile supply chain strategies, and identifying the implications for the focal firm's operational performance. In this study, we focus on a specific group of emerging I4.0 technologies known as I4.0 base technologies (i.e., cloud computing, Internet of Things, and Big Data analytics), whose complementary features can enhance the data collection, storage, and sharing, as well as the analysis processes. Drawing on the Dynamic Capabilities Theory, a structural equation model is used to analyze data collected from 256 Spanish focal manufacturing firms. Results indicate that I4.0 base technologies do not have the same effects on lean and agile supply chain strategies. While I4.0 base technologies can make supply chains leaner, they have been found to have no significant direct effect on agile supply chain implementation. Further, findings indicate a direct relationship between the lean and agile approaches and that the latter generates mediation effects between lean and operational performance. © 2023 The Authors Implications of using Industry 4.0 base technologies for lean and agile supply chains and performance Agile supply chain; Dynamic capabilities; Industry 4.0 technologies; Lean supply chain Cloud analytics; Computation theory; Data Analytics; Digital storage; Enterprise resource management; Supply chain management; Agile supply chains; Condition; Current supplies; Dynamics capability; Industry 4.0 technology; Lean supply chains; Operational performance; Supply chain modeling; Supply chain strategy; Supply performance; Industry 4.0",Strategic alignment
1823,How big data analytics use improves supply chain performance: considering the role of supply chain and information system strategies,"Purpose: Drawing on the dynamic capabilities theory, this paper proposes that supply chain (SC) strategies (i.e. the lean SC and agile SC strategies) will mediate the relationship between big data analytics (BDA) and SC performance. Furthermore, from the perspective of strategic alignment, this study hypothesizes that the effect of the SC strategy on SC performance is differently moderated by the information system (IS) strategy (i.e. the IS innovator and IS conservative strategies). Design/methodology/approach: This study used 159 match-paired questionnaires collected from Chinese firms to empirically test the hypotheses. Findings: Results show the positive direct and indirect impact of BDA on SC performance. Specifically, the lean and agile SC strategies mediate the relationship between BDA and SC performance. Furthermore, the results indicate that the IS innovator and IS conservative strategies differentially moderate the effect of the lean and agile SC strategies on SC performance. Specifically, the IS innovator strategy positively moderates the effect of the agile SC strategy on SC performance. By contrast, the IS conservative strategy positively moderates the effect of the lean SC strategy on SC performance but negatively moderates the effect of the agile SC strategy on SC performance. Originality/value: This study provides a comprehensive understanding of how SC and IS strategies can help firms leverage BDA to improve SC performance. © 2022, Emerald Publishing Limited. How big data analytics use improves supply chain performance: considering the role of supply chain and information system strategies Agile supply chain strategy; Big data analytics; IS conservative Strategy; IS innovator Strategy; Lean supply chain strategy; Supply chain performance ",Strategic alignment
1824,Lean in industry 4.0 is accelerating manufacturing excellence – A DEMATEL analysis,"Purpose: The purpose of this research paper is to study the digital accelerators in conjunction with lean manufacturing enablers in the technology driven Industry 4.0 (I4.0) and understand their interrelationship dynamics with a goal to accelerate the pace of manufacturing excellence. Design/methodology/approach: Literature review coupled with the focus group approach facilitated to cull the key accelerating enablers to lean in I4.0. Thereafter, application of the multi criteria decision making methodology–DEMATEL (Decision Making Trial and Evaluation Laboratory) was carried out for analysis. Findings: A total of 18 factors from the integration of lean in I4.0 were identified from the focus group approach. The analysis from DEMATEL approach reflected that big data analytics and technology driven talent were the two most important factors in the manufacturing excellence journey. Leadership standard work and continuous improvement culture were the two key because category factors, while, just in time the critical effect category factor. Practical implications: Analysis from DEMATEL approach has provided useful insights to industry leaders with the details of the degree of importance and type of influencing factors. It has given them direction in areas of investment to face the challenges of smart factories of tomorrow for sustainability. Originality/value: Application of DEMATEL approach for analyzing the dynamics of the 18 factors in the integrated lean systems in I4.0 for manufacturing excellence. © 2022, Emerald Publishing Limited. Lean in industry 4.0 is accelerating manufacturing excellence – A DEMATEL analysis Industry 4.0; Lean; Manufacturing Agile manufacturing systems; Data Analytics; Factor analysis; Industrial research; Industry 4.0; Lean production; DEMATEL; Design/methodology/approach; Digital accelerators; Focus groups; Lean; Literature reviews; Manufacturing; Manufacturing excellence; Multi criteria decision-making; Research papers; Decision making",Value management
1825,Achieving Competitive Sustainable Advantages (CSAs) by Applying a Heuristic-Collaborative Risk Model,"Increasing disruption and turmoil continuously challenges organizations regarding the achievement of short-and long-term objectives. Such a hostile environment results from both the natural evolution of the business landscape complexity and the emergence of unpredictable disruptive evets such as the COVID-19 pandemic. More than ever, organizations should continuously develop business strategies that help them to become more agile, adaptative, sustainable, and effectively respond to the countless business risks (threats and opportunities). Innovation, such as the development and implementation of new technology, new ways of thinking and executing work, are just some of the major factors that can help organizations to increase their likelihood of success. In this work, is proposed the incorporation of a heuristic risk model into a typical organizational business intelligence architecture, to identify collaborative critical success factors across the different phases of a project life cycle which can be used to guide, monitor, and increase the success outcome likelihood of ongoing and upcoming projects. Some benefits of the incorporation include: a higher speed regarding the collection and treatment process of project collaborative data, the output of more accurate results with residual bias associated, a timely and efficient 360◦ view regarding the identification of project collaborative risks, and the impact (positive or/and negative) of these on a project’s outputs and outcomes. Finally, the model capabilities of performing descriptive, predictive, and prescriptive analysis, enables the generation of unique and actionable project’s lessons learned which can be used to make more data-informed decisions, and thus enhances the achievement of sustainable competitive advantages. The development and implementation of the proposed incorporation is illustrated with a with a real case study. © 2022 by the authors. Licensee MDPI, Basel, Switzerland. Achieving Competitive Sustainable Advantages (CSAs) by Applying a Heuristic-Collaborative Risk Model Business intelligence architecture; Critical success factors; Industry 4.0; Organizational digital transformation; Organizational network analysis; Organizational risk management; Sustainability business development; COVID-19; landscape; life cycle; risk assessment; sustainability",Risk management
1826,Analyzing Truck Driver's Behavior on the Road Using YOLO v4 Tiny Algorithm,"The Philippine logistics industry is always in need of competent and qualified truck drivers to meet the increasing demands of the country's economic operations. Thus, this research paper investigates the driving competency of truck drivers driving behaviour. The industry plays a critical part in the country's economy, and any trucking accident creates a significant problem for the parties involved that causes a decline in productivity and loss of revenue. This research paper discusses the creation of a system which utilizes machine learning, data analytics, and internet-of-things for easy monitoring and assessment of the truck driver's driving behaviour. In collaboration with the Department of Science and Technology and the Technological Institute of the Philippines, the project aims to integrate various technologies and use it to assess the roadworthiness of truck drivers and use the YOLO v4 algorithm to train the model. It provides all the necessary details covering the statistics needed to assess the truck driver that aligns with the standards set required by the Land Transportation Office (LTO) administration. The Agile methodology was used throughout the development as there are constant changes being made which adds flexibility and puts fewer risks for the system development. Based on the model compiled and the results of the iteration on Yolov4-tiny the classes average precision has a minimum value of 75.45% and a maximum value of 99.89%. The results showed a high accuracy performance of the model created in information retrieval and object detection.  © 2022 ACM. Analyzing Truck Driver's Behavior on the Road Using YOLO v4 Tiny Algorithm Ant Design; AWS; IoT; MERN Stack; Rest APIs; Truck Drivers; Web Application; Web Development Application programming interfaces (API); Automobiles; Internet of things; Truck drivers; Trucks; Ant design; AWS; IoT; MERN stack; Philippines; Research papers; Rest API; WEB application; Web applications; Web development; Iterative methods",Strategic alignment
1828,Lessons learned to improve the UX practices in agile projects involving data science and process automation,"Context: User-Centered Design (UCD) and Agile methodologies focus on human issues. Nevertheless, agile methodologies focus on contact with contracting customers and generating value for them. Usually, the communication between end users (they use the software and have low decision power) and the agile team is mediated by customers (they have high decision power but do not use the software). However, they do not know the actual problems that end users (may) face in their routine, and they may not be directly affected by software shortcomings. In this context, UX issues are typically identified only after the implementation, during user testing and validation. Objective: Aiming to improve the understanding and definition of the problem in agile projects, this research investigates the practices and difficulties experienced by agile teams during the development of data science and process automation projects. Also, we analyze the benefits and the teams’ perceptions regarding user participation in these projects. Method: We collected data from four agile teams, in the context of an academia and industry collaboration focusing on delivering data science and process automation solutions. Therefore, we applied a carefully designed questionnaire answered by developers, scrum masters, and UX designers. In total, 18 subjects answered the questionnaire. Results: From the results, we identify practices used by the teams to define and understand the problem and to represent the solution. The practices most often used are prototypes and meetings with stakeholders. Another practice that helped the team to understand the problem was using Lean Inception (LI) ideation workshops. Also, our results present some specific issues regarding data science projects. Conclusion: We observed that end-user participation can be critical to understanding and defining the problem. They help to define elements of the domain and barriers in the implementation. We identified a need for approaches that facilitate user-team communication in data science projects to understand the data and its value to the users’ routine. We also identified insights about the need of more detailed requirements representations to support the development of data science solutions. © 2022 Elsevier B.V. Lessons learned to improve the UX practices in agile projects involving data science and process automation Agile; Data science; Lean inception; User experience; User involvement; User participation; User-centered design Automation; Data Science; Process control; Agile; Agile Methodologies; Agile teams; Decision power; End-users; Lean inception; Process automation; User involvement; User participation; Users' experiences; User centered design",Stakeholder management
1830,Accelerating Digital Transformation of SMEs,"Digital transformation is happening across all industries worldwide, including in Singapore. This book seeks to provide insights on how small and medium enterprises (SMEs) can embark on their own journeys of digital transformation, while at the same time remaining agile in responding to industry and consumer needs. It will outline how firms can foster a culture of business transformation to improve efficiency and productivity; elaborate on how the COVID-19 pandemic has accelerated the digital transformation process, and how it has provided new opportunities; present a roadmap on how SMEs can navigate through the artificial and data analytics revolution; and provide recommendations on how SMEs can partner with institutes of higher learning. It concludes by elaborating on the skillsets and capabilities needed to drive digitalisation. Copyright © 2023 by World Scientific Publishing Co. Pte. Ltd. All rights reserved. Accelerating Digital Transformation of SMEs  ",Strategic alignment
1831,Digital Transformation Handbook,"Digital transformation has become more than a buzzword from the media since companies figured out the importance of rethinking business processes during global challenges. On its own, the term assumes integration of digital technology into all areas of a business, resulting in fundamental changes to how the company operates and delivers value to customers. Taking care of and choosing the optimal ICT tools is a constant struggle; the final decision may depend on the consultant’s experience. Including all business stakeholders in this process is a must. Creating innovative company culture, continuous learning, and developing new skills with flexible and open communication and willingness to experiment are a challenge. This complex, comprehensive approach can include implementing new systems, integrating existing systems, and using data analytics and artificial intelligence (AI) or machine learning to drive better outcomes. By adapting and exploiting digital technology in new ways, businesses can gain better and more detailed customer experiences and build stronger relationships with their clients. In addition, digital transformation can help organizations to be more agile and responsive, which can lead to less time needed for different processes or the ability to adapt quickly to changing conditions. In a time when change is the only constant – and it is hitting us every day not to forget about that – it is essential to think about digital transformation constantly. Technology improvement, availability, and scalability give us no room for excuses for not using them. Who can say that we are not living in dynamic and exciting times? The Authors have taken their 20 years of practical experience and put it into this handbook, in which many cases can be found where not every time is a success story. This book is prepared to provide some insights, give you a fresh overview of what such change can enable, and set up an environment for new technology that will arrive shortly. © 2024 Krunoslav Ris and Milan Puvača. Digital Transformation Handbook  ",Strategic alignment
1832,An Agile Concept Inventory Methodology to Detect Large Sets of Student Misconceptions in Programming Language Courses,"While learning new subjects, students often develop misconceptions that affect their results and even their academic success. Concept inventories (CIs) – collections of multiple-choice questions – are widely used instruments for spotting misconceptions and allowing instructors to correct them promptly. While the main advantage of CIs is responsiveness in analyzing answers, their measurement speed (the number of misconceptions that can be tested in a time frame) is poor. In this paper, we introduce an improved CI methodology that allows for accurate detections of broader sets of misconceptions in classes and, thus, to obtain detailed pictures of the student difficulties. Our methodology is based on observing the answers to the individual options of the multiple-choice questions, rather than to the question as a whole. We are therefore able to get more information from an item and thus improve the measurement speed. We integrated our methodology in a CS1 Java Programming Language course, and we tested 89 distinct misconceptions in 15 sessions of 30 min. Our methodology showed a 4x speed up in the measurement speed compared to state-of-the-art CIs, while preserving satisfactory accuracy. Thanks to this extensive coverage of misconceptions, we built a new metric, the “knowledge fitness”, to objectively measure student difficulties. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG. An Agile Concept Inventory Methodology to Detect Large Sets of Student Misconceptions in Programming Language Courses concept inventories; learning analytics; learning metrics; misconceptions; pedagogy; programming; programming languages Computer programming languages; Concept inventories; Language course; Learning analytic; Learning metrics; Measurement speed; Misconception; Multiple-choice questions; Pedagogy; Programming; Student misconceptions; Students",Strategic alignment
1833,Machine learning (ML) on the Internet of Things (IoT) streaming data toward real-time insights,"The world generates an unfathomable amount of data every day. The speed with which the data gets generated, transmitted, ingested, and crunched is nothing but spectacular in the recent past. Expert thinkers and pundits across the globe are of the opinion that data is the transformation agent. Data is positioned as a strategic asset for any institution, innovator, and individual to grow and glow and is being reasoned as the new fuel for bringing in real and sustainable business transformation. It is a universally accepted fact that data can be methodically processed, mined, and analyzed to produce actionable insights. There are batch and real-time data processing methods to make sense of data heaps. Further on, there are integrated data analytics platforms in plenty, to extract hidden patterns, associations, and other useful insights out of data volumes. The data analytics ecosystem grows continuously considering the importance of datadriven insights and insights-driven decisions for businesses as well as people to be agile, adaptive and adroit in their dealings and deeds. Off late real-time data capture, storage, analytics, decision-making, and action are being insisted upon vehemently considering the evolving business dynamics. Any data or message has to be carefully captured, cleansed, and crunched immediately in order to be really beneficial for businesses and commoners. It is indisputable that the data value goes down sharply with time. Another facet is that agile and autonomous business systems are extremely event-driven. That is, any business event may trigger a suite of events across. Thus, all kinds of event data/messages have to be received and processed in real time in order to activate and automate one or more business operations. In short, for envisaging and realizing real-time services, applications and enterprises, real-time data analytics capability is very much indispensable. Enterprises are therefore keenly strategizing and setting up analytics infrastructure modules with all the clarity and alacrity to make sense out of both internal and external data in time. Such a futuristic and flexible capability helps business houses to be sagaciously steered in the right direction. © The Institution of Engineering and Technology 2022. Machine learning (ML) on the Internet of Things (IoT) streaming data toward real-time insights  ",Capacity management
1836,Holistic Framework to Data-Driven Sustainability Assessment,"In recent years, the Twin-Transition reference model has gained notoriety as one of the key options for decarbonizing the economy while adopting more sustainable models leveraged by the Industry 4.0 paradigm. In this regard, one of the most relevant challenges is the integration of data-driven approaches with sustainability assessment approaches, since overcoming this challenge will foster more agile sustainable development. Without disregarding the effort of academics and practitioners in the development of sustainability assessment approaches, the authors consider the need for holistic frameworks that also encourage continuous improvement in sustainable development. The main objective of this research is to propose a holistic framework that supports companies to assess sustainability performance effectively and more easily, supported by digital capabilities and data-driven concepts, while integrating improvement procedures and methodologies. To achieve this objective, the research is based on the analysis of published approaches, with special emphasis on the data-driven concepts supporting sustainability assessment and Lean Thinking methods. From these results, we identified and extracted the metrics, scopes, boundaries, and kinds of output for decision-making. A new holistic framework is described, and we have included a guide with the steps necessary for its adoption in a given company, thus helping to enhance sustainability while using data availability and data-analytics tools. © 2023 by the authors. Holistic Framework to Data-Driven Sustainability Assessment continuous improvement; data analytics; data-driven sustainability; decarbonizing; holistic framework; Industry 4.0; lean thinking; sustainability assessment analytical framework; assessment method; data assimilation; holistic approach; sustainability; sustainable development",Strategic alignment
1837,"IFIP WG 5.7 International Conference on Advances in Production Management Systems, APMS 2023","The proceedings contain 58 papers. The special focus in this conference is on IFIP WG 5.7 International Conference on Advances in Production Management Systems. The topics include: A Systematic Literature Review on Combinations of Industry 4.0 and Lean Production; lean and Digital Strategy Role in Achieving a Successful Digital Transformation; tying Digitalization to the Lean Mindset: A Strategic Digitalization Perspective; characterization of Digitally-Advanced Methods in Lean Production Systems 4.0; synergies Between Industry 4.0 and Lean on Triple Bottom Line Performance; Driving Sustainability Through a VSM-Indicator-Based Framework: A Case in Pharma SME; Design and Application of a Development Map for Aligning Strategy and Automation Decisions in Manufacturing SMEs; using the Lean Approach for Improving Eco-Efficiency Performance: A Case Study for Plastic Reduction; work Pattern Analysis with and without Site-Specific Information in a Manufacturing Line; lean and Digitalization Status in Manufacturing Companies Located in Norway; digital Transformation Towards Industry 5.0: A Systematic Literature Review; industry 5.0 and Manufacturing Paradigms: Craft Manufacturing - A Case from Boat Manufacturing; industry 4.0 Readiness Assessment of Enterprises in Kazakhstan; critical Factors for Selecting and Integrating Digital Technologies to Enable Smart Production: A Data Value Chain Perspective; business Process Reengineering in Agile Manufacturing – A Mixed Method Research; service-Oriented Architecture for Driving Digital Transformation: Insights from a Case Study; application of Digital Tools, Data Analytics and Machine Learning in Internal Audit; Consumer Engagement in the Design of PLM Systems: A Review of Best Practices; a Distributed Ledger Technology Solution for Connecting E-mobility Partners; human in Command in Manufacturing. IFIP WG 5.7 International Conference on Advances in Production Management Systems, APMS 2023  ",Strategic alignment
1838,"IFIP WG 5.7 International Conference on Advances in Production Management Systems, APMS 2023","The proceedings contain 58 papers. The special focus in this conference is on IFIP WG 5.7 International Conference on Advances in Production Management Systems. The topics include: A Systematic Literature Review on Combinations of Industry 4.0 and Lean Production; lean and Digital Strategy Role in Achieving a Successful Digital Transformation; tying Digitalization to the Lean Mindset: A Strategic Digitalization Perspective; characterization of Digitally-Advanced Methods in Lean Production Systems 4.0; synergies Between Industry 4.0 and Lean on Triple Bottom Line Performance; Driving Sustainability Through a VSM-Indicator-Based Framework: A Case in Pharma SME; Design and Application of a Development Map for Aligning Strategy and Automation Decisions in Manufacturing SMEs; using the Lean Approach for Improving Eco-Efficiency Performance: A Case Study for Plastic Reduction; work Pattern Analysis with and without Site-Specific Information in a Manufacturing Line; lean and Digitalization Status in Manufacturing Companies Located in Norway; digital Transformation Towards Industry 5.0: A Systematic Literature Review; industry 5.0 and Manufacturing Paradigms: Craft Manufacturing - A Case from Boat Manufacturing; industry 4.0 Readiness Assessment of Enterprises in Kazakhstan; critical Factors for Selecting and Integrating Digital Technologies to Enable Smart Production: A Data Value Chain Perspective; business Process Reengineering in Agile Manufacturing – A Mixed Method Research; service-Oriented Architecture for Driving Digital Transformation: Insights from a Case Study; application of Digital Tools, Data Analytics and Machine Learning in Internal Audit; Consumer Engagement in the Design of PLM Systems: A Review of Best Practices; a Distributed Ledger Technology Solution for Connecting E-mobility Partners; human in Command in Manufacturing. IFIP WG 5.7 International Conference on Advances in Production Management Systems, APMS 2023  ",Strategic alignment
1841,Successful operational integration of healthcare analytics at Seattle Children's,"Introduction: As the quantity and complexity of health data grows, it is critical for healthcare organizations to devise analytic strategies that power data innovation so they can take advantage of new opportunities and improve outcomes. Seattle Children's Healthcare System (Seattle Children's) is an example of an organization that has built an operating model that integrates analytics into their business and daily operations. We present a roadmap for how Seattle Children's consolidated its fragmented analytics operations into a unified cohesive ecosystem capable of supporting advanced analytics capabilities and operational integration to transform care and accelerate research. Methods: In-depth interviews were conducted with ten leaders at Seattle Children's who have been instrumental in developing their enterprise analytics program. Interviews included the following leadership roles: Chief Data & Analytics Officer, Director of Research Informatics, Principal Systems Architect, Manager of Bioinformatics and High Throughput Analytics, Director of Neurocritical Care, Strategic Program Manager & Neuron Product Development Lead, Director of Dev Ops,Director of Clinical Analytics, Data Science Manager, and Advance Analytics Product Engineer. The interviews were unstructured and consisted of conversations intended to gather information from leadership about their experiences in building out Enterprise Analytics at Seattle Children's. Results: Seattle Children's has built an advanced enterprise analytics ecosystem that is integrated into its daily operations by applying an entrepreneurial mindset and agile development practices that are common in a startup environment. Analytics efforts were approached iteratively by selecting high-value projects that were delivered through Multidisciplinary Delivery Teams that were integrated into service lines. Service line leadership, in partnership with the Delivery Team leads, were responsible for the success of the team by setting project priorities, determining project budgets, and maintaining overall governance of their analytics endeavors. This organizational structure has led to the development of a wide range of analytic products that have been used to improve both operations and clinical care at Seattle Children's. Conclusions: Seattle Children's has demonstrated how a leading healthcare system can successfully create a robust, scalable, near real-time analytics ecosystem- one that delivers significant value to the organization from the ever-expanding volume of health data we see today. © 2022 The Authors. Learning Health Systems published by Wiley Periodicals LLC on behalf of University of Michigan. This article has been contributed to by YOU.S. Government employees and their work is in the public domain in the USA. Successful operational integration of healthcare analytics at Seattle Children's analytics operating model; data ecosystems; healthcare analytics; organizational innovation; scientific computing ",Governance
1842,Analysis of Business Data and Cybersecurity as New Areas of Activity for Business Analysts in the Context of Digital Transformation,"The current development of the economy is marked by a significant increase in the volume of information and digital data. This manifests itself at the macro- and micro-levels. The context in which today’s business exists has changed globally; the use of descriptive, predictive, or normative analytics is no longer enough. Therefore, new approaches are needed that will ensure its successful development. World practice in the field of business analysis shows that it already covers many areas to solve business problems, such as agile technologies, data analysis using digital technologies, analysis in the field of cybersecurity, and the use of the capabilities of data mining. The paper discusses the newly listed areas of business analysis and defines its capabilities in terms of business data analysis and in the field of cybersecurity analysis. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. Analysis of Business Data and Cybersecurity as New Areas of Activity for Business Analysts in the Context of Digital Transformation Business analysis; Business data analysis; Cybersecurity; Digitalization Cybersecurity; Data mining; Predictive analytics; 'current; Area of activities; Business analysis; Business analysts; Business data; Business data analyse; Cyber security; Digital transformation; Digitalization; Information data; Metadata",Monitoring and control
1843,Enterprise-Level IS Research: Challenges and Potentials of Looking Beyond Enterprise Solutions,"For more than 40 years, enterprise solutions, specifically enterprise systems, allowed companies to integrate enterprises’ operations throughout. Starting with integrating core operational functions, the integration scope of enterprise solutions has increasingly widened, now often covering customer activities, activities along supply chains, and business analytics. IS research has contributed a wide range of explanatory and design knowledge dealing with this class of IS. During the last two decades, however, not only technological innovations, but also managerial / organizational innovations not only extend the affordances of enterprise solutions, but also challenge traditional approaches to their design and coordination. Particularly in large enterprises or complex business ecosystems, many IT/business alignment issues have not yet been fundamentally addressed, and novel, more decentralized (aka agile) forms of coordination have not yet been integrated with mainstream IS design and management practice. At the same time, IS complexity is not harnessed at all, and is increasingly threatening to impose limits to IS efficiency and flexibility gains. This position paper presents a cross-solution (= enterprise-level) perspective on IS, discusses the challenges of complexity and coordination for IS design and management, presents selected enterprise-level insights for IS coordination and governance, and explores avenues towards a more comprehensive body of knowledge on this important level of analysis. Copyright © 2023 by SCITEPRESS – Science and Technology Publications, Lda. Under CC license (CC BY-NC-ND 4.0) Enterprise-Level IS Research: Challenges and Potentials of Looking Beyond Enterprise Solutions Enterprise Level of Analysis; Enterprise Systems; Information Systems Complexity; Information Systems Coordination; Information Systems Governance; Information Systems Management Coordination reactions; Ecosystems; Enterprise resource planning; Information management; Information use; Supply chains; Enterprise level of analyse; Enterprise system; Information system complexity; Information system coordination; Information system governances; Information system management; IS design; IS managements; Levels of analysis; Systems complexity; Information systems",Governance
1844,Integrating design thinking and agile approaches in analytics development: The case of Aginic,"Drawing on the story of Aginic and its educational analytics platform edPortal, this teaching case study examines how applying agile methods and design thinking to analytics has helped unlock significant value for Aginic, its clients and the education sector overall. It describes key factors driving the successful integration of agile values and design approaches, allowing students to gain a deep understanding on how such integration can facilitate the development of innovative data analytics products. © Association for Information Technology Trust 2023. Integrating design thinking and agile approaches in analytics development: The case of Aginic agile development; Business analytics; creativity and innovation; design thinking; education; user experience ",Strategic alignment
1845,"A Review of Cloud Computing on Sustainable Development: Contribution, Exploration and Potential Challenges","Sustainable development is a strategy that brings into consideration both the immediate and long-term requirements, such as social and economic justice or environmental conservation and natural wealth. Cloud computing is the supply of computing environments, including analytics, software, networking, databases, storage and servers, in order to provide agile creativity, scalable capabilities, and scale economies. This research explores many different cloud features that bring advantages to sustainable development, such as easy maintenance, positive economic impact, resource pooling and large network access. This study also investigates various features of contemporary cloud service providers, namely Amazon Web Services (AWS), Google Cloud and Microsoft Azure, that assist sustainable development. Lastly, this paper elaborates on potential challenges in cloud sustainability in terms of security, management, hidden expenses and lack of expertise. © 2022 IEEE. A Review of Cloud Computing on Sustainable Development: Contribution, Exploration and Potential Challenges cloud computing; cloud deployment models; cloud features; cloud service models; economic sustainability; environmental sustainability; social sustainability; sustainable development Cloud analytics; Distributed database systems; Economic and social effects; Web services; Windows operating system; Cloud deployment model; Cloud deployments; Cloud feature; Cloud service models; Cloud-computing; Deployment models; Economic sustainability; Environmental conservation; Environmental sustainability; Social sustainability; Sustainable development",Value management
1847,"Lessons Learned From Interdisciplinary Efforts to Combat COVID-19 Misinformation: Development of Agile Integrative Methods From Behavioral Science, Data Science, and Implementation Science","Background: Despite increasing awareness about and advances in addressing social media misinformation, the free flow of false COVID-19 information has continued, affecting individuals’ preventive behaviors, including masking, testing, and vaccine uptake. Objective: In this paper, we describe our multidisciplinary efforts with a specific focus on methods to (1) gather community needs, (2) develop interventions, and (3) conduct large-scale agile and rapid community assessments to examine and combat COVID-19 misinformation. Methods: We used the Intervention Mapping framework to perform community needs assessment and develop theory-informed interventions. To supplement these rapid and responsive efforts through large-scale online social listening, we developed a novel methodological framework, comprising qualitative inquiry, computational methods, and quantitative network models to analyze publicly available social media data sets to model content-specific misinformation dynamics and guide content tailoring efforts. As part of community needs assessment, we conducted 11 semistructured interviews, 4 listening sessions, and 3 focus groups with community scientists. Further, we used our data repository with 416,927 COVID-19 social media posts to gather information diffusion patterns through digital channels. Results: Our results from community needs assessment revealed the complex intertwining of personal, cultural, and social influences of misinformation on individual behaviors and engagement. Our social media interventions resulted in limited community engagement and indicated the need for consumer advocacy and influencer recruitment. The linking of theoretical constructs underlying health behaviors to COVID-19–related social media interactions through semantic and syntactic features using our computational models has revealed frequent interaction typologies in factual and misleading COVID-19 posts and indicated significant differences in network metrics such as degree. The performance of our deep learning classifiers was reasonable, with an F-measure of 0.80 for speech acts and 0.81 for behavior constructs. Conclusions: Our study highlights the strengths of community-based field studies and emphasizes the utility of large-scale social media data sets in enabling rapid intervention tailoring to adapt grassroots community interventions to thwart misinformation seeding and spread among minority communities. Implications for consumer advocacy, data governance, and industry incentives are discussed for the sustainable role of social media solutions in public health. © 2023 JMIR Publications Inc. All rights reserved. Lessons Learned From Interdisciplinary Efforts to Combat COVID-19 Misinformation: Development of Agile Integrative Methods From Behavioral Science, Data Science, and Implementation Science community engagement; COVID-19; deep learning; health belief model; misinformation; social media ",Strategic alignment
1848,Intertwining green SCM- and agile SCM-based decision-making framework for sustainability using GIVTFNs,"Purpose: Supply chain management (SCM)-embedded valuable resources, such as capital, raw-materials, products, partners, customers and finished inventories, where the evaluation of environmental texture and flexibilities are needed to perceive sustainability. The present study aims to identify and evaluate the directory of green and agile (G-A) attributes based on decision support framework (DSF) for identifying dominating measures in SCM. Design/methodology/approach: DSF is developed by exploiting generalized interval valued trapezoidal fuzzy numbers (GIVTFNs). Two technical approaches, i.e. degree of similarity approach (DSA) and distance approach (DA) under the extent boundaries of GIVTFNs, are implicated for data analytics and for recognizing constructive G-A measures based on comparative study for robust decision. A fuzzy-based performance indicator, i.e. fuzzy performance important index (FPII), is presented to enumerate the weak and strong G-A characteristics to manage knowledge risks in allied business environment. Findings: The modeling is illustrated from the insights of decision-makers for augmenting business value based on cognitive identification of measures, where the best performance score is identified by the “sustainable packaging” under the traits of green supply chain management (GSCM). “The use of Web-based applications” under the traits of agile supply chain management (ASCM) and “Outsourcing flexibility” under traits of ASCM is found as the second and third most significant performance characteristics for business sustainability. Additionally, the “Reutilization (recycling) and reprocessing” under GSCM in manufacturing and “Responsiveness and speed toward customers needs” under ASCM are found difficult in attainment. Research limitations/implications: The G-A evaluation will assist in attaining performance excellence in day-to-day operations and overall functioning. The outcomes will help executives to plan strategic objectives and attaining success. Originality/value: To reinforce the capabilities of SCM, wide extent of G-A dimensions are presented, concept of FPII is reported to manage knowledge risks based on identification of strong attributes and two technical approaches, i.e. DSA and DA under GIVTFNs are presented for attaining robust decision and directing managerial decision-making process. © 2022, Emerald Publishing Limited. Intertwining green SCM- and agile SCM-based decision-making framework for sustainability using GIVTFNs Agile management; Decision support framework (DSF); Fuzzy performance important index (FPII); Green management; Supply chain management (SCM) ",Strategic alignment
1850,"Impact of COVID-19 on open universities worldwide: case studies from Asia, Africa and Europe","Purpose: The outbreak of COVID-19 in 2020 has had a profound impact on education institutions at all levels. Open universities, with their privileged delivery method, have been in an advantageous position. In the earlier stages of the pandemic, they made remarkable contributions to assuring learning continuity. However, with more and more conventional universities migrating online, great changes have taken place in the field of higher education, and it is imperative for open universities to adjust their strategies in order to maintain their leading role in a technology-enabled education context. This paper aims to examine what challenges have been faced by open universities during the pandemic and how they will transform in the future. Design/methodology/approach: Six open universities in Asia, Africa and Europe were selected as cases in this research to make a comparative study based on the papers in the volume beyond distance education [1]. Similarities and differences among the cases were analyzed in order to identify the developing trend for open universities in the international context. Findings: The results showed that (1) open universities in these regions demonstrated their resilience in the pandemic; examples were that new technologies have been leveraged to implement totally online delivery with short notice and huge amount of learning resources were offered to the society. (2) However, they encountered challenges of delivering fully online examination due to the lockdown and quarantine policies, and open universities in African and the sole private institution suffered financial pressure due to improving information and communication technology infrastructure and staff training. Another challenge was the fierce competition from conventional universities that open universities in Asia and Europe came across. (3) Four main areas were identified for future development in order to respond to the challenges: No.1 is that programs such as health care, psychology, epidemiology, virology, immunology, data analytics, biology and bio-informatics have stimulated interest for African open universities to develop in the future; No. 2 is that open universities were seeking to innovate their teaching formats; short courses, such as micro credentials, might be developed as agile and flexible offerings which are expected to be suitable to learners in the pandemic context; No 3 is that programs and courses for upskilling in the context of digitalization will be implemented; and No. 4 is that lifelong learning is given a higher priority in order for open universities to stand securely in the higher education sector. Originality/value: The study may give open university leaders a quick insight into their future development. © 2023, Songyan Hou. Impact of COVID-19 on open universities worldwide: case studies from Asia, Africa and Europe Challenges; COVID-19; Developing trend; Impacts; Open universities; Responses ",Strategic alignment
1852,Moving towards agile cybersecurity incident response: A case study exploring the enabling role of big data analytics-embedded dynamic capabilities,"Organizations are at risk of cyber-attacks more than ever before due to the ongoing digitalization of business operations. Industry reports indicate that it is not a matter of if but when organizations become victims of cyber-attacks or breaches. In this research, we argue that organizations must enable agility in their incident response (IR) to quickly respond to diverse cybersecurity threats, and big data analytics (BDA) plays a pivotal role in enabling agility in the IR. Drawing from dynamic capabilities theory, we conducted a field study using a case study approach to examine the following research question: What dimensions of big data analytics-embedded dynamic capabilities enable agility in cybersecurity incident response? We develop a framework that presents five key dimensions of BDA-embedded dynamic capabilities (data consolidation, threat intelligence, incident investigation, analytical skillset, and cybersecurity analytics warehouse) in IR at four specific stages, that is, manual analysis, basic analytics, advanced analytics, and pervasive analytics. The detail of the framework explains how BDA-embedded dynamic capabilities at the pervasive analytics stage enable agility in IR by infusing agile characteristics of flexibility, speed, and learning in IR. This study contributes to the knowledge of IT-embedded dynamic capabilities and cybersecurity IR agility. Detailed recommendations are also provided for potential practitioners. © 2023 The Author(s) Moving towards agile cybersecurity incident response: A case study exploring the enabling role of big data analytics-embedded dynamic capabilities Agility; Big data analytics; Cybersecurity; Dynamic capabilities; Incident response Big data; Crime; Cybersecurity; Network security; Agility; Big data analytic; Business operation; Case-studies; Cyber security; Cyber-attacks; Data analytics; Dynamics capability; Field studies; Incident response; Data Analytics",Capacity management
1853,From digital tools to digital methodology: About the agile introduction of BI: Support driver-based planning,"A service company in the human resources sector is considered, which already uses digital tools in the area of financial controlling. The creation of a plan and its periodic comparison to the realized values has been characterized by a considerable manual effort and limited to the initial design of the planning and controlling template. The resulting lack of flexibility is in contrast to a dynamic market environment as well as an expansion of the core areas of the company. The goals of digitization are to build an agile business intelligence supported controlling and reporting framework and the associated driver-based planning. This case study also highlights the technical relationship between the two topics. The timeline of the project and an ongoing success story about digitalization in financial controlling are described. © The Author(s), under exclusive license to Springer Fachmedien Wiesbaden GmbH, part of Springer Nature 2023. All rights reserved. From digital tools to digital methodology: About the agile introduction of BI: Support driver-based planning  ",Financial management
1855,The Review for Visual Analytics Methodology,"Big data usage evolves from previously looking into the capacity of big data's descriptive and diagnostic perspectives into currently feeding the demands for predictive big data analytics. The needs come about due to organizations that crave predictive analytics capabilities to reduce risk, make intelligent decisions, and generate different customer experiences. Similarly, visual analytics play an essential role in understanding and fitting the analytics prediction in their business decision. Hence, the combination of descriptive, diagnostics and predictive within Visual Analytics emerges as a balanced field to provide understandable predictive insight. Due to the organizational demand and multi-discipline area, the approach to developing visual analytics is still uncertain in the Big Data Project Lifecycle from methodological perspectives. While there are a few potential methodological approaches that could be used for visual analytics, they are scattered across numerous academic research and industrial practice. To date, there is no coherent review and analysis of the work that has been explored specifically for Visual Analytics methodology. This paper reports on a review of previous literature concerning how Visual Analytics has been executed in the big data life cycle to address the gap. The review is organized in this study from three perspectives: i) general ICT -related methodology (e.g. SDLC, Agile, DevOps), ii) Data Science-related methodology (e.g. CRISP-DM, SEMMA, KDD) and iii) Visual Analytics-related methodologies in which each method will be benchmarked based on the Visual Analytics major part of reality, computer and human, in terms of its width, Depth, and flows. This study found insufficiencies, non-specific and vague conditions in handling the Visual Analytics when using current methodological approaches based on the review conducted. The paper also highlights the Visual Analytics-related methodological review, which can she would some light on the approaches and ways of implementing analytics in the big data lifecycle, which can be beneficial for future studies in proposing a more comprehensive methodology for Visual Analytics in the big data lifecycle. © 2022 IEEE. The Review for Visual Analytics Methodology big data analytics; methodology; process; visual analytics Data Analytics; Industrial research; Life cycle; Predictive analytics; Big data analytic; Customer experience; Data analytics; Data lifecycle; Data usage; Intelligent decisions; Methodological approach; Methodology; Process; Visual analytics; Big data",Value management
1858,"23rd International Conference on Enterprise Information Systems, ICEIS 2021","The proceedings contain 26 papers. The special focus in this conference is on Enterprise Information Systems. The topics include: Blockchain-Based Enterprise Ballots in an Oil and Gas Consortium; automated Support for Risk Management in Scrum Agile Projects; defining Digital Legacy Management Systems’ Requirements; exploring Technical Debt Tools: A Systematic Mapping Study; NFR Evaluation in IoT Applications: Methods, Strategies and Open Challenges; on Persistent Implications of E2E Testing; applying Affordance Theory to Big Data Analytics Adoption; business Process Transformation During Covid-19; Self-organizing Federation of Autonomous MQTT Brokers; evaluation of Taxi Service with Regard to the Drivers Income Using Simulation Support; investigating Differential Privacy Outcomes; Eye-Tracking and Usability in (Mobile) ERP Systems; stressor Event Covid-19 Lockdown? A Multi-wave Study on Young People Starting Their Professional Careers; patterns for Using Fractal Enterprise Modelling in Operational Decision-Making; enabling Digital Twins in Industry 4.0; Dynamic Enterprise Business Development with Dual Capability EAM; adaptive Enterprise Architecture: Complexity Metrics in a Mixed Evaluation Method; an Approach to Evolution Management in Integrated Heterogeneous Data Sources; A Blockchain-Based Approach for COVID-19 Vaccine Lifecycle; a Complete Step-by-Step Methodology for Defining, Deploying and Monitoring a Blockchain Network in Industry 4.0; impact of Self-organization on Tertiary Objectives of Production Planning and Control; Application and Comparison of CC-Integrals in Business Group Decision Making; Edge Deep Learning Towards the Metallurgical Industry: Improving the Hybrid Pelletized Sinter (HPS) Process; online Algorithms for Prize-Collecting Optimization Problems. 23rd International Conference on Enterprise Information Systems, ICEIS 2021  ",Strategic alignment
1860,Reimagining the Fashion Retail Industry Through the Implications of COVID-19 in the Gulf Cooperation Council (GCC) Countries,"The COVID-19 pandemic has disrupted the fashion retail industry. The Gulf Cooperation Council Countries (GCC) is the home of family-centric shopping malls and brick and mortar stores (B&M). This article aims to provide a critical look at the business strategies which the fashion retail companies need to adopt to provide consumers with an integrated online and B&M service which will be essential to survive in the post-pandemic business environment. This article is based on the rich industry experience of the authors and extensive secondary research on the business strategies being employed by the leading fashion retailers in the GCC region to combat the pandemic disruption. The study highlights the importance of a comprehensive rethink on business strategy for the GCC fashion retailers with adoption of digitalization technologies and an adaptive supply chain as the pillars to survive the post-pandemic normal of business environments. The study concludes with a look to the future strategies for fashion retailers in developing a digitalization blueprint, using cloud technologies and big data analytics, leveraging social media, building an agile and adaptive supply chain with omnichannel capability, and ensuring that future products and services are sustainable and socially responsible. © 2021 Fortune Institute of International Business. Reimagining the Fashion Retail Industry Through the Implications of COVID-19 in the Gulf Cooperation Council (GCC) Countries B&M stores; business strategy; COVID-19; e-commerce; fashion retail; GCC ",Strategic alignment
1864,Monitoring Cloud-Native Applications: Lead Agile Operations Confidently Using Open Source Software,"Introduce yourself to the nuances of modern monitoring for cloud-native applications running on Kubernetes clusters. This book will help you get started with the concepts of monitoring, introduce you to popular open-source monitoring tools, and help with finding the correct set of use cases for their implementation. It covers the in-depth technical details of open-source software used in modern monitoring systems that are tailor made for environments running microservices. Monitoring Cloud-Native Applications is divided into two parts. Part 1 starts with an introduction to cloud-native applications and the foundational concepts of monitoring. It then walks you through the various aspects of monitoring containerized workloads using Kubernetes as the de-facto orchestration platform. You will dive deep into the architecture of a modern monitoring system and look at its individual components in detail. Part 2 introduces you to popular open-source tools which are used by enterprises and startups alike and are well established as the tools of choice for industry stalwarts. First off, you will look at Prometheus and understand its architecture and usage. You will also learn about InfluxDB, formerly called TICK Stack (Telegraf, InfluxDB, Chronograf, and Kapacitor). You will explore the technical details of its architecture and the use cases which it solves. In the next chapter, you will be introduced to Grafana, a multi-platform open source analytics and interactive visualization tool that can help you with visualization of data and dashboards. After reading this book, you will have a much better understanding of key terminologies and general concepts around monitoring and observability. You will also be able to select a suitable monitoring solution from the bouquet of open-source monitoring solutions available for applications, microservices, and containers. Armed with this knowledge, you will be better prepared to design and lead a successful agile operations team. What You Will Learn Monitor and observe of metrics, events, logs, and traces Carry out infrastructure and application monitoring for microservices architecture Analyze and visualize collected data Use alerting, reporting, and automated actions for problem resolution Who This Book Is For DevOps administrators, cloud administrators, and site reliability engineers (SREs) who manage and monitor applications and cloud infrastructure on a day-to-day basis within their organizations. © 2021 by Mainak Chakraborty and Ajit Pratap Kundan. All rights reserved. Monitoring Cloud-Native Applications: Lead Agile Operations Confidently Using Open Source Software Application Performance Monitoring; Container Monitoring; DevOps; etrics; Monitoring; Observability; Performance Analysis; Visualization ",Monitoring and control
1866,A Business Intelligence Tool for Explaining Similarity,"Agile Business often requires to identify similar objects (firms, providers, end users, products) between an older business domain and a newer one. Data-driven tools for aggregating similar resources are nowadays often used in Business Intelligence applications, and a large majority of them involve Machine Learning techniques based on similarity metrics. However effective, the mathematics such tools are based on does not lend itself to human-readable explanations of their results, leaving a manager using them in a “take it as is”-or-not dilemma. To increase trust in such tools, we propose and implement a general method to explain the similarity of a given group of RDF resources. Our tool is based on the theory of Least Common Subsumers (LCS), and can be applied to every domain requiring the comparison of RDF resources, including business organizations. Given a set of RDF resources found to be similar by Data-driven tools, we first compute the LCS of the resources, which is a generic RDF resource describing the features shared by the group recursively—i.e., at any depth in feature paths. Subsequently, we translate the LCS in English common language. Being agnostic to the aggregation criteria, our implementation can be pipelined with every other aggregation tool. To prove this, we cascade an implementation of our method to (i) the comparison of contracting processes in Public Procurement (using TheyBuyForYou), and (ii) the comparison and clustering of drugs (using k-Means) in Drugbank. For both applications, we present a fairly readable description of the commonalities of the cluster given as input. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG. A Business Intelligence Tool for Explaining Similarity Explainable Artificial Intelligence (XAI); Least Common Subsumer (LCS); Resource Description Framework (RDF) Artificial intelligence; Information analysis; K-means clustering; Learning systems; Business domain; Business intelligence applications; Business-intelligence; Data driven; End-users; Explainable artificial intelligence (XAI); Intelligence tool; Least common subsume; Resource description framework; Resources description frameworks; Resource Description Framework (RDF)",Strategic alignment
1868,BERT-Based Secure and Smart Management System for Processing Software Development Requirements from Security Perspective,"Software requirements management is the first and essential stage for software development practices, from all perspectives, including the security of software systems. Work here focuses on enabling software requirements managers with all the information to help build streamlined software requirements. The focus is on ensuring security which is addressed in the requirements management phase rather than leaving it late in the software development phases. The approach is proposed to combine useful knowledge sources like customer conversation, industry best practices, and knowledge hidden within the software development processes. The financial domain and agile models of development are considered as the focus area for the study. Bidirectional encoder representation from transformers (BERT) is used in the proposed architecture to utilize its language understanding capabilities. Knowledge graph capabilities are explored to bind together the knowledge around industry sources for security practices and vulnerabilities. These information sources are being used to ensure that the requirements management team is updated with critical information. The architecture proposed is validated in light of the financial domain that is scoped for this proposal. Transfer learning is also explored to manage and reduce the need for expensive learning expected by these machine learning and deep learning models. This work will pave the way to integrate software requirements management practices with the data science practices leveraging the information available in the software development ecosystem for better requirements management. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. BERT-Based Secure and Smart Management System for Processing Software Development Requirements from Security Perspective Bidirectional encoder transformers; Knowledge graphs; Requirements management; Software development; Transfer learning Deep learning; Human resource management; Learning systems; Requirements engineering; Signal encoding; Bidirectional encoder transformer; Financial domains; Knowledge graphs; Management IS; Management systems; Processing software; Requirement management; Software requirements; Software requirements management; Transfer learning; Software design",Governance
1869,"CAiSE-DC 2022 - Proceedings of the Doctoral Consortium Papers Presented at the 34th International Conference on Advanced Information Systems Engineering, CAiSE 2022","The proceedings contain 10 papers. The topics discussed include: towards better data selection for self-service business intelligence outputs: a local authorities case study; process-aware attack-graphs for risk quantification and mitigation in industrial infrastructures; the management of artificial intelligence: developing a framework based on the artificial intelligence maturity principle; complex behaviors detection and analysis in process mining; probabilistic and non-deterministic event data in process mining: embedding uncertainty in process analysis techniques; a new definition of unambiguous user stories: a proposed framework to identify and avoid ambiguity in user stories; the MERLIN method to create modeling assistants for model-driven development tools; a reference framework for agile transformation in public administration; strategy execution through enterprise architecture: capability-based management as a foundation for successful (digital) transformations; and sensemaking in multi-artefact information tasks. CAiSE-DC 2022 - Proceedings of the Doctoral Consortium Papers Presented at the 34th International Conference on Advanced Information Systems Engineering, CAiSE 2022  ",Strategic alignment
1874,Demonstration of latency-aware 5G network slicing on optical metro networks,"The H2020 METRO-HAUL European project has architected a latency-aware, cost-effective, agile, and programmable optical metro network. This includes the design of semidisaggregated metro nodes with compute and storage capabilities, which interface effectively with both 5G access and multi-Tbit/s elastic optical networks in the core. In this paper, we report the automated deployment of 5G services, in particular, a public safety video surveillance use case employing low-latency object detection and tracking using on-camera and on-the-edge analytics. The demonstration features flexible deployment of network slice instances, implemented in terms of European Telecommunications Standards Institute (ETSI) network function virtualization network services. We summarize the key findings in a detailed analysis of end-to-end quality of service, service setup time, and soft-failure detection time. The results show that the round-trip time over an 80 km link is under 800 μs and the service deployment time is under 180 s.  © 2009-2012 Optica Publishing Group. Demonstration of latency-aware 5G network slicing on optical metro networks  Cost effectiveness; Metropolitan area networks; Network function virtualization; Object detection; Quality of service; Queueing networks; Security systems; Cost effective; Elastic optical networks; European program; Latency-aware; Metro networks; Network slicing; Optical-; Public safety; Storage capability; Video surveillance; 5G mobile communication systems",Risk management
1876,Creating Value from Front End Digitalization Versus Conventional Work Processes,"PETRONAS has been developing the Concept Factory++ (CF++) Front End Digitalization application since early 2021. The CF++ application has been rolled out using Agile development methodology and is currently being utilized in Screening and Feasibility Front End studies. CF++ has unlocked significant value in quality, speed, cost, and user experience compared with conventional work processes. This paper details the CF ++ automated Front End work process, its integration within the digital development planning workflow, and provides an assessment of CF++ Value Creation compared to conventional work processes previously utilized. The key motivations for digitalization were a highly manual process, inability to integrate across domains and dataset, speed leakage and potential of non-conformances due to data handling and inconsistencies. PETRONAS' Front End transformation digitalizes the conventional work processes and enhances the concept ideation, technical-commercial definition, and knowledge capture processes. Process enhancements include integrated Front End technical and cost analytics engines, and broader enterprise digital Field Development processes to promote collaborative working space; maximize the use of extensive internal project data, and embedded Best in Class benchmarking. Throughout the development, CF++ was heavily stress tested to validate the techno-commercial engine and the end-to-end integration from data input to report automation. A project analysis comparing use of CF ++ vs. conventional work processes was completed to determine improvement in speed, quality and cost. The paper will present savings in speed, cost and accuracy by comparing CF++ outputs against the manual process deliverables from out turn projects. CF++ created value across all areas, with key savings drivers reviewed. Analysis shows that CF++ also provides significant accuracy improvement earlier in the FEL process, allowing reduced contingency to be applied. The Front End digitalization process identified significant value to stakeholders through increased pace of delivery, improved early concept definition with limited human intervention, increased cost accuracy, and increased confidence in the results through replicatability, improved data supply and benchmarking rigour. CF++ also enhanced Field Development Team's experience through improved collaboration and flexible integration with the subsurface and economics tools and database. A number of challenges through the transformation process were identified and will be presented. This includes the extensive verification requirements due to the tool complexity, to assure quality of coding at module level and during the end-to-end test. Fully integrated Front End process digitalization has rarely been attempted within E&P companies. Further, incorporating knowledge and learning insights within process analytics allows for optimization and Best in Class solutions to be delivered digitally. Using the agile approach, CF++ will be augmented with new business and sustainability modules to achieve strategic business objectives. Copyright © 2022, Society of Petroleum Engineers. Creating Value from Front End Digitalization Versus Conventional Work Processes  Data handling; Engines; Gasoline; Integration; Agile development methodologies; Development planning; Field development; Front end; Integrated front ends; Manual process; PETRONAS; Users' experiences; Work process; Work-flows; Benchmarking",Strategic alignment
1877,Emerging Market Retail: Transitioning from a Product-Centric to a Customer-Centric Approach,"In an environment with digital disruptions, retailers must adopt a customer-centric approach to survive and compete effectively. Retailers need to be agile and forward-looking in adopting the relevant analytics and performance metrics to bring a customer-centric approach across upstream and downstream activities in the retail value chain. However, retailers in emerging markets (EMs) need clarity on the specific analytics and performance metrics in the value chain that will enable them to transition from their current product-centric state to the desired customer-centric state. Employing a triangulation approach (i.e., literature review, marketplace evidence, and managerial interviews) in the fragmented retail landscape of EMs, this study provides an organizing framework that explains: (i) the need for a customer-centric approach across the retail value chain, (ii) the specific performance metrics that need to be adopted across upstream and downstream activities in the retail value chain to enable THEM retailers to achieve their desired customer-centric state, and (iii) the role of analytics in providing insights to achieve these performance metrics and improving monetary and non-monetary firm performance outcomes. We also provide firm-specific and macro-level conditions that can influence the THEM retailers’ adoption of relevant analytics and explain the different paths retail formats can follow to adopt analytics. We present a strategy matrix that enables retail managers to identify the appropriate analytics to be adopted at different retail value chain stages to achieve desired performance metrics. We also highlight future research opportunities in retailing in EMs. © 2021 New York University Emerging Market Retail: Transitioning from a Product-Centric to a Customer-Centric Approach Emerging Markets; Organized and Unorganized Retailers; Performance Metrics; Retail Analytics; Retail Value Chain; Strategic Matrix ",Strategic alignment
1881,A Case Study on Project Realization Digitalization from the Domain Perspective,"This paper provides a Case Study in Front End Project Realization digitalization from the Domain perspective, with a focus on the methodology used, process enhancements that were enabled through automation, and lessons learned during the transformation. The transformation has been an iterative process, first focusing on digitalizing modules within the Front End work process and evolving into a multi-discipline integrated digital application. Along the journey, application of Agile project strategies enabled continuous enhancements to be identified and implemented through lessons learned, formal design thinking reviews, new idea generation and informal engagements with other disciplines commencing their digital journey. The process enhancements include: • New Ways of Working to seamlessly integrate Front End technical and cost analytics engines, and across broader enterprise digital Field Development processes. • New Sources of Insight to expand ideation using cross industry learnings, maximize use of extensive internal project data, and embedded Best in Class benchmarking. The Front End digitalization process identified significant value to stakeholders through increased pace of delivery, improved early concept definition with limited human intervention, increased cost accuracy, and increased confidence in the results through replication, improved data supply and benchmarking rigor. Specific value unlocks are seen across Front End Loading (FEL) i.e., pre-FEL to FEL-2 stages and will be presented. Through incorporation of enhanced data and insights, improved cost compression and decision-making quality has also been identified and subsequently will improve the project economics. A number of challenges through the transformation process were identified. These included: integration or replacement of legacy technical and cost applications; identifying and digitalizing a wide range of internal engineering tools and data sources needed for a comprehensive digital Front End process; efficiency of translating technical requirements to the digital team through comprehensive mapping of design and experienced-based rules; and re-shaping Front End technical focus from deliverable generation to targeted assurance, value obsession and risk management. While major focus has been on the integration of internal technical and cost applications, significant challenges were also identified in integration of external applications and Application Programming Interface (API) readiness to allow interaction between the applications and the Front End digital application i.e., Concept Factory. And finally, challenges in achieving a high performance team with the right balance of Domain, translators and programmers will be discussed. © Copyright 2021, Society of Petroleum Engineers A Case Study on Project Realization Digitalization from the Domain Perspective  Application programming interfaces (API); Cost engineering; Costs; Decision making; Integration; Iterative methods; Risk management; Case-studies; Digital applications; Front end; Front-end loading; Is-enabled; Iterative process; Multi disciplines; Process enhancements; Project strategy; Work process; Benchmarking",Value management
1882,Minimising Risk—The Application of Kotter’s Change Management Model on Customer Relationship Management Systems: A Case Study,"Implementing a Customer Relationship Management (CRM) system requires significant consideration with respect to change management and the associated business risks. This paper describes how to best achieve the change goal and minimize these risks. The research question under investigation is: “How can Kotter’s change management model be used effectively to enhance the value and utilisation of a CRM system”. Kotter’s eight-stage change model is the adopted change model used by the organisation under study. As business intelligence (BI) is a growing field within industry and academia alike, limited substantive research has been done regarding how to manage the change process itself within a BI project. Often research either focuses on the technical development (e.g., agile methodology) or the change process from a holistic perspective. However, both are needed to effectively manage the risk of failure. The research design for this study was that of a single organisation case study. The research questions were addressed by using a deductive research style. To allow for multiple perspectives and triangulation of the data, a mixed-methods approach (Quant + QUAL) was used. Outcomes of the research showed that whilst there was some success in the implementation of Kotter’s change model, it could have been significantly improved if the competencies identified in this research were considered and incorporated prior and during the change journey. Building on Kotter’s classic work with change management, this research fills the gap by describing the pertinent competencies required in managing the change process, identifying common pitfalls and investigating the common threads between the ‘data to outcome’ process and the change management process to better mitigate the risk This paper adds value to current change literature/models by defining and describing the importance of these competencies when embarking on a change program related to BI tools and systems and how these competencies are incorporated into Kotter’s model. © 2021 by the authors. Minimising Risk—The Application of Kotter’s Change Management Model on Customer Relationship Management Systems: A Case Study business intelligence; case study; change competencies; change management; CRM; Kotter’s change model; risk ",Risk management
1883,Leveling up tech skills in the internal audit profession: A window into value creation in evolving digitalized business landscapes,"Purpose: This paper aims to review the latest management developments across the globe and pinpoint practical implications from cutting-edge research and case studies. Design/methodology/approach: This briefing is prepared by an independent writer who adds their own impartial comments and places the articles in context. Findings: This research paper unpacks the potential of a business' internal audit function (IAF) to evolve with digitalization. The results reveal three ways that IAFs adapt to emerging technological risks. Firstly, by being agile in extending the scope of internal audit roles to include activities like assessing cybersecurity risks. Secondly, by responding to senior management demands for more consulting-type support by morphing into an advisory role. Thirdly, by modifying IAF practices to incorporate a shift in importance towards digital skills like data analytics. The challenges in executing these adaptations rest in auditors upskilling themselves as digital hybrids, while always maintaining independence. Originality/value: The briefing saves busy executives, strategists and researchers hours of reading time by selecting only the very best, most pertinent information and presenting it in a condensed and easy-to-digest format. © 2020, Emerald Publishing Limited. Leveling up tech skills in the internal audit profession: A window into value creation in evolving digitalized business landscapes Artificial intelligence; Data analytics; Digitalization; Internal audit function; Internal auditors ",Strategic alignment
1884,Digital transformation by enabling strategic capabilities in the context of “BRICS”,"Purpose: This study characterizes the scenario of emerging countries (ECs) – “Brazil, Russia, India, China and South Africa (BRICS)” concerning digital transformation and its association with the Industry 4.0 (I4.0) value creation system. For such, the authors developed a discussion paper based on content analysis of 857 journals in business administration, describing in a proposed framework the institutionalization “BRICS” policies that nurture global competitiveness among ECs and development needs to catching up. Design/methodology/approach: Data from 16 official documents of government, ministries and economic studies were analyzed by applying Atlas TI contrasting theory of 875 papers to develop and discuss the framework. Content analysis showed research gaps, technological needs and governance to enable firms to sustain competitive advantages applying I4.0 value creation system. Results converged into a microfoundation of the agile journey of a digital transformation to global organizations in between BRICS. Findings: This paper's central question is to understand: How can organizations achieve a sustainable I4.0 value creation system adopting digital transformation in “BRICS”? The reduced transaction costs driven by platforms and ecosystems orchestration and the related or integrated multiple level sources of knowledge could speed benefits of domestic firms and subsidiaries of global organizations. Research gaps could be understood by a new combination of resources and knowledge, exploiting technologies and, also, the discussion of social economic relevance of I4.0. Research limitations/implications: Because of the complexity and the novelty of the framework, further studies could be discussed by its elements. New structures and paths for alternative strategic factors may be proposed in the future with the inclusion of new relationships in the adoption of platform business models and ecosystems. Future studies should consider digital knowledge-based assets attained to economic activities across national boundaries; data analytics or data-driven technology adoption and their effects on global attractiveness. Practical implications: The paper implicates in evaluating whether dynamic capabilities subsidize performance propitiating the catching up with a focus on the I4.0 system and digital transformation management journey. The proposed framework demonstrates the benefits of digital transformation by enabling strategic capabilities, making efforts to reduce a lack of research paths concerning the policy attributes that define the platform use strategy from an architectural standpoint and its benefits. Social implications: The particularities of turning either an I4.0 global organization or a digital organization operate in various environments, allowing access to the activities' digital context. Social implications concerning digital resources as strategic accelerators are determined by the BRICS peculiarities, such as social behavior, consumerism or communication pattern, leadership and workforce skills. Finally, political aspects and interference in the economy are deployed in society what must be considered. Originality/value: This paper proposes a conceptual framework to better understand whether the heterogeneity of resources could explain I4.0 and digital configurations, while new platforms have driven features in global industrial environments and ecosystems. The seizing opportunities in these countries and sense-making use of platforms and orchestration of ecosystems are found as the critical topics being the main value of this important discussion. © 2021, Cristiana Rennó D’Oliveira Andrade and Cláudio Reis Gonçalo. Digital transformation by enabling strategic capabilities in the context of “BRICS” 4.0 industry; Digital transformation; Emerging countries; Strategic capabilities ",Strategic alignment
1886,Data Virtual Machines: Enabling Data Virtualization,"Modern analytics environments are characterized by a data infrastructure that comprises a great variety of datasets, data formats, data management and processing systems. Such environments are dynamic and data analysis needs to be performed in a flexible and agile manner via data virtualization techniques. Towards this end, we have proposed the Data Virtual Machine (DVM), a graph-based conceptual model based on entities and attributes. The basic idea of the DVM is that the relations of entities and attributes are based and expressed as the output of data processing tasks. In this paper we discuss the notion of data virtualization and propose a set of goals for relevant techniques in terms of modeling capabilities, query formulation and schema flexibility. We also place DVMs with respect to these goals. © 2021, Springer Nature Switzerland AG. Data Virtual Machines: Enabling Data Virtualization Data virtual machines; Data virtualization Information management; Network security; Virtual machine; Virtual reality; Virtualization; Conceptual model; Data infrastructure; Data management system; Data virtual machine; Data virtualization; Data-processing system; Graph-based; Model-based OPC; Modelling capabilities; Virtualization Techniques; Graphic methods",Monitoring and control
1887,"Reimagining Digital Learning for Sustainable Development: How Upskilling, Data Analytics, and Educational Technologies Close the Skills Gap","Reimagining Digital Learning for Sustainable Development is a comprehensive playbook for education leaders, policy makers, and other key stakeholders leading the modernization of learning and development in their institutions as they build a high value knowledge economy and prepare learners for jobs that do not yet exist. Currently, nearly every aspect of human activity, including the ways we absorb and apply learning, is influenced by disruptive digital technologies. The jobs available today are no longer predicators of future employment, and current and future workforce members will need to augment their competencies through a lifetime of continuous upskilling and reskilling to meet the demands of the Fourth Industrial Revolution. This book features curated insights and real-world cases from thought leaders throughout the world and identifies major shifts in content formats, pedagogic approaches, technology frameworks, user and design experiences, and learner roles and expectations that will reshape our institutions, including those in emerging economies. The agile, lean, and cost-effective strategies proposed here will function in scalable and flexible bandwidth environments, enabling education leaders and practitioners to transform brick-and-mortar learning organizations into digital and blended ecosystems and to achieve the United Nation's ambitious Sustainable Development Goals by 2030. © 2021 selection and editorial matter, Sheila Jagannathan. All rights reserved. Reimagining Digital Learning for Sustainable Development: How Upskilling, Data Analytics, and Educational Technologies Close the Skills Gap  ",Risk management
1888,Log Sinks and Big Data Analytics along with User Experience Monitoring to Tell a Fuller Story,"Understanding and continually improving the user experience is critical to the success of web applications, especially those that are business-driven. There exist a multitude of tools to monitor user activities on a website, which then provide metrics to help developers and company leadership understand where their users experience pain-points. However, all tools suffer from their own limitations and ultimately it is important for companies to have as much control (as possible) over all data collected by third-party tools, and be able to make decisions related to its storage, retention, processing, etc., in an agile manner. At Dottid, we use a third-party tool to understand and enhance the user experience on our web application, recently however, we ran into a problem regarding failed logins that required us to architect a solution ourselves, since the tool cannot address this issue for us. The solution leverages log collection and big data analytics and its architecture has paved the way for us to build more actionable insights than if we were using the tool as-is. We transparently share our experiences, and the details of our solution and rationale, with the goal of benefiting others, and promoting further industry-academic collaboration, in this space. © 2022 Knowledge Systems Institute Graduate School. All rights reserved. Log Sinks and Big Data Analytics along with User Experience Monitoring to Tell a Fuller Story Analytics; Big Data; Logging; Monitoring; Software Data Analytics; Digital storage; Knowledge engineering; Software engineering; Analytic; Data analytics; ITS architecture; Logging; Software; Third-party tools; User activity; Users' experiences; WEB application; Web applications; Big data",Strategic alignment
1890,Evergreen Forecast & Predictive LTRO Using Machine Learning – Case Study from PDO South,"Digital transformation (Dx) is increasingly becoming a key enabler in oil and gas industry to reduce costs, make faster and better decisions and increase productivity. The difference between leading the next innovation wave or being left behind may depend on how proficiently we embrace digital enablers, and how quickly we can test, prototype and scale these digital solutions to create value for the business. Digital technologies are not new to Petroleum Development Oman (PDO). In fact, the company has a track record of testing and adopting a wide range of new technology and integrated organisational capabilities to improve its business performance. Significant investments have been made into instrumenting its fields, including the IT infrastructure, Real-Time Operations, Exception Based Surveillance, Collaborative Work Environment (CWE), Smart Fields, NIBRAS, data management, analytics trials, to name a few. Yet consensus that Dx has significant further upside across PDO, led to the initiation of an asset-led pilot to digitally transform an existing PDO South Field – ""S"". The focus of the pilot was to identify new Dx opportunities while leveraging on existing PDO investments into digitalization, leading to quantified improvement in business performance of field –S. The project workscope was based on the outcome of an Opportunity Framing Event (OFE), in which a total of 27 opportunities were identified and ranked in terms of business value vs. feasibility or cost of implementation (Figure 1). Technical Subject Matter Experts (SMEs), asset field - surface, sub-surface, data management teams and other relevant support functions participated in the OFE so that business improvement synergies could be identified across the multiple disciplines in an integrated fashion. Following an agile approach, 5 Valuestreams (VS) were selected for Minimum Viable Product (MVP) implementation, in phase 1 of the pilot. Focus of this paper, however, is to elaborate further only one of the 5 VSs i.e. use of machine learning for ""Evergreen Production Forecast for Field Development Plan (FDP) optimization and Locate the Remaining Oil (LTRO)"". Copyright 2022, Society of Petroleum Engineers. Evergreen Forecast & Predictive LTRO Using Machine Learning – Case Study from PDO South  Commerce; Gas industry; Gasoline; Human resource management; Information management; Machine learning; Petroleum industry; Business performance; Case-studies; Digital solutions; Digital technologies; Digital transformation; Machine-learning; Oil and Gas Industry; Petroleum development Oman; Reduce costs; Remaining oil; Investments",Strategic alignment
1891,Interactive Visualization for Statistical Modelling through a Shiny App in R,"The importance of analytics and visualization tools has been growing over the last decades to handle big data which steamed from all aspects of life. The focus of this paper was on visualization as a crucial tool in presenting complex raw data and modelling results to provide easy-to-understand actionable information that facilitate decision-making. However, limited research distinguished between 'data visualization' and 'model visualization', which has been clearly made in this paper. Furthermore, this paper aimed to she would light on the importance of interactive visualizations to compliment statistical data modelling using R and Shiny for its advanced capabilities. Specifically, a methodology has been proposed based on a hybrid development lifecycle that adopts the Agile Software Development Lifecycle and the Data Analytics Lifecycle. Finally, by presenting a case study to model the dynamics of COVID-19, it was found that R and Shiny alongside the proposed hybrid development lifecycle significantly reduced the amount of time required to build visually interactive applications. The reported results highlighted the effectiveness of the adopted approach in assisting and guiding researchers and developers in building interactive applications that leverage Big Data Analytics. © 2021 IEEE. Interactive Visualization for Statistical Modelling through a Shiny App in R Data Analytics; Interactive Applications; R; Shiny; Visualization Big data; Data Analytics; Data visualization; Decision making; Interactive devices; Life cycle; Software design; Statistical methods; Analytic tools; Data analytics; Decisions makings; Interactive applications; Interactive visualizations; Modeling results; R; Shiny; Statistic modeling; Visualization tools; Visualization",Strategic alignment
1893,Towards the Automation of Industrial Data Science: A Meta-learning based Approach,"In context of the fourth industrial revolution (industry 4.0), the industrial big data is subject to grow rapidly to respond the agile industrial computing and manufacturing technologies. This data evolution can be captured using ubiquitous integrated sensors and multiple smart machines. We believe the use of data science methodologies, for the selection of models and configuration of hyper-parameters, may help to better control such data evolution. But, at the same time, the industrial practitioners and researchers often lack machine-learning expertise to directly retrieve the benefit from valuable manufacturing big data. Such a lack poses the major obstacle to yield value from even-though familiar data. In this case, a collaboration with data scientists may become an exigence along with the extensive machine learning knowledge which presumably may result to pursue further delays and effort. Multiple approaches for automating machine learning (AutoML) have been proposed for the past recent years in order to alleviate this deficiency. These approaches are expected to perform better along with accomplishment of computing resources which are mostly not readily accessible. To address this research challenge, in this paper, we propose a meta-learning based approach that may serve an effective decision support system for the AutoML process. Copyright © 2021 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved. Towards the Automation of Industrial Data Science: A Meta-learning based Approach Automated Machine Learning; Industrial Data Science; Industry 4.0; Manufacturing Big Data; Meta-learning Big data; Data Science; Decision support systems; Learning systems; Machine learning; Automated machine learning; Automated machines; Data evolution; In contexts; Industrial data science; Industrial datum; Learning-based approach; Machine-learning; Manufacturing big data; Metalearning; Industry 4.0",Strategic alignment
1901,Building capacity of community health centers to overcome data challenges with the development of an agile COVID-19 public health registry: a multistate quality improvement effort,"Objective: During the coronavirus disease 2019 (COVID-19) pandemic, federally qualified health centers rapidly mobilized to provide SARS-CoV-2 testing, COVID-19 care, and vaccination to populations at increased risk for COVID-19 morbidity and mortality. We describe the development of a reusable public health data analytics system for reuse of clinical data to evaluate the health burden, disparities, and impact of COVID-19 on populations served by health centers. Materials and Methods: The Multistate Data Strategy engaged project partners to assess public health readiness and COVID-19 data challenges. An infrastructure for data capture and sharing procedures between health centers and public health agencies was developed to support existing capabilities and data capacities to respond to the pandemic. Results: Between August 2020 and March 2021, project partners evaluated their data capture and sharing capabilities and reported challenges and preliminary data. Major interoperability challenges included poorly aligned federal, state, and local reporting requirements, lack of unique patient identifiers, lack of access to pharmacy, claims and laboratory data, missing data, and proprietary data standards and extraction methods. Discussion: Efforts to access and align project partners' existing health systems data infrastructure in the context of the pandemic highlighted complex interoperability challenges. These challenges remain significant barriers to real-time data analytics and efforts to improve health outcomes and mitigate inequities through data-driven responses. Conclusion: The reusable public health data analytics system created in the Multistate Data Strategy can be adapted and scaled for other health center networks to facilitate data aggregation and dashboards for public health, organizational planning, and quality improvement and can inform local, state, and national COVID-19 response efforts. © 2021 Published by Oxford University Press on behalf of the American Medical Informatics Association. This work is written by US Government employees and is in the public domain in the US. Building capacity of community health centers to overcome data challenges with the development of an agile COVID-19 public health registry: a multistate quality improvement effort COVID-19; EHR data; health centers; public health informatics infrastructure; SARS-CoV-2 Capacity Building; Community Health Centers; COVID-19; COVID-19 Testing; Humans; Public Health; Quality Improvement; Registries; SARS-CoV-2; Article; capacity building; coronavirus disease 2019; data aggregation; data analysis; data extraction; data interoperability; electronic health record; health care access; health center; health data; health disparity; human; medical informatics; pandemic; patient registry; pharmacy (shop); preliminary data; public health; total quality management; capacity building; health center; register",Risk management
1905,Understanding business analytics continuance in agile information system development projects: an expectation-confirmation perspective,"Purpose: This paper seeks to examine how expectations from business analytics (BA) by members of agile information systems development (ISD) teams affect their perceptions and continuous use of BA in ISD projects. Design/methodology/approach: Data was collected from 153 respondents working in agile ISD projects and analysed using partial least squares structural equation modelling techniques (PLS-SEM). Findings: Perceived usefulness and technological compatibility are the most salient factors that affect BA continuance intention in agile ISD projects. The proposed model explains 48.4% of the variance for BA continuance intention, 50.6% of the variance in satisfaction, 36.7% of the variance in perceived usefulness and 31.9% of the variance in technological compatibility. Research limitations/implications: First, this study advances understanding of the factors that affect the continuous use of BA in agile ISD projects; second, it contextualizes the expectation-confirmation model by integrating technological compatibility in the context of agile ISD projects. Originality/value: This is the first study to investigate BA continuance intention from an employee perspective in the context of agile ISD projects. © 2021, Ransome Epie Bawack and Muhammad Ovais Ahmad. Understanding business analytics continuance in agile information system development projects: an expectation-confirmation perspective Agile project; Business analytics; Expectation-confirmation model; Information system development ",Strategic alignment
1907,Remote Digital Technologies Driving Innovation - A Case Study on Subsea Operations,"The energy industry operates in a world where a consistent connection over real-time information is expected, however, less than 1% of available data sets are currently leveraged in the industry and usually - when monitored - most of the evaluations are based on human assessment of raw data. Today's business complexity requires an evolution in the way we share project and service information in the energy industry - from a reporting culture to true transparency and collaboration. There is a need for a digital system able to embrace raw data, perform analytics, and display key recommendations and insights. This will play a key role in turning challenges into opportunities. Adopting a real-time modular platform for proactive asset management and instant troubleshooting can be leveraged during any stage in a field's life cycle. Built for collaboration, the platform unlocks value through minimizing equipment downtime, enabling proactive servicing and digitally optimizing the understanding of equipment needs throughout the life of field. This agile approach consolidates multiple data streams into one trusted digital source including: • Autonomous monitoring of subsea wells, wellheads and xmas tree integrity without the need for active power. Designed to integrate with existing subsea infrastructure, these battery-operated monitoring systems passively monitor for a breach of material and will alert the operator of any integrity issues. • A fully equipped remote communications tool that goes beyond simple audio and video chat to support effective remote support. • Integrated visualization of robotic cleaning and inspection data of subsea equipment. • Objective view of the overall health status asset(s), field(s) or infrastructure, considering those elements which have the greatest bearing on health and longevity, with the possibility to monitor health trends over time, prolong operable life and optimize productivity. The implementation of the right tool, feature-rich, intuitive, and easily scalable leverages data and analytics to unlock a collaborative model of proactive servicing, prolong asset operable life and reduce OPEX. The case study demonstrates the positive outcomes that can aid the decarbonization of monitoring operations in remote locations. This approach can yield exceptional cost savings when compared to traditional methods of on-site visual monitoring. A wider overview will include the benefits of extending the window for full intervention or P&A, including lower asset risk, and protecting the environment. This paper illustrates deployment examples of the platform on subsea fields in the North Sea and Asia, its value, approach, as well as recommendations for any operator willing to embark on a similar journey. Copyright © 2022, Society of Petroleum Engineers. Remote Digital Technologies Driving Innovation - A Case Study on Subsea Operations  Data Analytics; Data visualization; Gasoline; Health; Health risks; Monitoring; Offshore oil well production; Trees (mathematics); Case-studies; Data set; Digital technologies; Energy industry; Human assessment; Project informations; Real-time information; Reporting cultures; Service information; Sub-sea operations; Life cycle",Financial management
1908,Agile Field Development Study Leveraging Fast-Tracked Reservoir Modeling and Data Analytics While Incorporating Reservoir Uncertainties,"A small onshore brownfield in south Oman has low oil recovery because of its heavy oil and high water production, which together with reservoir uncertainties poses development challenges. Petrogas implemented an innovative field development planning approach to quantitatively compare multiple field development scenarios and optimize the operational choices within each. The workflow started with a single history matched model for each of the two geological structures in the field. A set of 14 field development scenarios were defined, on injection rates, well locations, and injection fluids. Identification and quantification of subsurface uncertainties were performed. These uncertainties were included in the geomodel for each scenario, which generated an ensemble of realizations and corresponding production forecasts. Two sets of economic results were produced-a simple, discounted cashflow model and the fiscal terms of the operator's service contract. Each ensemble was run against these models to generate probabilistic performance indicators for each scenario. Using cloud-computing capability, the field development study was drastically accelerated without losing on the quality. Almost 800 simulations were run over 5 days, covering 32 development scenarios in total (for two structures), automatically integrated with the economics workflow, providing in-depth analyses. The scenarios were compared in a series of dashboards that presented the economic metrics and their corresponding cumulative distribution functions. The analysis yielded several important insights: longer wells did not provide enough additional production to offset the increased costs. Moreover, peripheral drive with horizontal wells was more effective than irregular vertical wells. The waterflood scenarios improved production, but the polymer-injection option with short horizontal wells and peripheral infill well pattern was the highest-performing scenario. The study also helped identify areas where more detailed optimization studies should be performed, e.g., to optimize polymer-injection scheduling and polymer design. Traditionally, subsurface uncertainties analysis was restricted to a small number of discrete model realizations. Results were quantified in terms of production ranges only. Here, production forecasts were based on an ensemble of models, capturing the full range of uncertainties. In addition, evaluation criteria included economics. Copyright © 2022, Society of Petroleum Engineers. Agile Field Development Study Leveraging Fast-Tracked Reservoir Modeling and Data Analytics While Incorporating Reservoir Uncertainties  Crude oil; Data Analytics; Digital storage; Distribution functions; Economic analysis; Heavy oil production; Infill drilling; Offshore oil well production; Petroleum reservoir evaluation; Uncertainty analysis; Development scenarios; Development study; Field development; Polymer injection; Production forecasts; Reservoir data; Reservoir models; Reservoir uncertainty; Uncertainty; Work-flows; Horizontal wells",Value management
1909,The Digitalization of Well Intervention Operations: From Proof of Concept to Proof of Scale,"Objective/Scope: As fields mature, the need to perform well interventions drastically increases. The process of identifying, requesting, and preparing an intervention is complex and data can be neither reliable nor easily accessible. Well Guardian, an innovative digital tool, was developed to improve well health monitoring and to reduce time-to-intervention by optimizing engineering processes and providing well data to engineers quickly. Methods, Procedures, Process: Well Guardian followed the Scrum process, a specific Agile methodology that serves to facilitate a project. Scrum provides a framework for developing, delivering and sustaining products in a complex environment and relies on incremental development, typically done in uniform iterations called Sprints. Framing of the digital tool started in January 2021 and its first deployment in the UK affiliate was in June 2021 (initially to cover two assets). Subsequent features and functionalities of Well Guardian were released iteratively, achieving full scale-up to all the affiliate assets in September 2021. Results, Observations, Conclusions: Well Guardian is the result of an exceptional collaboration between technical disciplines enabled by a dedicated digital team. It has solved a variety of obstacles by centralizing many disparate data sources into one accessible portal. The outcome: it provides a single source of truth and live well monitoring, generating economic business value in less than one year. Well Guardian enables engineers to prioritize their time on the more complex parts of a well intervention, encouraging a proactive rather than reactive approach. The digitalization of the previously manual Well Intervention Request (WIR) process is an example of Well Guardian successfully optimizing an engineering process. The digital WIR allows all stakeholders to have clarity and visibility on the planning and execution of interventions; completing activities is more efficient and thus, restoring wells to their full production potential and accelerating production is possible at a quicker rate. Well Guardian's monitoring of real-time well data improves the transparency of a well's health and reduces the time taken to review a well. Both benefits enable safer interventions by: • Identifying outstanding safety actions. • Automatically notifying when annuli pressures reach operating limits, helping onsite operators and well integrity engineers in anticipating potential integrity threats. • Automatically notifying for expiring exemption to standards, enabling teams to prepare renewals more efficiently together with more rigorous risk assessments. • Automatically notifying users when a well's behavior enters an apparent anomalous state, detecting the early symptoms of a well intervention. • Including mandatory subtasks related to integrity/safety/chemical checks in the digitalized WIR. Novel/Additive Information: Well Guardian is a collaborative digital tool bringing the Drilling & Wells, Field Operations and Geoscience disciplines together, bridging the silos between them and consequently improving the overall efficiency and performance of the affiliate. It is a multidisciplinary, integrated digital platform that enables data visualization and analytics, while enhancing data quality and reliability by centralizing more than ten different database systems. Copyright © 2022, Society of Petroleum Engineers. The Digitalization of Well Intervention Operations: From Proof of Concept to Proof of Scale  Digital devices; Economics; Health; Investments; Iterative methods; Risk assessment; Safety engineering; Agile Methodologies; Complex environments; Digital tools; Engineering process; Health monitoring; Incremental development; Proof of concept; Reduce time; Well data; Well intervention; Engineers",Risk management
1910,Toward an Understanding of Big Data Analytics and Competitive Performance,"The most significant enterprise system topic over the last decade is business intelligence and analytics. Big data analytics is now viewed as a disruptive technology that will significantly transform business intelligence and analytics. Therefore, big data technology, as a way to analyze and comprehend turbulent environments, has attracted huge interest from researchers and practitioners. Such environments are highly challenging for enterpris-es; how can they deal with such turbulence and stay competitive? The valuable information that this technology can offer seems almost limitless. Can big data help organizations deal with turbulent environments, and if so, how? We address this question by closely examining the relations between (1) big data analytics capabilities and dynamic capabilities, (2) dynamic capabilities and competitive performance, (3) big data analytics capabilities and operational capabilities, and (4) operational capabilities and competitive performance. Big data analytics capabilities are necessary to use big data effectively. We developed a model with four hypotheses to investigate these relations. We conducted a survey among 107 respondents from the largest companies in the Nordic countries. All but one of our hypotheses were supported. This research contributes to a better understanding of how big data analytic investments support agile capabilities, in turn promoting competitive performance. © Scandinavian Journal of Information Systems,. Toward an Understanding of Big Data Analytics and Competitive Performance big data analytics; big data analytics capabilities; competitive performance; dynamic capabilities; operational capabilities ",Financial management
1911,"10th International Conference on Design, User Experience, and Usability, DUXU 2021, held as part of the 23rd International Conference, HCI International 2021","The proceedings contain 130 papers. The special focus in this conference is on Design, User Experience, and Usability. The topics include: Envisioning Educational Product User eXperience Through Participatory Design Practice; operational Usability Heuristics: A Question-Based Approach for Facilitating the Detection of Usability Problems; integration of User Experience and Agile Techniques for Requirements Analysis: A Systematic Review; is Your App Conducive to Behaviour Change? A Novel Heuristic Evaluation; understanding Customer Value Propositions Through the Lens of Value Equations Method: A Systematic Approach; Developing and Validating a Set of Usability and Security Metrics for ATM Interfaces; challenges and Opportunities on the Application of Heuristic Evaluations: A Systematic Literature Review; consumer Experience Research Based on the Background of Experience Economy and Digital Economy; a Study on Scale Construction of Adjective Pairs for Evaluating Audiovisual Effects in Video Games; A Framework Based on UCD and Scrum for the Software Development Process; a Review of Automated Website Usability Evaluation Tools: Research Issues and Challenges; web Analytics for User Experience: A Systematic Literature Review; validation of a Questionnaire to Evaluate the Usability in the Peruvian Context; design Research on Visualization of Life Behavior and Rhythm; YERKISH: A Visual Language for Computer-Mediated Communication by an Ape; the Canadian Cultural Diversity Dashboard: Data Storytelling and Visualization for the Cultural Sector; visual Writing at the State-of-the-Art?; expressions of Data: Natural State, Specific Application, and General Pattern; pasigraphy: Universal Visible Languages; emojitaliano: A Social and Crowdsourcing Experiment of the Creation of a Visual International Language. 10th International Conference on Design, User Experience, and Usability, DUXU 2021, held as part of the 23rd International Conference, HCI International 2021  ",Strategic alignment
1913,Emergency management system design for accurate data: a cognitive analytics management approach,"Purpose: This paper uses a cognitive analytics management approach to analyze, understand and solve the problems facing the implementation of information systems and help management do the needed changes to enhance such a critical process; the emergency management system in the health industry is analyzed as a case study. Design/methodology/approach: Cognitive analytics management (CAM) framework (Osman and Anouz, 2014) is used. Cognitive process: The right questions are asked to understand the behavior of every process and the flow of its corresponding data; critical data variables were identified, guidelines for identifying data sources were set. Analytics process: Techniques of data analytics were applied to the selected data sets, problems were identified in user–system interaction and in the system design. The analysis process helped the management in the management process to make right decisions for the right change. Findings: Using the CAM framework, the analysis to the Lebanese Red Cross case study identified system user-behavior problems and also system design problems. It identified cases where distributed subsystems are vulnerable to time keeping errors and helped the management make knowledgeable decisions to overcome major obstacles by implementing several changes related to hardware design, software implementation, human resource training, operational and human-technology changes. CAM is a novel and feasible software engineering approach for handling system failures. Originality/value: The paper uses CAM framework as an approach to overcome system failures and help management do the needed changes to enhance such a critical process. This work contributes to the software engineering literature by introducing CAM as a new agile methodology to be used when dealing with system failures. Furthermore, this study is an action research that validated the CAM theoretical framework in a health emergency context in Lebanon. © 2021, Emerald Publishing Limited. Emergency management system design for accurate data: a cognitive analytics management approach Action research; Clock drift; Cognitive analytics management; Emergency management dispatch; Time synchronization ",Strategic alignment
1917,"10th International Conference on Design, User Experience, and Usability, DUXU 2021, held as part of the 23rd International Conference, HCI International 2021","The proceedings contain 130 papers. The special focus in this conference is on Design, User Experience, and Usability. The topics include: Envisioning Educational Product User eXperience Through Participatory Design Practice; operational Usability Heuristics: A Question-Based Approach for Facilitating the Detection of Usability Problems; integration of User Experience and Agile Techniques for Requirements Analysis: A Systematic Review; is Your App Conducive to Behaviour Change? A Novel Heuristic Evaluation; understanding Customer Value Propositions Through the Lens of Value Equations Method: A Systematic Approach; Developing and Validating a Set of Usability and Security Metrics for ATM Interfaces; challenges and Opportunities on the Application of Heuristic Evaluations: A Systematic Literature Review; consumer Experience Research Based on the Background of Experience Economy and Digital Economy; a Study on Scale Construction of Adjective Pairs for Evaluating Audiovisual Effects in Video Games; A Framework Based on UCD and Scrum for the Software Development Process; a Review of Automated Website Usability Evaluation Tools: Research Issues and Challenges; web Analytics for User Experience: A Systematic Literature Review; validation of a Questionnaire to Evaluate the Usability in the Peruvian Context; design Research on Visualization of Life Behavior and Rhythm; YERKISH: A Visual Language for Computer-Mediated Communication by an Ape; the Canadian Cultural Diversity Dashboard: Data Storytelling and Visualization for the Cultural Sector; visual Writing at the State-of-the-Art?; expressions of Data: Natural State, Specific Application, and General Pattern; pasigraphy: Universal Visible Languages; emojitaliano: A Social and Crowdsourcing Experiment of the Creation of a Visual International Language. 10th International Conference on Design, User Experience, and Usability, DUXU 2021, held as part of the 23rd International Conference, HCI International 2021  ",Strategic alignment
1921,Organizational agility in industry 4.0: A systematic literature review,"Agility is the dynamic capability of an organization which helps it to manage a change and uncertainties in the environment. The purpose of this research is to review the literature from the perspective of agility in Industry 4.0. This paper systematically reviews 381 relevant articles from peer-reviewed academic journals in the period of the last five years. The results show that agility is important for an organization to adopt Industry 4.0 technologies as it helps companies to cope with the changes that arise along with the adoption of Industry 4.0 technologies. Further, it also indicates that by adopting Industry 4.0 technologies, companies can significantly enhance their agility capability into various aspects with different technologies. The technologies which enhance the agility are: smart manufacturing, internet of things, cyber-physical system, big data and analytics and cloud computing. On the other hand, important aspects of agility include supply chain, workforce, information system, facilities, management, manufacturing and technology agility. © 2021 by the authors. Licensee MDPI, Basel, Switzerland. Organizational agility in industry 4.0: A systematic literature review Agile; Agility; Environment; I-4.0 ecosystem; Industry 4.0; Organizational agility literature review; manufacturing; technological change; uncertainty analysis",Value management
1923,QaSD: A Quality-aware Strategic Dashboard for supporting decision makers in Agile Software Development,"Software and data analytics solutions support improving development processes and the quality of the software produced in Agile Software Development (ASD). However, decision makers in software teams (e.g., product owner, project manager) are demanding powerful tools providing evidence data that support their strategic decision-making processes. In this paper, we present and provide access to QaSD, a Quality-aware Strategic Dashboard supporting decision makers in ASD. The dashboard allows decision makers to define high-level strategic indicators (e.g., customer satisfaction, process performance) related to software quality and to measure, explore, simulate and forecast the values of those indicators in order to explain and justify their decisions. Moreover, we also provide the results of a conducted evaluation of the dashboard quality in a real environment that evaluated the QaSD as usable, easy to use, with good navigation, and reliable. © 2020 Elsevier B.V. QaSD: A Quality-aware Strategic Dashboard for supporting decision makers in Agile Software Development Agile; Decision-making Dashboard; Forecasting; Software analytics; What-if analysis Computer software selection and evaluation; Customer satisfaction; Data Analytics; Decision making; Human resource management; Quality control; Software design; Agile software development; Decision makers; Development process; Process performance; Project managers; Real environments; Software teams; Strategic decision making; Software quality",Strategic alignment
1925,Current approaches for executing big data science projects—a systematic literature review,"There is an increasing number of big data science projects aiming to create value for organizations by improving decision making, streamlining costs or enhancing business processes. However, many of these projects fail to deliver the expected value. It has been observed that a key reason many data science projects do not succeed is not technical in nature, but rather, the process aspect of the project. The lack of established and mature methodologies for executing data science projects has been frequently noted as a reason for these project failures. To help move the field forward, this study presents a systematic review of research focused on the adoption of big data science process frameworks. The goal of the review was to identify (1) the key themes, with respect to current research on how teams execute data science projects, (2) the most common approaches regarding how data science projects are organized, managed and coordinated, (3) the activities involved in a data science projects life cycle, and (4) the implications for future research in this field. In short, the review identified 68 primary studies thematically classified in six categories. Two of the themes (workflow and agility) accounted for approximately 80% of the identified studies. The findings regarding workflow approaches consist mainly of adaptations to CRISP-DM (vs entirely new proposed methodologies). With respect to agile approaches, most of the studies only explored the conceptual benefits of using an agile approach in a data science project (vs actually evaluating an agile framework being used in a data science context). Hence, one finding from this research is that future research should explore how to best achieve the theorized benefits of agility. Another finding is the need to explore how to efficiently combine workflow and agile frameworks within a data science context to achieve a more comprehensive approach for project execution © 2022 Saltz and Krasteva Current approaches for executing big data science projects—a systematic literature review Agile data science; Big data science; Big data science workflows; Process frameworks; Project execution Data Science; Decision making; Life cycle; 'current; Agile approaches; Agile data science; Big data science; Big data science workflow; Process framework; Project execution; Science projects; Systematic literature review; Work-flows; Big data",Value management
1926,The mediating role of knowledge management and information systems selection management capability on Big Data Analytics quality and firm performance,"This study draws on organizational learning and strategic decision-making theory to develop a conceptual framework to explore how the selection measures of BDA systems and external support partners are linked to BDA system quality, and how these influence firms’ competitive position. Through a cross-sectional survey of 523 IT professionals from the US, UK, and India, using path analysis and structural equation modeling, we found that the information systems selection process, enhanced by knowledge management capabilities, is positively related to the BDA system quality and firms’ performance. However, inconsistent with prior studies on transactional systems, we found no support for the hypothesis that software vendor criteria influence BDA quality. Also, in selecting systems and external facilitators, organizations appear to be pivoting towards parameters that are considered “emerging”, such as cloud computing, DevOps, and agile experiences, as they increase the likelihood of unlocking business value from BDA. © 2021 Informa UK Limited, trading as Taylor & Francis Group. The mediating role of knowledge management and information systems selection management capability on Big Data Analytics quality and firm performance agile; Big data analytics; business values and competitive advantage; cloud computing; DevOps; information system criteria; knowledge management; software vendor criteria; third-party consultant Agile manufacturing systems; Big data; Binary alloys; Cloud analytics; Competition; Computation theory; Decision making; Decision theory; Information systems; Information use; Knowledge management; Regression analysis; Agile; Big data analytic; Business value; Business value and competitive advantage; Cloud-computing; Competitive advantage; Data analytics; Information system criteria; Software vendor criteria; Software vendors; Third parties; Third-party consultant; Data Analytics",Strategic alignment
1927,The essential role of population health during and beyond COVID-19,"The coronavirus disease 2019 (COVID-19) pandemic has fundamentally changed how health care systems deliver services and revealed the tenuousness of care delivery based on face-to-face office visits and fee-for-service reimbursement models. Robust population health management, fostered by value-based contract participation, integrates analytics and agile clinical programs and is adaptable to optimize outcomes and reduce risk during population-level crises. In this article, we describe how mature population health programs in a learning health system have been rapidly leveraged to address the challenges of the pandemic. Population-level data and care management have facilitated identification of demographicbased disparities and community outreach. Telemedicine and integrated behavioral health have ensured critical primary care and specialty access, and mobile health and postacute interventions have shifted site of care and optimized hospital utilization. Beyond the pandemic, population health can lead as a cornerstone of a resilient health system, better prepared to improve public health and mitigate risk in a value-based paradigm. © 2021 Ascend Media. All rights reserved. The essential role of population health during and beyond COVID-19  COVID-19; Delivery of Health Care; Learning Health System; Population Health; Article; collaborative care team; coronavirus disease 2019; health care disparity; high risk patient; hospital utilization; human; learning health system; pandemic; population health; primary medical care; priority journal; psychological aspect; social distancing; telemedicine; health care delivery; organization and management; prevention and control",Risk management
1928,The Urgency of Business Agility During COVID-19 Pandemic: Distribution of Small and Medium Business Products and Services,"Purpose: Business agility is an important key to survival for SMEs in Indonesia, especially during the COVID-19 pandemic. Indonesian local product distribution and service distribution are mostly served by SMEs. Agile businesses will be able to assist them in the proper distribution of products and services. This research examines how the direct and indirect influence of IT capabilities on business agility through organizational learning and business intelligence for small and medium enterprises in the distribution of Indonesian products and services. Research design, data and methodology: This research uses SEM method with SmartPLS tool. The sample of this research was conducted on small and medium enterprises in the distribution of Indonesian products and services. The sample obtained in this study was 202 SME owners or managers (strategic level). Results: Business intelligence plays a key role in improving business agility. The results of IT capability can directly and indirectly affect business agility through organizational learning. Conclusions: Business intelligence has the biggest role in increasing business agility in SMEs in Indonesia. IT capability has an indirect effect on business agility through organizational learning. The findings of this study prove that IT capabilities do not indirectly affect business agility through business intelligence © Copyright: The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://Creativecommons.org/licenses/by-nc/4.0/) which permits unrestricted noncommercial use, distribution, and reproduction in any medium, provided the original work is properly cited. The Urgency of Business Agility During COVID-19 Pandemic: Distribution of Small and Medium Business Products and Services Business agility; Business intelligence; It capability; Organizational learning; Product distribution; Service distribution ",Governance
1929,Leveraging the serverless paradigm for realizing machine learning pipelines across the edge-cloud continuum,"The exceedingly exponential-growing data rate highlighted numerous requirements and several approaches have been released to maximize the added-value of cloud and edge resources. Whereas data scientists utilize algorithmic models in order to transform datasets and extract actionable knowledge, a key challenge is oriented towards abstracting the underline layers: the ones enabling the management of infrastructure resources and the ones responsible to provide frameworks and components as services. In this sense, the serverless approach features as the novel paradigm of new cloud-related technology, enabling the agile implementation of applications and services. The concept of Function as a Service (FaaS) is introduced as a revolutionary model that offers the means to exploit serverless offerings. Developers have the potential to design their applications with the necessary scalability in the form of nanoservices without addressing themselves the way the infrastructure resources should be deployed and managed. By abstracting away the underlying hardware allocations, the data scientist concentrates on the business logic and critical problems of Machine Learning (ML) algorithms. This paper introduces an approach to realize the provision of ML Functions as a Service (i.e., ML-FaaS), by exploiting the Apache OpenWhisk event-driven, distributed serverless platform. The presented approach tackles also composite services that consist of single ones i.e., workflows of ML tasks including processes such as aggregation, cleaning, feature extraction, and analytics; thus, reflecting the complete data path. We also illustrate the operation of the approach mentioned above and assess its performance and effectiveness exploiting a holistic, end-toend anti-fraud detection machine learning pipeline. © 2021 IEEE. Leveraging the serverless paradigm for realizing machine learning pipelines across the edge-cloud continuum artificial intelligence; cloud computing; edge computing; function as a service; machine learning; serverless Abstracting; Data mining; Information management; Infrastructure as a service (IaaS); Pipelines; Platform as a Service (PaaS); Added values; Algorithmic model; Business logic; Composite services; Critical problems; Edge resources; Fraud detection; Infrastructure resources; Machine learning",Strategic alignment
1931,"Transitioning from Legacy Air Traffic Management to Airspace Management through Secure, Cloud-Native Automation Solutions","Advancements in Cloud-native services, Machine-Learning (ML), Artificial Intelligence (AI), and Rapid Application Development (RAD) using the Agile methodology has led countless industries to achieving desirable levels of automation while reducing cost and improving quality software deployments, timely / iterative delivery, and accountability. Coupling this framework with the principle of security as a shared responsibility further enhances the efficacy of an integrated Development, Security, and Operations (DevSecOps) Team within organizations to deliver secured digital solutions. Air Navigation Service Providers (ANSPs) around the world are currently exploring and embracing the digital evolution shifting from monolithic, legacy automation platforms to an application framework of microservices to allow for flexible operations as capabilities and airspace operations evolve. Specific to the US, the ATM automation system of today is comprised of both safety and non-safety critical systems, with mission-essential, efficiency-critical, and mission-support services that are predominately maintained and evolved through multi-year, one contractor-led programs. Although the system has proven resilient, it has not proven to be agile and flexible to allow for advances in capabilities on-board aircraft or in the data integration and sharing with other NAS automation systems. This creates significant overhead in development, sustainability, and operations of the current automation system, and leaves modernization efforts - in terms of new capabilities - in constant investment decision planning cycles, costing agencies not just money, but more time to innovate. To advance aviation into a new generation of interoperability leveraging collaborative frameworks and application specific capabilities, ANSPs must adapt to innovative methods to collect, process, and deliver critical and essential aeronautical, weather, and flight information to air traffic control operators and ultimately to airspace users. Doing so can not only lead to sustaining NAS automation systems while reducing the costs to develop and operate these systems, but it also provides an opportunity to present strategies on how to dramatically reduce the time and integration efforts needed to deploy new capabilities. Leveraging cloud-native technologies and services is a way to realize this automation evolution vision for ANSPs.This paper examines the migration from today's systems to secure, cloud-native platforms to prove that Mission Services and Mission Applications can be rapidly available / deployable to operators who provide separation and flow management services, using a cyber-secured cloud-native environment. Aeronautical data typically used for tactical decision making is now seen as crucial to the decision-making process in Air Traffic Management (ATM). Integrating global and localized datasets into a digital aviation data platform enhances the capabilities of the solutions and opens the possibilities of leveraging big data analytics and microservices to compute trajectory predictions (TP), demand capacity balancing (DCB), arrival and departure sequencing, airspace delay, among others, in real-time to achieve operator-driven mission objectives. Technology has reached a state of maturity, especially in cloud and hybrid cloud solutions, to support safety of life operations, like ATM. This paper identifies approaches that are being considered for that migration to support the integration of new airspace entrants, the use of application services to provide a dynamic, evolutionary ATM platform, and addresses some of the safety and security strategies that must be considered for this evolution.  © 2021 IEEE. Transitioning from Legacy Air Traffic Management to Airspace Management through Secure, Cloud-Native Automation Solutions ATM; Automation; Cloud-native; Microservices Air navigation; Air traffic control; Artificial intelligence; Automation; Complex networks; Cost reduction; Data integration; Integration; Integration testing; Interoperability; Safety engineering; Sustainable development; Agile Methodologies; Air navigation service providers; Air Traffic Management; Airspace management; Automation solutions; Automation systems; Cloud-native; Machine-learning; Microservice; Rapid application development; Application programs",Capacity management
1932,Towards using self-generated data sources to decision support in agile software development; [Hacia el uso de fuentes autogeneradas de datos como soporte a la toma de decisiones en el desarrollo ágil de software],"Most of the agile software process activities are knowledge-intensive, which require environments and technology that ease its creation and transfer. There is a vast amount of knowledge in software companies, but it is commonly tacit and it can only be utilized if companies can identify these people to share it. However, there are untapped sources of knowledge from tools that are used daily in projects. This article presents the results of the identification of these knowledge sources in companies in southern Sonora, through the KoFI methodology. As a result, 3 types of useful knowledge sources were identified, coming from code repositories, issue trackers, and requirements managers. As a means to use and manage these sources of knowledge, we proposed the development of a technological tool that integrates the capabilities and benefits of Business Intelligence (BI) and Knowledge Management (KM), where BI identifies possible weaknesses and opportunities, while KM supports the process design, implementation, and monitoring.  © 2021 IEEE. Towards using self-generated data sources to decision support in agile software development; [Hacia el uso de fuentes autogeneradas de datos como soporte a la toma de decisiones en el desarrollo ágil de software] agile software development; business intelligence; knowledge management; knowledge sources Information analysis; Software design; Agile software development; Agile software process; Business knowledge; Business-intelligence; Data-source; Decision supports; Knowledge sources; Process activities; Software company; Technological tools; Knowledge management",Financial management
1933,Agile Architectural Model for Development of Time-Series Forecasting as a Service Applications,"Time-series data analysis and forecasting have become increasingly important due to its massive application and production. Working with time series, preparing and manipulating data, predicting future values, and forecasting results analysis are becoming more natural tasks in people’s everyday lives. Modelling of an architectural design for convenient and user-friendly applications that provide a range of functionality for collection, management, processing, analysis, and forecasting time-series data establishes this article’s goal. The system’s technical requirements are maintainability and the ability to expand it with new data manipulation methods and predictive models in the future (scalability). As a result of this paper an architectural model was built, field-testing and profiling of which determined that applications developed on its basis allow users to get the desired results faster than using alternative solutions. The reduction of required level of user technical skills was achieved by the presence of the front-end component. Possibility of precise time-series predictions provision regardless of the data domain was accomplished by creation of dynamically extendable predictive model management service and tools. Improvement of system maintenance and extension time-costs was the result of microservices architecture pattern usage. As a result, this work demonstrates an informational system for end-to-end workflow on time-series forecasting. Application implementation prototype (proof of concept) that was built on the basis of the described architectural model demonstrated the advantages of this design over existing analogues. During testing, scalability improvements and overall efficiency increase in terms of time and resource costs were recorded. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG. Agile Architectural Model for Development of Time-Series Forecasting as a Service Applications Big data; Data collection; Microservices; Scalability; Software-architecture; Time series analysis; Time-series forecasting Architectural design; Forecasting; Predictive analytics; Scalability; Time series; Alternative solutions; Architectural modeling; Architecture patterns; Forecasting time series; Technical requirement; Time series data analysis; Time series forecasting; Time series prediction; Time series analysis",Strategic alignment
1940,"Real-time analytics, incident response process agility and enterprise cybersecurity performance: A contingent resource-based analysis","Emerging paradigms of attack challenge enterprise cybersecurity with sophisticated custom-built tools, unpredictable patterns of exploitation, and an increasing ability to adapt to cyber defenses. As a result, organizations continue to experience incidents and suffer losses. The responsibility to respond to cybersecurity incidents lies with the incident response (IR) function. We argue that (1) organizations must develop ‘agility’ in their IR process to respond swiftly and efficiently to sophisticated and potent cyber threats, and (2) Real-time analytics (RTA) gives organizations a unique opportunity to drive their IR process in an agile manner by detecting cybersecurity incidents quickly and responding to them proactively. To better understand how organizations can use RTA to enable IR agility, we analyzed in-depth data from twenty expert interviews using a contingent resource-based view. The results informed a framework explaining how organizations enable agile characteristics (swiftness, flexibility, and innovation) in the IR process using the key features of the RTA capability (complex event processing, decision automation, and on-demand and continuous data analysis) to detect and respond to cybersecurity incidents as-they-occur which, in turn, improves their overall enterprise cybersecurity performance. © 2021 Real-time analytics, incident response process agility and enterprise cybersecurity performance: A contingent resource-based analysis Agility; Enterprise cybersecurity performance; Incident response; Real-time analytics; Resource-based view Digital storage; Rapid thermal annealing; Complex event processing; Contingent resources; Continuous data analysis; Cyber defense; Cyber security; In-depth datum; Incident response; Real-time analytics; Security of data",Risk management
1941,The role of artificial intelligence in shaping the future of Agile fashion industry,"Artificial intelligence (AI) has become an integral part of every industry. With the emergence of big data, the industries, and more especially textile and apparel (T&A) industry, are on the brink of relationships with consumers, suppliers, and competitors. They need to handle different scenarios with a multitude of complex correlations and dependencies between them and uncertainties arising from human interaction. It has become imperative for them to manage huge amounts of data for the optimization of decision-making processes. In such circumstances, AI techniques have shown promise in every segment of the T&A value chain, from product discovery to robotic manufacturing. The potential wide-ranging applications of AI in T&A industry have found their ways into design support systems to T&A recommendation systems, intelligent tracking systems, quality control, T&A forecasting, predictive analytics in supply chain management or social networks and T&A e-commerce. The research recourses to the qualitative method in the form of systematic literature review and in-depth interviews from senior management people and industry experts. Findings identify the dimensions of AI to develop dynamic capability along with its potential impact and probable challenges. As such, the findings contribute to relevant literature and offer useful insights for academia and practitioners. © 2022 Informa UK Limited, trading as Taylor & Francis Group. The role of artificial intelligence in shaping the future of Agile fashion industry Agile manufacturing; apparel; Artificial intelligence; big data analytics; dynamic capability; textile and fashion industry Artificial intelligence; Big data; Data Analytics; Decision making; Predictive analytics; Supply chain management; Textile industry; Textiles; Agile manufacturing; Apparel industry; Complex correlation; Dynamics capability; Fashion industry; Humaninteraction; Integral part; Optimisations; Textile and fashion industry; Uncertainty; Behavioral research",Strategic alignment
1942,Digital Lighthouse: A Scalable Model for Digital Transformation in Oil & Gas,"Energy companies are latecomers to digitization with respect to other business, but new technologies like Big Data, Cloud infrastructure and Artificial Intelligence offer great opportunities. Here we present an integrated approach to the digitalization of an O&G plant aiming to offer operator safety enhancement, production optimization and reduction of the environmental impact to maximize the asset value. This has been accomplished by complex and continuous work powered by the people who are the engine and the real target of the digital transformation process. In the key study hereby presented, an all-round effort has been made to empower the operator's everyday work with digital and innovative tools supporting reservoir, maintenance, production and HSE workflow. Starting from a number of various legacy systems, a single integrated dashboard was built: The Integrated Operation Centre (IOC). IOC is now available on PC and smartphones to all site personnel both at the operational and managerial level. New innovative systems were developed and deployed into IOC to capitalize on the data acquired during years of plant activities. Machine learning and advanced analytics solutions provide new daily insight on how to efficiently schedule maintenance operations and avoid off-specs and downtime on critical equipment, while complex production optimizers help technicians react to unexpected situations and maximize production. Via IoT (Internet of Things) and portable devices, new tools and workflows were deployed onsite to ease the work and enhance the safety of workers with focus on usage of PPE and providing rapid information to locate workers during emergency situations. People from both site and company headquarters ensured the success of the digital transformation by working together in an Agile Method during the development phase and by coaching in the roll-out phase. New professional roles, like data scientist and big data engineers, joined effort with experienced operators to ensure the success of this journey. This cooperation was at the basis of a comprehensive change management effort, which ensured a smooth and constant change in the way the personnel thinks, acts and reacts. This, we believe, is at the very heart of any fundamental transformation, being it digital or not. Copyright 2022, Society of Petroleum Engineers. Digital Lighthouse: A Scalable Model for Digital Transformation in Oil & Gas  Artificial intelligence; Big data; Data Analytics; Environmental impact; Human resource management; Legacy systems; Life cycle; Predictive analytics; Data clouds; Digital transformation; Digitisation; Energy companies; Integrated Operations; Oil gas; Operation center; Scalable Modelling; Work-flows; Workers'; Internet of things",Capacity management
1944,Value Oriented Engineering Solutions for Business Continuity,"This paper presents a systemic approach using Engineering and analytics methods to avail the fastest and safest responses to recovering business operations after Abqaiq Plants major disruption after the 2019 September 14th incident. This new approach using value and agile engineering, risk management methodologies combined with the business continuity model suggested was successfully applied to recover Abqaiq Plants Operations after catastrophic events occurred. This paper pretends to serve as example about how the business continuity plan should response to a major emergency and how this planning activity could be effectively supported using a Value Oriented Engineering Solutions (VOES). This VOES approach is based on Business continuity framework and adapted for use during emergency situations to generate effective and urgent responses to recover one of the most strategical operations in the Oil and Gas Industry worldwide ahead of the last year significant disruption. VOES approach vastly implemented during Abqaiq Plants Restoration allowed a 100% functional recovery on 9 days, 5 days in advance to the most optimistic scenario. This paper shows a case study implemented for major instrumentation and electrical equipment activities performed in UA Spheroids plant, one of the most affected area and responsible to process the 100% of the Abqaiq Plants Oil Production rate. This paper pretends to contribute with the research and practice on business continuity management. Considering a particular approach to BCM, incorporating value-oriented engineering solutions in the developing of continuity plans; we apply model-based techniques to provide quality assurance in the elaboration process, and to automate the generation/update of a BCP. On the practical side, this study converts Operational, Maintenance, Safety and Reliability perspectives in a holistic view provided from Engineering solutions responsible to generate the guidelines for an agile, effective and realizable recovery plan. © 2021, Society of Petroleum Engineers Value Oriented Engineering Solutions for Business Continuity  Gas industry; Gasoline; Quality assurance; Risk management; Agile engineerings; Analytic method; Business continuity; Business operation; Engineering methods; Engineering solutions; New approaches; Risks management; Solution approach; Systemic approach; Recovery",Capacity management
1945,"Digitally Transforming Electronic Governments into Smart Governments: SMARTGOV, an Extended Maturity Model","Smart government aims to provide intelligent and agile personalized public services to government stakeholders. Over the past few years, governments seek to employ innovative technologies, such as blockchain, analytics, artificial intelligence (AI), Internet of Things (IoT), cloud computing, robotics process automation, and machine learning (ML), to improve citizens’ experiences and quality of life as well as government decision-making processes. They also aim to increase the efficiency of business processes and to transform the delivery of public services. However, the extant e-government literature lacks an inclusive digital government maturity model that can be used as a unified reference for guiding public organizations through the journey to smart government. Accordingly, the main aim of this paper is to develop a smart government maturity model by extending and revising the extant literature related to e-government. The developed model is then used to empirically evaluate the smart government initiative in the United Arab Emirates (UAE). We applied the model on 41 UAE federal governments’ websites/portals and measured their maturity level using 73 indicators. The findings indicate that the UAE is quite advanced with regard to smart government, as the majority of governmental entities scored “High” and above on the use of innovative and disruptive technologies in various domains. The smart government maturity model proved its efficacy and value in evaluating the UAE's smart government initiative. © The Author(s) 2021. Digitally Transforming Electronic Governments into Smart Governments: SMARTGOV, an Extended Maturity Model e-government; innovation; maturity model; smart government; SMARTGOV; technology; UAE ",Strategic alignment
1951,Lean 4.0: Digital Technologies as Strategies to Reduce Waste of Lean Manufacturing,"Originated from Toyota Production System, Lean Manufacturing (LM) is a widely known production system that promotes efficient processes in the industry context, focusing on continuous improvement and value-adding activities to avoid waste. Industry 4.0 (I4.0), in turn, has recently emerged in the manufacturing sector, highlighting digitalization and its incorporation in industrial processes, ensuring improvements in production lines. This work intended to analyze the LM and I4.0 integration by studying the relationship between the classic types of waste with the digital technologies 4.0. Applying a three-phase methodology approach, the principles, attributes, and technologies of LM and I4.0 were identified. A relationship matrix and multidimensional diagrams were designed to investigate how digital technologies can contribute to the reduction of LM waste. A total of 27 associations were established and the emergent technologies with more combinations referred to Automated Guided Vehicles, Big Data Analytics, and Additive Manufacturing. Consequently, LM and I4.0 can work together and provide benefits to each other, highlighting that technologies 4.0 are critical to support waste reduction in manufacturing companies. The main contributions of this paper are the systematization and integrated operationalization of digital technologies in lean environments, aiming to optimize indicators of production processes through waste reduction. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG. Lean 4.0: Digital Technologies as Strategies to Reduce Waste of Lean Manufacturing Industry 4.0; Lean 4.0; Lean Manufacturing; Waste Agile manufacturing systems; Automatic guided vehicles; Data Analytics; Industry 4.0; Continuous improvements; Continuous value; Digital technologies; Efficient process; Industry contexts; Lean 4.0; Manufacturing IS; Production system; Toyota Production System; Waste reduction; Lean production",Strategic alignment
1952,Re-Imagining Performance Reviews: Automated Dashboards for Continuous Visibility of Engineers' Performance,"Traditional methods for performance appraisal are not suitable for agile fast-paced software companies. This has been a realization in the software industry since the early adoption of agile methodologies. Nonetheless, software companies are still struggling to find a practical solution that fits the highly dynamic nature of their teams. In particular, high-growth companies, scaleups, need to be creative at how they approach performance appraisals. In this talk, we introduce automated review dashboards to support a seamless appraisal process and continuous visibility of software engineers' performance. The proposed dashboards leverage structure from existing growth frameworks while automating the aggregation of relevant qualitative and quantitative performance metrics. We reflect on our experience using the dashboards at Robusta, a medium-sized software scaleup company. The dashboards enabled a team of four managers to provide timely feedback to 56 engineers with a base for continuous visibility. We explore the design of the dashboards, the customizable metrics and the overall review experience from the perspectives of different stakeholders. We conclude with the lessons learned and practical advice for scaleups facing the same challenge. © 2021 IEEE. Re-Imagining Performance Reviews: Automated Dashboards for Continuous Visibility of Engineers' Performance Agile performance reviews; Business intelligence; Engineering management; Entrepreneurship; Information management Engineers; Human resource management; Information management; Software engineering; Agile Methodologies; Agile performance review; Engineering management; Entrepreneurship; Performance; Performance appraisal; Performance reviews; Practical solutions; Software company; Software industry; Visibility",Governance
1960,Business Analytics in Strategic Purchasing: Identifying and Evaluating Similarities in Supplier Documents,"The increasing digitalization in the automotive industry is influencing the structure of the traditional value chain and calls for the handling of large amounts of data to remain competitive in a constantly changing environment. This results in new challenges for purchasing management, which has to cope with agile integration of service providers as well as interorganizational process automation using electronic data exchange platforms. This work analyzes the electronic document stream on supplier management platforms by proposing an automated text mining framework. Both textual components, e. g., requests for information and offers, and narrative material, e. g., financial and calculation data, are being analyzed by topic modeling and descriptive statistics. The methodological approach is introduced and illustrated by the use case of service provider documents in purchasing processes. The results reveal financial potential for purchasing and generally contribute to supply chain cost management. © 2021 Taylor & Francis. Business Analytics in Strategic Purchasing: Identifying and Evaluating Similarities in Supplier Documents  Advanced Analytics; Electronic data interchange; Purchasing; Sales; Supply chains; Text mining; Changing environment; Descriptive statistics; Electronic data exchange; Inter-organizational process; Large amounts of data; Methodological approach; Purchasing managements; Requests for information; Information management",Strategic alignment
1963,SoReady: An Extension of the Test and Defect Coverage-Based Analytics Model for Pull-Based Software Development,"Pull-based software development is a distributed development model that offers an opportunity to review a pull request before it gets merged into the main repository. A pull request addresses new features,bug fixing,and maintenance issues submitted by both integrators or contributors. It appears that many empirical studies are conducted to discover how pull request evaluation is done,and to our knowledge,limited research exists for assessing release readiness of pull requests. Studies also reported that the failure rate of pull-requests rapidly increases when there are many forks created. It is therefore,questions worth exploring are whether the code review really contributing to the code quality,and how to determine the release readiness of pull requests? In our previous work,test and defect coverage-based analytics model (TDCAM) has been proven to be suitable to determine the readiness of releases for software that is rapidly evolving,in which this is also a characteristic of pull-based software development. In this paper,the TDCAM has been extended to include pull request coverage indicators. The proposed model,namely as SoReady and the visualization analysis presented herein has enabled five developers in a commercial setting to make informed and evidence-based decisions regarding the test status of each pull request and overall reliability of an open source software through a prototype dashboard. ©,2019 IEEE. SoReady: An Extension of the Test and Defect Coverage-Based Analytics Model for Pull-Based Software Development agile; defect coverage; metrics; Pull based software development; pull requests; software release readiness; test analytics.; test coverage Defects; Failure analysis; Open source software; Open systems; Reliability analysis; Software design; Software reliability; Software testing; Testing; agile; Defect coverage; metrics; pull requests; Software release; Test coverage; Software prototyping",Risk management
1964,Analysis of barriers in implementation of digital transformation of supply chain using interpretive structural modelling approach,"Purpose: Digital supply chain (DSC) is an agile, customer-driven and productive way to develop different forms of returns for companies and to leverage efficient approaches with emerging techniques and data analytics. Though the advantages of digital supply chain management (DSCM) are many, its implementation is quite slow for several reasons. The purpose of this paper is to identify the major barriers which hinder the adoption of DSC and to analyse the interrelationship among them. The barriers of DSC are explored on the basis of existing literature and experts’ opinion. Design/methodology/approach: This paper uses the interpretive structural modelling (ISM) approach to develop a hierarchical structural model which shows the mutual dependence among the barriers of DSC. Cross-impact matrix multiplication applied to classification analysis was performed to represent these barriers graphically on the basis of their driving power and dependence. Findings: The research demonstrates that the barriers “no sense of urgency”, “lack of industry specific guidelines”, “lack of digital skills and talent” and “high implementation and running cost” are the most significant barriers to digital transformation of supply chain. This paper also suggests some managerial implications to overcome the barriers which hinder the implementation of digital transformation of supply chain. Practical implications: This paper assists managers and policymakers to understand the order in which these barriers must be tackled and adopts a roadmap for successful implementation of DSCM and reap its benefits. Originality/value: This is one of the initial research studies which has analysed the barriers of DSC using ISM approach. © 2019, Emerald Publishing Limited. Analysis of barriers in implementation of digital transformation of supply chain using interpretive structural modelling approach Barriers; Digital skills; Digital supply chain (DSC); Innovation; Interpretive structural modelling (ISM); Modelling; Supply chain management ",Strategic alignment
1965,Cyber Digital Twin Simulator for Automatic Gathering and Prioritization of Security Controls' Requirements,"The scale and complexity of cyber threats in digital enterprises hamper operators' ability to gather, prioritize and rationalize which security controls requirements should be handled first to achieve rapid risk reduction. This paper presents a cyber digital twin, based on attack graph analytics, that automatically gathers and prioritizes security controls requirements at scale over active networks. The first-of-A-kind twin collects information about the computer network, associates it with attack tactics, measures the efficiency of implemented security controls requirements and automatically detects missing security controls. The twin also evaluates a cyber risk value using the attack graph and proposes prioritization of the detected requirements to rapidly reduce risk within existing system constraints. The cyber digital twin simulator offers several new risk reduction methods for automatically selecting security controls requirements. The necessary data for constructing a contextual cyber digital twin is defined, including the relationship between security controls and attack tactics. The paper illustrates the calculations used for ranking security controls' risk impact, the algorithm for security controls' requirements prioritization, and finally demonstrates successful results using a field experiment conducted via an active network. © 2020 IEEE. Cyber Digital Twin Simulator for Automatic Gathering and Prioritization of Security Controls' Requirements Agile Security; Analytical Attack Graph; Attack Surface; Cyber Digital Twin; Requirements Prioritization; Security Controls Active networks; Digital twin; Requirements engineering; Cyber threats; Digital enterprise; Existing systems; Field experiment; Prioritization; Requirements prioritization; Risk reductions; Security controls; Network security",Risk management
1967,Agile CMII: A Methodology for Assessing Social Project Impacts Within Agile Contexts,"Agile methodologies have been gaining more and more momentum since its inception in the beginning of the year 2000s. Its inherent iterative and user-centered behavior helps teams to dynamically prioritize, plan and execute work packages, which tend to be appropriate for innovative projects where problem-solution duo co-evolve throughout time. Thus, agile adoption, which naturally began within software development teams, has now been widespread throughout other corporate departments and diverse segments. The present work continues an ongoing research over CMII (Conceição Moura Impact Index), which is an analytics model that combines NPS (net promote score), OKRs (objectives and key results) and SROI (social return over investment) methods for measuring project impacts. Specifically, this current research proposes an integration of Scrum (an agile representative) with CMII model. The main objective is to make the model adoptable and practical for agile teams on social projects. Overall, it functions under Scrum’s sprint ceremonies – planning, daily meeting, review and retrospective –, where CMII KPIs and methods will be timely handled or executed, respectively. The main result is a project assessment methodology – comprised of phases, methods and metrics –, making it applicable for professionals when measuring the overall impact of their initiatives. It aims to dynamically informs strategic decision-making and links it to projects’ operational aspects. © 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG. Agile CMII: A Methodology for Assessing Social Project Impacts Within Agile Contexts Agile; Agile transformation; Analytics; Metrics; Social project ",Strategic alignment
1968,Development and initial validation of the big data framework for agile business: Transformational innovation initiative,"This paper presents the development of Big Data Framework for Agile Business (BDFAB) and its initial validation. BDFAB is a strategic framework comprising values, roles, phases, artifacts, practices, business parameters, a compendium of transformation processes and the Big Data manifesto. BDFAB has direct and practical impact in reducing risks in digital business transformation capitalizing on Big Data. This position paper highlights the implications of Big Data from a strategic viewpoint and, thereby, contributes to both industrial and academic understanding of BDFAB. © Springer Nature Switzerland AG 2020. Development and initial validation of the big data framework for agile business: Transformational innovation initiative Big Data Framework for Agile Business; Big data strategies; Data science application; Risk reduction in big data adoption; Strategies for adopting big data ",Strategic alignment
1969,Architectural ML Framework for IoT Services Delivery Based on Microservices,"The Internet of Things (IoT) is the interconnection of devices and services that allows free data flow. Managing and analyzing this data is the actual added value that IoT is beneficial for. Machine learning plays an increasingly important role in performing data analysis in IoT solutions. This paper presents an architectural framework with machine learning solutions implemented as a service in the microservice group. This architectural framework for IoT services delivery is designed following the Agile methodology. The requirements for the software architecture and expected functionalities of the system are defined. The microservices collection is explained by providing a separate description for every service. Machine learning (ML) analytics on IoT (as the processing paradigm for intelligently handling the IoT data) is represented as a part of the microservice platform. Several strategic advantages of the proposed microservice-based IoT architecture over others are discussed together with implementation issues. © 2020, Springer Nature Switzerland AG. Architectural ML Framework for IoT Services Delivery Based on Microservices Internet of things (IoT); Machine learning (ML); Microservice architecture (MSA); MongoDB; Prediction; Scalability Data handling; Machine learning; Network architecture; Added values; Agile Methodologies; Architectural frameworks; Data flow; Internet of thing (IOT); Iot architectures; Iot services; Strategic advantages; Internet of things",Strategic alignment
1970,Why we need a small data paradigm,"Background: There is great interest in and excitement about the concept of personalized or precision medicine and, in particular, advancing this vision via various 'big data' efforts. While these methods are necessary, they are insufficient to achieve the full personalized medicine promise. A rigorous, complementary 'small data' paradigm that can function both autonomously from and in collaboration with big data is also needed. By 'small data' we build on Estrin's formulation and refer to the rigorous use of data by and for a specific N-of-1 unit (i.e., a single person, clinic, hospital, healthcare system, community, city, etc.) to facilitate improved individual-level description, prediction and, ultimately, control for that specific unit. Main body: The purpose of this piece is to articulate why a small data paradigm is needed and is valuable in itself, and to provide initial directions for future work that can advance study designs and data analytic techniques for a small data approach to precision health. Scientifically, the central value of a small data approach is that it can uniquely manage complex, dynamic, multi-causal, idiosyncratically manifesting phenomena, such as chronic diseases, in comparison to big data. Beyond this, a small data approach better aligns the goals of science and practice, which can result in more rapid agile learning with less data. There is also, feasibly, a unique pathway towards transportable knowledge from a small data approach, which is complementary to a big data approach. Future work should (1) further refine appropriate methods for a small data approach; (2) advance strategies for better integrating a small data approach into real-world practices; and (3) advance ways of actively integrating the strengths and limitations from both small and big data approaches into a unified scientific knowledge base that is linked via a robust science of causality. Conclusion: Small data is valuable in its own right. That said, small and big data paradigms can and should be combined via a foundational science of causality. With these approaches combined, the vision of precision health can be achieved. © 2019 The Author(s). Why we need a small data paradigm Artificial intelligence; Data science; Personalized medicine; Precision health; Precision medicine; Small data Cooperative Behavior; Data Interpretation, Statistical; Data Science; Datasets as Topic; Delivery of Health Care; High-Throughput Screening Assays; Humans; Learning; Precision Medicine; Small-Area Analysis; analytic method; Article; causality; chronic disease; clinical practice; data processing; health care system; human; personalized medicine; population health; public health; reference database; study design; cooperation; health care delivery; high throughput screening; information processing; learning; procedures; small-area analysis; statistical analysis",Strategic alignment
1971,"Rejuvenating late life field opportunities through a fast track full field re-evaluation, focused idle well reactivation, strategic data acquisition and data analytics in peninsular Malaysia waters","Managing late life declining assets often called for a holistic approach and integrated efforts to safeguard the production funnel and finding the upside potential for field rejuvenation to sustain the economic life. Identifying the remaining hydrocarbon potential and monetizing it during late field life often pose a challenge in terms of limited data availability, complex reservoir characterization, complex well profile as well as job doability. To overcome the challenges, a strategic maturation plan and fast track action is required to accelerate the field potential prior to abandonment. This paper will showcase few case studies from Peninsular Malaysia waters on the successful paths taken to reap the full benefits of bypassed hydrocarbon potential by leveraging on proactive full field analysis, fit-for-purpose technology and data analytics for petrophysical evaluation and focused execution of the production enhancement plan which seems impossible under normal operating work scope. Initiating proactive full field review (FFR) and fast track assessment on the remaining hydrocarbon potential prior to field abandonment had become the major enabler in identifying the untapped opportunities behind casing to prolong the field economic life. Commendable efforts to bring the opportunities into life also include advanced data acquisition to minimize uncertainty in the add-perf job, using data learning to assist shallow reservoir evaluation for work-over and infill drilling candidates, utilizing ceased production well for water injector slot and agile as well as adaptive workflow for safe job execution both for production enhancement and abandonment purposes. Through aggressive subsurface study and multi-disciplinary work integration process starting from identifying the opportunities till realizing the barrels into production pipeline, a significant and sustainable production gain had been successfully achieved which helps to prolong the net positive cash flow of the fields involved. The success is replicated to the rest of other fields where a nimble workflow has been established to unlock the late life field potential. Partnership in technology has also triggered more focus in delivering the results and value to the operation in addition to increased collaboration in new uncharted area such as advanced formation evaluation tool technology, nanotechnology material, geoengineering of shallow unconsolidated formation properties and enhancing operational success based on data-driven approach. Besides, some lessons learned and best practices in planning and executing well abandonment operation will be also highlighted. In a nutshell, field rejuvenation and exploiting maximum potential during late field life require a long projectile and strategic planning for value creation. Case studies demonstrated in this paper have provided an exemplary blueprint to best manage the late life declining assets by challenging the status quo and working through an integrated framework to bring in more barrels. Copyright © 2020, Offshore Technology Conference. Rejuvenating late life field opportunities through a fast track full field re-evaluation, focused idle well reactivation, strategic data acquisition and data analytics in peninsular Malaysia waters  Abandoned wells; Data acquisition; Data Analytics; Endocrinology; Hydrocarbons; Offshore oil well production; Offshore technology; Petroleum reservoir evaluation; Bypassed hydrocarbon; Data-driven approach; Hydrocarbon potential; Integrated frameworks; Petrophysical evaluations; Production enhancement; Sustainable production; Unconsolidated formations; Infill drilling",Capacity management
1972,Practical insights for sales force digitalization success,"Sales organizations are actively pursuing digitalization because they see value through greater efficiency and effectiveness. In this paper, we examine the challenges in implementing sales force digitalization and suggest ways in which organizations can overcome these challenges. We present a sales effectiveness framework and discuss how digitalization affects its key components, including customer strategy, organization design, talent management, customer engagement, and the supporting architecture. We propose a conceptual model for digitalization success which considers the digital readiness of the organization, usage and adoption of digital solutions by the sales force, and actions for driving sustained impact. We map three types of digitalization failure to this model, exploring the causes of slow progress, poor adoption, and low impact. Then, we share success factors for overcoming these challenges. These include getting the right team to lead the effort, using an agile approach, and putting the organizational support elements in place to sustain success. We present these success factors in the form of a checklist of critical dimensions that organizations tend to overlook. Finally, we discuss the value for practitioners, share limitations of the work, and propose areas for further research that can enhance our ideas and improve sales digitalization success rates. © 2021 Pi Sigma Epsilon National Educational Foundation. Practical insights for sales force digitalization success digital success checklist; omnichannel; sales analytics; sales force digitalization; Salespeople ",Strategic alignment
1973,"The effect of disruption technology, and the future knowledge management toward service innovation for telecommunication industry 4.0 in Indonesia","The disruption technological creates unprecedented opportunities and challenges that will be strengthened by the convergence of digital, physical technologies that characterize the newly emerging Fourth Industrial Revolution 4.0. This emerging technology is extraordinary and has the potential to become a source of growth. The telecommunications industry 4.0 is undergoing transformational development to deal with disruptive technological challenges. Globally, quality has declined from the past few decades, a combination of a stagnant economy and an increase in the quality of income has caused dissatisfaction. Inertia services that are often suffered by large companies are often difficult to shake, customer expectations have also shifted in the market, placing the company in a difficult situation in achieving customer satisfaction, agile and innovative technological trends, cost competition, government policy, and this will increase the level of global competition. Exploration case studies and literature reviews are used to test the antecedents of Service Innovation. The study identifies that disruption of technology services offers opportunities for business development to encourage the use of Internet-based services, increasing demand for cheaper and faster internet for consumers. To facing a competitive advantage, organizations have influenced by Service Innovation with the ability to improve big data analytics and organize future knowledge management capabilities, which are agile and flexible in providing information and solutions. From the managerial perspective, this research provides a comprehensive view of what the impact of Service Innovation is on organizations, how to achieve, what variables contribute, and how to relate with performance. The authenticity of this research lies in the description of how management emerges with a practical oriented framework of how organizations must be formed to be innovative and competitive through the general arrangement of antecedents of service innovation. This study, however, has limitations because the qualitative nature and conceptual framework need to have further investigated through large-scale surveys by quantitative research. © BEIESP. The effect of disruption technology, and the future knowledge management toward service innovation for telecommunication industry 4.0 in Indonesia Big Data Analytics; Competitive Advantage; Disruption Technology; Knowledge Management; Service Innovation ",Strategic alignment
1974,"Comparison of the Particulate Matter Index and Particulate Evaluation Index Numbers Calculated by Detailed Hydrocarbon Analysis by Gas Chromatography (Enhanced ASTM D6730) and Vacuum Ultraviolet Paraffin, Isoparaffin, Olefin, Naphthene, and Aromatic Analysis (ASTM D8071)","The Particulate Matter Index (PMI) is a tool that provides an indication of a fuel's tendency to produce Particulate Matter (PM) emissions. Currently, the index is being used by various fuel laboratories and the Automotive OEMs as a tool to understand the gasoline fuel's impact on both PM from engine hardware and vehicle-out emissions. In addition, a newer index that could be used to give an indication of the PM tendency of the gasoline range fuels, called the Particulate Evaluation Index (PEI), is shown to have a good correlation to PMI. The data used in those indices are collected from chemical analytical methods. This paper will compare gas chromatography (GC) methods used by three laboratories and discuss how the different techniques may affect the PMI and PEI calculation. Data from two fuel laboratories running an Enhanced ASTM D6730 method will be compared to the paraffin, isoparaffin, olefin, naphthene, or aromatic (PIONA) data from a modified ASTM D8071 method using a newly developed vacuum ultraviolet (VUV) detector from VUV Analytics. While values do trend together, they are not equivalent. The possible reasons for this are explored, and some recommendations are made for future work.  ©  Comparison of the Particulate Matter Index and Particulate Evaluation Index Numbers Calculated by Detailed Hydrocarbon Analysis by Gas Chromatography (Enhanced ASTM D6730) and Vacuum Ultraviolet Paraffin, Isoparaffin, Olefin, Naphthene, and Aromatic Analysis (ASTM D8071) D6730; D8071; Detailed Hydrocarbon Analysis; DHA; Honda Index; Particulate Matter Index; PEI; PIONA; PMI; VUV Aromatic hydrocarbons; Gasoline; Olefins; Paraffins; Particles (particulate matter); Analytical method; Evaluation index; Gasoline fuels; Good correlations; Hydrocarbon analysis; Particulate Matter; Particulate matter emissions; Vacuum ultraviolets; Gas chromatography",Monitoring and control
1975,Structural modeling of lean supply chain enablers: a hybrid AHP and ISM-MICMAC based approach,"Purpose: Today the role of industry 4.0 plays a very important role in enhancing any supply chain network, as the industry 4.0 supply chain uses Big Data and advanced analytics to inform the complete visibility. Latest data are available to bring clarity and support real-time decision-making in the entire supply chain that is why adopting optimization techniques such as lean manufacturing and lean supply chain concept for enhancing the supply chain network of the organizations is a good idea and would benefit them in increasing their cost efficiency and productivity. The purpose of this work is to develop a technique, which may be useful for future researchers and managers to identify and classification of the significant lean supply chain enablers. Design/methodology/approach: In this paper, the authors considered hybrid analytical hierarchy process to find the ranking of the identified lean supply chain enablers by calculating their weightage. Interpretive structural modeling (ISM) is applied to develop the structural interrelationship among various lean supply chain management enablers. Considering the results obtained from ISM the Matrices d'Impacts Croises Multiplication Appliqué a un Classement (MICMAC) analysis is done to identify the driving and dependence power of Lean Supply Chain Management Enablers (LSCMEs). Findings: Further, the best results applying these methodologies could be used to analyze their inter-relationships for successful Lean supply chain management implementation in an organization. The authors developed an integrated model after the identification of 20 key LSCMEs, which is very helpful to identify and classify the important enablers by ISM methodology and explore the direct and indirect effects of each enabler by MICMAC analysis on the LSCM implementation. This will help organizations optimize their supply chain by selective control of lean enablers. Practical implications: For lean manufacturing practitioners, the result of the study can be beneficial where the manufacturer is required to increase efficiency and reduce cost and wastage of resources in the lean manufacturing process, as well as in enhancing the supply chain. Originality/value: This paper is the first research paper that considered firstly deep literature review of identified lean supply chain enablers and second developed structured modeling of various lean enablers of supply chain with the help of various methodologies. © 2021, Emerald Publishing Limited. Structural modeling of lean supply chain enablers: a hybrid AHP and ISM-MICMAC based approach AHP; ISM; Lean supply chain enablers; Lean supply chain management; MICMAC Agile manufacturing systems; Decision making; Efficiency; Industry 4.0; Lean production; Supply chain management; Interpretive structural models; Lean supply chain enabler; Lean supply chain management; Lean supply chains; matrix; Matrix d'impact croises multiplication applique a un classement; Model matrices; Structural modeling; Supply chain network; Hierarchical systems",Strategic alignment
1976,Commentary: practical insights for sales force digitalization success—an executive’s key takeaways,"The author of the present commentary evaluates the implications of the Zoltners, Sinha, Sahay, Shastri, and Lorimer (2021) article and their view on the challenges associated with digitizing an organization’s sales force. The associated pitfalls of sales force digitalization are real—and so is the potential. Hence, the author commends Zoltners and colleagues (2021) for contributing such valuable insights pertaining to one of the inevitable challenges of today’s sales force management. The author’s aim is to contribute an executive’s perspective on the article by highlighting three key takeaways: (1) Master the thin line between the scope of digitalization and the risks involved, (2) ensure simplicity as well as user value, and (3) deploy agile processes when developing, implementing, and operating sales force digitalization initiatives. © 2021 Pi Sigma Epsilon National Educational Foundation. Commentary: practical insights for sales force digitalization success—an executive’s key takeaways Digital transformation; sales analytics; sales force digitalization ",Strategic alignment
1978,STAMP 4 NLP – An Agile Framework for Rapid Quality-Driven NLP Applications Development,"The progress in natural language processing (NLP) research over the last years, offers novel business opportunities for companies, as automated user interaction or improved data analysis. Building sophisticated NLP applications requires dealing with modern machine learning (ML) technologies, which impedes enterprises from establishing successful NLP projects. Our experience in applied NLP research projects shows that the continuous integration of research prototypes in production-like environments with quality assurance builds trust in the software and shows convenience and usefulness regarding the business goal. We introduce STAMP 4 NLP as an iterative and incremental process model for developing NLP applications. With STAMP 4 NLP, we merge software engineering principles with best practices from data science. Instantiating our process model allows efficiently creating prototypes by utilizing templates, conventions, and implementations, enabling developers and data scientists to focus on the business goals. Due to our iterative-incremental approach, businesses can deploy an enhanced version of the prototype to their software environment after every iteration, maximizing potential business value and trust early and avoiding the cost of successful yet never deployed experiments. © 2021, Springer Nature Switzerland AG. STAMP 4 NLP – An Agile Framework for Rapid Quality-Driven NLP Applications Development Avoiding pitfalls; Best practices; Machine learning; Natural language processing; Process model; Quality assurance Data Science; Quality assurance; Software prototyping; Applications development; Business opportunities; Continuous integrations; Incremental approach; Incremental process; NAtural language processing; Software engineering principles; Software environments; Natural language processing systems",Value management
1980,Sentiment Analysis Platform of Customer Product Reviews,"Opinions and reviews are very important data coming from the customer for the companies to improve their product or services. The study focuses on creating a platform that would be able to capture the data in the form of comments or reviews. The platform was able to create and generate quantitative and text data based on the reviews and comments. The researchers used the Scrum Agile methodology. This study utilized the descriptive research method with the use of the questionnaire as the instrument testing the acceptability of the system's application. It was evaluated by 100 respondents. The mean value was determined and compared using analysis of variance to determine if there is a significant difference. ANOVA was also used to test for critical value of the null hypothesis. Based on the results, there is a marked significant difference among the evaluation of the 4 groups of respondents as indicated in the F-value of 6.48972. The respondents gave an acceptable rating and all agreed that the proposed product sentiment analysis application is a useful tool in evaluating company's products. © 2019 IEEE. Sentiment Analysis Platform of Customer Product Reviews analytics; mining; opinion; product review; sentiment analysis Artificial intelligence; Instrument testing; Mining; Agile Methodologies; analytics; Critical value; Customer products; Null hypothesis; opinion; Product reviews; research methods; Sentiment analysis",Strategic alignment
1981,"10th International Conference on Design, User Experience, and Usability, DUXU 2021, held as part of the 23rd International Conference, HCI International 2021","The proceedings contain 130 papers. The special focus in this conference is on Design, User Experience, and Usability. The topics include: Envisioning Educational Product User eXperience Through Participatory Design Practice; operational Usability Heuristics: A Question-Based Approach for Facilitating the Detection of Usability Problems; integration of User Experience and Agile Techniques for Requirements Analysis: A Systematic Review; is Your App Conducive to Behaviour Change? A Novel Heuristic Evaluation; understanding Customer Value Propositions Through the Lens of Value Equations Method: A Systematic Approach; Developing and Validating a Set of Usability and Security Metrics for ATM Interfaces; challenges and Opportunities on the Application of Heuristic Evaluations: A Systematic Literature Review; consumer Experience Research Based on the Background of Experience Economy and Digital Economy; a Study on Scale Construction of Adjective Pairs for Evaluating Audiovisual Effects in Video Games; A Framework Based on UCD and Scrum for the Software Development Process; a Review of Automated Website Usability Evaluation Tools: Research Issues and Challenges; web Analytics for User Experience: A Systematic Literature Review; validation of a Questionnaire to Evaluate the Usability in the Peruvian Context; design Research on Visualization of Life Behavior and Rhythm; YERKISH: A Visual Language for Computer-Mediated Communication by an Ape; the Canadian Cultural Diversity Dashboard: Data Storytelling and Visualization for the Cultural Sector; visual Writing at the State-of-the-Art?; expressions of Data: Natural State, Specific Application, and General Pattern; pasigraphy: Universal Visible Languages; emojitaliano: A Social and Crowdsourcing Experiment of the Creation of a Visual International Language. 10th International Conference on Design, User Experience, and Usability, DUXU 2021, held as part of the 23rd International Conference, HCI International 2021  ",Strategic alignment
1982,How big data analytics improve supply chain performance: Considering the roles of supply chain and is strategy,"This paper aims to examine the relationship between big data analytics (BDA) and supply chain (SC) performance. Drawing on the dynamic capabilities theory, we hypothesize that supply chain strategy (i.e., lean and agile supply chain strategy) will positively mediate the relationship between BDA and SC performance. Further, from the perspective of strategy alignment, we also hypothesize that the effect of supply chain strategy on SC performance will be moderated by IS strategy (i.e., IS innovator and IS conservative strategy). Our study uses 159 match-paired questionnaires collected from Chinese firms to empirically test the hypotheses. This paper explains how BDA can improve SC performance and reveals the interaction effect of IS strategy and supply chain strategy on SC performance, which contributes to theoretical and practical implications. © ICIS 2020. All rights reserved. How big data analytics improve supply chain performance: Considering the roles of supply chain and is strategy Agile supply chain strategy; Big data analytics; IS conservative strategy; IS innovator strategy; Lean supply chain strategy; Supply chain performance Big data; Blending; Data Analytics; Information systems; Information use; Supply chains; Surveys; Agile supply chains; Chinese firms; Dynamic capabilities; Interaction effect; IS strategy; Strategy alignment; Supply chain performance; Supply chain strategy; Advanced Analytics",Strategic alignment
1983,Excess patient visits for cough and pulmonary disease at a large US health system in the months prior to the COVID-19 pandemic: Time-series analysis,"Background: Accurately assessing the regional activity of diseases such as COVID-19 is important in guiding public health interventions. Leveraging electronic health records (EHRs) to monitor outpatient clinical encounters may lead to the identification of emerging outbreaks. Objective: The aim of this study is to investigate whether excess visits where the word “cough” was present in the EHR reason for visit, and hospitalizations with acute respiratory failure were more frequent from December 2019 to February 2020 compared with the preceding 5 years. Methods: A retrospective observational cohort was identified from a large US health system with 3 hospitals, over 180 clinics, and 2.5 million patient encounters annually. Data from patient encounters from July 1, 2014, to February 29, 2020, were included. Seasonal autoregressive integrated moving average (SARIMA) time-series models were used to evaluate if the observed winter 2019/2020 rates were higher than the forecast 95% prediction intervals. The estimated excess number of visits and hospitalizations in winter 2019/2020 were calculated compared to previous seasons. Results: The percentage of patients presenting with an EHR reason for visit containing the word “cough” to clinics exceeded the 95% prediction interval the week of December 22, 2019, and was consistently above the 95% prediction interval all 10 weeks through the end of February 2020. Similar trends were noted for emergency department visits and hospitalizations starting December 22, 2019, where observed data exceeded the 95% prediction interval in 6 and 7 of the 10 weeks, respectively. The estimated excess over the 3-month 2019/2020 winter season, obtained by either subtracting the maximum or subtracting the average of the five previous seasons from the current season, was 1.6 or 2.0 excess visits for cough per 1000 outpatient visits, 11.0 or 19.2 excess visits for cough per 1000 emergency department visits, and 21.4 or 39.1 excess visits per 1000 hospitalizations with acute respiratory failure, respectively. The total numbers of excess cases above the 95% predicted forecast interval were 168 cases in the outpatient clinics, 56 cases for the emergency department, and 18 hospitalized with acute respiratory failure. Conclusions: A significantly higher number of patients with respiratory complaints and diseases starting in late December 2019 and continuing through February 2020 suggests community spread of SARS-CoV-2 prior to established clinical awareness and testing capabilities. This provides a case example of how health system analytics combined with EHR data can provide powerful and agile tools for identifying when future trends in patient populations are outside of the expected ranges. © Joann G Elmore, Pin-Chieh Wang, Kathleen F Kerr, David L Schriger, Douglas E Morrison, Ron Brookmeyer, Michael A Pfeffer, Thomas H Payne, Judith S Currier. Originally published in the Journal of Medical Internet Research (http://www.jmir.org), 10.09.2020. This is an open-access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work, first published in the Journal of Medical Internet Research, is properly cited. The complete bibliographic information, a link to the original publication on http://www.jmir.org/, as well as this copyright and license information must be included. Excess patient visits for cough and pulmonary disease at a large US health system in the months prior to the COVID-19 pandemic: Time-series analysis COVID-19; Electronic health record; Forecast; Pandemic; Prediction; Time-series analysis Acute Disease; Adult; Ambulatory Care Facilities; Betacoronavirus; California; Coronavirus Infections; Cough; Electronic Health Records; Emergency Service, Hospital; Female; Hospitalization; Humans; Male; Middle Aged; Pandemics; Pneumonia, Viral; Respiratory Insufficiency; Retrospective Studies; Seasons; acute respiratory failure; Article; coronavirus disease 2019; coughing; emergency care; health care system; hospital care; hospitalization; human; lung disease; major clinical study; observational study; outpatient care; pneumonia; retrospective study; seasonal variation; time series analysis; United States; winter; acute disease; adult; Betacoronavirus; California; Coronavirus infection; coughing; electronic health record; female; hospital emergency service; male; middle aged; outpatient department; pandemic; respiratory failure; season; virus pneumonia",Monitoring and control
1984,Sustainability benefits analysis of cyber-manufacturing systems,"Cyber-Manufacturing System (CMS) is a vision for the factory of the future, where manufacturing processes and physical components are seamlessly integrated with computational processes to provide agile, adaptive, and scalable manufacturing services. Functional elements of CMS are digitized, registered, and shared with users and stakeholders through various computer networks and the Internet. CMS incorporates recent advances in the Internet of Things, Cloud Computing, Cyber-Physical System, Service-Oriented Technologies, Modeling and Simulation, Sensor Networks, Machine Learning, Data Analytics, and Advanced Manufacturing Processes. CMS possesses intelligence such as self-monitoring, self-adjustment, self-prediction, self-allocation, self-configuration, self-scalability, self-remediating, and self-reusing. Such intelligent capabilities enable CMS to contribute to manufacturing sustainability. However, prior studies are limited in addressing a narrow scope of CMS or in covering only a subset of sustainability dimensions. This paper addresses the research gap by developing a holistic CMS infrastructure and adopting a Distance-to-Target based sustainability assessment approach to measure the sustainability benefits of CMS. To illustrate how the infrastructure and metrics are used to analyze the sustainability benefits of CMS, an example case is presented. The results show that CMS can deliver substantial sustainability benefits through increased productivity, profitability & energy efficiencies, and reduction of working-in-process (WIP) inventory levels & logistics costs. © 2020 The Author(s). This is an Open Access article under the CC BY license. Sustainability benefits analysis of cyber-manufacturing systems Cyber-Manufacturing system; Cyber-Physical system; Modeling and simulation; Sustainability assessment; Triple bottom line Data Analytics; Embedded systems; Metal drawing; Sensor networks; Sustainable development; Advanced manufacturing; Computational process; Increased productivity; Manufacturing process; Manufacturing service; Model and simulation; Sustainability assessment; Sustainability dimensions; Engineering education",Strategic alignment
1986,INFORMING AGILITY IN THE CONTEXT OF ORGANIZATIONAL CHANGES,"Aim/Purpose This paper, although conceived earlier than the emergence of COVID-19 pandemic, addresses the problem of informing agility as part of organizational agility that has become a rather important issue for business survival. Background While the general issues of business informing, and business intelligence (BI) in particular, have been widely researched, the dynamics of informing, their ability to act in accord with changes in business and preserve the key competencies has not been widely researched. In particular, the research on BI agility is rather scattered, and many issues need to be clarified. Methodology A series of in-depth interviews with BI professionals to determine relations between organizational agility and BI agility, and to round up a set of key factors of BI agility. Contribution The paper clarifies a candidate set of key factors of BI agility and gives ground for future research in relations with areas like corporate and BI resilience and culture. Findings The interview results show the relations between organizational changes, and changes in BI activities. BI has limited potential in recognizing important external changes but can be rather helpful in making decision choices and detecting internal problems. Lack of communication between business and IT people, existence of data silos and shadow BI, and general inadequacy of organizational and BI culture are the key factors impairing BI agility. Recommendations There are practical issues around BI agility that need solving, like the reason-for Practitioners able coverage of standards or creation of a dedicated unit to care about BI potential. Recommendations The research is still in its starting phase, but additional interesting directions for Researchers start to emerge, like relations between BI agility, resilience and corporate agil ity, or the role of informing culture and BI culture for BI agility issues. Impact on Society Agile business, especially in times of global shocks like COVID-19, loses less value and has more chances to survive. Future Research Most likely this will be focused on the relations between BI agility, resilience, and corporate agility, and the role of informing culture and BI culture for BI agility issues. © 2021. All Rights Reserved. INFORMING AGILITY IN THE CONTEXT OF ORGANIZATIONAL CHANGES business intelligence agility; organizational agility ",Governance
1988,"1st Conference on Production Systems and Logistics, CPSL 2020","The proceedings contain 49 papers. The special focus in this conference is on Production Systems and Logistics. The topics include: Contribution to the industry-specific identification and selection of a business model in machinery and equipment industry; development of the Supply Chain Management 2040 - Opportunities and Challenges; innovative logistics concepts for a versatile and flexible manufacturing of lot size one; dynamic criticality assessment as a supporting tool for knowledge retention to increase the efficiency and effectiveness of maintenance; Introduction of Traceability into the Continuous Improvement Process of SMEs; data-based identification of knowledge transfer needs in global production networks; evolution of a Lean Smart Maintenance Maturity Model towards the new Age of Industry 4.0; development of a Concept for Real-Time Control of Manual Assembly Systems; concept for the cost prognosis in the industrialization of highly iteratively developed physical products; methodology And Use Of Variant Fields In Factory Planning; value Stream Mapping and Process Mining: A Lean Method Supported by Data Analytics; a Framework for Data Integration and Analysis in Radial-Axial Ring Rolling; optimization approach for the combined planning and control of an agile assembly system for electric vehicles; generative Design In Factory Layout Planning: An Application Of Evolutionary Computing Within The Creation Of Production Logistic Concepts; self-organization and autonomous control of intralogistics systems in line with versatile production at Werk150; socio-technical requirements for production planning and control systems; analytical model for determining the manual consolidation time for large equipment manufacturers; part Based Mold Quotation With Methods Of Machine Learning. 1st Conference on Production Systems and Logistics, CPSL 2020  ",Strategic alignment
1993,Predictive Analysis of Manpower Requirements in Scrum Projects Using Regression Techniques,"Flexible iterative development life cycle, adaptive nature and fast delivery has given Agile an upper edge as compared to all other software development frameworks. In the current industry scenario agile methods are gaining popularity, owing to its people centric approach, hence organizations are adopting agile development methodologies at a large scale. Agile projects work in self-organizing small collaborative teams. Team size varies according to the project requirement however, agile development focus on smaller team size. Supervised machine learning is applied in this study to provide optimum prediction model. All the available regression models in Matlab R2019b are used to predict number of team members required for an agile project. Iterations from five different open source projects are considered for this study. The results after training all the variants of each regression model, namely Linear Regression models, Support Vector Machine models, Tree models, Ensemble models and Gaussian Process Regression models are compared using Root Mean Square Error (RMSE) score and R-squared values. On the basis of evaluative and comprehensive analysis, the most significant model to predict manpower requirement for an agile project has been chosen. © 2020 The Authors. Published by Elsevier B.V. Predictive Analysis of Manpower Requirements in Scrum Projects Using Regression Techniques Agile projects; Regression models; Scrum; Team size Forecasting; Intelligent computing; Iterative methods; Life cycle; MATLAB; Mean square error; Open source software; Software design; Support vector machines; Support vector regression; Agile development methodologies; Comprehensive analysis; Gaussian process regression model; Linear regression models; Root mean square errors; Software development framework; Supervised machine learning; Support vector machine models; Predictive analytics",Strategic alignment
1995,Marketing ecosystem: An outside-in view for sustainable advantage,"In the modern business environment, consumers are increasingly influenced by megatrends involving marketplace, technology, socioeconomics, geopolitics, and natural environment. Simultaneously, the data and insights that can inform consumer attitudes and behaviors often reside outside of firms' direct control. Consciously incorporating these interdependent factors into firms' decision-making is essential for adaptability and sustainable profitability. Building on the “outside-in” perspective, we propose that firm strategies should be informed through the lens of the marketing ecosystem that considers the interrelated and dynamic megatrends. By leveraging advances in data and technology, firms can sense-make the marketplace by extracting insights from massive amounts of diverse consumer data with modern-day analytics. By mapping out the megatrends with marketing analytics, firms can 1) more accurately predict consumers' changing preferences and formulate appropriate strategies to engage with them; and 2) become more market-adaptable and competitive in the present and the future. To deliver sustainably compelling value to customers, firms should adopt an ecosystem mindset and cooperate with various stakeholders. A broad-thinking, agile, and humble firm culture can enable the development of more robust outside-in capabilities. We elaborate on the megatrends in the interconnected world of the marketing ecosystem, and propose emerging research directions in each area. © 2020 Elsevier Inc. Marketing ecosystem: An outside-in view for sustainable advantage Consumer privacy; Marketing strategy; Outside-in marketing; Socioeconomics; Sustainability; Unstructured data ",Value management
1996,Just-in-time modeling with DataMingler,"DataMingler is a prototype tool that implements a novel conceptual model, the Data Virtual Machine (DVM) and can be used for agile just-in-time modeling of data from diverse sources. The DVM provides easy-to-understand semantics and fast and flexible schema manipulations. An important and useful class of queries in analytics environments, dataframes, is defined in the context of DVMs. These queries can be expressed either visually or through a novel query language, DVM-QL. We demonstrate DataMingler’s capabilities map relational sources and queries on the latter in a DVM schema and augment it with information from semi-structured and unstructured sources. We also show how to express on the DVM easily complex relational queries or queries on structured, semi-structured and unstructured sources combined. Copyright © 2021 for this paper by its authors. Just-in-time modeling with DataMingler Data virtualization; Just-in-time modeling Just in time production; Query languages; Virtualization; Conceptual model; Data virtualization; Just-in-time modeling; Prototype tools; Relational queries; Semi-structured; Semantics",Monitoring and control
1999,Developing strategies to improve agility in the project procurement management (PPM) process: Perspective of business intelligence (BI),"Purpose: The ability of an organization to observe varying demands and efficiently meet them can be described as agility. Project procurement management (PPM) in the past was stable as things did not change very often and were very predictable. Due to hyper-competition, less predictable market and exponential innovation, the existing PPM becomes very unstable which marks the requirement of an agile model to manage procurement projects effectively. The paper aims to discuss this issue. Design/methodology/approach: For achieving the improvements, various barriers to improving agility in PPM were identified from the literature and experts’ review, followed by obtaining quantified impacts of identified barriers from the experts using the Delphi technique. Finally, interpretive structural modeling along with Matrice d’ Impacts Croises Multiplication Appliqué an Classement analysis was used to analyze the interactions among barriers to prioritize and strategize their mitigation. Findings: As per the analysis, the lack of top management alignment and commitment, lack of digital strategy, lack of new technology competencies and inefficiencies of financial factors were the most critical barriers that would come across while improving agility in PPM for any organization. Industries should have a stable, well-established and supportive top management that has a vision for digital transformation along with upgrading the companies’ technology layer for automating most of the manual processes to have intelligent decision-making capability. Originality/value: Industries need to be agile in their operations for being more competitive and responsive to the market. PPM being the most critical part of the entire value chain needs to be agile in the first place. The strategies developed as an output of this research can be utilized by industries for improving agility in their business processes. © 2019, Emerald Publishing Limited. Developing strategies to improve agility in the project procurement management (PPM) process: Perspective of business intelligence (BI) Agility; Business intelligence; Delphi; Interpretive structural modelling; MICMAC; Project procurement management ",Governance
2000,Production optimization under constraints: Development and application of software combining data science and petroleum engineering knowledge,"OMV New Zealand gas/condensate fields’ gas production is limited by commercial demand, which also constrain production of associated condensate. No test separators nor individual well multiphase flow meters are installed, only single-phase gas flow meters (V-cones flow meters and orifice plate) for each individual well. In order to produce the maximum revenue for the fields, the wells with the highest condensate-gas-ratio need to be prioritized, while still ensuring that well and facilities constraints are managed. An agile crew of engineers, developers and data scientists, have been mobilized to design and create reliable, easy to use and easy to maintain software solutions to solve three different parts of the optimization problem: A live, dynamic visualization of the wells operating envelopes for dynamic monitoring of the current status of individual wells versus the constraints and direct comparison with simulation models results. A software solution to automatically identify step-changes in well gas, water and condensate rates at facility output level, using these changes to improve CGR and WGR allocated value for each individual well. A software application to calculate the best combination of individual well rates to meet gas export demand while maximizing condensate production, within facility limits and well operating envelopes. © EAGE 2019. Production optimization under constraints: Development and application of software combining data science and petroleum engineering knowledge  Application programs; Data Science; Data visualization; Flow measurement; Flow of gases; Flowmeters; Gas condensates; Gases; Petroleum engineering; Phase meters; Condensate gas ratios; Condensate production; Development and applications; Dynamic visualization; Multi-phase flow meters; Optimization problems; Production optimization; Single phase gas flows; Gas industry",Capacity management
2002,Digital transformation and the impact in knowledge management,"Nowadays, digital transformation is forcing companies to reach a new level of productivity and digital evolution. Small and autonomous is winning over large and centralized. Digital transformation requires the adoption of more agile business processes and the development of new customer-facing digital services. It means creating scale through reusable services and enabling self-service consumption of those services. Business processes and transactions can be automated with the composition of microservices. We will see that the principle of composability allows microservices to deliver value to the business in different contexts. The paper also explains how a BizDevOps philosophy with references to microservices allows rapid adaptations of requirements to fast-changing needs in businesses, outlining the importance of business process automation for companies to acquire the know-how to implement a just-in-time diachronic dialogue. It presents the alignment of the proposed framework with a digital strategy. Assembling a multidisciplinary team is foreseen as a key factor in developing innovative capabilities to react to new customer demands, enabling the company to stay competitive and continuously address customer expectations, differentiating tacit from explicit knowledge.  Copyright © 2020 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved. Digital transformation and the impact in knowledge management Bizdevops; Context-awareness; Decision support and data analytics; Ux engineering Information services; Sales; Search engines; Technology transfer; Business process automation; Customer expectation; Digital strategies; Digital transformation; Explicit knowledge; Innovative capability; Multi-disciplinary teams; Reusable services; Knowledge management",Strategic alignment
2006,Refinery digital transformation: Driving an end-to-end value chain optimization strategy with ai and prescriptive models,"Objectives/Scope: Please list the objectives and/or scope of the proposed paper. (25-75 words): The intent of this paper is to recognize and address the challenges that downstream companies face across their value chain including a history of having siloed businesses processes that have created 'value leaks'. We propose an unified operations management approach that orchestrates all activities across the value chain as part of an enterprise digital transformation strategy. Business process can be deeply transformed when operations, supply chain and process optimization are connected in a collaborative environment. Methods, Procedures, Process: Briefly explain your overall approach, including your methods, procedures and process. (75-100 words): Refineries are at different stages in their digital transformation journey and have built their business workflows over many years across multiple point solutions. We propose a digital transformation strategy for operations across five key characteristics, that is: • Comprehensive: an enterprise solution across all layers of the value chain that breaks down business silos and creates collaboration • Flexible: allows businesses to start at any point in their own transformation journey with solutions for each specific component of the value chain • Connected: opens new possibilities and workflows to drive unique business process improvement • Modern: leveraging latest technologies for the best user experience, an agile performance and focus on reducing cost of ownership • Transparent: visualisation and advanced analytics that improve management decision making across the value chain Results, Observations, Conclusions: Please describe the results, observations and conclusions of the proposed paper. (100-200 words): Across a refinery's value chain there are significant benefits that can be realized by approaching digital transformation across their operations enabling end-to-end value chain optimization including: • Supply Chain ($20-150M/year): Crude Purchases - Reduced cost of purchases, reduced maintenance Planning, Scheduling and Logistics: Increased throughput, increased yield • Process Performance ($10-50M/year): Longer equipment life, increased availability, increased quality, increased yields • Blending and Oil Movements ($5-50M/year): Reduced giveaway, no rework, minimum inventory, minimum downgrades, higher fuels agility • Energy Management ($5-25M/year): Reduced energy conversion cost, reduced energy consumption cost, reduced cost of crude for energy • Production Management ($2-5M/year): Reduced accounting losses, reduced inventory, reduced hydrogen and steam consumption, increased throughput • Operations Management ($2-5M/year): Reduced unplanned shutdowns, increased yield, reduced hydrogen and steam consumption, increased throughput Novel/Additive Information: Please explain how this paper will present novel (new) or additive information to the existing body of literature that can be of benefit to and/or add to the state of knowledge in the petroleum industry. (25-75 words): This paper is novel as it takes a wholistic look at a refinery operations value chain and eliminating existing value leaks end-to-end. It recognizes how existing systems have been built-up on outdated technology and siloed business process and proposes a path forward bringing operations, supply chain and process optimization together as a key element of a digital transformation strategy. The paper also explores how AI & prescriptive models pave the future for optimization. © 2020, Society of Petroleum Engineers Refinery digital transformation: Driving an end-to-end value chain optimization strategy with ai and prescriptive models  Advanced Analytics; Blending; Cost reduction; Crude oil; Decision making; Energy conversion; Energy utilization; Fuel additives; Gasoline; Hydrogen fuels; Mobile telecommunication systems; Optimization; Petroleum additives; Petroleum industry; Process control; Scheduling; Supply chains; Throughput; User experience; Business process improvement; Collaborative environments; Digital transformation; Maintenance planning; Management decision-making; Operations management; Production management; Value chain optimizations; Petroleum refineries",Capacity management
2007,Enduring Industry Dynamics through Agile Business Strategy,"This paper is conceptual in nature, aims to discuss the phenomenon of globalization in ICT industry and its strategic implications. In this context the paper specifically studies two firms- an Indian MNC, HCL and American MNC, CISCO who have made forte in their respective field. The study aims to explore strategic propositions of these two ICT companies that has exhibited agile strategy phenomenally. Also, an attempt is made to explore the strategic approach of other contemporary firms in order to have critical observations laying down conclusion. It suggests that distributing assets from existing activities and redeploying them to new opportunities is foremost imperative in contemporary business environment; the technological trend is seen departing from traditional stand while adopting tactical and innovative approach for agile business intelligence. MNC (multinational corporations) executives ought to map their worldwide competency scope, accordingly, divesting redundant business and build astuteness to adapt future challenges with agile strategies. © 2020 Ecological Society of India. All rights reserved. Enduring Industry Dynamics through Agile Business Strategy Artificial intelligence; Information flux; Innovation capability; Transforming enterprises; Value creation system ",Capacity management
2008,The ecology of open innovation units: adhocracy and competing values in public service systems,"There have been concerted efforts to encourage innovation and to foster a more innovative and “open” culture to government and public service institutions. Policy and service innovation labs constitute one part of a broader “open innovation” movement which also includes open data, behavioral insights, digital services, data science units, visualization capabilities, and agile and lean methods. This article argues that we need to step back and better understand these “ecologies” of innovation capabilities that have emerged across public service institutions, and to recognize that as fellow “innovation” traveling companions they collectively seek to transform the culture of government and public service institutions, producing more effective, efficient and tailored policies and services. This article introduces analytic frameworks that should help locate policy and innovation labs amidst these other innovating entities. First, it delineates the various units and initiatives which can be seen as committed to new ways of working and innovating in public service institutions, often relying on “open innovation” rhetoric and approaches. Second, it shows how–despite the diversity among these entities–they nevertheless share similar attributes as “adhocracies” and are located as part of a broader movement and class of organizations. Third, we locate these diverse OI entities amidst broader public service systems using the Competing Values Framework. Fourth, this article situates the challenges confronting OI units developing and sustaining or broadening niches in public service systems. Finally, it identifies future research questions to take up. © 2021 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group. The ecology of open innovation units: adhocracy and competing values in public service systems adhocracies; competing values framework; policy labs; innovation labs; service labs; Open innovation; public service ",Stakeholder management
2011,Software Engineering for Machine Learning: A Case Study,"Recent advances in machine learning have stimulated widespread interest within the Information Technology sector on integrating AI capabilities into software and services. This goal has forced organizations to evolve their development processes. We report on a study that we conducted on observing software teams at Microsoft as they develop AI-based applications. We consider a nine-stage workflow process informed by prior experiences developing AI applications (e.g., search and NLP) and data science tools (e.g. application diagnostics and bug reporting). We found that various Microsoft teams have united this workflow into preexisting, well-evolved, Agile-like software engineering processes, providing insights about several essential engineering challenges that organizations may face in creating large-scale AI solutions for the marketplace. We collected some best practices from Microsoft teams to address these challenges. In addition, we have identified three aspects of the AI domain that make it fundamentally different from prior software application domains: 1) discovering, managing, and versioning the data needed for machine learning applications is much more complex and difficult than other types of software engineering, 2) model customization and model reuse require very different skills than are typically found in software teams, and 3) AI components are more difficult to handle as distinct modules than traditional software components-models may be 'entangled' in complex ways and experience non-monotonic error behavior. We believe that the lessons learned by Microsoft teams will be valuable to other organizations. © 2019 IEEE. Software Engineering for Machine Learning: A Case Study Artifical Intelligence; Data; Machine Learning; Process; Software Engineering Computer software reusability; Engineering education; Learning systems; Machine components; Machine learning; Processing; Software engineering; Artifical intelligence; Data; Development process; Engineering challenges; Information technology sector; Machine learning applications; Software applications; Software engineering process; Application programs",Strategic alignment
2014,A scoping review of the use of data analytics for the evaluation of mhealth applications,"Background: The development and use of mobile health applications (mHealth Apps) have increased drastically over the last few years. There is an evident lack of the needed evidence to support the scalability and sustainability of these mHealth Apps. The vast number of mHealth Apps commercially available on App Stores makes it difficult to select the most effective and trustworthy Apps. Continuous evaluation of these Apps needs to be implemented or conducted to establish the needed evidence-base. Traditional methods for evaluating health interventions are not suitable for the quick and agile approach needed to develop and maintain Apps. In search of alternative and consistent standards and guidelines new evaluation methods are proposed. The aim of this article is to investigate the state of the literature in this regard. A scoping review is conducted to identify how mHealth Apps are currently evaluated, with a focus on data analytics. Method: Five stages of conducting a scoping review are implemented: 1) identifying the research questions 2) identifying the relevant studies 3) study selection 4) data charting and 5) collecting, summarising, and reporting the results. Electronic databases used for the literature search included Scopus, Google Scholar, PubMed, and JMIR. Using the identified search terms for the scope-specific contents and two levels of screening 43 documents were included for the final data extraction. Results: There is an increasing trend within the published works in our database, specifically peaking in 2018 and 2019. Usability and Effectiveness Evaluations are the most common reported evaluations for mHealth Apps. User surveys are the most frequently used traditional evaluation method, together with statistical analysis. Newly proposed evaluation methods include log or usage data analytics or implementing new frameworks. Conclusion: This scoping review highlighted the value of conducting further research for the standardization of the evaluation of mHealth Apps. A growing trend and implementation of data analytics are observed within the health domain. We conclude that a framework that incorporates data analytic tools and techniques would be of great value to mHealth App developers, evaluators, and users. © 2020 Towards the Digital World and Industry X.0 - Proceedings of the 29th International Conference of the International Association for Management of Technology, IAMOT 2020. All rights reserved. A scoping review of the use of data analytics for the evaluation of mhealth applications Data analytics; Evaluation; MHealth Apps; Scoping Review Data Analytics; Industrial management; Data analytic tools; Effectiveness evaluation; Electronic database; Health interventions; Mobile health application; New evaluation methods; Research questions; Standards and guidelines; mHealth",Risk management
2015,Large scale quality transformation in hybrid development organizations – A case study,"As the software industry transitions to a subscription-based software-as-a-service (SaaS) model, software development companies are transforming to hybrid development organizations with increased adoption of Agile and Continuous Integration/ Continuous Delivery (CI/CD) development practices for newer products while continuing to use Waterfall methods for older products. This transformation is a huge undertaking impacting all aspects of the software development life cycle (SDLC), including the quality management system. This paper presents a case study of a large-scale transformation of a legacy quality management system to a modern system developed and implemented at Cisco Systems. The framework for this transformation is defined by six distinct areas: metrics, process, measurement, reporting, quality analytics, and culture & leadership. Our implementation leveraged recent advances in Machine Learning (ML), Artificial Intelligence (AI), connected data, integrated operations, and big data technologies to solve the challenges created by a hybrid software development organization. We believe this case study will help researchers and industry leaders understand the benefits and potential challenges of such sizeable transformations. © 2020 The Authors Large scale quality transformation in hybrid development organizations – A case study Agile; Hybrid development organization; Quality management system; Quality transformation; Waterfall Artificial intelligence; Computer software; Legacy systems; Life cycle; Quality management; Service industry; Software design; Continuous integrations; Development practices; Integrated Operations; Quality management systems; Scale transformation; Software development life cycle; Software development organizations; Waterfall methods; Software as a service (SaaS)",Strategic alignment
2017,Big data analytics capability and co-innovation: An empirical study,"Business; Economics; Information science; Big data analytics capabilities; Co-innovation; Big data; Co-creation © 2019 The Author(s); There are numerous emerging studies addressing big data and its application in different organizational aspects, especially regarding its impact on the business innovation process. This study in particular aims at analyzing the existing relationship between Big Data Analytics Capabilities and Co-innovation. To test the hypothesis model, structural equations by the partial least squares method were used in a sample of 112 Colombian firms. The main findings allow to positively relate Big Data Analytics Capabilities with better and more agile processes of product and service co-creation and with more robust collaboration networks with stakeholders internal and external to the firm. © 2019 The Author(s) Big data analytics capability and co-innovation: An empirical study Big data; Big data analytics capabilities; Business; Co-creation; Co-innovation; Economics; Information science ",Stakeholder management
2018,Modelling the relationship of digital technologies with lean and agile strategies,"As the world becomes globalised, companies fight for survival by connecting their in-house processes with external suppliers/customers. To remain competitive, companies must integrate innovative capabilities like ‘industry-4.0 technologies’ with their operation and supply chain (SC) strategies. The integration of various strategies has been investigated with the associated effect on performance; however, studies on how industry 4.0 technologies might support integrated strategies are still incipient. This work investigates the hierarchical relationships of ‘industry 4.0 technologies’ with lean and agile strategies. Adopting the ‘Interpretive Structural Modelling (ISM)’ technique to present a model depicting the linkage, the work also classifies the technologies and practices according to their ‘driving’ and ‘dependency’ powers. The findings revealed that the technologies have a high affinity to enable the implementation of lean and agile strategies. Among the nine technologies included in the study, ‘Cyber-Physical-System’, ‘Internet-of-Things’, ‘Cloud-Computing’, and ‘Big-Data-Analytics’ have the highest driving powers, signifying their higher affinity with the practices. Meanwhile, all the practices have a high enough affinity to be influenced by the technologies, except for a few (3/16 of lean and 2/9 of agile) that possess affinities too low to be driven by these technologies. The theoretical and managerial impacts of the research are also emphasised. © 2021 Kedge Business School. Modelling the relationship of digital technologies with lean and agile strategies agile SCM; digital technologies; industry 4.0; interpretive structural modelling; Lean SCM ",Financial management
2020,Agile analytics: Adoption framework for business intelligence in higher education,"Agile analytics is a strategy that focuses on improving DW and BI that characterizes program developers on how they should own the characteristics of DW. In Higher Education, data is an important asset. The unmanaged data available in the database is only transactional because it has to be transferred to a data warehouse. In Higher Education Data Warehouse is a method in data processing, data processing must provide the final results that can be evaluated and provide recommendations. The value of suitable methods or structures is needed to reduce the risk of technological failure in the data warehouse growth environment. The goal of this study is to develop an adoption framework by applying the concept of agile analytics. The System Development process included Manado State University as a research case study by performing business process analysis, and even recognizing issues and problems in the design of DW and BI. Adoption Framework testing is conducted at the end of the project construction to see the level of acceptance, and the architecture and framework feedback. © 2005 - ongoing JATIT & LLS Agile analytics: Adoption framework for business intelligence in higher education Adoption Framework; Agile Analytics; Business Intelligence; Data Warehouse; Higher Education ",Strategic alignment
2021,Managing the professional skills of the future: A model to support competence management,"Big data and digitalization are transforming the world of work, introducing an epochal change. This wild digital phenomenon, also thanks to the introduction of 4.0 industry, is changing the relationship between workers and machines and, if properly governed, can represent a great chance for companies to attain advantages and create value. This stimulating scenario embodies a huge opportunity for HRM. It provides impulses to improve positive social change, as well as develop and adopt new digital systems and innovative organizational solutions. HR professionals can help employees use digital 4.0 modes to manage, organize and drive change. To address this opportunity, HRM 4.0 has to collaborate with IT, spread an agile mind to execute projects, adopt design thinking and use integrated analytics. This paper presents an organizational model based on a technology platform designed for business workers and able to fill the gap between own skills and the request from the labour market. A theoretical framework is proposed, based on an innovative integrated system able to implement the entire workflow of evaluation, selection and training of candidates with the final aims of allowing companies to identify, manage and build business workers’ competencies. We conclude presenting opportunities and challenges for future studies. © 2019 Fondazione Gerardo Capriglione Onlus. All rights reserved. Managing the professional skills of the future: A model to support competence management  ",Strategic alignment
2023,Exploring critical success factors in agile analytics projects,"Via updating Chow and Cao's list of success factors for agile projects, attributes of potential critical success factors (CSF's) for agile analytics projects were identified from the literature. Ten new attributes were added to Chow and Cao's original list. Seven new attributes from the general agile project literature address: risk appetite, team diversity and availability, engagement, project planning, shared goals, and methods uncertainty. Three attributes specific to analytics projects were added: data quality, model validation, and building customers' trust in model solution. The potential validity of the various CSF's and attributes was explored via data from case studies of two analytics projects that varied in deployment success. The more successful project was found to be stronger in almost all the factors than the failed project. The findings can help researchers and analytics practitioners understand the environmental conditions and project actions that can help get business value from their analytics initiatives. © 2020 IEEE Computer Society. All rights reserved. Exploring critical success factors in agile analytics projects  Systems science; Business value; Critical success factor; Environmental conditions; Model solution; Model validation; Project planning; Success factors; Team diversity; Agile manufacturing systems",Capacity management
2028,"Baltic-DB and IS-Forum-DC 2020 - Joint Proceedings of Baltic DB and IS 2020 Conference Forum and Doctoral Consortium, co-located with the 14th International Baltic Conference on Databases and Information Systems, Baltic DB and IS 2020","The proceedings contain 11 papers. The topics discussed include: a mobility data model for web-based tourists tracking; do we really know how to measure software quality?; development of ontology based competence management process model for non-formal education; analysis of legacy monolithic software decomposition into microservices; business capabilities utilization enhancement using archimate for EAS projects delivery in an agile environment; aspect-oriented analytics of big data; a linguistic analysis of startups in the context of the air transport industry management; towards transforming natural language queries into SPARQL queries; importance of the use of analytics in requirements engineering; and a hybrid method for textual data classification based on support vector machine with particle swarm optimization metaheuristic and k-means clustering. Baltic-DB and IS-Forum-DC 2020 - Joint Proceedings of Baltic DB and IS 2020 Conference Forum and Doctoral Consortium, co-located with the 14th International Baltic Conference on Databases and Information Systems, Baltic DB and IS 2020  ",Monitoring and control
2030,Long-term creep behavior prediction of polymethacrylimide foams using artificial neural networks,"To study the long-term creep behavior prediction of polymethacrylimide (PMI) foams, the gradation loading creep tests were proposed at four different temperatures in this paper. The Stepped Isostress (SSM) and TTSP methods were combined to obtain the master curve under reference stress and temperature. The artificial Neural Networks (ANN) technique was used to build the long-term creep behavior prediction model of PMI materials. The effects of different activation functions, hidden layer structures, and other super parameters on the prediction performance were investigated. The results suggest that the SSM plus TTSP method can be used to construct the master curve, which could predict a larger time scale of material creep behavior based on a short-term test. It is of great significance and feasible to predicts the long-term creep life of materials accurately using advanced artificial intelligence technology. According to the statistical analysis, the logistic type activation function has a more accurate and stable prediction performance on long-term creep behavior prediction of PMI. To avoid overfitting, the number of hidden layers should be as small as possible, and the prediction performance of a single hidden layer structure with 8 neurons is sufficient for long-term creep behavior prediction in the engineering area. The statistical value of the correlation coefficient was greater than 0.995. The application range of advanced artificial intelligence technology in this field can be further expanded in the preceding research, such as in the prediction of long-term creep behavior on the composition level of the material. © 2020 Elsevier Ltd Long-term creep behavior prediction of polymethacrylimide foams using artificial neural networks Artificial neural networks; Creep; Polymethacrylimide form; Stepped isostress method Activation analysis; Chemical activation; Creep; Forecasting; Predictive analytics; Activation functions; Application range; Artificial intelligence technologies; Correlation coefficient; Material creep behavior; Prediction performance; Reference stress; Short-term test; Neural networks",Financial management
2032,Supply chain business intelligence and the supply chain performance: The mediating role of supply chain agility,"The study has planned to examine the impact of the supply chain business intelligence on the supply chain performance of the Indonesian firms. Additionally, the study has examined the mediating role of agile capability and supply chain capability. For data gathering, total 450 questionnaires were distributed and obtained only 325 questionnaires. During data screening process 23 questionnaires were excluded, since they were incomplete. Therefore, we obtained 67% response rate for the survey in present study. Partial Least Square-Structural Equation Modeling (PLS-SEM) is an important statistical procedure to carry out multivariate data analysis, Empirical analysis in this study supports the supply chain business intelligence competence with respect to technical, cultural and managerial competence. This indicates that it is necessary to utilize appropriate technologies and tools and have well-defined processes, although these are not sufficient conditions to develop business intelligence product efficiently and effectively, such as, relevant knowledge and information which facilitate in the decision-making functions of the supply chain. Besides, some inter-and intra-organizational culture elements also affect the business intelligence product creation. The prior research further supported the significance of knowledge/information-related competences to enhance agile performance characteristics and competitive performance of the supply chain. © ExcelingTech Pub, UK. Supply chain business intelligence and the supply chain performance: The mediating role of supply chain agility Agility; Indonesia; Performance; Supply Chain ",Value management
2033,HawkEye: A visual framework for agile cross-validation of deep learning approaches in financial forecasting,"Financial forecasting represents a challenging task, mainly due to the irregularity of the market, high fluctuations and noise of the involved data, as well as collateral phenomena including investor mood and mass psychology. In recent years, many researchers focused their work on predicting the performance of the market by exploiting novel Machine Learning and Deep Learning tools and techniques. However, many of the approaches proposed in the literature do not take adequately into account some important specific domain issues in the analysis of the results. Among these, it is worth to mention the bias introduced by the choice of model weights initialization and the considered observation periods, as well as the narrow separation between significant results and noise, typical of the financial domain. A thorough analysis of these peculiar issues lead to a substantial increase of the experiments and results to analyze, making the discovery of meaningful hidden patterns very difficult and time consuming to perform. To cope with these concerns and accompanying the current Machine Learning Interpretability trend, in this paper we propose a visual framework for in-depth analysis of results obtained from Deep Learning approaches, tackling classification tasks within the financial domain and aiming at a better interpretation and explanation of the trained Deep Learning models. Our framework offers a modular view, both general and targeted, of results data, providing several financial specific metrics, including Sharpe and Sortino ratios, Equity curves and Maximum Drawdown, as well as custom period analysis and reports, experiment comparison tools, and evaluation features for different algorithms. © 2020 ACM. HawkEye: A visual framework for agile cross-validation of deep learning approaches in financial forecasting Deep Learning; Financial Forecasting; Time-series analysis; Visual analytics. Commerce; Distributed database systems; Forecasting; Investments; Learning systems; Petroleum reservoir evaluation; Classification tasks; Cross validation; Financial domains; Financial forecasting; In-depth analysis; Interpretability; Learning approach; Observation Period; Deep learning",Strategic alignment
2034,"Applying IIoT and AI - Opportunities, requirements and challenges for industrial machine and equipment manufacturers to expand their services","Theroretically, servitization benefits industrial companies to generate more revenue, integrate themselves deeper into the value chains of their customers and improves the competitiveness. The ongoing digital transformation can enable servitization towards more advanced services with a more customer centric view. Macro economically, the industrial sector is very important for most of the developed countries. The digital transformation is posing a triple challenge to the machining and equipment manufacturers and will require a continued development of the existing business models, a change of organizational structures and a strong leadership to remain successful. The companies will need to re-evaluate their market justification and define their value proposition to both existing and potentially new customers. New skills are required as data and analytics, represented by IIoT and AI, will play an ever-larger role in the companies' interaction with their present and new customers. For the industrial companies, servitization is both linked to higher risk and to a higher earnings potential. Implications for Central European audience: The implications for Central European industrial companies of the ongoing digitalisation and servitization will be profound. The European car industry is undergoing significant changes and not only due to E-mobility. Also Products-as-Service will have an impact on their whole value chain. The industrial sector in general and the machining industry in particular will need to re-assess its business models and revenue generation. In addition, senior management is already confronted with the need for both new skill-sets and possibly more agile organisational structures, where the industrial mind-set will be challenged by new service models and the thinking of the digital natives. © 2020, Economics - Prague, Faculty of Business Administration. Applying IIoT and AI - Opportunities, requirements and challenges for industrial machine and equipment manufacturers to expand their services Artificial intelligence; Digital transformation; Iiot; Industrial services; Machining and equipment manufacturers; Servitization; Sscs; Ssps ",Strategic alignment
2035,Simulating a virtual machining model in an agent-based model for advanced analytics,"Monitoring the performance of manufacturing equipment is critical to ensure the efficiency of manufacturing processes. Machine-monitoring data allows measuring manufacturing equipment efficiency. However, acquiring real and useful machine-monitoring data is expensive and time consuming. An alternative method of getting data is to generate machine-monitoring data using simulation. The simulation data mimic operations and operational failure. In addition, the data can also be used to fill in real data sets with missing values from real-time data collection. The mimicking of real manufacturing systems in computer-based systems is called “virtual manufacturing”. The computer-based systems execute the manufacturing system models that represent real manufacturing systems. In this paper, we introduce a virtual machining model of milling operations. We developed a prototype virtual machining model that represents 3-axis milling operations. This model is a digital mock-up of a real milling machine; it can generate machine-monitoring data from a process plan. The prototype model provides energy consumption data based on physics-based equations. The model uses the standard interfaces of Step-compliant data interface for Numeric Controls and MTConnect to represent process plan and machine-monitoring data, respectively. With machine-monitoring data for a given process plan, manufacturing engineers can anticipate the impact of a modification in their actual manufacturing systems. This paper describes also how the virtual machining model is integrated into an agent-based model in a simulation environment. While facilitating the use of the virtual machining model, the agent-based model also contributes to the generation of more complex manufacturing system models, such as a virtual shop-floor model. The paper describes initial building steps towards a shop-floor model. Aggregating the data generated during the execution of a virtual shop-floor model allows one to take advantage of data analytics techniques to predict performance at the shop-floor level. © 2017, Springer Science+Business Media, LLC (outside the USA). Simulating a virtual machining model in an agent-based model for advanced analytics Advanced analytics; Data generator; Manufacturing simulation; Milling; MTConnect; Simulation; STEP-NC Agile manufacturing systems; Autonomous agents; Computational methods; Energy utilization; Floors; Milling (machining); Virtual reality; Advanced analytics; Data generator; Manufacturing simulation; MTConnect; Simulation; STEP-NC; Monitoring",Monitoring and control
2039,Methodologies for designing healthcare analytics solutions: A literature analysis,"Healthcare analytics has been a rapidly emerging research domain in recent years. In general, healthcare solution design studies focus on developing analytic solutions that enhance product, process and practice values for clinical and non-clinical decision support. The objective of this study is to explore the scope of healthcare analytics research and in particular its utilisation of design and development methodologies. Using six prominent electronic databases, qualifying articles between 2010 and mid-2018 were sourced and categorised. A total of 52 articles on healthcare analytics solutions were selected for relevant content on public healthcare. The research team scrutinised the articles, using established content analysis protocols. Analysis identified that various methodologies have been used for developing analytics solutions, such as prototyping, traditional software engineering, agile approaches and others, but despite its clear advantages, few show the use of design science. Key topic areas are also identified throughout the content analysis suggesting topical research priorities in the field. © The Author(s) 2019. Methodologies for designing healthcare analytics solutions: A literature analysis design research; health analytics; information system artefact; solution design Delivery of Health Care; Health Facilities; Humans; Information Systems; article; artifact; content analysis; human; information system; research priority; software; systematic review; topical drug administration; health care delivery; health care facility",Governance
2040,Applying Scrum in Data Science Projects,"The rise of big data has led to an increase in data science projects conducted by organizations. Such projects aim to create valuable insights by improving decision making or enhancing an organization's service offering through data-driven services. However, the majority of data science projects still fail to deliver the expected value. To increase the success rate of projects, the use of process models or methodologies is recommended in the literature. Nevertheless, organizations are hardly using them because they are considered too rigid and they do not support the typical iterative and open nature of data science projects. To overcome this problem, this research suggests applying Agile methodologies to data science projects. Agile methodologies were originally developed in the software engineering domain and are characterised by their iterative approach towards software development. In this study, we selected the Scrum approach and integrated it into the CRISP-DM methodology for data science projects using a Design Science Research approach. This new methodology was then evaluated in three different case organizations using expert interviews. Analysis of the expert interviews resulted in a further refinement of the Agile data science methodology proposed by this research. © 2020 IEEE. Applying Scrum in Data Science Projects Agile; Data Science; Scrum Decision making; Iterative methods; Software design; Agile Methodologies; Design-science researches; Expected values; Iterative approach; Science methodologies; Science projects; Service offering; Software engineering domain; Data Science",Value management
2042,Transforming communication channels to the co-creation and diffusion of intangible heritage in smart tourism destination: Creation and testing in Ceutí (Spain),"Creating smart tourism destinations requires innovative solutions which cover the main pillars of sustainability as sociocultural, environmental, and economic aspects, in order to spread the cultural heritage of these tourist destinations to their visitors. One of the most demanded approaches by the new hyper-connected visitors is the expectation of plunging and becoming a short-term resident to receive a real experience during their visit. Therefore, the scope of this research covers the objective of designing an innovative communication channel between a visitor and a point of interest (POI), which in turn allows agile experiences to be built and provided and increases the dissemination of cultural heritage through new technologies, considering the real needs of the territories and the new digital visitors. In order to address these topics, this paper proposes an innovative and co-created progressiveWeb-App for visitors called Be Memories in order to spread the intangible heritage of a tourist destination, where the content is co-created by residents of the destination. The tool has been tested in Ceutí, a Spanish village with a high cultural value, which needs to be disseminated through new innovative tools. The trial was launched during local festivities of the village using an Internet of Things device, called a Smart Spot, to establish a communication channel between the visitor and POI. The results of the test were measured using Google Analytics, the reactions of Be Memories in social networks, and the acceptance of other cities and European committees. The results have concluded that Be Memories is able to enable a local experience via agile, fresh, and crowd-sourced content that people enjoy. This channel presents a complementary level of information with respect to official sources, documentaries, and local guide tours, at the same time enabling a mechanism to promote physical visits, walking tours, and cultural heritage via low-cost and sustainable infrastructure. © 2019 by the authors. Transforming communication channels to the co-creation and diffusion of intangible heritage in smart tourism destination: Creation and testing in Ceutí (Spain) Co-creation; Hyper-connected users; ICTs; Internet of Things (IoT); Point of interest; Prosumers; Residents; Smart tourism destination; Sustainability; Tourist guides cultural heritage; ecotourism; information and communication technology; Internet; sustainability; territory; testing method; tourist destination",Stakeholder management
2043,Towards Agile Integration: Specification-based Data Alignment,"Utilizing data sets from multiple domains is a common procedure in scientific research. For example, research on the performance of buildings may require data from multiple sources that lack a singular standard for data reporting. The Building Management System might report data at regular 5minute intervals, whereas an air-quality sensor might capture values only when there has been significant change from the previous value. Many systems exist to help integrate multiple data sources into a single system or interface. However, such systems do not necessarily make it easy to modify an integration plan, for example, to accommodate data exploration, new and changing data sets or shifts in the questions of interest. We propose an agile data-integration system to enable quick and adaptive analysis across many data sets, concentrating initially on the data alignment step: combining data values from multiple time-series based data sets whose time schedules. To this end, we adopt a Domain Specific Language approach where we construct a domain model for alignment, provide a specification language for describing alignments in the model and implement an interpreter for specification in that language. Our implementation exploits a rank-based join in SQL that produces faster alignment times than the commonly suggested method of aligning data sets in a database. We present experiments to demonstrate the advantage of our method and exploit data properties for optimization.  © 2020 IEEE. Towards Agile Integration: Specification-based Data Alignment data alignment; data integration; time series Agile manufacturing systems; Air quality; Alignment; Data integration; Data Science; Information use; Intelligent buildings; Problem oriented languages; Specification languages; Specifications; Time series analysis; Agile integration; Building management system; Data integration system; Domain-specific language approaches; Multiple data sources; Multiple time series; Performance of buildings; Scientific researches; Information management",Strategic alignment
2044,HR 4.0 case studies,"With the transformations being brought in by Industry 4.0, the function of people management is evolving to restructure workplaces and workforce related practices and processes. Practitioners, academicians and industry captains are in agreement that the function of Human Resource Management will move away from regulatory and administrative aspects to something more automated and focussed on strategy. With fast transforming market spaces, businesses have to be agile enough to respond to those changes and leverage opportunities. The human resource function has to become a reliable partner in identifying the best available talent and selecting the right fit to enable the organisations to achieve their objectives. Today, businesses are looking at developing leadership talent to direct Industry 4.0 changes, overseeing tech integration into work culture, improving employee experiences, offering customised learning, determining and evaluating human capital related metrics and following values like diversity and inclusion. This is in keeping with the understanding of new business models that are coming up, increase in the number of digital natives in the workforce and contemporary societal expectations. Several organisations have already started their disruptive journeys into HR 4.0 and are utilising Virtual Reality, Artificial Intelligence, Robotic Process Automation, Analytics and Machine Learning to improve their employee experience. Adoption of technology by organization all helps in branding building (Rana, S., & Sharma, R. (2018). This chapter highlights the use and impact of such technologies in four organisations: Deutsche Bahn - the largest railway operator in Europe, L’Oreal - the world’s largest cosmetics company, Mariott International - a hospitality company with over 7000 hotels and resorts, and Lyft - an American ridesharing company. © 2021 Nova Science Publishers, Inc. HR 4.0 case studies  ",Financial management
2045,Reputational intelligence: innovating brand management through social media data,"Purpose: Companies are currently facing the challenge of understanding how their business is affected by the large volume of opinions continually generated by their stakeholders in social media regarding their intangible assets (experiences, emotions and attitudes). With this in mind, the purpose of this paper is to present an innovative management model, named E2AB, to measure and analyse reputational intangibles from digital ecosystems and their impacts on tangible assets. Design/methodology/approach: The methodology applied was big data and business intelligence techniques. These methods were used in the computing process to obtain daily data from every asset guarantees that the model is validated with robust data. This model has been corroborated using data from the banking sector, specifically 402,383 net data inputs from the digital ecosystems. Findings: This study illustrates the existence of a holistic influence of intangible assets over tangible assets. The findings demonstrate complex relationships between tangible and intangible assets, determined not only by the type of variable but also by its valence and intensity. Practical implications: These findings may help chief communication officers and general managers a better understanding of how intangible assets extracted from online users’ opinions are related to their organisation’s tangible assets plus a chance to find out about their impact and how to manage them for a practical and agile decision making in real time. Originality/value: It is a pioneering work in establishing a model, which demonstrates transversal and holistic relationships between relational intangible and tangible assets of firms from digital ecosystems, using business intelligence techniques. © 2019, Emerald Publishing Limited. Reputational intelligence: innovating brand management through social media data Business intelligence; Customer perceptions; Intangible management; Reputational intelligence; Reputational model; Social media data Competitive intelligence; Decision making; Ecosystems; Information analysis; Agile decision makings; Complex relationships; Customer perceptions; Design/methodology/approach; Digital ecosystem; Intangible assets; Reputational intelligence; Social media datum; Social networking (online)",Strategic alignment
2046,Journey towards the unconventional digital well factory,"Integrated Well Delivery (IWD) Program also known as Digital Well Factory Program is a large cross-functional transformation effort designed to help unconventional oil & gas operators successfully transition and ramp up from   exploration phase to development phase. The purpose of the program is to establish a state-of-the-art integrated well delivery capability (referred to as ""agile well factory"") leveraging the best practices of the unconventional  resources  industry as well as the latest digital technologies & tools. IWD program relies on five fundamental concepts that underpin the ""digital well factory"" vision: Transparency, Connectivity, Adaptability, Collaboration &  Cross- Functional Integration. The program was designed to look at well factory performance from a 360-degree perspective and thus is structured around a comprehensive framework of 6 interdependent building blocks: 1. Well Delivery  Performance  Advanced Analytics 2. Well Delivery Zero-based Process Transformation 3. Well Delivery Integrated Planning & Scheduling Optimization 4. Well Delivery Process Agility & Automation 5. Well Delivery Mobility &  Integrated  Planning Center 6. Well Delivery Collaborative Operating Model The end goal of the program is to evolve the operator’s organization into an agile, intelligent, automated & highly efficient well factory. On the one hand,  the program  aims at making the company capable of smoothly delivering hundreds of wells per year on schedule and in record time & cost. And on the other hand, it aims at making the operator capable of continuously adapting itself  based on  lessons learned, data driven-insights, operational challenges, strategic changes, etc. This paper describes the end state vision of the program, its value, approach, framework, key initiatives, management, as well as  recommendations for  any operator willing to embark on a similar journey. Copyright 2020, International Petroleum Technology Conference. Journey towards the unconventional digital well factory  Advanced Analytics; Gasoline; Petroleum prospecting; Resource valuation; Cross functional integration; Delivery performance; Digital technologies; Operational challenges; Process transformation; Scheduling optimization; Unconventional resources; Well delivery process; Oil field development",Strategic alignment
2047,How to make intelligent automation projects agile? Identification of success factors and an assessment approach,"Purpose: Organizations are fast adopting new technologies such as automation, analytics and artificial intelligence, collectively called intelligent automation, to drive digital transformation. When adopting intelligent automation, there is a need to understand the success factors of these new technologies and adapt agile software development (ASD) practices to meet customer expectations. The purpose of this paper is to explore the success factors of intelligent automation and create a framework for managers and practitioners to meet dynamic business demands. Total interpretive structural modeling (TISM) framework is a suitable approach to integrate quantitative measurement with qualitative semi-structured interviews capturing the context of the individual organization environment. Design/methodology/approach: This paper identified agility factors and their interrelationships using a TISM framework. TISM results were validated using a one-tailed t-test to confirm the interrelationships between factors. Furthermore, the agility index of a case project organization was assessed using a graph-theoretic approach (GTA) to identify both the triggering factors for agility success and improvement proposals. Findings: Results showed that leadership vision, organization structure and program methodology were driving factors. The TISM model was validated statistically and the agility index of the intelligent automation case project organization was calculated to be79.5%. Here, a GTA was applied and the triggering factors for improvement of the agility index were identified. Research limitations/implications: The limitations of the study are described along with the opportunities for future research as the field evolves through the rapid innovation of technology and products. Practical implications: The increasing role of digital transformation in enterprise strategy and operations requires practitioners to understand how ASD practices must be planned, measured and/or improved over time through the implementation of automation, analytics and artificial intelligence programs. The TISM digraph provides a framework of hierarchical structure to organize the influencing factors, which assists in achieving organizational goals. This study highlights the driving factors which contribute to the success of intelligent automation projects and project organizations. Originality/value: This is a first attempt to analyze the interrelationships among agility factors in intelligent automation projects (IAP) using TISM and the assessment of the agility index of a case IAP organization using a GTA. © 2021, Emerald Publishing Limited. How to make intelligent automation projects agile? Identification of success factors and an assessment approach Agility in automation projects; Digital transformation; Intelligent automation; Organizational transformation; Project organizations; Total interpretive structural modeling ",Governance
2048,Data as a strategic asset: Improving results through a systematic data governance framework,"This paper presents a systematic proven and effective nine-step data governance framework that helps to enable oil and gas organizations to improve results by treating data as a strategic asset. The world is becoming increasingly data driven and technologically disrupted, and this momentum is expected to continue. According to the World Economic Forum (Kirk Bresniker 2018), ""every two years we create more data than we have created in all of history. Our ambitions are growing faster than our computers can improve."" Our industry's success will depend on timely development and appropriate application of machine learning, artificial intelligence (AI), and other advanced analytics. However, these tools may be ineffective and even destructive if the data used is compromised. In our study, we identified the factors that contribute to the problem of compromised data. Technical data and business landscapes are highly complex, segmented and largely disjointed. Most organizations are fragmented, working in silos rather than collaboratively. Production data owners in the field are traditionally far removed from data users at the office. Often ""raw"" data owners in the field have limited appreciation of the importance of ""trusted data,"" while data users at the office spend 80% of their time looking for and cleansing data, and only 20% of their time transforming data into actionable insights to drive informed decisions (Gabernet and Limburn 2017). The disconnect between departments often results in significant ""hard-dollar"" loss, which contributes to unknown potential value loss. Our data governance framework (Fig. 1) is a nine-step, methodical procedure that helps address these problems and positions the company to be more agile moving forward. 1. Determine organizational priorities and define the scope 2. Invest in organizational change management - key for adoption and sustainability 3. Establish the data governance organization, demonstrating comprehensive leadership support 4. Connect and align teams through a fit-for-purpose data catalog 5. Establish data governance policies to support priorities 6. Define new operating model by transforming policies into new ways of working 7. Design enabling technologies to support the data governance objectives 8. Implement ongoing data quality and availability monitoring 9. Facilitate sustainability via periodic audits based on pre-defined key performance indicators (KPIs) We developed and applied this systematic procedure to create trusted data in a world-leading natural resources company. Quality production data clearly enabled better informed and more strategic decisions and helped protect the company against potential litigations. Specific benefits were quantified to the following annual recurring economic value (in the context of sub-USD 60/bbl oil price environment) for 1,700 wells in the YOU.S. shale: • Improved information and decision quality (USD 3.7 million) • Reduced cost and cycle time (USD 4.4 million) • Increased production (USD 3.2 million) Trusted production data converted into robust, actionable insights improved performance in the following functions: • Well performance • Predictive maintenance • Operational intervention • Revenue and joint venture accounting • Regulatory reporting • Reservoir analysis In this paper we detail our tested and effective nine-step data governance framework, developed based on years of experience in the oil and gas industry, ranging from operations, engineering, and production volumes allocations to information technology and continuous improvement. Key to the importance of this work is treating data as a strategic asset and recognizing that, without trustworthy data, planning and performance analysis, whether performed by human colleague or robotics/advanced analytics, is of limited and misleading value. Copyright © 2020, Society of Petroleum Engineers Data as a strategic asset: Improving results through a systematic data governance framework  Advanced Analytics; Artificial intelligence; Benchmarking; Cost engineering; Crude oil price; Digital storage; Gas industry; Gasoline; Industrial economics; Oil well production; Oil wells; Petroleum industry; Quality control; Robot programming; Social robots; Sustainable development; Continuous improvements; Enabling technologies; Key performance indicators; Oil and Gas Industry; Operational intervention; Organizational change management; Performance analysis; Strategic decisions; Metadata",Capacity management
2049,From Ad-Hoc data analytics to DataOps,"The collection of high-quality data provides a key competitive advantage to companies in their decision-making process. It helps to understand customer behavior and enables the usage and deployment of new technologies based on machine learning. However, the process from collecting the data, to clean and process it to be used by data scientists and applications is often manual, non-optimized and error-prone. This increases the time that the data takes to deliver value for the business. To reduce this time companies are looking into automation and validation of the data processes. Data processes are the operational side of data analytic workflow. DataOps, a recently coined term by data scientists, data analysts and data engineers refer to a general process aimed to shorten the end-to-end data analytic life-cycle time by introducing automation in the data collection, validation, and verification process. Despite its increasing popularity among practitioners, research on this topic has been limited and does not provide a clear definition for the term or how a data analytic process evolves from ad-hoc data collection to fully automated data analytics as envisioned by DataOps. This research provides three main contributions. First, utilizing multi-vocal literature we provide a definition and a scope for the general process referred to as DataOps. Second, based on a case study with a large mobile telecommunication organization, we analyze how multiple data analytic teams evolve their infrastructure and processes towards DataOps. Also, we provide a stairway showing the different stages of the evolution process. With this evolution model, companies can identify the stage which they belong to and also, can try to move to the next stage by overcoming the challenges they encounter in the current stage.  © 2020 ACM. From Ad-Hoc data analytics to DataOps Agile Methodology; Continuous Monitoring; Data Pipelines; Data technologies; DataOps; DevOps Automation; Competition; Data Analytics; Decision making; Life cycle; Competitive advantage; Customer behavior; Decision making process; Evolution modeling; Evolution process; High quality data; Mobile telecommunications; Verification process; Data acquisition",Strategic alignment
2051,Effectiveness of groundwater heavy metal pollution indices studies by deep-learning,"Globally, groundwater heavy metal (HM) pollution is a serious concern, threatening drinking water safety as well as human and animal health. Therefore, evaluation of groundwater HM pollution is essential to prevent accompanying hazardous ecological impacts. In this aspect, the effectiveness of various groundwater HM pollution evaluation approaches should be examined for their level of trustworthiness. In this study, 226 groundwater samples from Arang of Chhattisgarh state, India, were collected and analyzed. Measured concentration for various HMs were further used to calculate six groundwater pollution indices, such as the HM pollution index (HPI), HM evaluation index (HEI), contamination index (CI), entropy-weight based HM contamination index (EHCI), Heavy metal index (HMI), and principal component analysis–based metal index (PMI). Groundwater in the study area was mainly contaminated by elevated Cd, Fe, and Pb concentrations due to natural and anthropogenic pollution. Moreover, this study explored the performance of deep learning (DL)-based predictive models via comparative study. Two hidden layers with 26 and 19 neurons in the first and second hidden layers, respectively, were optimised along with rectified linear unit activation function. A mini-batch gradient descent was also applied to ensure smooth convergence of the training dataset into the model. Results demonstrated that the DL-PMI scored lowest errors, 0.022 for mean square error (MSE), 0.140 for mean absolute error (MAE), and 0.148 for root mean square error (RMSE), in the model validation than the other DL-based groundwater HM pollution model. Prediction performances of all pollution indices were also verified using artificial neural network (ANN)-based models, which also highlighted the lowest validation error for ANN-PMI (MSE = 3.93, MAE = 1.38, and RMSE = 1.98). Furthermore, the prediction accuracies of PMI using both ANN and DL models scored the highest R2 value of 0.95 and 0.99, respectively. Therefore it is suggested that groundwater HM pollution using PMI as the best indexing approach in the present study area. Moreover, compared to benchmark, ANN, the DL performed better; hence, it could be concluded that the proposed DL model may be suitable approach in the field of computational chemistry by handling overfitting problems. © 2020 Elsevier B.V. Effectiveness of groundwater heavy metal pollution indices studies by deep-learning Deep learning; Geographical information system; Groundwater; HM pollution indices; India; Pollution zones Deep Learning; Environmental Monitoring; Groundwater; Humans; India; Metals, Heavy; Risk Assessment; Water Pollutants, Chemical; Chhattisgarh; India; Animalia; Computational chemistry; Deep learning; Errors; Gradient methods; Groundwater; Heavy metals; Mean square error; Potable water; Predictive analytics; Veterinary medicine; arsenic; cadmium; chromium; copper; ground water; heavy metal; iron; lead; manganese; nickel; zinc; heavy metal; Activation functions; Anthropogenic pollution; Contamination index; Heavy metal pollution; Over fitting problem; Pollution evaluation; Prediction performance; Root mean square errors; artificial neural network; comparative study; concentration (composition); data set; GIS; groundwater pollution; heavy metal; machine learning; numerical model; principal component analysis; Article; artificial neural network; comparative study; concentration (parameter); contamination index; controlled study; deep learning; entropy weight based heavy metal contamination index; heavy metal evaluation index; heavy metal index; heavy metal pollution index; India; mathematical analysis; prediction; principal component analysis based metal index; priority journal; water contamination; water pollution; water quality; water sampling; environmental monitoring; human; risk assessment; water pollutant; Groundwater pollution",Monitoring and control
2053,Designing a Sourcing Ecosystem for Strategic Innovation Through “Big Data” Applications,"Published research on innovation from Information Technology and Business Process Outsourcing (ITO/BPO) is rare [1]. Strategic innovation involves high uncertainties better addressed through agile methods and a collaborative approach [1–3]. Key success factors in delivering ITO/BPO innovation are high-quality relationships, trust and collaborative cultures [1–4], and establishing an effective governance configuration. The authors report on longitudinal case studies of a global mining company (“GMC”) and a group of its suppliers aimed at understanding how GMC is developing “big data” applications to generate game-changing innovation. This paper describes how GMC has developed a “big data” platform to support internal staff, customers, consultants and third party suppliers to create applications that can transform global mining and smelting industries to deliver a price premium for GMC’s products. GMC has encountered a shortage of suitably experienced data scientists in its key operating locations resulting in a significant skills gap in its big data program. GMC’s sourcing strategy aims to build an open and collaborative ecosystem that draws upon secondary markets to help fill the skills gap. To create an environment in which open innovation [5] can flourish, GMC established an Analytics Speed Team (AST) as an internal consulting and program management group to drive faster progress with big data applications. A contribution of this research is to identify the role of AST in establishing an effective governance configuration for open innovation. A practical contribution is made by analysing the value of secondary markets for ITO services in a sourcing ecosystem optimised for delivering innovation. © 2020, Springer Nature Switzerland AG. Designing a Sourcing Ecosystem for Strategic Innovation Through “Big Data” Applications Artificial intelligence; Big data; Cloud services; Innovation in outsourcing/offshoring; Secondary markets; Sourcing configuration; Sourcing ecosystem; “Gig” economy Application programs; Big data; Commerce; Ecosystems; Human resource management; Big data applications; Business process outsourcing; Collaborative approach; Key success factors; Longitudinal case study; Program management group; Sourcing strategies; Strategic innovations; Digital storage",Financial management
2055,Using Big Data Analytics to Create a Predictive Model for Joint Strike Fighter,"The amount of information needed to acquire knowledge on today's acquisition systems is growing exponentially due to more complex, higher resolution, software-intensive acquisition systems that need to operate in System-of-Systems (so is), Family-of-Systems (FoS), Joint, and Coalition environments. Unfortunately, the tools and methods necessary to rapidly collect, aggregate, and analyze this information have not evolved as a whole in conjunction with this increased system complexity and, therefore, has made analysis and evaluation increasingly deficient and ineffective. The Test Resource Management Center's (TRMC's) vision is to build a DoD test and evaluation (TE) knowledge management (KM) and analysis capability that leverages commercial big data analysis and cloud computing technologies to improve evaluation quality and reduce decision-making time. An evaluation revolution, starting with the Joint Strike Fighter (JSF) program, is underway to ensure the TE community can support the demands of next-generation weapon systems.The true product of TE is knowledge ascertained through the collection of information about a system or item under test. However, the TE community's ability to provide this knowledge is hampered by more complex systems, more complex environments, and the need to be more agile in support of strategic initiatives, such as agile acquisition and the 3rd Offset Strategy. This increased complexity and need for speed because delayed analysis and problems that go undetected during TE. The primary reason for these shortfalls is antiquated tools and processes that make data hard to locate, aggregate, and convert into knowledge. In short, DoD has not evolved its evaluation infrastructure as its weapon systems have evolved.Conversely, commercial entities, such as medical observation and diagnosis, electric power distribution, retail, and industrial manufacturing, have embraced agility in their methodologies while modernizing analytics capabilities to keep up with the massive influx of data. Raw physical sensors could provide data, higher-quality image or video cameras, radio frequency identification (RFID) devices, faster data collectors, more detailed point-of-sale information or digitized records, and ultimately is providing more data to analysts in size and complexity than ever before. As more data has become available, an interrelated phenomenon is the desire of analysts to ask more detailed questions about their consumers and their business infrastructure. To drive the process of implementing big data analytics, businesses have begun establishing analytics centers which either take pre-defined business cases and apply methods to address them or implement existing knowledge within the data architecture to create a higher level of awareness to business groups or the company at-large. To meet these demands, data storage and computation architectures have become more sophisticated, dozens of technologies were developed for large-scale processing (such as Apache Hadoop or GreenPlum), and streaming architectures which allow data to be processed and actioned on in real-time as it is collected have become commonplace. The net result of these commercial best practices is a solid foundation for the DoD to transform how it uses data to achieve faster, better, and smarter decisions throughout the acquisition lifecycle. © 2018 IEEE. Using Big Data Analytics to Create a Predictive Model for Joint Strike Fighter Big Data; Cloud Computing; Data Analytics; Data Management; Department of Defense; Knowledge Management; Predictive Maintainance; Test and Evaluation; Virtualization Aggregates; Big data; Data Analytics; Digital storage; Knowledge management; Predictive analytics; Quality control; Virtualization; Acquisition systems; Cloud-computing; Data analytics; Department of Defence; Joint Strike Fighters; Predictive maintainance; Predictive models; Test and evaluation; Virtualizations; Weapon system; Decision making",Strategic alignment
2057,"Understand, develop and enhance the learning process with big data","Purpose: With the advent of the internet and communication technology, the penetration of e-learning has increased. The digital data being created by the educational and research institutions is also on the ascent. The growing interest in recent years toward big data, educational data mining and learning analytics has motivated the development of new analytical ways and approaches and advancements in learning settings. The need for using big data to handle, analyze this large amount of data is prime. This trend has started attracting the interest of educational institutions which have an important role in the development skills process and the preparation of a new generation of learners. “A real revolution for education,” it is based on this kind of terms that many articles have paid attention to big data for learning. How can analytics techniques and tools be so efficient and become a great prospect for the learning process? Big data analytics, when applied into teaching and learning processes, might help to improvise as well as to develop new paradigms. In this perspective, this paper aims to investigate the most promising applications and issues of big data for the design of the next-generation of massive e-learning. Specifically, it addresses the analytical tools and approaches for enhancing the future of e-learning, pitfalls arising from the usage of large data sets. Globally, this paper focuses on the possible application of big data techniques on learning developments, to show the power of analytics and why integrating big data is so important for the learning context. Design/methodology/approach: Big data has in the recent years been an area of interest among innovative sectors and has become a major priority for many industries, and learning sector cannot escape to this deluge. This paper focuses on the different methods of big data able to be used in learning context to understand the benefits it can bring both to teaching and learning process, and identify its possible impact on the future of this sector in general. This paper investigates the connection between big data and the learning context. This connection can be illustrated by identifying the several main analytics approaches, methods and tools for improving the learning process. This can be clearer by the examination of the different ways and solutions that contribute to making a learning process more agile and dynamic. The methods that were used in this research are mainly of a descriptive and analytical nature, to establish how big data and analytics methods develop the learning process, and understand their contributions and impacts in addressing learning issues. To this end, authors have collected and reviewed existing literature related to big data in education and the technology application in the learning context. Authors then have done the same process with dynamic and operational examples of big data for learning. In this context, the authors noticed that there are jigsaw bits that contained important knowledge on the different parts of the research area. The process concludes by outlining the role and benefit of the related actors and highlighting the several directions relating to the development and implementation of an efficient learning process based on big data analytics. Findings: Big data analytics, its techniques, tools and algorithms are important to improve the learning context. The findings in this paper suggest that the incorporation of an approach based on big data is of crucial importance. This approach can improve the learning process, for this, its implementation must be correctly aligned with educational strategies and learning needs. Research limitations/implications: This research represents a reference to better understanding the influence and the role of big data in educational dynamic. In addition, it leads to improve existing literature about big data for learning. The limitations of the paper are given by its nature derived from a theoretical perspective, and the discussed ideas can be empirically validated by identifying how big data helps in addressing learning issues. Originality/value: Over the time, the process that leads to the acquisition of the knowledge uses and receives more technological tools and components; this approach has contributed to the development of information communication and the interactive learning context. Technology applications continue to expand the boundaries of education into an “anytime/anywhere” experience. This technology and its wide use in the learning system produce a vast amount of different kinds of data. These data are still rarely exploited by educational practitioners. Its successful exploitation conducts educational actors to achieve their full potential in a complex and uncertain environment. The general motivation for this research is assisting higher educational institutions to better understand the impact of the big data as a success factor to develop their learning process and achieve their educational strategy and goals. This study contributes to better understand how big data analytics solutions are turned into operational actions and will be particularly valuable to improve learning in educational institutions. © 2018, Emerald Publishing Limited. Understand, develop and enhance the learning process with big data Algorithm; Big data; E-learning; Higher education; Learning analytics; Learning process ",Strategic alignment
2058,Leveraging Governance to Derive Value from Advanced Analytics,"In today’s ever-changing health care market, the need for more agile data to support population health, value-based revenue models, and market consolidation is only increasing. To leverage emerging advanced analytics technologies, you must have a clear picture of the who, what, where and why of your data. This commentary explores how to utilize governance capabilities and technologies to ensure you derive value from your analytics investments. © 2019 by the North Carolina Institute of Medicine and The Duke Endowment. All rights reserved. Leveraging Governance to Derive Value from Advanced Analytics  Data Analysis; Health Care Sector; Humans; data analysis; health care cost; human",Financial management
2059,"Rejuvenating late life field opportunities through a fast track full field re-evaluation, focused idle well reactivation, strategic data acquisition and data analytics in Peninsular Malaysia waters","Managing late life declining assets often called for a holistic approach and integrated efforts to safeguard the production funnel and finding the upside potential for field rejuvenation to sustain the economic life. Identifying the remaining hydrocarbon potential and monetizing it during late field life often pose a challenge in terms of limited data availability, complex reservoir characterization, complex well profile as well as job doability. To overcome the challenges, a strategic maturation plan and fast track action is required to accelerate the field potential prior to abandonment. This paper will showcase few case studies from Peninsular Malaysia waters on the successful paths taken to reap the full benefits of bypassed hydrocarbon potential by leveraging on proactive full field analysis, fit-for-purpose technology and data analytics for petrophysical evaluation and focused execution of the production enhancement plan which seems impossible under normal operating work scope. Initiating proactive full field review (FFR) and fast track assessment on the remaining hydrocarbon potential prior to field abandonment had become the major enabler in identifying the untapped opportunities behind casing to prolong the field economic life. Commendable efforts to bring the opportunities into life also include advanced data acquisition to minimize uncertainty in the add-perf job, using data learning to assist shallow reservoir evaluation for work-over and infill drilling candidates, utilizing ceased production well for water injector slot and agile as well as adaptive workflow for safe job execution both for production enhancement and abandonment purposes. Through aggressive subsurface study and multi-disciplinary work integration process starting from identifying the opportunities till realizing the barrels into production pipeline, a significant and sustainable production gain had been successfully achieved which helps to prolong the net positive cash flow of the fields involved. The success is replicated to the rest of other fields where a nimble workflow has been established to unlock the late life field potential. Partnership in technology has also triggered more focus in delivering the results and value to the operation in addition to increased collaboration in new uncharted area such as advanced formation evaluation tool technology, nanotechnology material, geoengineering of shallow unconsolidated formation properties and enhancing operational success based on data-driven approach. Besides, some lessons learned and best practices in planning and executing well abandonment operation will be also highlighted. In a nutshell, field rejuvenation and exploiting maximum potential during late field life require a long projectile and strategic planning for value creation. Case studies demonstrated in this paper have provided an exemplary blueprint to best manage the late life declining assets by challenging the status quo and working through an integrated framework to bring in more barrels. Copyright 2019, Society of Petroleum Engineers. Rejuvenating late life field opportunities through a fast track full field re-evaluation, focused idle well reactivation, strategic data acquisition and data analytics in Peninsular Malaysia waters  Blueprints; Data Analytics; Endocrinology; Hydrocarbons; Infill drilling; Petroleum reservoir evaluation; Case-studies; Data analytics; Economic life; Fast tracks; Field potential; Hydrocarbon potential; Malaysia; Production enhancement; Re-evaluation; Strategic data; Data acquisition",Capacity management
2061,Leadership decision-making processes in the context of data driven tools,"Digital economy vast streams of data have created a new paradigm for the business intelligence processes, increasing the potential of advanced analytics and cognitive data tools. Big data structures are used in business intelligence to work with massive amount of dataset to extract value for effective business decision. The current research seeks to address the following question: how can leaders integrate new technology in their decision process to achieve business goals? Emerging technologies directly created organizational power shift and internal bureaucracy adjustments as a result of data transparency trend and decision-making levels changes. A new type of organizational culture and the leadership role in the organizational development becomes necessary. The significant impact over the organizational systems and business goals requires a strategic approach in implementing data driven decision-making processes. © 2018, SRAC - Romanian Society for Quality. All rights reserved. Leadership decision-making processes in the context of data driven tools Advanced analytics; Agile; Artificial intelligence; Big data; Business intelligence; Continuous development; Data management; Digital economy; Machine learning; Organization culture; Technology ",Value management
2062,Health and safety in smart industry: State-of-the-art and future trends,"Industry 4.0, also known as smart manufacturing, is the name of what is considered as the fourth industrial revolution. The drivers behind are globalization, mass customization, and the rapid increase in computational power which makes it now possible to process big data and perform advanced analytics. In response, manufacturers require production systems that have more resilient, predictive and adaptable capabilities to achieve a truly agile production that is capable of lean, competitive production of mass customized products. The digitization of manufacturing has many implications not only on the shop floor. The biggest potential improvements identified in the survey is that Industry 4.0 brings to health and safety in the form of advanced monitoring of the worker and their environment through wearables, vision, and other sensor systems. The implementation of these technologies in turn also challenges that require further addressing, these challenges are not only technical but also organizational in nature. © 2021 Taylor & Francis Group, London. Health and safety in smart industry: State-of-the-art and future trends  Accident prevention; Predictive analytics; Reliability; Wearable sensors; Agile production; Computational power; Future trends; Globalisation; Health and safety; Manufacturing IS; Mass customization; Production system; Smart manufacturing; State of the art; Industry 4.0",Value management
2064,"Quality, IT and Business Operations Modeling and Optimization conference, 2018","The proceedings contain 35 papers. The special focus in this conference is on Quality, IT and Business Operations Modeling and Optimization. The topics include: A Conceptual Architectural Design for Intelligent Health Information System: Case Study on India; Development of QFD Methodology; fixing of Faults and Vulnerabilities via Single Patch; a General Framework for Modeling of Multiple-Version Software with Change-Point; modeling Fault Detection Phenomenon in Multiple Sprints for Agile Software Environment; preservation of QoS and Energy Consumption-Based Performance Metrics Routing Protocols in Wireless Sensor Networks; reliability Analysis for Upgraded Software with Updates; quantitative Software Process Improvement Program Using Lean Methodology; Selection of Optimal Software Reliability Growth Models: A Fuzzy DEA Ranking Approach; an Improved Scoring System for Software Vulnerability Prioritization; software Release and Patching Time with Warranty Using Change Point; two-Dimensional Framework to Optimize Release Time and Warranty; when to Start Remanufacturing Using Adopter Categorization; analytics for Maintenance of Transportation in Smart Cities; business Strategy Prediction System for Market Basket Analysis; Defending the OSN-Based Web Applications from XSS Attacks Using Dynamic JavaScript Code and Content Isolation; analysis of Module-Based Software Reliability Growth Model Incorporating Imperfect Debugging and Fault Reduction Factor; analysis of Impediments to Sustainability in the Food Supply Chain: An Interpretive Structural Modeling Approach; technological Capabilities Impacting Business Intelligence Success in Organisations; Significance of Parallel Computation over Serial Computation Using OpenMP, MPI, and CUDA; Performance Enhancement of AODV Routing Protocol Using ANFIS Technique; six Sigma Implementation in Cutting Process of Apparel Industry; LC-ELM-Based Gray Scale Image Watermarking in Wavelet Domain. Quality, IT and Business Operations Modeling and Optimization conference, 2018  ",Capacity management
2066,The era of big data and path towards sustainability,"This paper is an attempt to throw light on the applications of big data analytics in nurturing sustainable develoment through a descriptive metadata study. Big data is widely used in many areas such as hospitality, transporataion, health, governance, e-commerce etc. Common applications of big data include consumer profiling, personalised pricing, marketing, advertising and predictive analysis. One of the formidable challanges confronted by the businesses in the contemporary period is to reconcile the conflicting interests of profit maximisation and fostering sustainability. The unprecedented magnitude of data generated within the organisations do have the potential to bring gainful insights for efficient resource utilisation and waste minimisation. The advent of big data aids in making these conflicting interests complementary by providing efficient and precise predicitions. A number of studies are going on to explore possible options avialable to leverage big data analytics to create social and environmental value. Novel analytical approaches, enormous amounts of data and new technology would help in gaining insights to frame more agile and efficient policies to protect the environment. This paper discusses how big data is applied in different areas to foster sustainability. © 2018 Elsevier Ltd. All rights reserved. The era of big data and path towards sustainability Big data; Gartner's model; Sustainability Advanced Analytics; Big data; Conservation; Data Analytics; Information management; Marketing; Analytical approach; Consumer profiling; Descriptive metadata; Gaining insights; Gartner; Resource utilisation; Social and environmental; Waste minimisation; Sustainable development",Value management
2067,"18th International Conference on Software Process Improvement and Capability Determination, SPICE 2018","The proceedings contain 26 papers. The special focus in this conference is on Software Process Improvement and Capability Determination. The topics include: Teaching software processes and standards: a review of serious games approaches; interpretation and reporting of process capability results: focus on improvement; Introducing requirements change management process into ISO/IEC 12207; measuring change in software projects through an earned value lens; semantic model based approach for knowledge intensive processes; Transforming SPEM 2.0-compatible process models into models checkable for compliance; ensuring conformance to process standards through formal verification; specification and deployment of a semantic database for system configuration management; adopting agile in the sports domain: A phased approach; techniques based on data science for software processes: A systematic literature review; systematic mapping study on process mining in agile software development; a comprehensive evaluation of agile maturity self-assessment surveys; agile usage in embedded software development in safety critical domain–A systematic review; An enterprise SPICE capability profile for an ISO 9001:2015 compliant organization; A process reference and assessment model for ECU software calibration data; An ontology to support TMMi-based test process assessment; permissioned blockchains and smart contracts into agile software processes; A systematic literature review on the gamification monitoring phase: How SPI standards can contribute to gamification maturity; towards a taxonomy of process quality characteristics for assessment; process risk determination supporting data protection impact assessment; a novel model for development project assessment in automotive; Evolving PRO2PI methodology considering recent challenges and changes in the SPI context. 18th International Conference on Software Process Improvement and Capability Determination, SPICE 2018  ",Financial management
2068,"Agile values or plan-driven aspects: Which factor contributes more toward the success of data warehousing, business intelligence, and analytics project development?","Practically all organizations are developing data warehousing, business intelligence, and analytics (DW/BIA) projects for achieving customer value. A DW/BIA development project may be characterized by both agile and plan-driven aspects. The reported study investigated two research questions: (1) Which factor, agile values or plan-driven aspects, contributes more toward the success of DW/BIA? (2) What are the significant antecedents of agile values and plan-driven aspects? 124 respondents engaged in DW/BIA development filled a 30-item questionnaire on seven constructs. The partial least squares structural equation modeling (PLS-SEM) method was used to determine the strength of the relationships among the following factors: technological capability, shared understanding, top management commitment, and complexity as antecedents; agile values and plan-driven aspects as mediating; and project success as the dependent construct. Based on a prediction-oriented segmentation (PLS-POS) analysis, the findings indicate that there are two groups, agile-plan balanced and agile-heavy, which represent different approaches to DW/BIA development. Top management commitment and shared understanding emerge as strong antecedents to agile values and plan-driven aspects. Overall, the factor agile values contributes more toward the success of DW/BIA development. © 2018 Agile values or plan-driven aspects: Which factor contributes more toward the success of data warehousing, business intelligence, and analytics project development? Agile values; Analytics; Business intelligence; Plan-driven; Shared understanding; Top management commitment Competitive intelligence; Data warehouses; Information analysis; Least squares approximations; Warehouses; Agile values; Analytics; Plan-driven; Shared understanding; Top management commitment; Computer supported cooperative work",Strategic alignment
2069,Ambidextrous organization and agility in big data era: The role of business process management systems,"Purpose: The purpose of this paper is to explore the effect of big data analytics-capable business process management systems (BDA-capable BPMS) on ambidextrous organizations’ agility. In particular, how the functionalities of BDA-capable BPMS may improve organizational dynamism and reactiveness to challenges of Big Data era will be explored. Design/methodology/approach: A theoretical analysis of the potential of BDA-capable BPMS in increasing organizational agility, with particular attention to the ambidextrous organizations, has been performed. A conceptual framework was subsequently developed. Next, the proposed conceptual framework was applied in a real-world context. Findings: The research proposes a framework highlighting the importance of BDA-capable BPMS in increasing ambidextrous organizations’ agility. Moreover, the authors apply the framework to the cases of consumer-goods companies that have included BDA in their processes management. Research limitations/implications: The principal limitations are linked to the need to validate quantitatively the proposed framework. Practical implications: The value of the proposed framework is related to its potential in helping managers to fully understand and exploit the potentiality of BDA-capable BPMS. Moreover, the implications show some guidelines to ease the implementation of such systems within ambidextrous organizations. Originality/value: The research offers a model to interpret the effects of BDA-capable BPMS on ambidextrous organizations’ agility. In this way, the research addresses a significant gap by exploring the importance of information systems for ambidextrous organizations’ agility. © 2018, Emerald Publishing Limited. Ambidextrous organization and agility in big data era: The role of business process management systems Agile; Agility; Ambidexterity; Big data; Information systems ",Strategic alignment
2071,Developments in knowledge discovery processes and methodologies: Anything new?,"The process of turning data into knowledge is referred to as “knowledge discovery” (KD) and originated in the 1990s. Since that time many different process models and methodologies have been developed. A genealogy presented in 2010, showed how the different models evolved and presented a refined process model, which represents a synthesis of the models presented before. However, the rise of data analytics and big data have changed how organizations do business. The key to these changes is to use data and turn it into knowledge to create value for the organization. Therefore, this study aims to update our understanding of knowledge discovery processes by reviewing the research into KD processes since 2010 in order to understand if there have been considerable changes and developments in this field. The developments in KD process models and methodologies that were found are threefold: tasks, steps and agile practices. © 2019 Association for Information Systems. All rights reserved. Developments in knowledge discovery processes and methodologies: Anything new? Agile practice; Big data; Knowledge discovery process; Process methodology; Process model Big data; Data Analytics; Information systems; Information use; Agile practices; Knowledge discovery process; Process model; Process Modeling; Agile manufacturing systems",Strategic alignment
2073,"4th International Conference on Big Data and Information Analytics: Theories, Algorithms and Applications in Data Science, BigDIA 2018","The proceedings contain 8 papers. The topics discussed include: machine learning techniques for variable annuity valuation; L1 correlation-based penalty in high-dimensional quantile regression; engineering big data to small businesses: lessons learned from a case study; feature learning and classification in neuroimaging: predicting cognitive impairment from magnetic resonance imaging; an optimization model to evaluate dynamic assignment capability of agile organization; on the application of energy inspired model to label denoising; convolutional neural networks for breast cancer histopathological image classification; and channel predictions for cable TV users by modified SPEED algorithms. 4th International Conference on Big Data and Information Analytics: Theories, Algorithms and Applications in Data Science, BigDIA 2018  ",Strategic alignment
2074,"Rejuvenating late life field opportunities through a fast track full field re-evaluation, focused idle well reactivation, strategic data acquisition and data analytics in peninsular Malaysia waters","Managing late life declining assets often called for a holistic approach and integrated efforts to safeguard the production funnel and finding the upside potential for field rejuvenation to sustain the economic life. Identifying the remaining hydrocarbon potential and monetizing it during late field life often pose a challenge in terms of limited data availability, complex reservoir characterization, complex well profile as well as job doability. To overcome the challenges, a strategic maturation plan and fast track action is required to accelerate the field potential prior to abandonment. This paper will showcase few case studies from Peninsular Malaysia waters on the successful paths taken to reap the full benefits of bypassed hydrocarbon potential by leveraging on proactive full field analysis, fit-for-purpose technology and data analytics for petrophysical evaluation and focused execution of the production enhancement plan which seems impossible under normal operating work scope. Initiating proactive full field review (FFR) and fast track assessment on the remaining hydrocarbon potential prior to field abandonment had become the major enabler in identifying the untapped opportunities behind casing to prolong the field economic life. Commendable efforts to bring the opportunities into life also include advanced data acquisition to minimize uncertainty in the add-perf job, using data learning to assist shallow reservoir evaluation for work-over and infill drilling candidates, utilizing ceased production well for water injector slot and agile as well as adaptive workflow for safe job execution both for production enhancement and abandonment purposes. Through aggressive subsurface study and multi-disciplinary work integration process starting from identifying the opportunities till realizing the barrels into production pipeline, a significant and sustainable production gain had been successfully achieved which helps to prolong the net positive cash flow of the fields involved. The success is replicated to the rest of other fields where a nimble workflow has been established to unlock the late life field potential. Partnership in technology has also triggered more focus in delivering the results and value to the operation in addition to increased collaboration in new uncharted area such as advanced formation evaluation tool technology, nanotechnology material, geoengineering of shallow unconsolidated formation properties and enhancing operational success based on data-driven approach. Besides, some lessons learned and best practices in planning and executing well abandonment operation will be also highlighted. In a nutshell, field rejuvenation and exploiting maximum potential during late field life require a long projectile and strategic planning for value creation. Case studies demonstrated in this paper have provided an exemplary blueprint to best manage the late life declining assets by challenging the status quo and working through an integrated framework to bring in more barrels. Copyright 2019, Society of Petroleum Engineers. Rejuvenating late life field opportunities through a fast track full field re-evaluation, focused idle well reactivation, strategic data acquisition and data analytics in peninsular Malaysia waters  Abandoned wells; Data acquisition; Data Analytics; Endocrinology; Hydrocarbons; Infill drilling; Oil well testing; Bypassed hydrocarbon; Data-driven approach; Hydrocarbon potential; Integrated frameworks; Petrophysical evaluations; Production enhancement; Sustainable production; Unconsolidated formations; Petroleum reservoir evaluation",Capacity management
2075,Ad hoc data analytics and business intelligence service framework for construction projects,"This paper presents a framework of an ad-hoc data analytic and Business Intelligence service tailored to a construction project. Mandates of delivering integrated information solutions and effective reporting are commonly required nowadays in large capital projects. Due to the nature of construction projects with schedule and budget constraints, poorly defined business problems prohibited the team to deploy full scale data analytic and Business Intelligence (BI) services on site. On the other hand, the increasingly complex data coming from multiple applications and organizations on projects requires more powerful data integration tools and techniques. The proposed framework outlines an agile and ad hoc best practice for job site data analytics and effective reporting based on a real use case from a large pharmaceutical project. Processes in the framework include data alignment, Level of Detail (LoD) data articulation and analytical model establishment. It also illustrates how to resolve complex data analytic challenges for unforeseen cost disputes and how to deliver solutions within a short period of time. © IGLC 2018 All Rights Reserved. Ad hoc data analytics and business intelligence service framework for construction projects Complex; Customization; Data Analytics; Integrated Information; Integration; Waste Budget control; Information analysis; Integration; Lean production; Wastes; Complex; Construction projects; Customization; Data analytics; Integrated information solutions; Integrated informations; Multiple applications; Pharmaceutical projects; Data integration",Strategic alignment
2076,Building a rig state classifier using supervised machine learning to support invisible lost time analysis,"This paper covers the development of a key component of an internal system to report invisible lost time (ILT) metrics across drilling operations. Specifically this paper covers the development of a generalizable rig state engine based on the application of supervised machine learning. The same steps used in the creation of the production rig state engine are appled here to a smaller data set to demonstrate both the tractability of the problem and the methods used to create the rig state engine in the production system. The project objective was to provide efficiency and engineering metrics in a central repository covering operated regions. The system is designed to require minimal user configuration and management and provides both historic and near real time analysis to deliver a rich resource for offset comparison and benchmarking. Identifying rig-state is at the heart of every performance and engineering analysis system. This can be thought of as a machine learning classification problem. A large supervised learning set was constructed and used to train classification models which were compared for accuracy. A key success metric was the ability to generalise the selected model across different operations. Output from the rig-state classifier was then used to derive KPI data which was presented through a web based front end. A pilot system was then developed using agile principles allowing for rapid user engagement. Testing demonstrated that the system can support all real time operations within the company simultaneously and rapidly process historic well data for offset benchmarking. The cloud-based architecture allows rapid deployment of the system to new groups significantly reducing deployment costs. The system provides a foundation for onward data science and more advanced functionality. Minimal configuration, cloud storage and processing, combining contextual data with real-time rig data, near-real-time and historic analysis capabilities, rapid deployment, low cost, high accuracy and consistent metrics are all key and proven value drivers for the system. The output data is aso a valuable resource for additional machine learning and data science projects. Copyright 2019, SPE/IADC International Drilling Conference and Exhibition. Building a rig state classifier using supervised machine learning to support invisible lost time analysis  Benchmarking; Cost benefit analysis; Data Science; Digital storage; Engines; Infill drilling; Supervised learning; Well testing; Analysis capabilities; Classification models; Cloud-based architectures; Configuration and managements; Engineering analysis; Engineering metrics; Machine learning classification; Supervised machine learning; Learning systems",Monitoring and control
2077,How digital technology can transform Steel and metals value chains,"Steel and metals manufacturers have a huge opportunity to transform their value chain by implementing digital technology, enabling them to simultaneously improve customer service, profit margins, and inventory levels. Moreover, capturing these benefits does not require a full IT transformation. Rather, companies can quickly capitalize on new technologies like predictive analytics and data lakes through agile pilot tests and begin generating results in weeks, at a fraction of the cost of previous technology. © 2018 Verlag Stahleisen GmbH. All rights reserved. How digital technology can transform Steel and metals value chains  Customer-service; Digital technologies; Inventory levels; Metal values; Pilot tests; Profit margin; Service profits; Value chains; Predictive analytics",Monitoring and control
2079,Metric based cloud infrastructure monitoring,"Goal of the paper is to design dynamic, model-driven solution for agile service and network monitoring based on metrics suitable for managing large infrastructures and clustered applications in baremetal or cloud environments. The solution uses decetralised model where each node computes its states and passes the results to upper level cluster monitoring services. This is opposite approach than traditional monitoring where all logic is stored on central monitoring node or cluster of nodes. Our proposed Monitoring solution offers automation of repetitive administration tasks and makes creation of monitoring infrastructure part of routine configuration tasks and make use of meta-data models already in place. Various monitoring scenarious are covered as Cluster and inter datacenter monitoring, advanced time-series analytics, corellations and trending with prediction algorythms and querying log event databases for unusual patterns. Emphasis is given to limiting the amount of output when failures occur presented to human operators, reducing the time to solve the problem and automating solution of most common issues. © Springer International Publishing AG 2018. Metric based cloud infrastructure monitoring Cloud application; Cloud infrastructure; Logging; Metric; Monitoring; Time-series Advanced Analytics; Administration tasks; Cloud environments; Cloud infrastructures; Cluster monitoring; Cluster of nodes; Human operator; Moni-toring nodes; Network Monitoring; Monitoring",Monitoring and control
2081,Threat intelligence computing,"Cyber threat hunting is the process of proactively and iteratively formulating and validating threat hypotheses based on security-relevant observations and domain knowledge. To facilitate threat hunting tasks, this paper introduces threat intelligence computing as a new methodology that models threat discovery as a graph computation problem. It enables efficient programming for solving threat discovery problems, equipping threat hunters with a suite of potent new tools for agile codifications of threat hypotheses, automated evidence mining, and interactive data inspection capabilities. A concrete realization of a threat intelligence computing platform is presented through the design and implementation of a domain-specific graph language with interactive visualization support and a distributed graph database. The platform was evaluated in a two-week DARPA competition for threat detection on a test bed comprising a wide variety of systems monitored in real time. During this period, sub-billion records were produced, streamed, and analyzed, dozens of threat hunting tasks were dynamically planned and programmed, and attack campaigns with diverse malicious intent were discovered. The platform exhibited strong detection and analytics capabilities coupled with high efficiency, resulting in a leadership position in the competition. Additional evaluations on comprehensive policy reasoning are outlined to demonstrate the versatility of the platform and the expressiveness of the language. © 2018 Association for Computing Machinery. Threat intelligence computing Computing methodology; Intrusion detection; Threat hunting Iterative methods; Real time systems; Visualization; Computing methodologies; Computing platform; Design and implementations; Domain knowledge; Graph computations; Interactive visualizations; Threat detection; Threat hunting; Intrusion detection",Risk management
2082,Creating value from business intelligence and analytics in SMEs: Insights from experts,"This paper reports from an exploratory study that examines utilization of Business Intelligence and Analytics (BI&A) in Small-and-Medium-sized Enterprises (SMEs). In total, 24 semi-structured interviews of BI&A experts were conducted. The experts highlighted several critical issues that SMEs should consider: (1) to ""start Small, think Big"" was emphasized as an appropriate BI&A investment strategy for SMEs to obtain value in terms of both ""quick wins"" and long-term assets and impacts, (2) to consider BI&A investment without implementing a traditional data warehouse, and (3) to consider the automated data warehouse approach. In addition, the experts underscored to pay more attention to data governance. A recognized value framework from the literature was applied as an analytical lens to interpret the findings. We suggest modification of this framework to make it less ""waterfall"" oriented and more iterative and agile to create value from BI&A in SMEs. Future research should assess SMEs' readiness and capabilities for BI&A. In addition, we need to understand the exclusive needs for decision-making in SMEs across industries. © 2018 Association for Information Systems. All rights reserved. Creating value from business intelligence and analytics in SMEs: Insights from experts BI&A value framework; Business intelligence and analytics; Data governance; SMEs Data warehouses; Decision making; Information systems; Information use; Automated data; Critical issues; Data governances; Exploratory studies; Investment strategy; Semi structured interviews; Small and medium-sized enterprise; SMEs; Information analysis",Governance
2083,The Agile Deployment of Machine Learning Models in Healthcare,"The continuous delivery of applied machine learning models in healthcare is often hampered by the existence of isolated product deployments with poorly developed architectures and limited or non-existent maintenance plans. For example, actuarial models in healthcare are often trained in total separation from the client-facing software that implements the models in real-world settings. In practice, such systems prove difficult to maintain, to calibrate on new populations, and to re-engineer to include newer design features and capabilities. Here, we briefly describe our product team's ongoing efforts at translating an existing research pipeline into an integrated, production-ready system for healthcare cost estimation, using an agile methodology. In doing so, we illustrate several nearly universal implementation challenges for machine learning models in healthcare, and provide concrete recommendations on how to proactively address these issues. Copyright © 2019 Jackson, Yaqub and Li. The Agile Deployment of Machine Learning Models in Healthcare agile; analytics engineering; continuous delivery; health informatics; machine learning ",Strategic alignment
2086,"Joint Proceedings of the AHFE 2018 International Conference on Human Factors in Artificial Intelligence and Social Computing, Software and Systems Engineering, The Human Side of Service Engineering and Human Factors in Energy, 2018","The proceedings contain 53 papers. The special focus in this conference is on Human Factors in Artificial Intelligence and Social Computing. The topics include: Gamified approach in the context of situational assessment: A comparison of human factors methods; estimation of risks in scrum using agile software development; software development practices in Costa Rica: A survey; convincing systems engineers to use human factors during process design; Knowledge management model based on the enterprise ontology for the KB DSS system of enterprise situation assessment in the SME sector; Re-defining the role of artificial intelligence (AI) in wiser service systems; an approach for a quality-based test of industrial smart service concepts; smart services conditions and preferences – an analysis of chinese and german manufacturing markets; using digital trace analytics to understand and enhance scientific collaboration; a case study of user adherence and software project performance barriers from a sociotechnical viewpoint; smart university for sustainable governance in smart local service systems; using augmented reality and gamification to empower rehabilitation activities and elderly persons. A study applying design thinking; Method cards – a new concept for teaching in academia and to innovate in SMEs; correlations between computer-related causal attributions and user persistence; emotionalizing e-Commerce pages: Empirical evaluation of design strategies for increasing the affective customer response; patient-centered design of an e-Mental health app; New « intelligence » coming to the cockpit…again?; beyond the chatbot: Enhancing search with cognitive capabilities; a comparative analysis of similarity metrics on sparse data for clustering in recommender systems. Joint Proceedings of the AHFE 2018 International Conference on Human Factors in Artificial Intelligence and Social Computing, Software and Systems Engineering, The Human Side of Service Engineering and Human Factors in Energy, 2018  ",Financial management
2087,"Artificial intelligence, big data, strategic flexibility, agility, and organizational resilience: A conceptual framework based on existing literature","In today’s economically turbulent times, it is imperative that organizations remain flexible and resilient in order to adapt themselves to an ever-changing environment. To facilitate this, organizations should rely upon pliant structures of information, whilst simultaneously continuing to incorporate more rigid infrastructures in order to allow for the collection and analysis of large amounts of both internal and external data. This juxtaposition gives rise to the need for a trade-off. While academic literature has stressed that information systems may represent a burden for organizations pursuing strategic agility, flexibility, and organizational resilience, this paper highlights the ways in which Analytical, Automatic, Adaptive, and Agile information systems - or Big Data Analytics (BDA) capable information systems - may be helpful. In particular, this paper proposes BDA capable information systems, tied with artificial intelligence capabilities, as a trade-off solution. Alongside this, it also proposes some further implications of the topic for scholars and practitioners. © 2018 IADIS Press. All Rights Reserved. Artificial intelligence, big data, strategic flexibility, agility, and organizational resilience: A conceptual framework based on existing literature Artificial Intelligence; Big Data Analytics; Information Systems; Organizational Resilience; Strategic Agility; Strategic Flexibility Artificial intelligence; Economic and social effects; Information systems; Information use; Academic literature; Agile information system; Big Data Analytics; Changing environment; Conceptual frameworks; Organizational Resilience; Strategic Agility; Strategic flexibility; Big data",Governance
2088,Exploring collaborative writing of user stories with multimodal learning analytics: A case study on a software engineering course,"Software engineering is the application of principles used in engineering design, development, testing, deployment, and management of software systems. One of the software engineering's approaches, highly used in new industries, is agile development. User stories are a commonly used notation to capture user requirements in agile development. Nevertheless, for the elaboration of user stories, a high level of collaboration with the client is necessary. This professional skill is rarely measured or evaluated in educational contexts. The present work approaches collaboration in software engineering students through multimodal learning analytics, modeling, and evaluating students' collaboration while they are writing user stories. For that, we used multidirectional microphones in order to derive social network analysis metrics related to collaboration (permanence and prompting) together with human-annotated information (quality of the stories and productivity). Results show that groups with a lower productivity in writing user stories and less professional experience in managing software requirements present a non-collaborative behavior more frequently, and that teams with a fewer number of interventions are more likely to produce a greater number of user stories. Moreover, although low experience subjects produced more user stories, a greater productivity of the most experienced subjects was not statistically verified. We believe that these types of initiatives will allow the measurement and early development of such skills in university students. © 2013 IEEE. Exploring collaborative writing of user stories with multimodal learning analytics: A case study on a software engineering course agile development; multimodal learning analytics; Requirements engineering; social network analysis Application programs; Productivity; Quality control; Requirements engineering; Social networking (online); Software testing; Students; Agile development; Collaborative writing; Educational context; Multi-modal learning; Professional experiences; Software engineering course; Software engineering students; Software requirements; Engineering education",Risk management
2089,Real-time drilling advisor appstore – An agile development and deployment program,"In the new wave of digitization, real-time advisory mode drilling data analytics is both an intermediate step to achieve long term vision of autonomous drilling and an agile response to the need of delivering beyond best in class wells. An operator has constructed an agile program for drilling data analytics to lead the development of analytics solutions, deployment to collect business values, and handover to productization. The product under development is a Real-Time Drilling Advisor (RTDA) Appstore, which is both an R&D template for prototyping and a hosting environment for rapid field trials and early deployments. The RTDA Appstore consists of a witsml data pipeline, template of drilling data analytics functionalities, modularized computational kernels, machine learning packages for historical data analysis, an in-memory database for both historical and real-time processed data, customized drilling visualization widgets, and an open-source web app deployment framework. A hybrid methodology combining conventional engineering practice, data-driven approaches, novel visualization formats, and physics-based modeling is adopted to develop real-time data analytics solutions. It focuses on three areas: failure detection, performance improvement, and advisory directional drilling. A list of applications and features in the RTDA Appstore includes: interactive offset data map, drilling parameter roadmap, drilling efficiency monitoring, bit/reamer wear prediction, smart torque/drag, payzone tracking, auto slide sheets, sliding KPI, real-time casing wear, real-time BIC benchmarking, etc. So far, about 30 wells of the company's Deepwater wells and about 150 of the company's unconventional wells were drilled using RTDA applications. RTDA deployments generated time and cost savings through early warnings or bespoke visualization of information for improved decision making. The RTDA Appstore is built in-house and thus provides flexibility and modularization options and heavily focused on the analytics layer. The agile program creates an interactive channel to not only meet business demand, but also to use feedback from operations to steer the digital R&D strategy. As the half open-source nature of this program, the digital capability cultivated through the development and deployment of RTDA will also be cascaded to local wells teams through format of digital training, code repository, and a single portal Appstore. Copyright 2019, SPE/IADC International Drilling Conference and Exhibition. Real-time drilling advisor appstore – An agile development and deployment program  Benchmarking; Data Analytics; Data visualization; Decision making; Modular construction; Open systems; Visualization; Wear of materials; Computational kernels; Data-driven approach; Drilling parameters; Engineering practices; Hybrid methodologies; Novel visualizations; Physics-based modeling; Visualization of information; Infill drilling",Capacity management
2090,ESP well and component failure prediction in advance using engineered analytics - A breakthrough in minimizing unscheduled subsurface deferments,"A failed Electrical Submersible Pump (ESP) well is generally identified when there is no flow to the surface. The process of reviving well production can take weeks leading to huge unwanted deferment. Through a Proof-Of-Concept (PoC), the objective is to prototype and evaluate the results of an early failure detection for ESP wells using Machine Learning (ML), without reserving focus on implementation. By demonstrating the feasibility of this approach and verifying that the concept has practical potential, the tool can be used to reduce deferment and identify failure prone component to either devise mitigation strategy for extending time-to-failure or work on an improved design before failure. The paper details all the work undertaken to develop a Predictive Analytics model based on ML algorithms using field sensor data, real time physics-based model calculated data and well failure history to predict ESP well failure and identify failed component in advance. The approach of database standardization, data pre-processing, machine-learning algorithm selection, supervised training and validation dataset creation shall be discussed. ESP domain knowledge used for Feature Engineering across multiple modeling iterations to consistently improve well and component level model metrics shall be detailed. After the evaluation by well owners at Petroleum Development Oman (PDO), refered as Operator's blind test, the prediction of the ML algorithm shows a good accuracy in its ability to capture historical failures ranging between days to months in advance. The Well Level Failure model captures failure prone wells with a precision of 90% and accuracy of 76%. The Component Level Failure model correctly identifies pump failure from other failures with a precision of 92% and accuracy of 88%. These numbers show the reliability of future predictions that could enable users to make high stake workover and operating envelope optimization decisions with confidence. Following benefits are estimated from both Well failure and Pump Component failure prediction models metrics respectively: • 28.35% savings from total unscheduled ESP deferment • 1% increase in Overall Mean Time to Failure (MTTF) based on optimization of predicted pump component failure wells. In an organization where over thousand ESP wells are managed by limited production engineers, post ESP failure, the effort invested for hoist scheduling, raising new well proposal, rig mobilization, new ESP installation and commissioning utilizes huge time and leads to long undesired oil deferment. Implementation of engineered analytics to predict ESP failures and failed components in advance can support production engineers to plan early for workover operations, increase well run life and minimize oil deferment losses. Methodologically assessed by Senior Petroleum Engineers in selected clusters (using historical data and in the context of each failure and non-failure cases), the Predictive Analytics journey has started. It is ready to be operationalized at a small scale to build confidence as an advisory tool for Production Engineers in real-time to evaluate multiple wells' failure probability on a daily basis and generate massive savings from well deferment. This agile journey focused on value generation is achieved with combined efforts between technology, domain knowledge and data. © 2019, Society of Petroleum Engineers ESP well and component failure prediction in advance using engineered analytics - A breakthrough in minimizing unscheduled subsurface deferments  Building components; Engineers; Forecasting; Gasoline; Learning algorithms; Machine learning; Oil well production; Predictive analytics; Submersible pumps; Time-to-failure; Well workover; Component level models; Early failure detection; Electrical submersible pumps; Failure Probability; Feature engineerings; Mitigation strategy; Petroleum development Oman; Supervised trainings; Petroleum reservoir evaluation",Strategic alignment
2091,Chasing the Crowd: Digital Transformations and the Digital Driven System Design Paradigm,"These days business successes and economic opportunities steadily depend on IT-capabilities and digital driven business transformations. Digitalization continues to advance with new business models and growing prospects in various dimensions from higher margins and greater public sector funding to new customers, markets and more diverse suppliers’ networks. The key digital transformation drivers of change –fundamental redesign of business activities, processes, and models– are each significantly facilitated by integrated mix of digital technologies that includes, but not limited to, social, mobile, analytics and cloud (SMAC) and sometimes SMACIT when counting the Internet of Things. Increasingly the actual digital transformation has much more to do with information, data, analytics, workflow, culture, and management than traditional IT/IS. In the last decades IT/IS has been essential driver of organizational changes, the recent developments in digitalization and innovations shifted the emphasis on the information rather than the technology in “IT” abbreviation. To understand the prospects and challenges associate with the digital driven business strategy and operational transformations, a modified model of the socio-technical system is created and presented. It depicts at macro level how digital technologies and utility type computing platforms act as drivers for supporting digital business strategies and advancing with the customers. While it simplifies the relations of the internal organization socio-technical model with the external world, the enhanced socio-technical (EST) model indicates the interactions of the four organization’s forces: structure, people, technology and processes with the platform technologies such as O2O and SMACIT and further with the crowds. The paper confers the impact of digital driven transformations to the system design and development process and specifically emphasizes on agile transformational practices aligning better with business agility, asset utilization, and greater customers’ satisfaction. © 2019, Springer Nature Switzerland AG. Chasing the Crowd: Digital Transformations and the Digital Driven System Design Paradigm Agile transformational approach; Crowd business models; Digital technologies; Digital transformation; Enhanced Socio-Technical (EST) model; SMAC; SMACIT; Systemic effect; Utility type digital platforms Agile manufacturing systems; Customer satisfaction; Metadata; Sales; Strategic planning; Systems analysis; Systems engineering; Business models; Digital platforms; Digital technologies; Digital transformation; SMAC; SMACIT; Sociotechnical; Systemic effect; Transformational approach; Software design",Governance
2092,Predictive Simulation Modeling and Analytics of Value Stream Mapping for the Implementation of Lean Manufacturing: A Case Study of Small and Medium-Sized Enterprises (SMEs),Predictive simulation analytics can be used for Small and medium-sized enterprises (SMEs) to become more competitive. SMEs tried to implement the lean manufacturing for improving the competitiveness. The future-state Value Stream Mapping (VSM) of the lean manufacturing could not be implemented at the shop floor level. So the simulation is used as predictive analytics for the future-state VSM. The objective of this paper is to analysis the future-state mapping of VSM for the implementation of lean manufacturing using predictive analytics. The predictive simulation models were analyzed to understand the current state and future state mapping of VSM. Predictive simulation analytics of VSM is an appropriate analytical tool for implementation of lean manufacturing in SMEs for improving competitiveness. © 2018 IEEE. Predictive Simulation Modeling and Analytics of Value Stream Mapping for the Implementation of Lean Manufacturing: A Case Study of Small and Medium-Sized Enterprises (SMEs) Discrete Event Simulation; Lean manufacturing; Predictive Analytics; SMEs; Value stream Mapping Agile manufacturing systems; Competition; Control systems; Discrete event simulation; Intelligent computing; Lean production; Mapping; Analytical tool; Predictive simulation modeling; Predictive simulations; Shop floor; Small and medium-sized enterprise; SMEs; State values; Value stream mapping; Predictive analytics,Monitoring and control
2093,Digital employee experience engagement paradox: Futureproofing retention practice,"The digital disruption of everything technology, society, business models is impacting the future of work inclusive of the evolution of the role of both the generalist and specialist human capital practitioner. Futureproofing the employee experience through design thinking, robust workplace people analytics and agile transformation ways of working is the new normal for evidence based human resource practitioners. This chapter explores how specifically digital disruption is impacting future of work and what work in the future is likely to look like. Possible answers to which digital capabilities, competencies and business culture might enable not a race and battle against the smart machines but rather a constructive collaborative augmentation. Which of these future digital technology work processes or digital workforce contexts will enable sustainability despite massive automation and deskilling of knowledge work? The chapter makes recommendations for futureproofing retention and engagement via the (re) design of next generation employee experience. © Springer Nature Switzerland AG 2018. Digital employee experience engagement paradox: Futureproofing retention practice Agile transformation; Design thinking workforce analytics; Digital disruption; Employee experience; Engagement; Retention ",Financial management
2095,Growing smart farming services: How to get the best out of farming data,"Many enterprises in the agricultural sector can access a lot of data, but often (still) fail to treat this data as an asset. Even though farming data has great potential for the creation of new business models, most manufacturers of agricultural equipment do not know which data they possess as it is spread across different data silos of complex and historically grown application landscapes. In order to come up with innovative data-driven smart farming services, seamless collaboration of cross-functional teams staffed with different roles like software engineers, data scientists, business experts, and end users is crucial for the successful realization of smart farming services. In a joint research cooperation, John Deere and Fraunhofer IESE have developed an agile engineering method for the value-driven creation of smart farming services. In this paper, we present our method, which is based on different design thinking techniques used to identify concrete data-driven user stories that are of true value to farmers. We also present a data science notation that we developed to facilitate the targeted analysis of the data-science-significant requirements of data-driven user stories and to foster interdisciplinary collaboration and knowledge dissemination in our cross-functional team. We also demonstrate the Insights Collaboration (short: ICSpace) app, which supports the application of the notation. © VDI Verlag GmbH · Düsseldorf 2019. Growing smart farming services: How to get the best out of farming data  Agricultural robots; Agriculture; Agile engineerings; Agricultural equipment; Agricultural sector; Cross-functional teams; Interdisciplinary collaborations; Knowledge dissemination; New business models; Research cooperation; Data Science",Strategic alignment
2097,Automated VNF Testing with Gym: A Benchmarking Use Case,"In the growing landscape of Virtualized Network Function (VNF) development processes and methodologies fueled by enabling technologies for virtualization, the myriad of customization options unveil unprecedented SW/HW configuration knobs and hazards. Underlying execution environments multiplex resources imposing hard-to-predict relationships between VNF performance metrics (e.g., latency), allocated infrastructure assets (e.g., vCPU), and stimuli workloads. Gym is a framework designed to enable automated testing and to extraction of such relationships by the means of VNF performance profiles, walking through a because-effect path towards agile DevOps methodologies for NFV. The demo showcases the implementation of Gym through exemplified live extraction of VNF metrics for analytics use cases, such as comparison factors between physical network functions and pre-deployment infrastructure dimensioning. © 2018 IFIP. Automated VNF Testing with Gym: A Benchmarking Use Case  Extraction; Transfer functions; Comparison factors; Development process; Enabling technologies; Execution environments; Infrastructure assets; Network functions; Performance metrics; Performance profile; Network function virtualization",Financial management
2098,"15th International Conference on Information Technology: New Generations, ITNG 2018","The proceedings contain 101 papers. The special focus in this conference is on. The topics include: Modified Huffman Code for Bandwidth Optimization Through Lossless Compression; A Self Proxy Signature Scheme Over NTRU Lattices; towards an Ontology of Security Assessment: A Core Model Proposal; integration of Mobile Forensic Tool Capabilities; deep Packet Inspection: A Key Issue for Network Security; what Petya/NotPetya Ransomware Is and What Its Remidiations Are; Analysis of Security Vulnerability and Analytics of Internet of Things (IOT) Platform; new Techniques for Public Key Encryption with Sender Recovery; cloud Intrusion Detection and Prevention System for M-Voting Application in South Africa: Suricata vs. Snort; protecting Personal Data with Blockchain Technology; cybersecurity as People Powered Perpetual Innovation; The Role of CAE-CDE in Cybersecurity Education for Workforce Development; Software Optimizations for DES; Design and Development of VXLAN Based Cloud Overlay Network Monitoring System and Environment; end to End Delay Analysis in a Two Tier Cluster Hierarchical Wireless Sensor Networks; Tools to Support SMEs to Migrate to the Cloud: Opportunities and Challenges; a Dual Canister Medicine IoT Dispenser; local Clock Synchronization Without Transmission Delay Estimation of Control Messages in Wireless Sensor Networks; denial of Service Attacks in Wireless Sensor Networks with Proposed Countermeasures; internet-Based Education: A New Milestone for Formal Language and Automata Courses; teaching Communication Management in Software Projects Through Serious Educational Games; performance Study of the Impact of Security on 802.11ac Networks; gamification Applied in the Teaching of Agile Scrum Methodology; alignment of Requirements and Testing in Agile: An Industrial Experience. 15th International Conference on Information Technology: New Generations, ITNG 2018  ",Strategic alignment
2099,A model for software development cost estimation with system dynamic approach,"Software development is a complex and intricate process due to the number of factors involved, such as human factors, the complexity of developing products, the diversity of development levels, and the management problems of large projects. In Software development cost estimation, we are faced with many variables that change over time, interact with each other and make prediction difficult. In order to solve this problem, using systems and software dynamic approach, a model for estimation software development costs has been designed and implemented in this research. In this research, the system dynamics approach and fuzzy logic are used to modeling. This research consists of two statistical populations. The first statistical population in this research includes IT managers and software development project managers. The second population in this research includes experts of Magfa Company. The tool used to collect data is a questionnaire. The data and information related to the project of development of BI-software (Business intelligence software) were at Magfa Company and has been gathered from the people involved in the project. After simulating and testing the model, three scenarios have been defined to reduce software development costs, which include: increasing personnel experience and increasing the experience of project managers, increasing the capabilities and competencies of manpower and changing the lifecycle of the system from cascading to agile methodology scenario. The findings indicate that the company will see a further reduction in software development costs by using the agile life cycle model. © 2019 Iranian Research Institute for Scientific Information and Documentation. All rights reserved. A model for software development cost estimation with system dynamic approach Fuzzy Logic; Software Cost; Software Development; System Dynamics ",Financial management
2101,ACT Testbot and 4S Quality Metrics in XAAS Framework,"The purpose of this paper is to analyze all Cloud based Service Models, Continuous Integration, Deployment and Delivery process and propose an Automated Continuous Testing and testing as a service based TestBot and metrics dashboard which will be integrated with all existing automation, bug logging, build management, configuration and test management tools. Recently cloud is being used by organizations to save time, money and efforts required to setup and maintain infrastructure and platform. Continuous Integration and Delivery is in practice nowadays within Agile methodology to give capability of multiple software releases on daily basis and ensuring all the development, test and Production environments could be synched up quickly. In such an agile environment there is need to ramp up testing tools and processes so that overall regression testing including functional, performance and security testing could be done along with build deployments at real time. To support this phenomenon, we researched on Continuous Testing and worked with industry professionals who are involved in architecting, developing and testing the software products. A lot of research has been done towards automating software testing so that testing of software product could be done quickly and overall testing process could be optimized. As part of this paper we have proposed ACT TestBot tool, metrics dashboard and coined 4S quality metrics term to quantify quality of the software product. ACT testbot and metrics dashboard will be integrated with Continuous Integration tools, Bug reporting tools, test management tools and Data Analytics tools to trigger automation scripts, continuously analyze application logs, open defects automatically and generate metrics reports. Defect pattern report will be created to support root because analysis and to take preventive action. © 2019 IEEE. ACT Testbot and 4S Quality Metrics in XAAS Framework 4S Quality Metrics; ACT (Automated Continuous Testing); Auto Bug Logging and Tracking; Cloud; Continuous Delivery; Continuous Deployment; Continuous Integration; Continuous Testing; T-Model; TestBot; XaaS (Everything as a Service) Automation; Big data; Clouds; Data Analytics; Defects; Information management; Integration; Machine learning; Continuous Delivery; Continuous Deployment; Continuous integrations; Continuous testing; Quality metrics; T-model; TestBot; XaaS (Everything as a Service); Integration testing",Strategic alignment
2103,Content-based multimedia analytics: US and NATO research,"The US and nations of the NATO Alliance are increasingly threatened by the global spread of terrorism, humanitarian crises/disaster response, and public health emergencies. These threats are influenced by the unprecedented rise of information sharing technologies and practices, where mobile access to social networking sites is ubiquitous. In this new information environment, agile data algorithms, machine learning software, and threat alert mechanisms must be developed to automatically create alerts and drive quick response. US science and technology investments in Artificial Intelligence and Machine Learning (AI/ML) and Human Agent Teaming (HAT) are increasingly focused on developing capabilities toward that end. A critical foundation of these technologies is the awareness of the underlying context to accurately interpret machine-processed warnings and recommendations. In this sense, context can be a dynamic characteristic of the operating environment and demands a multi-analytic approach. In this paper, we describe US doctrine that formulates capability requirements for operations in the information environment. We then describe a promising social computing approach that brings together information retrieval strategies using multimedia sources that include text, video, and imagery. Social computing is used in this case to increase awareness of societal dynamics at various scales that influence and impact military operations in both the physical and information domains. Our focus, content based information retrieval and multimedia analytics, involves the exploitation of multiple data sources to deliver timely and accurate synopses of data that can be combined with human intuition and understanding to develop a comprehensive worldview. © 2018 SPIE. Downloading of the abstract is permitted for personal use only. Content-based multimedia analytics: US and NATO research information environment; multimedia analytics; social computing; text and video analytics Artificial intelligence; Digital storage; Information retrieval; Learning systems; Military operations; Content-based information retrieval; Content-based multimedia; Information environment; Machine learning software; multimedia analytics; Social computing; Social networking sites; Video analytics; Social sciences computing",Financial management
2105,Smart hospitality—Interconnectivity and interoperability towards an ecosystem,"The Internet and cloud computing changed the way business operate. Standardised web-based applications simplify data interchange which allow internal applications and business partners systems to become interconnected and interoperable. This study conceptualises the smart and agile hospitality enterprises of the future, and proposes a smart hospitality ecosystem that adds value to all stakeholders. Internal data from applications among all stakeholders, consolidated with external environment context form the hospitality big data on the cloud that enables members to use business intelligence analysis to generate scenarios that enhance revenue management performance. By connecting to smart tourism network, sensors and content extractors can assist to collect external information, and beacons to deliver context-based promotion messages and add value. The proposed model enables fully integrated applications, using big data to enhance hospitality decision making as well as strengthen competitiveness and improve strategies performance. © 2017 Elsevier Ltd Smart hospitality—Interconnectivity and interoperability towards an ecosystem Big data; Hospitality ecosystem; ICT; Interconnectivity and interoperability; Sensor and beacon; Smart hospitality ",Stakeholder management
2106,"8th International Conference on the Economies of the Balkan and Eastern European Countries in the Changing World, EBEEC 2016","The proceedings contain 61 papers. The special focus in this conference is on Economies of the Balkan and Eastern European Countries in the Changing World. The topics include: The Barriers for the Development of the Social Cooperative Enterprises in Greece; tourism and Ecologically Sensitive Areas: The Case the Prefecture of Preveza from Citizens’ Point of View; the Analysis of Tourism and Economic Growth Relationship in Central and Eastern European Countries; monitoring the Impacts of Corporate Activities on Environment in Tourism and Communicating Through Corporate Reports; wine Tourism Development in Northern Greece: Evidence From Ktima Gerovassiliou; trends in the Balkans and Eurasia Under Globalization: Geostrategic Analysis; selected Aspects of Human Resource Management Supporting and Limiting Organizational Learning; the Agile Revolution in Software Engineering; metaeconomic Approaches in Global Management; defining Decision-Making Process for Student Learning Support System; Knowledge Transfer and Trust Among Partners: The Case of Greek IJVs; examination of Two Companies’ Liability Management; marketing Expansion Strategies in Multinational Marketing: The Role of e-Business; is Marketing Research Still Necessary in the Digital World?; conspicuous Consumption in Relation to Self-Esteem, Self-Image and Social Status: An Empirical Study; measuring Citizens Satisfaction From Public Sector Organizations in Greece: The Case of the Regional District of Xanthi; the Impact of Scandinavian Inward Foreign Direct Investment on the Baltic States; social Marketing in the Public Sector - The Case of a Municipality in Croatia; decision Support System for Marketing Metrics; designing an Intelligent Digital Signage System for Business Marketing; research Areas in Big Data Analytics Studies; preface. 8th International Conference on the Economies of the Balkan and Eastern European Countries in the Changing World, EBEEC 2016  ",Strategic alignment
2107,"Digital assurance for oil and gas 4.0: Role, implementation and case studies","“Barrel through Byte” transformation is the new revolution in the oil and gas industry. Technologies such as the Internet of Things (IoT), Cloud, Blockchain, Data Analytics, Big Data, Robotics (autonomous vehicles, drones, RPA), Artificial Intelligence (AI), Machine Learning (ML), Digital Twins, AR/VR has catalyzed this transformation. For instance, the deployment of AI systems to help optimize Bakken Shale wells running on SRP's, implementation of Blockchain drilling for well construction activities, VR/AR visualization of Canada SAGD oilfields, proactive adjustments to ESP using real-time data analytics in Kazakhstan gas condensate wells are some of the recent transformations the industry has witnessed. As the Oil and Gas 4.0 simplifies, modernizing and securing the legacy environments for the new digital era, a robust Digital Assurance is essential. Digital Assurance is an exclusive digital service employing an end-to-end ecosystem approach with intelligent and automated processes to gain quality and speed for promoting faster business, technology change and better customer experience. The service shall offer evaluation of new tools and testing environments such as the cloud, software testing tools, mobile environment, and application performance. Digital Assurance is more than the traditional QA; the evolution of Agile methods with DevOps tools and techniques for software/simulator development and testing has made its deployment quick, safe and reliable. The end-to-end testing process serves to simulate the real user on-field scenarios and validate the system under test and its components for integration and data integrity. Leveraging Digital 2.0 transformations, smart oilfields (by implementing autonomous robots) can eliminate human intervention in potential risk areas and thereby providing a safe, compliant workplace for engineers, managers, crewmembers and other personnel. Especially, robotics has potential applications in inspection, maintenance, and safety operations. In alignment with this, our personalized robotic testing solution can automate various peripheral devices and has resulted in reduced testing cycle time up to 30% and cost by 40%. The solution enables automated end-to-end IoT device interactions spanning smart devices such as camera, thermostat, sensors and web applications in oil and gas facilities. Adding, our various other in-house digital framework and solutions such as Big Data validation framework, cloud-based on-demand testing, and automated test scriptwriters have eased digital transformations. Digital Assurance by rigorous testing practices can also immune oil and gas majors from the cyber threats for the cloud (grid, storage) and blockchain. To overcome such compromises in hardware failure, our smart and innovative IoT Test Workbench solution for integration testing can reduce the hardware dependency up to 100%. This novel solution virtualizes Rapid IoT Solution development replacing smart devices during testing under extreme conditions such as HPHT wells and permafrost conditions in the Arctic Circle. Furthermore, Digital Assurance compliance coupled with digitalization knowledge and experience can play a paramount role in averting disasters such as GOM deepwater explosion and Montara oil spill in the future. Copyright 2019, Society of Petroleum Engineers Digital assurance for oil and gas 4.0: Role, implementation and case studies  Application programs; Arctic vehicles; Big data; Blockchain; Construction industry; Data Analytics; Data visualization; Digital storage; Digital twin; Gas condensates; Gas industry; Gases; Human resource management; Internet of things; Metadata; Oil fields; Oil spills; Real time systems; Robotics; Well testing; Application performance; Development and testing; Digital transformation; Gas-condensate wells; Internet of thing (IOT); Knowledge and experience; Oil and Gas Industry; Well construction activities; Integration testing",Governance
2108,Leading Growth through the Digital Leader,"As Millennials, and next in line the iGen, begin to take over the workforce, they are bringing with them leadership trends that will shape the future of organizational leadership. Modern organizations must respond to an increased pace of the workplace, and the nature of executives’ tasks is increasingly complex. Despite the evidence suggesting that focusing on growth versus performance will lead to better long-term performance, our business schools continue to emphasize managing performance instead of leading growth. Trends such as real-time feedback, agile networks of teams, advanced people analytics, micro-learning, personalized learning, and artificial intelligence enable the digitally minded leader to shape the future of leadership. We know that many leadership dimensions have a causal relationship with desirable organizational outcomes, and that the foundational pillars of leadership, such as shared values and vision, talent development, change management, and reward and recognition, will likely continue to drive these outcomes. However, how we lead in these areas is changing. The current paper explored these phenomena based on current literature with an eye toward the future. © 2018 University of Phoenix Leading Growth through the Digital Leader  ",Risk management
2109,Advanced Customer Analytics: Strategic Value Through Integration of Relationship-Oriented Big Data,"As more firms adopt big data analytics to better understand their customers and differentiate their offerings from competitors, it becomes increasingly difficult to generate strategic value from isolated and unfocused ad hoc initiatives. To attain sustainable competitive advantage from big data, firms must achieve agility in combining rich data across the organization to deploy analytics that sense and respond to customers in a dynamic environment. A key challenge in achieving this agility lies in the identification, collection, and integration of data across functional silos both within and outside the organization. Because it is infeasible to systematically integrate all available data, managers need guidance in finding which data can provide valuable and actionable insights about customers. Leveraging relationship marketing theory, we develop a framework for identifying and evaluating various sources of big data in order to create a value-justified data infrastructure that enables focused and agile deployment of advanced customer analytics. Such analytics move beyond siloed transactional customer analytics approaches of the past and incorporate a variety of rich, relationship-oriented constructs to provide actionable and valuable insights. We develop a customized kernel-based learning method to take advantage of these rich constructs and instantiate the framework in a novel prototype system that accurately predicts a variety of customer behaviors in a challenging environment, demonstrating the framework’s ability to drive significant value. Copyright © Taylor & Francis Group, LLC. Advanced Customer Analytics: Strategic Value Through Integration of Relationship-Oriented Big Data  Competition; Data integration; Digital storage; Sales; Sustainable development; Agile deployments; Big Data Analytics; Customer behavior; Data infrastructure; Dynamic environments; Kernel-based learning; Relationship marketing; Sustainable competitive advantages; Big data",Strategic alignment
2110,A knowledge management and sharing business model for dealing with disruption: The case of Aramex,"The current study investigates the global logistics player Aramex and how it deals with disruptive technologies. In particular, it focuses on the unique business model that the case organisation has adopted and that allows for disruption to be managed through collaborative knowledge management. The study is qualitative and uses video, document/text and interview material for the case organisation. Data was analysed in two coding stages to derive at the categories/themes that have the most explanatory power. Aramex, a global logistics providers originating from the Middle East, is utilised to illustrate their business concept that determines and permeates their organisational culture. Disruptive technological innovations, such as Big Data Analytics, new hardware, smart apps that can connect individuals to the corporation in different contexts, feature strongly, to manage their collective knowledge of innovation and value creation. Disruption is embedded in their business model and an important part of their business operations. © 2017 Elsevier Inc. A knowledge management and sharing business model for dealing with disruption: The case of Aramex Agile; Aramex; Asset-Light; Business Model; Disruptive Technologies; Knowledge sharing and management ",Capacity management
2111,Midas: Towards an Interactive Data Catalog,"This paper presents the ongoing work on the Midas polystore system. The system combines data cataloging features with ad-hoc query capabilities and is specifically tailored to support agile data science teams that have to handle large datasets in a heterogeneous data landscape. Midas consists of a distributed SQL-based query engine and a web application for managing and virtualizing datasets. It differs from prior systems in its ability to provide attribute level lineage using graph-based virtualization, sophisticated metadata management, and query offloading on virtualized datasets. © 2019, Springer Nature Switzerland AG. Midas: Towards an Interactive Data Catalog Data catalog; Metadata management; Polystore Data Science; Graphic methods; Health care; Large dataset; Metadata; Search engines; Attribute levels; Data catalog; Data cataloging; Heterogeneous data; Large datasets; Metadata management; Polystore; WEB application; Information management",Risk management
2113,Real-time detection of under-reamer failure: An example of agile data analytics development and deployment,"In response to the lower for longer oil price environment and the rapidly changing digital opportunities, agile development and deployment of digital solutions using data analytics can become essential in collecting short term business value through cost reduction and building flexible technology infrastructure for long term digital transformation. The new agile development/deployment adopted by the company consists of the following process: identify operational need, develop digital solutions through a combination of conventional physics based model and machine learning techniques, deliver through a flexible real-time web platform, and feedback learnings from deployment to facilitate next round of development. ReamerVision, the real-time early detection of under-reamer failure during hole enlargement while drilling at deepwater Gulf of Mexico is one of the successful examples: Operational need identified: Two active cutting structures (drill bit and expandable under-reamer) are often used for hole enlargement while drilling in Deepwater. A potential operational failure is the under-reamer may subject to premature cutter damage while drill bit is still in good condition. Without detection of this failure, an undergauge hole might be drilled and causes difficulty for subsequent casing running. Data analytics methodology: Historical data of both surface and downhole mechanics data are collected. Several metrics based on physics model are constructed to quantify cutter conditions, drilling efficiency, and formation variation. Supervised learning of historical dataset processed by those metrics yields thresholds for failure detection in real-time. Deliverable format: a web-based real-time data platform is constructed for deployment of this technology. A workflow is established between remote control center, drilling engineers, and domain-experts to establish real-time monitoring of under-reamer condition and provide actionable information to operation. Continue improvement: since April 2016, ReamerVision has been deployed to 27 deepwater reamer runs as a risk-mitigation tool. Positive feedbacks from end-users are utilized for calibration of model and development of new features. Copyright © 2018, The Society of Naval Architects and Marine Engineers. Real-time detection of under-reamer failure: An example of agile data analytics development and deployment Data analytics; Deepwater drilling; Machine learning; Underreamer Artificial intelligence; Bits; Cost reduction; Drills; Failure (mechanical); Infill drilling; Learning systems; Metadata; Offshore oil wells; Outsourcing; Reamers; Remote control; Signal detection; Data analytics; Digital transformation; Flexible technologies; Machine learning techniques; Operational failures; Physics-based modeling; Real time monitoring; Underreamer; Deepwater drilling",Monitoring and control
2115,Technology adoption in the SME sector for promoting agile manufacturing practices,"The Small and Medium Enterprises have always been constrained by technology and resources. In many instances, information technology has not penetrated enough to provide competitive advantage, and this is often due to skills and financial constraints. The study and analysis, in this paper is with regard to adoption of Information technology in the SME sector and the benefits obtained therein. The central entity is a large auto manufacturer, which has adopted Agile manufacturing practices and needs very tight integration of Information technology, manufacturing plans and market responses. This needs the downstream vendors, to respond in a timely manner, in order to achieve agility in the entire supply chain. This study involves, implementing information technology measure in select small enterprises and their response has been recorded in terms of lead time for production and inventory planning. This paper also discusses few more suggestions to achieve further improvement in the metrics. © Springer Nature Singapore Pte Ltd. 2019. Technology adoption in the SME sector for promoting agile manufacturing practices Agile cloud; Agile manufacturing; Data analytics; Supply chain management Competition; Intelligent computing; Supply chain management; Agile manufacturing; Competitive advantage; Data analytics; Financial constraints; Manufacturing plans; Production and inventory; Small and medium enterprise; Technology adoption; Manufacture",Financial management
2116,Sprint performance forecasts in agile software development: The effect of futurespectives on team-driven dynamics,"In agile software development, the sprint performances and dynamics of teams often imply tendencies for the success of a project. Post mortem strategies, e.g., retrospectives help the team to report and share individually gained experiences (positives and negatives) from previous sprints, and enable them to use these experiences for future sprint planning. The interpretation of effects on sprint performance is often subjective, especially with concern to social-driven factors in teams. Involving strategies from predictive analytics in sprint retrospectives could reduce potential interpretation gaps of dynamics, and enhance the pre-knowledge, also awareness situation when preparing for the next sprint. In a case study involving 15 software projects with a total of 130 involved undergraduate students, we investigated the post-effects on team performances and behavioral-driven factors when providing predictive analytics in retrospectives. Besides measures for productivity, we consider human factors, e.g., team structures, communication, meetings and mood affects in teams as well as project success metrics. We developed a unique JIRA plugin called ProDynamics that collects performance information from projects and derives trend-insights for next sprints. The ProDynamics plugin enables the use of a times series and neural network model within a JIRA system to interpret factorial dependencies and behavioral pattern, thus to show the next sprint course of a team. © 2019 Knowledge Systems Institute Graduate School. All rights reserved. Sprint performance forecasts in agile software development: The effect of futurespectives on team-driven dynamics Agile; Data analytics; Futurespectives; Human factors; Sprint performances; Team dynamics Data Analytics; Dynamics; Human engineering; Knowledge engineering; Predictive analytics; Students; Agile; Agile software development; Behavioral patterns; Futurespectives; Neural network model; Sprint performances; Team dynamics; Undergraduate students; Software design",Financial management
2117,The design and evaluation of an antimicrobial resistance surveillance system for neonatal intensive care units in Iran,"Introduction: Neonatal intensive care units (NICUs) have complex patients in terms of their diagnoses and required treatments. Antimicrobial treatment is a common therapy for patients in NICUs. To solve problems pertaining to empirical therapy, antimicrobial stewardship programs have recently been introduced. Despite the success of these programs in terms of data collection, there is still inefficiency in terms of analyzing and reporting the data. Thus, to successfully implement these stewardship programs, the design of antimicrobial resistance (AMR) surveillance systems is recommended as a first step. As a result, this study aimed to design an AMR surveillance system for use in the NICUs in northwestern Iranian hospitals to cover these information gaps. Methods: The recommended system is compatible with the World Health Organization (WHO) guidelines. The business intelligence (BI) requirements were extracted in an interview with a product owner (PO) using a valid and reliable checklist. Following this, an AMR surveillance system was designed and evaluated in relation to user experiences via a user experience questionnaire (UEQ). Finally, an association analysis was performed on the database, and the results were reported by identifying the important multidrug resistances in the database. Results: A customized software development methodology was proposed. The three major modules of the AMR surveillance are the data registry, dashboard, and decision support modules. The data registry module was implemented based on a three-tier architecture, and the Clinical Decision Support System (CDSS) and dashboard modules were designed based on the BI requirements of the Scrum product owner (PO). The mean values of UEQ measures were in a good range. This measures showed the suitable usability of the AMR surveillance system. Conclusion: Applying efficient software development methodologies allows for the systems’ compatibility with users’ opinions and requirements. In addition, the construction of interdisciplinary communication models for research and software engineering allows for research and development concepts to be used in operational environments. © 2018 Elsevier B.V. The design and evaluation of an antimicrobial resistance surveillance system for neonatal intensive care units in Iran Drug resistance; Informatics; Iran; Microbial; Neonatal; Surveillance Anti-Bacterial Agents; Decision Support Systems, Clinical; Drug Resistance, Bacterial; Humans; Infant, Newborn; Intensive Care Units, Neonatal; Iran; Artificial intelligence; Client server computer systems; Decision support systems; Diagnosis; Drug therapy; Intensive care units; Microorganisms; Neonatal monitoring; Security systems; Space surveillance; amikacin; ampicillin; antibiotic agent; cefazolin; cefepime; cefixime; cefotaxime; ceftazidime; ceftizoxime; ceftriaxone; ciprofloxacin; clindamycin; cotrimoxazole; erythromycin; gentamicin; imipenem; metronidazole; nalidixic acid; oxacillin; tetracycline; vancomycin; antiinfective agent; Drug resistance; Informatics; Iran; Microbial; Neonatal; antibiotic resistance; Article; biosurveillance; checklist; clinical decision support system; data base; human; interview; Iran; knowledge discovery; multidrug resistance; neonatal intensive care unit; personal experience; practice guideline; priority journal; register; software; world health organization; newborn; Software design",Strategic alignment
2120,Smart learning analytics: Conceptual modeling and agile engineering,"Learning analytics focuses on collecting, cleaning, processing, visualization and analyzing teaching and learning related data or metrics from a variety of academic sources. Our vision for engineering of smart learning analytics – the next generation of systems and tools for learning analytics - is based on the concept that this technology should strongly support “smartness” levels of smart academic institutions such as adaptivity, sensing, inferring, anticipation, self-learning, and self-organization. This paper presents the up-to-date findings and outcomes of research, design and development project at the InterLabs Research Institute at Bradley University (Peoria, IL, YOU.S.A.) that is focused on conceptual modeling of smart learning analytics systems, including identification of goals, objectives, features and functions, main components, inputs and outputs, hierarchical and smartness levels, mathematical methods and algorithms for those systems. Agile software engineering approach has been used for a development of a series of software prototypes to verify the design and development process and validate the obtained outcomes for smart learning analytics systems. © 2019, Springer International Publishing AG, part of Springer Nature. Smart learning analytics: Conceptual modeling and agile engineering Conceptual modeling; Smart education; Smart learning analytics Agile manufacturing systems; Data mining; Data visualization; E-learning; Functions; Hierarchical systems; Software prototyping; Academic institutions; Agile software engineering; Conceptual model; Design and Development; Design and development process; Learning analytics; Mathematical method; Teaching and learning; Engineering education",Risk management
2121,Implementing self-service business analytics supporting lean manufacturing: A state-of-the-art review,"Piloting lean manufacturing projects requires dynamically tailoring suitable sets of metrics. Quick turnaround in implementing such metrics is critical, as the typical duration of a Lean Six Sigma project is 3 to 6 months. Self-service Business Analytics (SSBA) can provide managers with the much-needed flexibility to efficiently design and redesign comprehensive metrics in fragmented information system contexts. A review of state-of-the-art practices for SSBA implementation is performed, which lays down the foundations for an upcoming framework geared towards lean manufacturing. Key SSBA planning and architecture findings are summarized. Practical evaluation of the framework in a complex information system landscape through Design Science Research (DSR) is projected with the Canadian division of an international steel parts manufacturing company © 2018 Implementing self-service business analytics supporting lean manufacturing: A state-of-the-art review BPMN; Constructive Research; Implementation; Lean manufacturing; Self-service analytics Advanced Analytics; Agile manufacturing systems; Industrial research; Information systems; Information use; Process engineering; BPMN; Complex information systems; Design science researches (DSR); Implementation; Manufacturing companies; Manufacturing projects; Self-service analytics; State-of-the art reviews; Lean production",Governance
2122,Business Intelligence for Small and Medium-Sized Enterprises: An Agile Roadmap toward Business Sustainability,"Business intelligence (BI) has evolved over several years as organizations have extended their online transaction processing (OLTP) capabilities and applications to support their routine operations. With online analytical processing (OLAP), organizations have also established the capability to extract internal and external data from a variety of sources to specifically obtain intelligence about non-routine and often less-structured arrangements. BI therefore refers to applications and technologies that are used to gather, provide access to, and analyze data and information about the operations of an organization. It has the capability of providing comprehensive insight into the more volatile factors affecting the business and its operations, thereby facilitating enhanced decision-making quality and contributing to the creation of business value. Larger and more sophisticated organizations have long been exploiting these capabilities. Business Intelligence for Small and Medium-Sized Enterprises (SMEs) guides SMEs in replicating this experience to provide an agile roadmap toward business sustainability. The book points out that successful BI implementations have generated significant increases in revenue and cost savings, however, the failure rates are also very high. More importantly, it emphasizes that a full range of BI capabilities is not the exclusive purview of large organizations. It shows how SMEs make extensive use of BI techniques to develop the kind of agility endowing them with the organizational capability to sense and respond to opportunities and threats in an increasingly dynamic business environment. It points to the way to a market environment in which smaller organizations could have a larger role. In particular, the book explains that by establishing the agility to leverage internal and external data and information assets, SMEs can enhance their competitiveness by having a comprehensive understanding of the key to an agile roadmap for business sustainability. © 2020 by Taylor & Francis Group, LLC. Business Intelligence for Small and Medium-Sized Enterprises: An Agile Roadmap toward Business Sustainability  ",Governance
2123,A predictive government decision based on citizen opinions - tools & results,"Research on citizen satisfaction with respect to public policies has significant public and political value. Politicians are generally seeking effective public policies that favourably impacts citizens' satisfaction. Citizen satisfaction index is a plausible mechanism for public policy makers to monitor and evaluate the public policies. While surveys on citizen satisfaction are common among agile and progressive public administration and governments, automating the computation of citizen's' satisfaction is challenging. Given that surveys and evaluations related to citizen satisfaction are retrospective, remedial actions when necessary are always somewhat late. We describe in this poster a predictive analytics framework for citizen satisfaction with respect to public policy based on the previous citizen sentiments past related policies. © 2018 Copyright is held by the owner/author(s). A predictive government decision based on citizen opinions - tools & results Citizen Satisfaction; Decision Analytics; Government Decision Support; Opinion Mining; Policy Acceptance Prediction; Policy Aspects; Semantic Relatedness; Sentiment Analysis; Social Media; Topic Modeling; Unstructured Text Analysis Computation theory; Data mining; Decision support systems; Government data processing; Natural language processing systems; Predictive analytics; Semantics; Sentiment analysis; Surveys; Citizen Satisfaction; Decision Analytics; Government decisions; Semantic relatedness; Social media; Topic Modeling; Unstructured texts; Public administration",Strategic alignment
2124,Digital lean cyber-physical production systems: The emergence of digital lean manufacturing and the significance of digital waste,"This paper explores the emergence of the next cyber/digital frontier for lean manufacturing practices. It focuses on (a) the new capabilities of information and operational technologies (ITs/OTs) for proactively detecting and eliminating potential ‘physical waste’ in production processes, preventing its manifestation in the real world through powerful virtual models and simulations as well as real-time performance monitoring systems based on advanced data analytics, and (b) on identifying and eliminating ‘digital waste’ that may come into existence in the cyber world due to the non-use (e.g. lost digital opportunities) and/or over-use (e.g. abused digital capabilities) of new digital/smart manufacturing technologies. © 2018, IFIP International Federation for Information Processing. Digital lean cyber-physical production systems: The emergence of digital lean manufacturing and the significance of digital waste Cyber-Physical Production Systems; Digital Lean Enterprise; Digital Lean Manufacturing; Digital manufacturing; Digital waste; Industry 4.0; Lean manufacturing; Smart manufacturing; Waste Agile manufacturing systems; Cyber Physical System; Industry 4.0; Real time systems; Wastes; Digital manufacturing; Digital wastes; Lean enterprise; Production system; Smart manufacturing; Lean production",Monitoring and control
2127,Real-time bit wear monitoring and prediction using surface mechanics data analytics: A step toward digitization through agile development,"Severe bit damage is a known issue in west Texas land drilling due to abrasive sand formation and interbedded hard stringers. Operational performance and rig cost are often impacted by penalty fee of bit DBR (damage beyond repairable), low ROP (rate of penetration) with worn bit, and inefficient decision-making on tripping. A real-time data analytics application is developed aiming to provide actionable information to operation to expedite decision making process. A historical dataset of surface mechanics data and bit records is collected from 40 bit runs drilled in 2016 and early 2017. A hybrid data analytics procedure consisting of conventional physical modeling of drilling mechanics and supervised learning using machine learning technique is conducted to separate bit failure pattern from normal formation transition and drilling parameters adjustment. A metric based algorithm is constructed for real-time monitoring of bit drilling performance and early warning on bit cutter wear conditions. A web-based real-time software is developed and field trialed on three wells with satisfactory results. Subsequent deployments in DART (Drilling Automation Remote Technology) center and field offices have been quickly rolled out for five rigs in west Texas. Positive feedback is generated from operation and engineers. Attributed to the success of agile development framework and adaptive software architecture, other advisory mode features such as motor life monitoring, smart-tripping evaluation, and sliding diagnosis etc. are under development. The application discussed in this paper combines expert's domain knowledge with machine learning techniques and provides actionable information to support on-site operational decisions. The development and deployment of this application follows an agile mode innovation framework, through which operational need and technical solution are quickly bridged and tangible business value is able to be delivered in short term. Copyright 2018, IADC/SPE Drilling Conference and Exhibition. Real-time bit wear monitoring and prediction using surface mechanics data analytics: A step toward digitization through agile development  Abrasives; Data Analytics; Decision making; Learning algorithms; Machine learning; Mechanics; Stringers; Wear of materials; Decision making process; Drilling performance; Machine learning techniques; Operational decisions; Operational performance; Rate of penetration; Real time monitoring; Technical solutions; Infill drilling",Strategic alignment
2130,Conceptual Modeling Education as a “Design Problem”,"This article frames Conceptual Modeling education as a design problem, in the sense of the Design Science research framework, motivated by student preconceptions and oversimplifications causing a gap between how the discipline is perceived at bachelor level and the holistic understanding of model value that is required for research work. The treatment to this design problem must comprise teaching approaches and artifacts capable of positioning Conceptual Modeling as a standalone discipline having a value proposition for any application domain, rather than a technique subordinated to other disciplines. The underpinning thesis is that modeling languages should be primarily understood as purposeful knowledge schemas that can be subjected to agile adaptations in support of model-driven systems or knowledge processes, by analogy to how a database schema is evolved in response to changing requirements of a data-driven system or data analytics needs. This thesis is supported by enablers provided by the Open Models Laboratory and the Agile Modeling Method Engineering framework – resources that support the development of treatments to the design problem framed by the article. © 2019 Robert Andrei Buchmann et al. Conceptual Modeling Education as a “Design Problem” Agile Modeling Method Engineering; Conceptual Modeling Education; Design Science; OMiLAB; Teaching Artifacts ",Strategic alignment
2131,Information technology integration for accelerated knowledge sharing practices: Challenges and prospects for small and medium enterprise,"This paper argues that business enterprises in this competitive global market cannot compete and remain sustainable without effective knowledge sharing to improve business intelligence processes. The central argument hinges on the deployment and use of information technology (IT) as strategic tools to promote business decision making through quick business data analysis and dissemination of business ideas across business units and locations. The study reiterated the critical role IT plays in facilitating a culture of organizational learning and knowledge sharing practices. The study utilized surveys and questionnaires that were distributed to 230 small and medium enterprises (SMEs), and both descriptive and inferential statistics were used to present the results. Findings showed that firms are still using one-on-one meeting to share knowledge, while knowledge sharing activities are controlled through a rigid and inflexible process at the top management level, thereby hindering knowledge flow that is crucial for real-Time decision making. The advances in IT have not been used advantageously to improve knowledge sharing and to advance business management. The paper concludes that without strong positive correlation between IT infrastructure integration, and communication strategies and knowledge sharing, the SMEs may not be able to compete in a highly competitive knowledge economy. Consequently, they may lose leverage to another competitor with more robust and mature IT infrastructure alignment for sharing business analytics and intelligence efficiently. A technologically driven, open, and informal approach to knowledge sharing for productive and innovative engagement is recommended. Furthermore, the use of IT that can promote agile and real-Time knowledge sharing is recommended. © 2019 LLC CPC Business Perspectives. All rights reserved. Information technology integration for accelerated knowledge sharing practices: Challenges and prospects for small and medium enterprise Business information systems; Business intelligence information; Information technology; KM; Knowledge sharing; SMEs ",Governance
2132,Evolving Green Chemistry Metrics into Predictive Tools for Decision Making and Benchmarking Analytics,"Designing efficient and green approaches to complex molecules is a challenge faced by any organization seeking to deliver modern pharmaceutical compounds to patients. The outcome of any route design effort, in terms of efficiency, is largely governed by the disconnections and synthetic strategies generated during the process of route scouting, coupled with the decisions made by the individuals responsible for the research. In this article, we delineate an approach, based on historical data, capable of quantifying the probable efficiency of a proposed synthesis prior to any research being conducted. This decision-making strategy can be used to both aid the decision-making process of innovators and to benchmark the outcome performance of the developed process, improving the efficiency of any manufacturing process developed. Through improved decision making and benchmarking, this approach could minimize the environmental impact of pharmaceutical production. © 2017 American Chemical Society. Evolving Green Chemistry Metrics into Predictive Tools for Decision Making and Benchmarking Analytics Green chemistry; Option analysis; PMI; Predictive analytics; Synthetic efficiency Benchmarking; Chemical analysis; Efficiency; Environmental impact; Predictive analytics; Decision making process; Decision-making strategies; Green chemistry; Manufacturing process; Option analysis; Pharmaceutical compounds; Pharmaceutical production; Synthetic efficiency; Decision making",Stakeholder management
2134,New Trends in Mobile Technologies Education in Slovakia: An Empirical Study,"In this paper, we propose a concept of teaching mobile technologies development as a specific application of the software development lifecycle. We have deployed this concept into the terms of the study program called Business Informatics at the Technical University of Kosice, in Slovakia. Students get familiar with some traditional methods of software development life cycle like waterfall and with Scrum as a representative of the Agile methodologies. We presented the obtained results and experiences from three courses creating a necessary basic for one direction of our master and bachelor's thesis. Our motivation was to cover all necessary stages like design with User Experience, development in line with current trends, testing using appropriate methods and metrics, deployment with a tracking system, monetization and appropriate licensing. From the hardware point of view, we offer different intelligent devices like wearable sensors, bracelets or watches to create various mobile applications for data analytics and visualization methods. Moreover, we invented relevant lectures with some techniques from active learning area like retrieval practice and case-based learning. During first 2 years we have tested suitability of various supporting tools like e.g. YouTrack, JIRA, Slack or Sli.do to improve the students' readiness for real IT projects by simulating working environment in the IT companies. © 2018 IEEE. New Trends in Mobile Technologies Education in Slovakia: An Empirical Study  Application programs; Data visualization; E-learning; Life cycle; Telecommunication equipment; Agile Methodologies; Business informatics; Case based learning; Intelligent devices; Software development life cycle; Technical universities; Visualization method; Working environment; Software design",Governance
2136,A quality model for actionable analytics in rapid software development,"Background: Accessing relevant data on the product, process, and usage perspectives of software as well as integrating and analyzing such data is crucial for getting reliable and timely actionable insights aimed at continuously managing software quality in Rapid Software Development (RSD). In this context, several software analytics tools have been developed in recent years. However, there is a lack of explainable software analytics that software practitioners trust. Aims: We aimed at creating a quality model (called Q-Rapids quality model) for actionable analytics in RSD, implementing it, and evaluating its understandability and relevance. Method: We performed workshops at four companies in order to determine relevant metrics as well as product and process factors. We also elicited how these metrics and factors are used and interpreted by practitioners when making decisions in RSD. We specified the Q-Rapids quality model by comparing and integrating the results of the four workshops. Then we implemented the Q-Rapids tool to support the usage of the Q-Rapids quality model as well as the gathering, integration, and analysis of the required data. Afterwards we installed the Q-Rapids tool in the four companies and performed semi-structured interviews with eight product owners to evaluate the understandability and relevance of the Q-Rapids quality model. Results: The participants of the evaluation perceived the metrics as well as the product and process factors of the Q-Rapids quality model as understandable. Also, they considered the Q-Rapids quality model relevant for identifying product and process deficiencies (e.g., blocking code situations). Conclusions: By means of heterogeneous data sources, the Q-Rapids quality model enables detecting problems that take more time to find manually and adds transparency among the perspectives of system, process, and usage. © 2018 IEEE. A quality model for actionable analytics in rapid software development Agile; H2020; Q-rapids; Quality model; Rapid software development; Software analytics; Software quality Application programs; Data integration; Petroleum reservoir evaluation; Quality assurance; Quality control; Software design; Agile; H2020; Q-rapids; Quality modeling; Software Quality; Computer software selection and evaluation",Strategic alignment
2137,Building novel capabilities to enable business intelligence agility: results from a quantitative study,"The class of business intelligence (BI) systems is widely used to guide decisions in all kinds of organizations and across hierarchical levels and functions. Organizations have launched many initiatives to accomplish adequate and timely decision support as an important factor to achieve and sustain competitive advantage. Given today’s turbulent environments it is increasingly challenging to bridge the gap between establishing a long-term strategy and quickly adopting to the dynamics in market competition. BI must address this field of tension as it was originally used to retrospectively reflect an organization’s performance and build upon stability and efficiency. This study aims to understand and achieve an agile BI from a dynamic capability perspective. Therefore, we investigate how dynamic BI capabilities, i.e., adoption of assets, market understanding, and intimacy as well as business operations, impact the agility of BI. Starting from a literature review of dynamic capabilities in information systems and BI, we propose hypotheses to connect dynamic BI capabilities with the agility to provide information. The derived hypotheses were tested using partial least squares structural equation modeling on data collected in a questionnaire-based survey. The results show that the lens of dynamic capabilities provides useful means to foster BI agility. The study identifies that technological advancements like in-memory technology seem to be a technology enabler for BI agility. However, an adequate adoption and integration of BI assets as well as the focus on market orientation and business operations are crucial to reach BI agility. © 2017, Springer-Verlag GmbH Germany. Building novel capabilities to enable business intelligence agility: results from a quantitative study Agility; Business intelligence; Dynamic capabilities ",Financial management
2141,Application of Lean principles and software solutions for maintenance records in continuing airworthiness management organisations,"This paper discusses how the processes of Continuing Airworthiness Management Organisations can be made more efficient by applying Lean principles and digital solutions, to obtain costs savings and improved compliance standards. Lean management is the process of eliminating waste by maximising value through balancing workloads and cutting out inefficient processes. Though Lean was initially applied to manufacturing production lines, an opportunity exists to apply the principles of Lean in airworthiness management. One such aspect of airworthiness management is the processing of records which is an essential part of safety and compliance. Software solutions designed with Lean philosophy and focus on airworthiness management processes can provide benefits in managing the flow of records and extracting actionable information from them for cost reduction and improving safety. Further value can be extracted by applying additional software solutions that will lay a strong foundation for machine learning and predictive analytics - taking aircraft maintenance from a reactive model towards a proactive model. © Royal Aeronautical Society 2018. Application of Lean principles and software solutions for maintenance records in continuing airworthiness management organisations airworthiness management; Lean; software; workflows Agile manufacturing systems; Application programs; Computer software; Computer software maintenance; Cost reduction; Learning systems; Predictive analytics; Regulatory compliance; Aircraft maintenance; Digital solutions; Lean; Maintenance records; Management process; Production line; Software solution; Work-flows; Waste management",Strategic alignment
2145,Big Data: A Tutorial-Based Approach,"Big Data: A Tutorial-Based Approach explores the tools and techniques used to bring about the marriage of structured and unstructured data. It focuses on Hadoop Distributed Storage and MapReduce Processing by implementing (i) Tools and Techniques of Hadoop Eco System, (ii) Hadoop Distributed File System Infrastructure, and (iii) efficient MapReduce processing. The book includes Use Cases and Tutorials to provide an integrated approach that answers the 'What', 'How', and 'Why' of Big Data. Features Identifies the primary drivers of Big Data Walks readers through the theory, methods and technology of Big Data Explains how to handle the 4 V's of Big Data in order to extract value for better business decision making Shows how and why data connectors are critical and necessary for Agile text analytics Includes in-depth tutorials to perform necessary set-ups, installation, configuration and execution of important tasks Explains the command line as well as GUI interface to a powerful data exchange tool between Hadoop and legacy r-dbms databases. © 2019 by Taylor & Francis Group, LLC. Big Data: A Tutorial-Based Approach  ",Strategic alignment
2147,"3rd International Conference of Reliable Information and Communication Technology, IRICT 2018","The proceedings contain 103 papers. The special focus in this conference is on Reliable Information and Communication Technology. The topics include: A systematic mapping study on microservices; a theoretical framework for improving software project monitoring task of Agile Kanban method; non-functional ontology requirements specifications: Islamic Banking domain; A proposed framework using exploratory testing to improve software quality in SME’s; data quality issues in big data: A review; comparative study of segmentation and feature extraction method on finger movement; realizing the value of big data in process monitoring and control: Current issues and opportunities; big data analytics in the malaysian public sector: The determinants of value creation; Comparing the performance of FCBF, Chi-Square and relief-F filter feature selection algorithms in educational data mining; the study of co-occurrences index’s keywords for Malaysian publications; the impacts of singular value decomposition algorithm toward indonesian language text documents clustering; pairwise test suite generation using adaptive teaching learning-based optimization algorithm with remedial operator; novel multi-swarm approach for balancing exploration and exploitation in particle swarm optimization; energy-efficient resource allocation technique using flower pollination algorithm for cloud datacenters; computational analysis of dynamics in an agent-based model of cognitive load and reading performance; analysis the arabic authorship attribution using machine learning methods: Application on islamic fatwā; a fuzz logic-based cloud computing provider’s evaluation and recommendation model; self-adaptive population size strategy based on flower pollination algorithm for T-way test suite generation; spell checker for somali language using Knuth-Morris-Pratt string matching algorithm; Schema proposition model for NoSQL applications. 3rd International Conference of Reliable Information and Communication Technology, IRICT 2018  ",Strategic alignment
2149,ACM International Conference Proceeding Series,The proceedings contain 34 papers. The topics discussed include: is virtual reality product development different? an empirical study on VR product development practices; ThrustHetero: a framework to simplify heterogeneous computing platform programming using design abstraction; an approach to identify use case scenarios from textual requirements specification; change-proneness of object-oriented software using combination of feature selection techniques and ensemble learning techniques; enhancing test cases generated by concolic testing; a knowledge centric approach to conceptualizing robotic solutions; making sense of actor behavior: an algebraic filmstrip pattern and its implementation; a software framework for adaptive signal analytics based on autonomic service components; modeling and coverage analysis of programs with exception handling; the value of software architecture recovery for maintenance; evolution traceability roadmap for business processes; key factors in scaling up agile team in matrix organization; and a case study exploring supply chain systems using actor based simulation. ACM International Conference Proceeding Series  ,Strategic alignment
2150,How does open government data driven co-creation occur? Six factors and a ‘perfect storm’; insights from Chicago's food inspection forecasting model,"It is becoming increasingly clear that the concepts of open government data (OGD) and co-creation are related; however, there is currently only limited empirical material available exploring the link between the two. This paper aims to help clarify the relationship between these two concepts by exploring a recently coined phenomenon: OGD-driven co-created public services. These services 1) utilize or are driven by OGD; 2) are co-created by stakeholders from different groups; and 3) produce public value for society. Due to the relative newness of the phenomenon an inductive exploratory case study is undertaken on Chicago's use of OGD in the co-creation of their food safety inspection forecasting model. This model forecasts critical food safety violations at food serving establishments and sends inspectors to the highest risk establishments first. The results of this exploratory work led to the discovery of a ‘perfect storm’ of six factors that seem to play a key role in allowing OGD-driven public service co-creation to take place. These factors are motivated stakeholders, innovative leaders, proper communication, an existing OGD portal, external funding, and agile development. © 2018 The Authors How does open government data driven co-creation occur? Six factors and a ‘perfect storm’; insights from Chicago's food inspection forecasting model Co-creation; Open government data; Predictive analytics; Public service innovation ",Stakeholder management
2152,Big data framework for agile business (BDFAB) As a basis for developing holistic strategies in big data adoption,"The Big Data Framework for Agile Business (BDFAB) is the result of exploration of the value of Big data technologies and analytics to business. BDFAB is based on literature review, modeling, experimentation and practical application. BDFAB incorporates multiple disciplines of Information Technology, Business Innovation, Sociology and Psychology (people and behavior, Social- Mobile media), Finance (ROI), Processes (Agile), User Experience, Analytics (descriptive, predictive and prescriptive) and Staff Up-skilling (HR). This paper presents the key elements of the framework comprising agile values, roles, building blocks, artifacts, conditions, agile practices and a compendium (repository). The building blocks themselves are made up of five modules: business decisions, Data-technology and analytics, user experience-operational excellence, quality dimensions and people-capabilities. As such, BDFAB exhibits an interdisciplinary approach to Big Data adoption in practice. © Springer International Publishing AG 2017. Big data framework for agile business (BDFAB) As a basis for developing holistic strategies in big data adoption  Predictive analytics; Building blockes; Business decisions; Business innovation; Data technologies; Literature reviews; Multiple disciplines; Operational excellence; Quality dimension; Big data",Value management
2153,Agile local governments: Experimentation before implementation,"This paper discusses how local governments can team up for joint service provision, be more adaptive towards new technological and organisational changes and introduce novel services following main industry trends (e.g. predictive analytics, autonomous vehicles and artificial intelligence). The conceptual approach is to use Public Value (PV) as the framework for the organisation and management of government performance, one of the most important successor ‘paradigmettes’ of the New Public Management (NPM). Based on the PV concept, the ‘adaptive model’ for local governments is introduced according to which each procured ICT solution is preceded by agile, open, bottom-up and experimental trial. This model is corroborated via recent empirical evidence from the case of Helsinki and Tallinn which was obtained by observing how city governments collaborate on joint innovation-lab-type structures and conduct agile trials in the field of smart mobility before traditional procurement. © 2017 Elsevier Inc. Agile local governments: Experimentation before implementation  ",Strategic alignment
2155,Agile Condor: A scalable high performance embedded computing architecture,"The Air Force Research Laboratory Information Directorate Advanced Computing and Communications Division is developing a new computing architecture, designed to provide high performance embedded computing (HPEC) pod solution to meet operational and tactical real-time processing intelligence surveillance and reconnaissance (ISR) missions. This newly designed system, Agile Condor, is a scalable and HPEC system based on open industry standards that will increase, far beyond the current state-of-the-art, computational capability within the restrictive size, weight and power constraints of unmanned aircraft systems' external 'pod' payloads. The objective with such a system is to explore and develop innovative system solutions to meet future Air Force real-time HPEC; e.g., multi-mission, multi-function ISR processing and exploitation. While the core compute capability can be placed in various environments, our baseline design utilizes a 12-inch diameter flight-certified aeronautics pod that is scalable in length. Agile Condor can be connected to external data sources, or the nose and tail can be made of Radio Frequency (RF) transparent material, enabling the use of various RF sensing technologies within the same aeronautics enclosure. Inside this pod is a lightweight, thermally-efficient industry standard 3U VPX conduction cooled (with unconditioned ambient air) chassis that supports the required board and interface hardware. Agile Condor brings high-performance computing closer to sensors and immediately enables future research and development efforts in neuromorphic computing and autonomous system operations. © 2015 IEEE. Agile Condor: A scalable high performance embedded computing architecture Autonomous Operations; Big Data; High Performance Computing; High-Performance Embedded Computing; Information Processing; Massive Analytics; Moving Target Indication; Pod-based Computing; Scalable Computing; Synthetic Aperture Radar Agile manufacturing systems; Big data; Data processing; Fighter aircraft; Military aviation; Research laboratories; Synthetic aperture radar; Unmanned aerial vehicles (UAV); Autonomous operations; Embedded computing; High performance computing; Massive Analytics; Moving target indication; Pod-based Computing; Scalable computing; Computer architecture",Capacity management
2157,2017 SIGED International Conference on Information Systems Education and Research,The proceedings contain 9 papers. The topics discussed include: identifying the relationship between technology curiosity and knowledge absorptive capability in an augmented reality smart class; using scrum in an information systems capstone course: a case study; using the ERP simulation games to teach managerial decision-making; using teaching cases for achieving Bloom's high- order cognitive levels: an application in technically-oriented information systems course; life long learning and the role of ICT-innovation; adoption of learning analytics in the UK: identification of key factors using the toe framework; gamifying the first programming class: outcomes and antecedents of continued engagement intention; information systems in cc2020: comparing key structural elements of curriculum recommendations in computing; and chasing the next fad: the changing nature of the is discipline. 2017 SIGED International Conference on Information Systems Education and Research  ,Monitoring and control
2158,"10th IFIP WG 8.1 Working Conference on the Practice of Enterprise Modelling, PoEM 2017","The proceedings contain 24 papers. The special focus in this conference is on Practice of Enterprise Modelling. The topics include: An analysis of enterprise architecture for federated environments; from indicators to predictive analytics: A conceptual modelling framework; product life-cycle assessment in the realm of enterprise modeling; towards reasoning about pivoting in startups and large enterprises with i; evolution models for information systems evolution steering; Toward GDPR-compliant socio-technical systems: Modeling language and reasoning framework; development of capability driven development methodology: Experiences and recommendations; management structure based government enterprise architecture framework adaption in situ; NEXT: Generating tailored ERP applications from ontological enterprise models; information security risk management; An integrated enterprise modeling framework using the RUP/UML business use-case model and BPMN; a method for effective use of enterprise modelling techniques in complex dynamic decision making; streamlining structured data markup and agile modelling methods; integrating local and global optimization in capability delivery; the digital business architect – Towards method support for digital innovation and transformation; using grounded theory for domain specific modelling language design: Lessons learned from the smart grid domain; towards meta model provenance: A goal-driven approach to document the provenance of meta models; light touch identification of cost/risk in complex socio-technical systems; The OntoREA© accounting and finance model: A retroactive DSRM demonstration evaluation; goals, workflow, and value: Case study experiences with three modeling frameworks; an evaluation framework for design-time context-adaptation of process modelling languages; capability-driven digital service innovation: Implications from business model and service process perspectives. 10th IFIP WG 8.1 Working Conference on the Practice of Enterprise Modelling, PoEM 2017  ",Strategic alignment
2159,Agile big data analytics development: An architecture-centric approach,"Agile development for big data analytics has become the new normal. However, research questions remain: 1) how should a big data system be designed and developed to effectively support advanced analytics? and 2) how should the agile process be adapted for big data analytics development? This article contributes an Architecture-centric Agile Big data Analytics (AABA) development methodology evolved and validated in 10 case studies through Collaborative Practice Research. Our studies showed that architecture agility is the key for successful agile big data analytics development. Employing an architecture-centric approach, the AABA methodology integrates the Big Data system Design (BDD) method and Architecture-centric Agile Analytics with architecture-supported DevOps (AAA) model for effective value discovery and rapid continuous delivery of value. The uses of a design concepts catalog and architectural spikes are advancements to architecture design methods that have proven to be critical to agile big data analytics development. © 2016 IEEE. Agile big data analytics development: An architecture-centric approach Agile devleopment methodology; Big data and analytics; Big data design method; Software architecture Agile manufacturing systems; Data mining; Design; Software architecture; Agile development; Agile devleopment methodology; Architecture designs; Architecture-centric; Collaborative practices; Data design; Development methodology; Research questions; Big data",Strategic alignment
2160,Increasing firm agility through the use of data analytics: The role of fit,"Agility, which refers to a dynamic capability within firms to identify and effectively respond to threats and opportunities with speed, is considered as a main business imperative in modern business environments. While there is some evidence that information technology (IT) capabilities can help organizations to be more agile, studies have reported mixed findings regarding such effects. In this study, we identify the conditions under which IT capabilities translate into agility gains. We focus on a specific and critical IT capability, the use of data analytics, which is often leveraged by firms to improve decision making and achieve agility gains. We leverage dynamic capability theory to understand the influence of data analytics use as a lower-order dynamic capability on firm agility as a higher-order dynamic capability. We also draw on the fit perspective to suggest that this impact will only accrue if there is a high degree of fit between several elements that are closely related to the use of data analytics tools within firms including the tools themselves, the users, the firm tasks, and the data. The proposed research model is empirically validated using survey data from 215 senior IT professionals confirming the importance of high levels of fit between data analytics tools and key related elements. The findings provide the understanding of the impacts of data analytics use on firm agility, while also providing guidance to managers on how they could better leverage the use of such technologies. These findings could be more broadly used to inform the effective use of other forms of IT in organizations. © 2017 Elsevier B.V. Increasing firm agility through the use of data analytics: The role of fit Data analytics use; Data-tools fit; Firm agility; People-tools fit; Tasks-tools fit Decision support systems; Information systems; Data analytics; Data tools; Firm agility; People-tools fit; Tasks-tools fit; Enterprise resource management",Financial management
2161,"A data-driven strategy for predicting greenness scores, rationally comparing synthetic routes and benchmarking PMI outcomes for the synthesis of molecules in the pharmaceutical industry","Cumulative Process Mass Intensity (PMI) is one of the most popular greenness metrics tracked during the lifecycle of a pharmaceutical compound. Its use is wide-spread, having come to represent the foundation of many assessments of efficiency. These metrics are critical during the development of a compound as analysis of efficiency data (such as PMI outcomes) can help minimize the environmental impact of pharmaceutical manufacturing, highlight areas for potential improvement and thus drive sustainability. However, there are several issues with many of the current metrics, one of the most pressing being the absence of such information when key synthetic strategy decisions are made in early development; many metrics articulate the impact of strategy decisions made in the absence of efficiency data. In this article, we develop a predictive analytics framework, coupled to Monte Carlo simulation, to address this issue and enable a rich understanding of potential PMI outcomes during both the decision making process (prediction) and the outcome review process (comparison). This method leverages real-world data to predict probable PMI ranges for a potential synthesis being considered, utilizing accumulated data which spans a range of molecules and phases of development. The approach can serve two critical functions lacking in current methods: (1) it can act as a decision-aiding tool during the route discovery process, predicting probable PMI outcomes for proposed, potential or unoptimized synthetic routes; (2) it can enable the direct comparison of the PMI outcome of a synthesis to all comparable chemistry, thus providing a benchmarking methodology capable of comparing PMIs across molecules. We envision that this approach will deliver significant impact to the green chemistry community by enabling greener decisions to be made at critical phases of invention, namely the ideation, route selection and development processes (designing green), along with providing a rational method to compare a specific outcome to prior art (benchmarking). © The Royal Society of Chemistry 2017. A data-driven strategy for predicting greenness scores, rationally comparing synthetic routes and benchmarking PMI outcomes for the synthesis of molecules in the pharmaceutical industry  ",Strategic alignment
2162,Designing a ‘concept of operations’ architecture for next-generation multi-organisational service networks,"Networked service organisations are increasingly adopting a ‘smarter networking’ philosophy in their design of more agile and customer-focused supply models. Changing consumer behaviours and the emergence of transformative technologies—industry 4.0, artificial intelligence, big data analytics, the Internet of Things—are driving a series of innovations, in terms of ‘products’ and business models, with major implications for the industrial enterprise, in their design of more ‘digitalised’ supply chains. For B2B systems, emerging ‘product-service’ offerings are requiring greater visibility, alignment and integration across an increasingly complex network of multiple partners and collaborators, in order to deliver a better service and customer ‘experience’. To support the design and operation of these multi-organisational service networks, we outline a concept of operations architecture here, underpinned by the literature and network theory, and demonstrate application using a series of exemplar case studies. Focusing on relational elements and the processes key to network integration within service supply networks, the cases inform a set of operating principles and protocols—applicable to all stakeholders ‘cooperating’, within a ‘shared’ environment. Equally critical is to understand how digital technologies may influence future operating philosophies. This article extends our theoretical understanding of network organisations, from a traditional ‘product’ perspective to that of ‘services’, and presents the case for developing a common, unified approach to designing diverse forms of multi-partner service networks. © 2016 The Author(s) Designing a ‘concept of operations’ architecture for next-generation multi-organisational service networks Digital technologies; Multi-organisational service networks; Network and data integration; Operating principles and protocols; Product-service systems Artificial intelligence; Big data; Complex networks; Data integration; Network architecture; Philosophical aspects; Product design; Supply chains; Concept of operations; Design and operations; Digital technologies; Industrial enterprise; Network organisation; Operating principles; Product-service systems; Service network; Next generation networks",Governance
2163,Adopting flow analytics in software development projects,"Anecdotal evidence suggests that use and effectiveness of flow tools and analytics is rapidly increasing across the software community. The differentiating feature of flow methods is their speed, epitomised by terms such as lead-time, cycle time, and cost of delay. Yet, existing research tends to largely focuses on the textbook version of flow, rather than any rigorous examination of its use in the turbulent and constantly changing ‘real world’ environment within which it is applied. This is a significant limitation given that software development is a highly metric oriented, complex and socially embedded activity. This study draws on a single case with four software development teams to identify the benefits realized when flow practices and analytics are integrated with a contemporary business intelligence and analytical software. It also provides an immediate practical contribution by identifying a set of lessons drawn from the case studied that may be applicable in future implementations of flow tools and analytics and the lessons learned can then be tailored and applied to other software development contexts. © 2017 Copyright is held by the owner/author(s). Adopting flow analytics in software development projects Agile; Analytics; Flow Software engineering; Agile; Analytical software; Analytics; Anecdotal evidences; Flow; Software community; Software development projects; Software development teams; Software design",Monitoring and control
2165,On the model design of integrated intelligent big data analytics systems,"Purpose - Although big data analytics has reaped great business rewards, big data system design and integration still face challenges resulting from the demanding environment, including challenges involving variety, uncertainty, and complexity. These characteristics in big data systems demand flexible and agile integration architectures. Furthermore, a formal model is needed to support design and verification. The purpose of this paper is to resolve the two problems with a collective intelligence (CI) model. Design/methodology/approach - In the conceptual CI framework as proposed by Schut (2010), a CI design should be comprised of a general model, which has formal form for verification and validation, and also a specific model, which is an implementable system architecture. After analyzing the requirements of system integration in big data environments, the authors apply the CI framework to resolve the integration problem. In the model instantiation, the authors use multi-agent paradigm as the specific model, and the hierarchical colored Petri Net (PN) as the general model. Findings - First, multi-agent paradigm is a good implementation for reuse and integration of big data analytics modules in an agile and loosely coupled method. Second, the PN models provide effective simulation results in the system design period. It gives advice on business process design and workload balance control. Third, the CI framework provides an incrementally build and deployed method for system integration. It is especially suitable to the dynamic data analytics environment. These findings have both theoretical and managerial implications. Originality/value - In this paper, the authors propose a CI framework, which includes both practical architectures and theoretical foundations, to solve the system integration problem in big data environment. It provides a new point of view to dynamically integrate large-scale modules in an organization. This paper also has practical suggestions for Chief Technical Officers, who want to employ big data technologies in their companies. ï¿½ Emerald Group Publishing Limited. On the model design of integrated intelligent big data analytics systems Big data analytics; Collective intelligence; Model design; System integration Agile manufacturing systems; Computer architecture; Data integration; Design; Multi agent systems; Petri nets; Systems analysis; Collective intelligences; Data analytics; Design/methodology/approach; Hierarchical colored petri nets; Managerial implications; Model design; System integration; Verification-and-validation; Big data",Strategic alignment
2166,"3rd International Conference on Information Systems Design and Intelligent Applications, INDIA 2016","The proceedings contain 75 papers. The special focus in this conference is on Information Systems Design and Intelligent Applications. The topics include: Implementations of secure reconfigurable cryptoprocessor a survey; a comparative study of various configuration management tools for IAAS cloud; adaptive fractal image compression based on adaptive thresholding in DCT domain; improved resource exploitation by combining hadoop map reduce framework with virtualbox; density based outlier detection technique; systematic evaluation of seed germination models; business modeling using agile; an efficient hybrid encryption technique based on DES and RSA for textual data; application of machine learning on process metrics for defect prediction in mobile application; automatic insurance and pollution challan generator system in india; effectively implementation of KNN-search on multidimensional data using quadtree; edge detectors based telegraph total variational model for image filtering; cloud based k-means clustering running as a mapreduce job for big data healthcare analytics using apache mahout; symbolic decision tree for interval data-an approach towards predictive streaming and rendering of 3D models; predictive 3D content streaming based on decision tree classifier approach; quantitative characterization of radiographic weld defect based on the ground truth radiographs made on a stainless steel plates; research and topology of shunt active filters for quality of power; development of 3D high definition endoscope system; excel solver for deterministic inventory model; DSL approach for development of gaming applications and methods of evidence identification, segregation, collection and partial analysis. 3rd International Conference on Information Systems Design and Intelligent Applications, INDIA 2016  ",Strategic alignment
2167,Big Data—Challenges,"The hallmark of a successful analytics strategy ultimately is one that results in sustainable data delivery, as this demonstrates value to the enterprise and alignment of critical resources. Sustainable institutional analytics competency delivers data in an agile manner from a platform that minimizes rework, rediscovery, and churn. Creating a data governance model requires buy-in from stakeholders across an organization, and building the case for this is time consuming. Effective data governance is a foundational element for sustained success. In a mature Analytics 1.0 environment, analytics data should be clean, well understood, and trusted. Building a strategy implies synthesis, and with synthesis of purpose follows efficacy and agility. Stewardship is a core component of data governance. Organizations have numerous sources of data, and as they are brought together, they gain increased value when seen in the context of other information. © 2017 by Taylor & Francis Group, LLC. Big Data—Challenges  ",Governance
2170,Proactive manufacturing-a big-data driven emerging manufacturing paradigm,"The incoming new industrial revolution will transform manufacturing radically, which will result manufacturing in the form of Socio-Cyber-Physical System (SCPS) with big data of structures, semi-structures and unstructures. To deal with such big data in manufacturing, a new manufacturing model, called proactive manufacturing was proposed with the introduction of big data analytics and proactive computing, especially proactive event-driven computing. A big-data driven general architecture fused with organizational semiotics and Observe-Orient- Decide-Act (OODA) loop was set up, which was further extended to a big-data driven proactive manufacturing architecture for SCPS-based manufacturing. The model was compared with existing manufacturing models from the viewpoint of the use of big-data value in depth and width. In particular, the correlations between proactive manufacturing and paradigms such as wisdom manufacturing, smart manufacturing, predictive manufacturing and long tail of making products were addressed. © 2017, Editorial Department of CIMS. All right reserved. Proactive manufacturing-a big-data driven emerging manufacturing paradigm Integrated manufacturing system; Proactive computing; Proactive manufacturing; Proactivity; Wisdom manufacturing Agile manufacturing systems; Cyber Physical System; Embedded systems; Manufacture; Event-driven computing; General architectures; Industrial revolutions; Integrated manufacturing systems; Manufacturing paradigm; Organizational semiotics; Pro activities; Proactive computing; Big data",Strategic alignment
2173,"Big data strategies for agile business: Framework, practices, and transformation roadmap","Agile is a set of values, principles, techniques, and frameworks for the adaptable, incremental, and efficient delivery of work. Big Data is a rapidly growing field that encompasses crucial aspects of data such as its volume, velocity, variety, and veracity. This book outlines a strategic approach to Big Data that will render a business Agile. It discusses the important competencies required to streamline and focus on the analytics and presents a roadmap for implementing such analytics in business. © 2018 by Taylor & Francis Group, LLC. Big data strategies for agile business: Framework, practices, and transformation roadmap  ",Strategic alignment
2174,Cognitive hyperconnected digital transformation: Internet of things intelligence evolution,"Cognitive Hyperconnected Digital Transformation provides an overview of the current Internet of Things (IoT) landscape, ranging from research, innovation and development priorities to enabling technologies in a global context. It is intended as a standalone book in a series that covers the Internet of Things activities of the IERC-Internet of Things European Research Cluster, including both research and technological innovation, validation and deployment. The book builds on the ideas put forward by the European Research Cluster, the IoT European Platform Initiative (IoT-EPI) and the IoT European Large-Scale Pilots Programme, presenting global views and state-of-the-art results regarding the challenges facing IoT research, innovation, development and deployment in the next years. Hyperconnected environments integrating industrial/business/consumer IoT technologies and applications require new IoT open systems architectures integrated with network architecture (a knowledge-centric network for IoT), IoT system design and open, horizontal and interoperable platforms managing things that are digital, automated and connected and that function in real-time with remote access and control based on Internet-enabled tools. The IoT is bridging the physical world with the virtual world by combining augmented reality (AR), virtual reality (VR), machine learning and artificial intelligence (AI) to support the physical-digital integrations in the Internet of mobile things based on sensors/actuators, communication, analytics technologies, cyber-physical systems, software, cognitive systems and IoT platforms with multiple functionalities. These IoT systems have the potential to understand, learn, predict, adapt and operate autonomously. They can change future behaviour, while the combination of extensive parallel processing power, advanced algorithms and data sets feed the cognitive algorithms that allow the IoT systems to develop new services and propose new solutions. IoT technologies are moving into the industrial space and enhancing traditional industrial platforms with solutions that break free of device-, operating system- and protocol-dependency. Secure edge computing solutions replace local networks, web services replace software, and devices with networked programmable logic controllers (NPLCs) based on Internet protocols replace devices that use proprietary protocols. Information captured by edge devices on the factory floor is secure and accessible from any location in real time, opening the communication gateway both vertically (connecting machines across the factory and enabling the instant availability of data to stakeholders within operational silos) and horizontally (with one framework for the entire supply chain, across departments, business units, global factory locations and other markets). End-to-end security and privacy solutions in IoT space require agile, context-aware and scalable components with mechanisms that are both fluid and adaptive. The convergence of IT (information technology) and OT (operational technology) makes security and privacy by default a new important element where security is addressed at the architecture level, across applications and domains, using multi-layered distributed security measures. Blockchain is transforming industry operating models by adding trust to untrusted environments, providing distributed security mechanisms and transparent access to the information in the chain. Digital technology platforms are evolving, with IoT platforms integrating complex information systems, customer experience, analytics and intelligence to enable new capabilities and business models for digital business. © 2017 River Publishers. All rights reserved. Cognitive hyperconnected digital transformation: Internet of things intelligence evolution Artificial intelligence; Cloud computing; Cognitive systems; Cognitive transformation; Cyber-physical systems; Deep learning; Intelligence evolution; Internet of things; Virtualization; Wireless networks ",Stakeholder management
2179,Blockchain technology innovations,"The digital supply chain has produced efficiencies, new innovative products, and close customer relationships globally by the effective use of mobile, IoT (Internet of Things), social media, analytics and cloud technology to generate models for better decisions. One of the new technologies is blockchain. While initially popularized by Bitcoin, blockchain is much more than the foundation for crypto currency. It offers a secure way to exchange any kind of good, service, or transaction. Industrial growth increasingly depends on trusted partnerships; but increasing regulation, cybercrime and fraud are inhibiting expansion. To address these challenges, blockchain will enable more agile value chains, faster product innovations, closer customer relationships, and faster integration with the Internet of Things (IoT). For example, IBM Global Financing freed up 40% more capital and decreased dispute resolution time from 40+ days to less than 10 with a pilot blockchain solution. Since the technology is decentralized, blockchain provides a lower cost of trade with a trusted contract monitored with out intervention from third parties who may not add direct value. Blockchain allows immediate contracts, engagements, and agreements with inherent, robust cyber security features. This paper presents how blockchain is changing the supply chain. Blockchain technology innovations Blockchain; Cloud technology; Cognitive computing; Industrial engineering; Supply chain management Electronic money; Engineers; Industrial engineering; Innovation; Public relations; Supply chain management; Trusted computing; Block-chain; Cloud technologies; Cognitive Computing; Customer relationships; Digital supply chain; Inhibiting expansion; Internet of thing (IOT); Technology innovation; Internet of things",Financial management
2180,Earned Value Analysis Deployment in an Enterprise Using BI Software,"Software development is growing exponentially, driven by the growing need for solutions for the processing of information and the increased use of mobile devices. This growth requires software companies achieve projects more efficients. To achieve this, the monitoring of projects need tools that can provide the timely control of tasks and information to enable decisions to ensure project success. This article presents a solution that implements the use of the Earned Value Analysis in a Business Intelligence (BI) and reporting this experiment in a company that develops software using an agile methodology. The solution proved to be viable for micro and small companies. However, a deeper study of the needs of these companies is required. © 2016 IEEE. Earned Value Analysis Deployment in an Enterprise Using BI Software Business Intelligence; Earned Value; Software Budget control; Competitive intelligence; Computer software; Information analysis; Mobile devices; Software engineering; Value engineering; Agile Methodologies; Earned value; Earned value analysis; Project success; Small companies; Software company; Software design",Financial management
2182,Time to Market—Enabling the Specific Efficiency and Cooperation in Product Development by the Institutional Role Model,"This paper discusses major and effective aspects that in future will be important for R&D management in the automotive industry and that will have an enormous impact on the entire value creation chain. Firstly, there are efficiency potentials for reducing development time in design and management of the entire product development process (for example, implementation of an IT system with real-time information, predictive analytics and total networking of all value creation partners). Secondly, the complexity involved in the new technologies and the associated networking being set up between business environment and automotive industry. For example car-to-customer, car-to-car or cooperative intelligent transport systems increasingly demand horizontal and agile cooperation among various industry branches (for example, car industry, telecommunications industry, over-the-top players) and the legislatures of the different markets. Moreover, it becomes clear that the particular public or private highway operators either support or hinder the market success of the new car technologies. Here, the urgent need for close cooperation has been recognized and already exists. However, the process with its milestones is still in a very early phase of product development. Nevertheless, the companies leading the market and the government agencies will cooperate intensely in future and thus also make application of the new technologies possible. For this reason, a form of business architecture was chosen that permits competitively neutral and non-discriminating cooperation with the stakeholders. The theory of the Institutional Role Model (IRM) created a multifunctional approach that permits a holistic type of cooperation and creates very good prerequisites for improving efficiency. The concept of the Institutional Role Model was used successfully for three national research projects (e.g. CONVERGE; Market Design for C-ITS). Within these projects the IRM was used for market phase introduction and penetration. This paper integrates the IRM early into the agile digital product development process. This process redesign will enable managers in the automotive industry to generate unique optimization opportunities in future. © 2016, Springer International Publishing AG. Time to Market—Enabling the Specific Efficiency and Cooperation in Product Development by the Institutional Role Model Agile digital product development process; Development time; Institutional rule model; Market entry; Spheres of activity ",Strategic alignment
2183,Agility in business intelligence,"Organizations of all sizes are surrounded by data on an ongoing basis, both internally and externally. Individuals at all levels of the organization need access to critical business information and the ability to analyze and share with the appropriate parties. With accurate and timely information, individuals can use it to make informed business decisions rather than “going with the gut” or performing “trial and error” (Mrdalj, 2007). Clive Humby is credited with the following analogy and its use over the past ten years that “data is the new oil.” The analogy is appropriate as unrefined crude oil has little market value, but refined oil can be made into plastics, chemicals, and gasoline. A single data element, by itself, is meaningless unless transformed into meaningful information, compared with company or industry trends, and analyzed to make future predictions (ANA Marketing Maestros, 2006; Rotella, 2012; Vanian, 2016). The challenge is to ensure that the right people have the right information in a timely manner. In order for organizations to make effective decisions based on data-driven evidence in a timely manner, an agile approach to managing business intelligence (BI) is essential for competitive advantage in a global environment. © 2018 Taylor & Francis. Agility in business intelligence  ",Governance
2184,Effectiveness of agile implementation methods in business intelligence projects from an end-user perspective,"The global Business Intelligence (BI) market grew by 10% in 2013 according to the Gartner Report. Today organizations require better use of data and analytics to support their business decisions. Internet power and business trend changes have provided a broad term for data analytics -Big Data. To be able to handle it and leverage a value of having access to Big Data, organizations have no other choice than to get proper systems implemented and working. However traditional methods are not efficient for changing business needs. The long time between project start and go-live causes a gap between initial solution blueprint and actual user requirements in the end of the project. This article presents the latest market trends in BI systems implementation by comparing Agile with traditional methods. It presents a case study provided in a large telecommunications company (20K employees) and the results of a pilot research provided in the three large companies: telecommunications, digital, and insurance. Both studies prove that Agile methods might be more effective in BI projects from an end-user perspective and give first results and added value in a much shorter time compared to a traditional approach. © 2016 Informing Science: The International Journal of an Emerging Transdiscipline. Effectiveness of agile implementation methods in business intelligence projects from an end-user perspective Advanced analytics; Agile methods; Business intelligence; Efficiency; End-users needs; Iteration; Sprint ",Governance
2185,Visual analytics in enterprise architecture management: A systematic literature review,"In times of dynamic markets, enterprises have to be agile to be able to quickly react to market influences. Due to the increasing digitization of products, the enterprise IT often is affected when business models change. Enterprise Architecture Management (EAM) targets a holistic view of the enterprise’ IT and their relations to the business. However, Enterprise Architectures (EA) are complex structures consisting of many layers, artifacts and relationships between them. Thus, analyzing EA is a very complex task for stakeholders. Visualizations are common vehicles to support analysis. However, in practice visualization capabilities lack flexibility and interactivity. A solution to improve the support of stakeholders in analyzing EAs might be the application of visual analytics. Starting from a systematic literature review, this article investigates the features of visual analytics relevant for the context of EAM. © Springer International Publishing AG 2017. Visual analytics in enterprise architecture management: A systematic literature review Decision support; EAM; Literature review; Visual analysis Commerce; Decision support systems; Complex structure; Decision supports; Enterprise Architecture; Enterprise architecture managements; Literature reviews; Support analysis; Systematic literature review; Visual analysis; Visualization",Governance
2187,Agile visual analytics in data science systems,"The practice of data science is ad hoc and agile, where needs and requirements evolve continuously and are resolved through collaboration among stakeholders. To support such practice visual analytics systems need to evolve as the needs and requirements in terms of data, users, tasks, medium, visualization, and interaction capabilities change continuously. In this paper we present a case study and illustrate several dimensions of the requirements in visual analytics. We put forward a vision of a dynamic agile visual analytics process and system model in support of data science, in which the user and system can cooperate to facilitate discovery while requirements change on demand. We argue that such a system needs an underlying language and algebra that defines not only operands and operators for performing visual analytics but also specifies guidelines that take them into account and produce useful visual analytics transformations leading to a specific insight. Our intent is not to present a fully developed system but rather a vision, illustrated through a use case. While the algebra presented here is sufficiently defined to illustrate our viewpoints, further refinement and completion is necessary to facilitate application in a visual analytics system. © 2016 IEEE. Agile visual analytics in data science systems Agile process; Data analytics; Design guidelines; Relational algebra; Requirements analysis; Visual analytics Agile manufacturing systems; Algebra; Data visualization; Smart city; Visualization; Agile process; Data analytics; Relational algebra; Requirements analysis; Visual analytics; Visual languages",Stakeholder management
2188,"How does the pore-throat size control the reservoir quality and oiliness of tight sandstones? The case of the Lower Cretaceous Quantou Formation in the southern Songliao Basin, China","Pore-throat size is a very crucial factor controlling the reservoir quality and oiliness of tight sandstones, which primarily affects rock-properties such as permeability and drainage capillary pressure. However, the wide range of size makes it difficult to understand their distribution characteristics as well as the specific controls on reservoir quality and oiliness. In order to better understand about pore-throat size distribution, petrographic, scanning electron microscopy (SEM), pressure-controlled mercury injection (PMI), rate-controlled mercury injection (RMI), quantitative grain fluorescence (QGF) and environmental scanning electron microscopy (ESEM) investigations under laboratory pressure conditions were performed on a suite of tight reservoir from the fourth member of the Lower Cretaceous Quantou Formation (K1q4) in the southern Songliao Basin, China. The sandstones in this study showed different types of pore structures: intergranular pores, dissolution pores, pores within clay aggregates and even some pores related to micro fractures. The pore-throat sizes vary from nano- to micro-scale. The PMI technique views the pore-throat size ranging from 0.001 μm to 63 μm and revealed that the pore-throats with radius larger than 1.0 μm are rare and the pore-throat size distribution curves show evident fluctuations. RMI measurements indicated that the pore size distribution characteristics of the samples with different porosity and permeability values look similar. The throat size and pore throat radius ratio distribution curves had however significant differences. The overall pore-throat size distribution of the K1q4 tight sandstones was obtained with the combination of the PMI and RMI methods. The permeability is mainly contributed by a small part of larger pore-throats (less than 30%) and the ratio of the smaller pore-throats in the samples increases with decreasing permeability. Although smaller pore-throats have negligible contribution on reservoir flow potential, they are very significant for the reservoir storage capacity. The pore-throats with average radius larger than 1.0 μm mainly exist in reservoirs with permeability higher than 0.1mD. When the permeability is lower than 0.1mD, the sandstones are mainly dominated by pore-throats with average radius from 0.1 μm to 1.0 μm. The ratio of different sized pore-throats controls the permeability of the tight sandstone reservoirs in different ways. We suggest that splitting or organizing key parameters defining permeability systematically into different classes or functions can enhance the ability of formulating predictive models about permeability in tight sandstone reservoirs. The PMI combined with QGF analyses indicate that oil emplacement mainly occurred in the pore-throats with radius larger than about 0.25-0.3 μm. This result is supported by the remnant oil micro-occurrence evidence observed by SEM and ESEM. © 2016 Elsevier Ltd. How does the pore-throat size control the reservoir quality and oiliness of tight sandstones? The case of the Lower Cretaceous Quantou Formation in the southern Songliao Basin, China Pore throat size; Quantou formation; Reservoir oiliness; Reservoir quality; Songliao Basin; Tight sandstone oil reservoir China; Songliao Basin; Petroleum reservoirs; Pore size; Pore structure; Predictive analytics; Quality control; Sandstone; Scanning electron microscopy; Size distribution; Textures; Tight gas; Oil reservoirs; Pore-throat size; Quantou formation; Reservoir quality; Songliao basin; capillary pressure; Cretaceous; hydrocarbon exploration; hydrocarbon reservoir; mercury (element); permeability; reservoir characterization; sandstone; size distribution; Petroleum reservoir engineering",Monitoring and control
2189,Blockchain technology innovations,"Digital world has produced efficiencies, new innovative products, and close customer relationships globally by the effective use of mobile, IoT (Internet of Things), social media, analytics and cloud technology to generate models for better decisions. Blockchain is recently introduced and revolutionizing the digital world bringing a new perspective to security, resiliency and efficiency of systems. While initially popularized by Bitcoin, Blockchain is much more than a foundation for crypto currency. It offers a secure way to exchange any kind of good, service, or transaction. Industrial growth increasingly depends on trusted partnerships; but increasing regulation, cybercrime and fraud are inhibiting expansion. To address these challenges, Blockchain will enable more agile value chains, faster product innovations, closer customer relationships, and quicker integration with the IoT and cloud technology. Further Blockchain provides a lower cost of trade with a trusted contract monitored without intervention from third parties who may not add direct value. It facilitates smart contracts, engagements, and agreements with inherent, robust cyber security features. This paper is an effort to break the ground for presenting and demonstrating the use of Blockchain technology in multiple industrial applications. A healthcare industry application, Healthchain, is formalized and developed on the foundation of Blockchain using IBM Blockchain initiative. The concepts are transferable to a wide range of industries as finance, government and manufacturing where security, scalability and efficiency must meet. © 2017 IEEE. Blockchain technology innovations Blockchain; Business; Cloud computing; Cloud services; Control systems; Cybersecurity; DevOps; Finance; Government; Healthcare; Industry 4.0; IoT Cloud computing; Control systems; Distributed computer systems; Efficiency; Electronic money; Finance; Health care; Industry; Innovation; Public relations; Trusted computing; Block-chain; Cloud services; Cyber security; DevOps; Government; Internet of things",Value management
2193,Improving Operational Efficiency of Applications via Cloud Computing,"Enterprises migrate or deploy their software-based applications to the cloud both to deliver new service and value faster to drive their top line, and to improve operational efficiency to boost their bottom line. This paper defines operational efficiency for business-to-consumer and other applications operating on public, private or hybrid cloud platforms. A cost model of application service production is proposed with six efficiency improvement levers: Automation; scalable capacity; advanced self-service; agile service creation; application execution efficiency; and efficiency analytics. Three key indicators are also proposed to help drive continuous efficiency improvement by the enterprise: The unit cost of service production; the cost of capacity waste; and the cost of creating functionality. © 2014 IEEE. Improving Operational Efficiency of Applications via Cloud Computing activity based; application services; cloud; Cloud computing; cost modeling Application programs; Cloud computing; Clouds; Costs; Activity-based; Application execution; Application services; Business to Consumer; Cost modeling; Efficiency improvement; Operational efficiencies; Scalable capacity; Efficiency",Stakeholder management
2195,Profit per Hour as a Target Process Control Parameter for Manufacturing Systems Enabled by Big Data Analytics and Industry 4.0 Infrastructure,"The rise of Industry 4.0 and in particular Big Data analytics of production parameters offers exciting new ways for optimization. The majority of factories in process industries currently aim for example, either for output maximization, yield increase, or cost reduction. The availability of real-time data and online processing capability with advanced algorithms enables a profit per hour operational management approach. Profit per hour as a target control metric allows running factories at the optimal available operating point taking all revenue and cost drivers into account. This paper describes the suitability of profit per hour as a target process control parameter for production in process industries. The authors explain how this management approach helps to make better operational decisions, trading off yield, energy, throughput, among other factors, and the resulting cumulative benefits. They also lay out how Big Data and advanced algorithms are the key enabler to this new approach, as well as a standardized methodology for implementation. With profit per hour an agile control approach is presented which aims to optimize the performance of industrial manufacturing systems in a world of ever increasing volatility. © 2017 The Authors. Published by Elsevier. Profit per Hour as a Target Process Control Parameter for Manufacturing Systems Enabled by Big Data Analytics and Industry 4.0 Infrastructure Advanced Process Control; Agile Manufacturing; Big Data Analytics; Manufacturing Systems 4.0; Operations Management; Profit per Hour Agile manufacturing systems; Big data; Cost reduction; Data handling; Information management; Intelligent control; Manufacture; Optimization; Profitability; Advanced Process Control; Agile manufacturing; Data analytics; Industrial manufacturing; Operational decisions; Operational management; Operations management; Production parameters; Process control",Monitoring and control
2200,Health worker focused distributed simulation for improving capability of health systems in Liberia,"Introduction: The main goal of this study was to produce an adaptable learning platform using virtual learning and distributed simulation, which can be used to train health care workers, across a wide geographical area, key safety messages regarding infection prevention control (IPC). Methods: A situationally responsive agile methodology, Scrum, was used to develop a distributed simulation module using short 1-week iterations and continuous synchronous plus asynchronous communication including end users and IPC experts. The module contained content related to standard IPC precautions (including handwashing techniques) and was structured into 3 distinct sections related to donning, doffing, and hazard perception training. Outcome: Using Scrum methodology, we were able to link concepts applied to best practices in simulation-based medical education (deliberate practice, continuous feedback, self-assessment, and exposure to uncommon events), pedagogic principles related to adult learning (clear goals, contextual awareness, motivational features), and key learning outcomes regarding IPC, as a rapid response initiative to the Ebola outbreak in West Africa. Gamification approach has been used to map learning mechanics to enhance user engagement. Conclusions: The developed IPC module demonstrates how high-frequency, low-fidelity simulations can be rapidly designed using scrum-based agile methodology. Analytics incorporated into the tool can help demonstrate improved confidence and competence of health care workers who are treating patients within an Ebola virus disease outbreak region. These concepts could be used in a range of evolving disasters where rapid development and communication of key learning messages are required. © 2016 Society for Simulation in Healthcare. Health worker focused distributed simulation for improving capability of health systems in Liberia Gamification of learning; Infection prevention control; Simulation; Virtual reality Clinical Competence; Communicable Disease Control; Formative Feedback; Health Personnel; Hemorrhagic Fever, Ebola; Humans; Quality of Health Care; Simulation Training; User-Computer Interface; clinical competence; communicable disease control; computer interface; constructive feedback; education; health care personnel; health care quality; Hemorrhagic Fever, Ebola; human; organization and management; procedures; simulation training; transmission",Risk management
2201,Architecture for Complex Event Processing Using Open Source Technologies,"Big Data, more than ever, is playing a vital role in IT decision making, with such decisions increasingly moving towards being made in real-time. Organisations are optimizing service performance, better handling capacity across overall organisations, and effectively making decisions utilising operational analytics. Realizing the full value of business data is a key challenge for today's operational analytics. Complex Event Processing (CEP) is a technique for tracking, analyzing, and processing data as an event happens and is useful for Big Data because it is intended to manage data in motion. Data in motion is processed and communicated based on business rules and processes. For decisions to be better-informed, data used for decision making has to be timely, complete, accurate, trusted, valid, reliable, and relevant. CEP utilizes data generated from moment-to-moment from different emerging sources such as sensor, sentiment, geo-locational, etc. There is a need to bridge the gap between traditional business intelligence with new Big Data technologies such as CEP. Bridging of this gap will enable organisations to become agile and data-driven so that business outcomes can be maximized by delivering better-informed decisions about a customer and delivering a better service to them. In this paper we discuss the architecture developed for CEP using open source technologies and show how CEP is applied to the use case of an Electronic Coupon Distribution Service (ECDS), using location information, past shopping/travel history, gender, likes/dislikes, etc. We further explore how different types of data such as static information (gender, age, etc.), previous history (where the person travelled to, what they bought, etc.), as well as real-time information about a customer (current location, current shopping habits, etc.) would all be utilised in CEP. © 2016 IEEE. Architecture for Complex Event Processing Using Open Source Technologies Big Data; Business Intelligence; Complex Event Processing; Conventional Coupon Distribution Service; Electronic Coupon Distribution Service Competitive intelligence; Data handling; Decision making; Information analysis; Location; Management science; Societies and institutions; Complex event processing; Complex event processing (CEP); Distribution services; Location information; Open-source technology; Operational analytics; Real-time information; Service performance; Big data",Strategic alignment
2205,"Distributing computing in the internet of things: Cloud, fog and edge computing overview","The main postulate of the Internet of things (IoT) is that everything can be connected to the Internet, at anytime, anywhere. This means a plethora of objects (e.g. smart cameras, wearables, environmental sensors, home appliances, and vehicles) are ‘connected’ and generating massive amounts of data. The collection, integration, processing and analytics of these data enable the realisation of smart cities, infrastructures and services for enhancing the quality of life of humans. Nowadays, existing IoT architectures are highly centralised and heavily rely on transferring data processing, analytics, and decision-making processes to cloud solutions. This approach of managing and processing data at the cloud may lead to inefficiencies in terms of latency, network traffic management, computational processing, and power consumption. Furthermore, in many applications, such as health monitoring and emergency response services, which require low latency, delay caused by transferring data to the cloud and then back to the application can seriously impact their performances. The idea of allowing data processing closer to where data is generated, with techniques such as data fusion, trending of data, and some decision making, can help reduce the amount of data sent to the cloud, reducing network traffic, bandwidth and energy consumption. Also, a more agile response, closer to real-time, will be achieved, which is necessary in applications such as smart health, security and traffic control for smart cities. Therefore, this chapter presents a review of the more developed paradigms aimed to bring computational, storage and control capabilities closer to where data is generated in the IoT: fog and edge computing, contrasted with the cloud computing paradigm. Also an overview of some practical use cases is presented to exemplify each of these paradigms and their main differences. © Springer International Publishing AG 2018. Distributing computing in the internet of things: Cloud, fog and edge computing overview Cloud computing; Distributed processing; Edge computing; Fog computing; Internet of things ",Risk management
2206,Industrial big data visualization: A case study using flight data recordings to discover the factors affecting the airplane fuel efficiency,"The dawn of the Industrial Internet era has she would light on the value of the big data associated with many industrial areas, such as aviation, resources, and manufacturing. Although the promising industrial big data could offer competitive advantages, it also poses profound challenges to the traditional analytics methods. The exploration and visualization capabilities are falling short for the fast-growing industrial data. Moreover, new value-adding insights need to be discovered in the fields where the domain experts have been consistently and scientifically improving. This paper introduces an industrial big data visualization case study using flight recorder data to discover the factors affecting the airplane fuel efficiency. The visualization challenge, the overall solution and the outcome are described in detail. In the meanwhile, the supporting methodology and tools for industrial data visualization are summarized into five components, including: agile data preparation, interactive exploration, large data visualization, statistical validation and report automation. © 2017 IEEE. Industrial big data visualization: A case study using flight data recordings to discover the factors affecting the airplane fuel efficiency Big Data Competition; Flight Recorder Data; Industrial Analytics; Large Data Visualization Competition; Data communication systems; Data privacy; Data visualization; Efficiency; Embedded software; Embedded systems; Recording instruments; Visualization; Competitive advantage; Data preparation; Flight recorders; Industrial Analytics; Industrial datum; Interactive exploration; Large data visualization; Statistical validation; Big data",Strategic alignment
2207,Agile analytics: Applying in the development of data warehouse for business intelligence system in higher education,"Majority of the Higher Learning Institutions are being implemented with information management systems for all the core activities ranging from admission, registration, alumni, graduate and academy operations. The data generated by these integrated information systems are transactional in nature and has been increasing exponentially. However, the use and value of the data has not been fully explored for driving decision-making process. This paper explores the importance of Agile Analytics in Business Intelligence and Data Warehouse development among Higher Learning Institutions. The paper concludes by outlining future directions relating to the development and implementation of an institutional project on data analytics. © 2018, Springer International Publishing AG, part of Springer Nature. Agile analytics: Applying in the development of data warehouse for business intelligence system in higher education Agile analytics; BI; Data warehouse; Framework Agile manufacturing systems; Bismuth; Data warehouses; Decision making; Information analysis; Information systems; Information use; Management information systems; Agile analytics; Business intelligence systems; Data warehouse development; Driving decision makings; Framework; Higher learning institutions; Information management systems; Integrated information system; Information management",Capacity management
2208,Strong agile metrics: Mining log data to determine predictive power of software metrics for continuous delivery teams,"ING Bank, a large Netherlands-based internationally operating bank, implemented a fully automated continuous delivery pipe-line for its software engineering activities in more than 300 teams, that perform more than 2500 deployments to production each month on more than 750 different applications. Our objective is to examine how strong metrics for agile (Scrum) DevOps teams can be set in an iterative fashion. We perform an exploratory case study that focuses on the classification based on predictive power of software metrics, in which we analyze log data derived from two initial sources within this pipeline. We analyzed a subset of 16 metrics from 59 squads. We identified two lagging metrics and assessed four leading metrics to be strong. © 2017 Association for Computing Machinery. Strong agile metrics: Mining log data to determine predictive power of software metrics for continuous delivery teams Agile metrics; Continuous delivery; Data mining; DevOps; Prediction modelling; Scrum; Software analytics; Software economics Data mining; Software engineering; Agile metrics; Continuous delivery; DevOps; Prediction modelling; Scrum; Software economics; Application programs",Capacity management
2209,"17th International Conference on Web Engineering, ICWE 2017","The proceedings contain 49 papers. The special focus in this conference is on Web Engineering. The topics include: Evaluating knowledge anchors in data graphs against basic level objects; decentralized evolution and consolidation of RDF graphs; spatially cohesive service discovery and dynamic service handover for distributed IoT environments; an agile model to model and model to text transformation framework; a big data analysis framework for model-based web user behavior analytics; from search engines to augmented search services; trading off popularity for diversity in the results sets of keyword queries on linked data; improving reliability of crowdsourced results by detecting crowd workers with multiple identities; maturity model for liquid web architectures; twisting web pages for saving energy; a semantic similarity metric based on translation; improved developer support for the detection of cross-browser incompatibilities; proximity-based adaptation of web content on public displays; ontology-enhanced aspect-based sentiment analysis; collaborative item embedding model for implicit feedback data; impact of referral incentives on mobile app reviews; web intelligence linked open data for website design reuse; exploratory search of web data services based on collective intelligence; the dimensions of crowdsourcing task design; public transit route planning through lightweight linked data interfaces; a query log analysis of dataset search; towards automatic generation of web-based modeling editors; harvesting forum pages from seed sites and public debates on the web. 17th International Conference on Web Engineering, ICWE 2017  ",Financial management
2211,Deploying Software Team Analytics in a Multinational Organization,"Implementing a software engineering analytics solution poses challenges and offers significant value for the globally distributed software development organization at ABB. Because software development activities in agile methodologies revolve around the team, ABB decided to implement an analytics solution focused on team metrics as part of its Software Development Improvement Program. Using key indicators focused around team improvement, researchers found that teams could manage their activities with metrics such as cycle time. Key lessons learned include paying attention to visual design and navigation and providing drill-down capabilities for the user. This article is part of a special issue on Actionable Analytics for Software Engineering. © 1984-2012 IEEE. Deploying Software Team Analytics in a Multinational Organization ABB; software development; software development management; software engineering; software metrics; software quality Computer software selection and evaluation; Software engineering; Agile Methodologies; Development activity; Globally distributed software development; Multinational organizations; Software development management; Software metrics; Software Quality; Software teams; Software design",Risk management
2212,Multi-perspective digitization architecture for the internet of things,"Social networks, smart portable devices, Internet of Things (IoT) on base of technologies like analytics for big data and cloud services are emerging to support flexible connected products and agile services as the new wave of digital transformation. Biological metaphors of living and adaptable ecosystems with service-oriented enterprise architectures provide the foundation for self-optimizing and resilient run-time environments for intelligent business services and related distributed information systems. We are extending Enterprise Architecture (EA) with mechanisms for flexible adaptation and evolution of information systems having distributed IoT and other micro-granular digital architecture to support next digitization products, services, and processes. Our aim is to support flexibility and agile transformation for both IT and business capabilities through adaptive digital enterprise architectures. The present research paper investigates additionally decision mechanisms in the context of multi-perspective explorations of enterprise services and Internet of Things architectures by extending original enterprise architecture reference models with state of art elements for architectural engineering and digitization. © Springer International Publishing AG 2017. Multi-perspective digitization architecture for the internet of things Decision support; Digital transformation; Digitization architecture; Internet of things; Multi-perspective adaptable enterprise architecture Agile manufacturing systems; Arts computing; Big data; Biology; Decision support systems; Digital devices; Distributed computer systems; Information systems; Internet of things; Metadata; Network architecture; Social sciences computing; Web services; Architectural engineering; Decision supports; Digital transformation; Distributed information systems; Enterprise Architecture; Internet of things architectures; Multi-perspective exploration; Service-oriented enterprise architectures; Service oriented architecture (SOA)",Governance
2214,Model-based analytics for profiling workloads in virtual network functions,"With the flexibility and programmability levels offered by Network Functions Virtualization (NFV), it is expected to catalyze the upcoming 'softwarization' of the network through software implementation of networking functionalities on virtual machines (VMs). While looking into the different issues thrown at NFV, numerous works have demonstrated how performance, power consumption and, consequently, the optimal resource configuration and VM allocation vary with the statistical features of the workload - specifically, the 'burstiness' of the traffic. This paper proposes a model-based analytics approach for profiling (virtual) network function (VNF) workloads that captures traffic burstiness, considering - and adding value to - hardware/software performance monitor counters (PMCs) available in Linux host servers. Results show good estimation accuracies for the chosen PMCs, which can be useful to enhance current methods for finegrained provisioning, usage-based pricing and anomaly detection, and facilitate the way towards an agile network. © 2017 IEEE. Model-based analytics for profiling workloads in virtual network functions  Computer operating systems; Transfer functions; Anomaly detection; Hardware/software; Network functions; Performance monitors; Resource configurations; Software implementation; Statistical features; Traffic burstiness; Network function virtualization",Financial management
2215,Sparkbench – A spark performance testing suite,"Spark has emerged as an easy to use, scalable, robust and fast system for analytics with a rapidly growing and vibrant community of users and contributors. It is multipurpose—with extensive and modular infrastructure for machine learning, graph processing, SQL, streaming, statistical processing, and more. Its rapid adoption therefore calls for a performance assessment suite that supports agile development, measurement, validation, optimization, configuration, and deployment decisions across a broad range of platform environments and test cases. Recognizing the need for such comprehensive and agile testing, this paper proposes going beyond existing performance tests for Spark and creating an expanded Spark performance testing suite. This proposal describes several desirable properties flowing from the larger scale, greater and evolving variety, and nuanced requirements of different applications of Spark. The paper identifies the major areas of performance characterization, and the key methodological aspects that should be factored into the design of the proposed suite. The objective is to capture insights from industry and academia on how to best characterize capabilities of Spark-based analytic platforms and provide cost-effective assessment of optimization opportunities in a timely manner. © Springer International Publishing Switzerland 2016. Sparkbench – A spark performance testing suite  Artificial intelligence; Big data; Cost effectiveness; Learning systems; Agile development; Graph processing; Methodological aspects; Performance assessment; Performance characterization; Performance testing; Performance tests; Statistical processing; Benchmarking",Monitoring and control
2216,Towards methods for systematic research on big data,"Big Data is characterized by the five V's - of Volume, Velocity, Variety, Veracity and Value. Research on Big Data, that is, the practice of gaining insights from it, challenges the intellectual, process, and computational limits of an enterprise. Leveraging the correct and appropriate toolset requires careful consideration of a large software ecosystem. Powerful algorithms exist, but the exploratory and often ad-hoc nature of analytic demands and a distinct lack of established processes and methodologies make it difficult for Big Data teams to set expectations or even create valid project plans. The exponential growth of data generated exceeds the capacity of humans to process it, and compels us to develop automated computing methods that require significant and expensive computing power in order to scale effectively. In this paper, we characterize data-driven practice and research and explore how we might design effective methods for systematizing such practice and research [19, 22]. Brief case studies are presented in order to ground our conclusions and insights. © 2015 IEEE. Towards methods for systematic research on big data Agile; Data Science; Data-driven research; Experimental Methods; Methodology Agile; Data driven; Data Science; Experimental methods; Methodology; Big data",Strategic alignment
2217,Big data analytics initiatives using business intelligence maturity model approach in public sector,"Big Data requirement in Public Sector is gaining focus in many countries. Described in the Eleventh Malaysia Plan 2016–2020, Big Data Analytics (BDA) has been identified as key focus for Information and Communications Technology (ICT). Especially with the recent participation of Trans-Pacific Partnership Agreement, Malaysia will be competing amongst other nations to make quick and accurate decision contributing to economic and GDP growth. With good understanding and guided implementation framework of Business Intelligence, right and agile decision can be made using the various available information. Malaysia Public Sector requires ICT involvement in Business Intelligence domain to perform Big Data Analytics. With evolving of data availability and needs for building correlation with different sets of data, Big-Data offers an emerging space to analyze wealth of information form structure, semi-structured and un-structured data set. Business Intelligence provides an organization the insight required to make the key and critical decision for leading competitive edge. © 2017 American Scientific Publishers All rights reserved. Big data analytics initiatives using business intelligence maturity model approach in public sector Big data analytics (BDA); Business intelligence maturity model (BIMM); Capability maturity model integration (CMMI) ",Strategic alignment
2218,Advanced big data analytics improves HSE management,"Safety enhancement is the priority of every organization. Annually millions of dollars are spent to develop procedures and practices to diminish the health, safety and environmental disasters. Monitoring incidents and hazards and performing timely analysis leads to effective remediation efforts. In oil and gas upstream industry, a thorough understanding of HSE data and intelligent data analytics is essential to improve safety and particularly to reduce injuries and fatalities. Most of the conventional HSE management and hazard identification systems are incapable of agile and automated data integration and smart decision making. Furthermore, HSE incident database is often too intricate to comprehend and its analysis is solely dependent on personal skills of individuals. In order to extract explanatory features' relationship and intelligently make execution strategies, a big data analytics platform is developed. This code enhances the value and quality of information entered into the HSE management systems, consequently prevents occupational hazards and results in a safer workplace. This smart platform quantifies the associated risk with every HSE decision. Oil and gas industry incidents based on area, data type and case were queried from the big data database. This algorithm was executed on a large public industry data base of occupational injury and illnesses to enhance big data HSE management. This code explored 3 million records categorized based on several parameters including super sector (major industry), industry (sub sector), data type, case type (data sub type), area (i.e. states), year. This paper exemplifies the value of advanced big data analytics to improve safety leadership. Capturing and analyzing such extremely large datasets is impossible with traditional methods. Copyright 2016, Society of Petroleum Engineers. Advanced big data analytics improves HSE management  Accident prevention; Advanced Analytics; Data Analytics; Data integration; Decision making; Gas industry; Hazards; Large dataset; Occupational risks; Execution strategies; Hazard identification systems; Health , safety and environmental; Occupational hazards; Occupational injury; Oil and gas industry incidents; Quality of information; Safety enhancement; Information management",Strategic alignment
2219,Big data analytics in logistics and supply chain management: Certain investigations for research and applications,"The amount of data produced and communicated over the Internet is significantly increasing, thereby creating challenges for the organizations that would like to reap the benefits from analyzing this massive influx of big data. This is because big data can provide unique insights into, inter alia, market trends, customer buying patterns, and maintenance cycles, as well as into ways of lowering costs and enabling more targeted business decisions. Realizing the importance of big data business analytics (BDBA), we review and classify the literature on the application of BDBA on logistics and supply chain management (LSCM) - that we define as supply chain analytics (SCA), based on the nature of analytics (descriptive, predictive, prescriptive) and the focus of the LSCM (strategy and operations). To assess the extent to which SCA is applied within LSCM, we propose a maturity framework of SCA, based on four capability levels, that is, functional, process-based, collaborative, agile SCA, and sustainable SCA. We highlight the role of SCA in LSCM and denote the use of methodologies and techniques to collect, disseminate, analyze, and use big data driven information. Furthermore, we stress the need for managers to understand BDBA and SCA as strategic assets that should be integrated across business activities to enable integrated enterprise business analytics. Finally, we outline the limitations of our study and future research directions. © 2016 Elsevier B.V. All rights reserved. Big data analytics in logistics and supply chain management: Certain investigations for research and applications Big data; Holistic business analytics; Maturity model; Methodologies and techniques; Supply chain analytics Supply chain management; Business activities; Business analytics; Future research directions; Logistics and supply chain management; Maintenance cycles; Maturity model; Methodologies and techniques; Research and application; Big data",Strategic alignment
2220,Library data labs: using an agile approach to develop library analytics in UK higher education,"Purpose: The purpose of this paper is to give an overview of the Jisc and Higher Education Statistics Agency (HESA) Library Data Labs project and its outputs. This collaboration involved bringing together cross-institutional library teams to produce proof of concept data-visualised dashboards using library analytics data that could be made available to others via the Heidi Plus service. Design/methodology/approach: The teams used an agile approach, which adapted the agile methodology for non-technical and disparate team members. The key agile elements were followed, including the Scrum approach, whereby teams had a product owner, several development team members, a data wrangler and a scrum master. Many of the dashboards took inspiration from some of the earlier Jisc work on library analytics. Findings: A wide variety of proof of concept dashboards were created addressing a range of library issues. These fell into two main categories for the cross-institutional teams, namely, comparing the Society of College, National and University Libraries (SCONUL) annual statistics results against the National Student Survey (NSS) data and collection management and analysis. Research limitations/implications: Some of the HESA data were potentially sensitive. In effect, this created a walled garden as some of the data were not designed for sharing. Furthermore, the data that the Jisc team used were restricted by publisher agreements, meaning that specific institutions’ usage could not be identified to others. Originality/value: The paper provides insight into the Library Data Labs project and discusses a number of implications from the outcomes of the project. These are now being investigated by HESA, Jisc and individual institutions. © 2018, Emerald Publishing Limited. Library data labs: using an agile approach to develop library analytics in UK higher education Academic libraries; Analytics; Business intelligence; Library usage; Student attainment; Undergraduate students; Visualizations ",Strategic alignment
2222,Position paper - Proposal for a scientific software lifecycle model,"Improvements in computational capabilities have lead to rising complexity in scientific modeling, simulation, and analytics and thus the software implementing them. In addition, a paradigm shift in platform architectures has added another dimension to complexity, to the point where software productivity (or the time, effort, and cost for software development, maintenance, and support) has emerged as a growing concern for computational science and engineering. Clearly communicating about the lifecycle of scientific software provides a foundation for community dialogue about processes and practices for various lifecycle phases that can improve developer productivity and software sustainability-key aspects of overall scientific productivity. While the mainstream software engineering community have produced lifecycle models that meet the needs of software projects in business and industry, none of the available models adequately describes the lifecycle of scientific computing software. In particular, software for end-to-end computations for obtaining scientific results has no formalized development model. Examining development approaches employed by teams implementing large multicomponent codes reveals a great deal of similarity in their strategies. In earlier work, we organized related approaches into workflow schematics, with loose coupling between submodels for development of scientific capabilities and reusable infrastructure. Here we consider an orthogonal approach, formulating models that capture the workflow of software development in slightly different scenarios, and we propose a scientific software lifecycle model based on agile principles. © 2017 Copyright held by the owner/author(s). Position paper - Proposal for a scientific software lifecycle model Scientific computing; Software engineering; Software lifecycle Computer software; Cost engineering; Digital storage; Lead compounds; Life cycle; Natural sciences computing; Productivity; Software engineering; Computational capability; Computational science and engineerings; End-to-end computations; Engineering community; Platform architecture; Scientific computing software; Software life cycles; Software productivity; Software design",Monitoring and control
2224,Local government unit analytics for program planning & policy-making in Caloocan city,"The purpose of this paper is to demonstrate the value of an information system for a Philippine local government unit (LGU) namely, the Caloocan City Hall's Planning Department Social Division. With difficulties in identifying relationships between various issues raised surrounding education and health, appropriate interventions or programs are not given. Thus, using IS benchmarking and agile methodology, the researchers proposed a system design framework in developing a data management and analytics system that is comprised of modules and visualizations customized to the needs of a Philippine LGU in uncovering relationships with their city hall data to define root causes of issues and discover trends and outliers useful in program planning for improvement of the educational and social welfare of the city. © 2017 IEEE. Local government unit analytics for program planning & policy-making in Caloocan city Data Analytics; E-Government; ICT4D; Local Government Unit; Planning Government data processing; Hall effect devices; Planning; Agile Methodologies; Analytics systems; Data analytics; Design frameworks; E-governments; ICT4D; Local government units; Program planning; Information management",Stakeholder management
2225,Cloud Transformation Analytics Services: A Case Study of Cloud Fitness Validation for Server Migration,"Migration of IT infrastructure to the Cloud transforms enterprise data, applications, and services to one or more other Cloud environments. Cloud migration engagements often rely on an in-depth discovery of the client's (source) IT environment, which is rather costly and can take up to six weeks before any meaningful conversations with customers can begin about the migration itself. There is a demand for a more agile approach to enable sales teams to perform rapid qualification of cloud fitness and reason about the benefits of Cloud using minimal information from the clients. The existing, consulting based approach typically relies on a number of discovery and analysis tools, yet the entire process is manual, expensive, and time consuming. In this paper we present a suite of cloud transformation analytics (CTA) services designed to streamline the process of premigration and migration analysis, such as cloud fitness validation and consolidation recommendations. CTA supports reasoning about diverse target clouds, as well as various transformation methods to match clients' needs, such as image migration, workload migration, and cross platform migration. We discuss our key insights and lessons learned from employing cloud fitness validation capability on datasets of up to 2000 servers to enable and accelerate the process of migration. © 2015 IEEE. Cloud Transformation Analytics Services: A Case Study of Cloud Fitness Validation for Server Migration Analytics; Cloud; Data Center; Migration Clouds; Computer programming; Analytics; Cloud transformations; Data centers; Migration; Minimal information; Transformation methods; Validation capability; Workload migration; Health",Capacity management
2226,Advanced text and video analytics for proactive decision making,"Today's warfighters operate in a highly dynamic and uncertain world, and face many competing demands. Asymmetric warfare and the new focus on small, agile forces has altered the framework by which time critical information is digested and acted upon by decision makers. Finding and integrating decision-relevant information is increasingly difficult in data-dense environments. In this new information environment, agile data algorithms, machine learning software, and threat alert mechanisms must be developed to automatically create alerts and drive quick response. Yet these advanced technologies must be balanced with awareness of the underlying context to accurately interpret machine-processed indicators and warnings and recommendations. One promising approach to this challenge brings together information retrieval strategies from text, video, and imagery. In this paper, we describe a technology demonstration that represents two years of tri-service research seeking to meld text and video for enhanced content awareness. The demonstration used multisource data to find an intelligence solution to a problem using a common dataset. Three technology highlights from this effort include 1) Incorporation of external sources of context into imagery normalcy modeling and anomaly detection capabilities, 2) Automated discovery and monitoring of targeted users from social media text, regardless of language, and 3) The concurrent use of text and imagery to characterize behaviour using the concept of kinematic and text motifs to detect novel and anomalous patterns. Our demonstration provided a technology baseline for exploiting heterogeneous data sources to deliver timely and accurate synopses of data that contribute to a dynamic and comprehensive worldview. © 2017 SPIE. Advanced text and video analytics for proactive decision making anomaly detection; machine learning; normalcy modeling; text and video analytics Artificial intelligence; Decision making; Demonstrations; Education; Learning systems; Modeling languages; Signal detection; Advanced technology; Anomaly detection; Asymmetric warfares; Automated discovery; Heterogeneous data sources; Information environment; Machine learning software; Video analytics; Digital storage",Financial management
2227,Digital Manufacturing and Flexible Assembly Technologies for Reconfigurable Aerospace Production Systems,"Reconfigurability is an important aspect of modern manufacturing systems as it facilitates the seamless introduction of new products to production and the adaptation to demand volatility. Advanced manufacturing technologies broadly used in automotive industry have limited application for typical UK aerospace manufacturing, as they require production volume and repetition of operations to deliver value. This paper discusses a framework of key technologies ranging from digital manufacturing concepts to flexible fixturing that enable reconfigurability in aerospace manufacturing systems. Initially, the overall architecture of the framework is presented illustrating the key components such as a cloud based data storage mechanism, an intelligent multi-product assembly station, kitting boxes embedded with sensors, a manufacturing network management portal and a decision support tool that combines data analytics and discrete event simulation. Afterwards, the main functionalities and technologies of the components are described and finally an industrial application scenario for the proposed framework is presented. © 2016 The Authors. Digital Manufacturing and Flexible Assembly Technologies for Reconfigurable Aerospace Production Systems Aerospace; Digital Manufacturing; M4; Manufacturing; Reconfiguration Aerospace engineering; Agile manufacturing systems; Automotive industry; Decision support systems; Digital storage; Discrete event simulation; Information management; Reconfigurable hardware; Tracking (position); Advanced manufacturing technologies; Aerospace; Aerospace manufacturing; Decision support tools; Digital manufacturing; Manufacturing networks; Production volumes; Reconfiguration; Manufacture",Value management
2229,"Digital transformation at thyssenkrupp: Challenges, strategies and examples","The digital transformation is changing the world in a continuously accelerating pace. Traditional industrial companies have a good chance to be the winner of the digital transformation. They can create additional value to their customer by optimizing and extending their current business and by creating new business models offering smart services. The paper describes thyssenkrupp’s strategy for the digital transformation illustrated by real examples. © Springer International Publishing AG 2017. Digital transformation at thyssenkrupp: Challenges, strategies and examples Agile processes; Big data; Digital transformation; Industrial internet of things; Industrie 4.0; Internet business; PLM; Predictive analytics; Smart services Big data; Information systems; Predictive analytics; Systems engineering; Agile process; Digital transformation; Industrial companies; Industrie 4.0; New business models; Real example; Smart services; ThyssenKrupp; Metadata",Strategic alignment
2231,"Utilizing high-performance embedded computing, agile condor, for intelligent processing: An artificial intelligence platform for remotely piloted aircraft","A newly invented, high performance, pod-based computer architecture, called Agile Condor (patent pending), has been designed and developed. Agile Condor is supporting autonomous operations by providing a platform for the innovative use of artificial intelligence, machine learning, and decision making algorithms upstream, near the information source, where the data is collected. In September 2016, experimental tests successfully demonstrated the ability to implement advanced neural networks and deep learning techniques on Agile Condor. We continue to use this new processing architecture, algorithms and bio-inspired computing methods to demonstrate existing, refine emerging and invent new artificial intelligence techniques that are highly applicable and needed for sensor platforms. For the first time ever, and in real-time, the system demonstrated: image processing, video processing and pattern recognition through the use of deep convolutional neural networks. Because of Agile Condor's modular architecture and performance characteristics, the system is providing flexible computational resources that will continue to bring new artificial intelligence (AI) capabilities closer to sensor platforms. © 2017 IEEE. Utilizing high-performance embedded computing, agile condor, for intelligent processing: An artificial intelligence platform for remotely piloted aircraft Artificial intelligence platform; autonomous operations; big data; high-performance embedded computing; information processing; machine learning platform; massive analytics Big data; Computer architecture; Deep neural networks; Image processing; Network architecture; Pattern recognition; Video signal processing; Artificial intelligence platform; Autonomous operations; Embedded computing; High-performance embedded computing; Learning platform; Machine learning platform; Machine-learning; Massive analytic; Performance; Sensor platform; Decision making",Financial management
2233,Continuously experiment to assess values early on,"Most modern software development activities are focusing on domains of emergence where experts cannot know a priori what kind of software provides value to users and customers. This is fundamentally different to traditional software engineering for large systems where a priori analysis by experts is used to identify requirements. While the latter is gaining a niche software category, developing and establishing development practices for domains of emergence is becoming significantly important and urgent. A major challenge is to find the right scope for software development. There are many options on what to deliver. This article presents principles and practices for steering software development towards the right scope by continuously conducting experiments. © 2016 Elsevier Inc. All rights reserved. Continuously experiment to assess values early on Agile Methods; Business Model Generation; Corporate Startup; Customer development; Data-driven Software Development; Deep Customer Insight; Design Sprint; Design Thinking; Experimentation; Hypothesis-driven development; Innovation; Lean Analytics; Lean Software Development; Lean Startup; MVP; Product Management; Requirements Engineering; Startup; Value-oriented Software Development ",Strategic alignment
2235,What can be learnt from experienced data scientists? A case study,"Data science has the potential to create value and deep customer insight for service and software engineering. Companies are increasingly applying data science to support their service and software development practices. The goal of our research was to investigate how data science can be applied in software development organisations. We conducted a qualitative case study with an industrial partner. We collected data through a workshop, focus group interview and feedback session. This paper presents the data science process recommended by experienced data scientists and describes the key characteristics of the process, i.e., agility and continuous learning. We also report the challenges experienced while applying the data science process in customer projects. For example, the data scientists highlighted that it is challenging to identify an essential problem and ensure that the results will be utilised. Our findings indicate that it is important to put in place an agile, iterative data science process that supports continuous learning while focusing on a real business problem to be solved. In addition, the application of data science can be demanding and requires skills for addressing human and organisational issues. © Springer International Publishing AG 2017. What can be learnt from experienced data scientists? A case study Data science; Service engineering; Software development Process engineering; Software engineering; Continuous learning; Data science; Essential problems; Industrial partners; Key characteristics; Qualitative case studies; Service engineering; Software development practices; Software design",Value management
2236,"The quantified self in precarity: Work, technology and what counts","Humans are accustomed to being tool bearers, but what happens when machines become tool bearers, calculating human labour via the use of big data and people analytics by metrics? The Quantified Self in Precarity highlights how, whether it be in insecure 'gig' work or office work, such digitalisation is not an inevitable process-nor is it one that necessarily improves working conditions. Indeed, through unique research and empirical data, Moore demonstrates how workplace quantification leads to high turnover rates, workplace rationalisation and worker stress and anxiety, with these issues linked to increased rates of subjective and objective precarity. Scientific management asked us to be efficient. Now, we are asked to be agile. But what does this mean for the everyday lives we lead? With a fresh perspective on how technology and the use of technology for management and self-management changes the 'quantified', precarious workplace today, The Quantified Self in Precarity will appeal to undergraduate and postgraduate students interested in fields such as Science and Technology, Organisation Management, Sociology and Politics. © 2018 Phoebe V. Moore. All rights reserved. The quantified self in precarity: Work, technology and what counts  ",Financial management
2238,CEUR Workshop Proceedings,The proceedings contain 17 papers. The topics discussed include: normalization of processes toward an integrated view of business process system; task characteristics that fit use of business intelligence; the smart production laboratory: a learning factory for Industry 4.0 concepts; closing the IT development-operations gap: the DevOps knowledge sharing framework; a repository for pattern governance supporting capability driven development; a framework for effort estimation in BPMS migration; automated process model annotation support: building blocks and parameters; ontology based PEP identification; identifying contributions for the evaluation of cloud services in state entities; agile software engineering practices and ERP: is a sprint too fast for ERP implementation; preference to use aggregators rather than individual deal sites: impact of big five inventory personality traits; the Industry 4.0 journey: start the learning journey with the reference architecture model industry 4.0; simplifying the DevOps adoption process; timber tracking reducing complexity of due diligence by using blockchain technology; managing complexity in process digitalisation with dynamic condition response graphs; and which factors are significant for obtaining business intelligence success in the public sector?. CEUR Workshop Proceedings  ,Financial management
2239,"Proceedings - 2017 IEEE 21st International Enterprise Distributed Object Computing Conference, EDOC 2017","The proceedings contain 23 papers. The topics discussed include: reference architecture for integration platforms; from data-centric business processes to enterprise process frameworks; network science applied to enterprise architecture analysis: towards the foundational concepts; a viewpoint for analyzing enterprise architecture evolution; a viewpoint for analyzing enterprise architecture evolution; standards-based function shipping - how to use TOSCA for shipping and executing data analytics software in remote manufacturing environments; an interactive platform to simulate dynamic pricing competition on online marketplaces; re-engineering enterprises using data warehouse as a driver and requirements as an enabler; from natural language to SBVR model authoring using structured English for compliance checking; discovering instance-spanning constraints from process execution logs based on classification techniques; visualisation of compliant declarative business processes; flexible task management support for knowledge-intensive processes; toward application integration with multimedia data; investigating the role of architects in scaling agile frameworks; enabling fine-grained access control in flexible distributed object-aware process management systems; a metric for evaluating the privacy level of a business process logic in a multi-cloud deployment; multi-criteria decision analysis for change negotiation in process collaborations; and modification operations for context-aware business rule management. Proceedings - 2017 IEEE 21st International Enterprise Distributed Object Computing Conference, EDOC 2017  ",Monitoring and control
2242,SlicerAstro: A 3-D interactive visual analytics tool for HI data,"SKA precursors are capable of detecting hundreds of galaxies in HI in a single 12 h pointing. In deeper surveys one will probe more easily faint HI structures, typically located in the vicinity of galaxies, such as tails, filaments, and extraplanar gas. The importance of interactive visualization in data exploration has been demonstrated by the wide use of tools (e.g. Karma, Casaviewer, VISIONS) that help users to receive immediate feedback when manipulating the data. We have developed SlicerAstro, a 3-D interactive viewer with new analysis capabilities, based on traditional 2-D input/output hardware. These capabilities enhance the data inspection, allowing faster analysis of complex sources than with traditional tools. SlicerAstro is an open-source extension of 3DSlicer, a multi-platform open source software package for visualization and medical image processing. We demonstrate the capabilities of the current stable binary release of SlicerAstro, which offers the following features: (i) handling of FITS files and astronomical coordinate systems; (ii) coupled 2-D/3-D visualization; (iii) interactive filtering; (iv) interactive 3-D masking; (v) and interactive 3-D modeling. In addition, SlicerAstro has been designed with a strong, stable and modular C++  core, and its classes are also accessible via Python scripting, allowing great flexibility for user-customized visualization and analysis tasks. © 2017 Elsevier B.V. SlicerAstro: A 3-D interactive visual analytics tool for HI data Agile software development; Empirical software validation; Object oriented development; Radio lines: galaxies; Scientific visualization; Visual analytics C++ (programming language); Computer programming; Data visualization; Galaxies; Image processing; Medical imaging; Object oriented programming; Open source software; Open systems; Software design; Software engineering; Visualization; Agile software development; Object oriented development; Radio lines: galaxies; Software validation; Visual analytics; Three dimensional computer graphics",Capacity management
2244,Agile big data analytics: AnalyticsOps for data science,"Big data analytic (BDA) systems leverage data distribution and parallel processing across a cluster of resources. This introduces a number of new challenges specifically for analytics. The analytics portion of the complete lifecycle has typically followed a waterfall process - completing one step before beginning the next. While efforts have been made to map different types of analytics to an agile methodology, the steps are often described as breaking activities into smaller tasks while the overall process is still consistent with step-by-step waterfall. BDA changes a number of the activities in the analytics lifecycle, as well as their ordering. The goal of agile analytics - to reach a point of optimality between generating value from data and the time spent getting there. This paper discusses the implications of an agile process for BDA in cleansing, transformation, and analytics. © 2017 IEEE. Agile big data analytics: AnalyticsOps for data science advanced analytics; agile development; analytics lifecycle; AnalyticsOps; big data analytics; data science; data science process models; Deep Learning; DevOps; Knowledge Discovery in Data Science; machine learning Big data; Data Analytics; Data Science; Deep learning; Learning systems; Life cycle; Parallel processing systems; Agile development; AnalyticsOps; DevOps; Knowledge discovery in data; Process model; Advanced Analytics",Strategic alignment
2245,Capabilities to achieve business intelligence agility - Research model and tentative results,"The class of business intelligence (BI) systems is used as a basis for decision making in most big organizations. Extensive initiatives have been launched to accomplish adequate and timely decision support as an important factor to achieve and sustain competitive advantage. Within turbulent market environments it is challenging to keep up a distinguishable long-term strategy while quickly reacting to changing circumstances. This area of conflicts holds particularly true for BI as it is originally used to retrospectively reflect an organization's performance and built upon stability and efficiency. Therefore, we investigate how dynamic BI capabilities, i.e. adoption of assets, market understanding and intimacy as well as business operations, impact the agility of BI. We approach our goal from a dynamic capability perspective. Starting from a literature review of dynamic capabilities of information systems (IS) and BI, we propose hypotheses to connect dynamic BI capabilities and BI agility. Derived hypotheses based on existing literature will be tested in our prospective research agenda. A small pre-study showed promising results. In-memory (I AM) technology seems to be a technology enabler for agile BI. However, adoption of BI assets and the focus on market orientation and business operations may even intensify the positive effect. Capabilities to achieve business intelligence agility - Research model and tentative results Agility; Business intelligence; Dynamic capabilities; PLS-SEM Commerce; Competition; Competitive intelligence; Decision making; Decision support systems; Enterprise resource management; Information systems; Agility; Business intelligence systems; Business operation; Competitive advantage; Dynamic capabilities; Dynamic capability perspective; Literature reviews; Market environment; Information analysis",Financial management
2246,Using Analytics to Guide Improvement during an Agile-DevOps Transformation,"Over the past three years, Fannie Mae IT has transformed from a traditional waterfall organization to a lean culture enabled by agile methods and DevOps. Software analytics were used to guide improvements and evaluate progress. Project-level analytics enabled agile teams to improve structural quality and evaluate their practices as they delivered greater functionality over shrinking delivery intervals. Aggregated enterprise metrics displayed an average 38 percent improvement in structural quality, with some applications achieving 48 percent to 70 percent gains. These quality improvements accompanied a 28 percent increase in productivity. The frequency of releases shrank from cycles of nine to 18 months, down to releases every couple of one- to two-month sprints. The article discusses examples of how analytics were used, along with challenges in selecting measures and implementing analytics in an agile-DevOps transformation. This article is part of a special issue on Actionable Analytics for Software Engineering. © 1984-2012 IEEE. Using Analytics to Guide Improvement during an Agile-DevOps Transformation agile development; agile transformation; DevOps; software analytics; software development; software engineering; software measurement; software productivity; software quality Computer software selection and evaluation; Productivity; Quality control; Software design; Agile development; Agile transformations; DevOps; Software Measurement; Software productivity; Software Quality; Software engineering",Risk management
2249,Self-Corrective Dynamic Networks via Decentralized Reverse Computations,"The feasibility of large-scale decentralized networks for local computations, as an alternative to big data systems that are often privacy-intrusive, expensive and serve exclusively corporate interests, is usually questioned by network dynamics such as node leaves, failures and rejoins in the network. This is especially the case when decentralized computations performed in a network, such as the estimation of aggregation functions, e.g. summation, are linked to the actual nodes connected in the network, for instance, counting the sum using input values from only connected nodes. Reverse computations are required to maintain a high aggregation accuracy when nodes leave or fail. This paper introduces an autonomic agent-based model for highly dynamic self-corrective networks using decentralized reverse computations. The model is generic and equips the nodes with the capability to disseminate connectivity status updates in the network. Highly resilient agents to the dynamic network migrate to remote nodes and orchestrate reverse computations for each node leave or failure. In contrast to related work, no other computational resources or redundancy are introduced. The self-corrective model is experimentally evaluated using real-world data from a smart grid pilot project under highly dynamic network adjustments that correspond to catastrophic events with up to 50% of the nodes leaving the network. The model is highly agile and modular and is applied to the large-scale decentralized aggregation network of DIAS, the Dynamic Intelligent Aggregation Service, without major structural changes in its design and operations. Results confirm the outstanding improvement in the aggregation accuracy when self-corrective actions are employed with a minimal increase in communication overhead. © 2017 IEEE. Self-Corrective Dynamic Networks via Decentralized Reverse Computations accuracy; adaptation; agent; aggregation; data analytics; decentralized network; fault-tolerance; migration; reverse computation; robustness; self-correction Agents; Agglomeration; Autonomous agents; Computational methods; Fault tolerance; Robustness (control systems); accuracy; adaptation; Data analytics; Decentralized networks; migration; Reverse computation; Self-correction; Big data",Risk management
2250,EFFECTIVENESS of AGILE COMPARED to WATERFALL IMPLEMENTATION METHODS in IT PROJECTS: ANALYSIS BASED on BUSINESS INTELLIGENCE PROJECTS,"The global Business Intelligence (BI) market grew by 7.3% in 2016 according to the Gartner report (2017). Today, organizations require better use of data and analytics to support their business decisions. Internet power and business trend changes have provided a broad term for data analytics - Big Data. To be able to handle it and leverage a value of having access to Big Data, organizations have no other choice than to get proper systems implemented and working. However, traditional methods are not efficient for changing business needs. Long time between project start and go-live causes a gap between initial solution blueprint and actual user requirements at the end of the project. This article presents the latest market trends in BI systems implementation by comparing agile with traditional methods. It presents a case study provided in a large telecommunications company (350 BI users) and the results of a pilot research provided in the three large companies: media, digital, and insurance. Both studies prove that agile methods might be more effective in BI projects from an end-user perspective and give first results and added value in a much shorter time compared to a traditional approach. © Faculty of Management, Warsaw University of Technology 2017. EFFECTIVENESS of AGILE COMPARED to WATERFALL IMPLEMENTATION METHODS in IT PROJECTS: ANALYSIS BASED on BUSINESS INTELLIGENCE PROJECTS agile; analytics; big data; business intelligence; end-users requirements; sprint; waterfall ",Governance
2253,"Business intelligence competence, agile capabilities, and agile performance in supply chain An empirical study","Purpose - The purpose of this paper is to study the role of business intelligence (BI) in achieving agility in supply chain context by examining the relationship between BI competence, agile capabilities, and agile performance of the supply chain. Design/methodology/approach - A theoretical framework is developed drawing on the resourcebased view, the dynamic capabilities perspective, and the competence-capability relationship paradigm, as well as an extensive review of the literature. Structural equation modeling is employed to analyze the data collected from Iranian manufacturers in the automotive industry. Findings - The empirical results support the conceptualization of supply chain BI competence as a multi-dimensional construct comprising managerial, technical, and cultural competence, and confirm that it is a key enabler of supply chain agility in terms of both agile capabilities and agile performance. The results also provide support for partial mediation of agile capabilities on the relationship between BI competence and agile performance of the supply chain. Originality/value - This paper provides a response to the identified need for empirical evidence on the benefits derived from BI, especially in the supply chain context. It also contributes to the existing supply chain agility literature by providing insight into the value and role of BI in enhancing agile capabilities and performance in the inter-organizational supply chain. © Emerald Group Publishing Limited. Business intelligence competence, agile capabilities, and agile performance in supply chain An empirical study Agile capabilities; Agile performance; BI competence; Business intelligence (BI); Structural equation modelling; Supply chain agility ",Capacity management
2254,Taking a different approach to drilling data aggregation to improve drilling performance,"Currently, there is a multitude of commercially available real-time drilling data aggregation and distribution systems, yet the industry remains plagued with issues that limit the usability and effectiveness of data before, during, and after a well is drilled. There are challenges with moving, merging, analyzing, qualifying, and formatting data as well as having access to like-data in sufficient quantity and on a reliable data frequency. This paper discusses a novel, adaptable, and low cost approach to building a system to drive drilling performance and set the stage for future automation. The Operator embarked on a project to develop a powerful, low cost system in order to leverage both high and low frequency data to gain value from real-time data models and algorithms at the rig site. High frequency data is defined as 1 to 100 Hertz data frequency. Low frequency data is defined as longer than once per hour or asynchronous, and is usually contextual - BHA information, mud reports, rig state, etc. Existing commercial systems fail to meet the requirements due to multiple factors. These include an inability to handle and process high frequency data, communicate with different protocols, and work across different proprietary systems. The result leads to higher costs, extra human resources and efforts, and a lack of consistency across a diverse rig fleet. Druing this process severe data quality issues were discovered at the rig site and needed the flexibility to modify, replace, or add sensors and data streams to remedy the problem. After evaluating more than thirty potential process controls and other industry applications, a software solution was selected, prototyped, tested and deployed to seven North American land rigs within a ten month period. This effort employed the agile development methodology which is an incremental, iterative work cadence using empirical feedback for rapid deployment of updated versions. The system was designed to take in all forms of data, file types, and communication protocols for seamless integration. The system includes rig state determination, data quality verification, a real time Bayesian model for analytics and smart alarms, integration to the Daily Drilling Report (DDR) database, real-time visualizations, and an open application layer with a Human Machine Interface (HMI) - all at the rig site. Ultimately the platform can also be used as a building block to assist automated drilling due to it being a Supervisory Control Advisory and Data Acquisition (SCADA) system although this is not the goal for this project. Copyright © 2017, SPE/IADC Drilling Conference and Exhibition. Taking a different approach to drilling data aggregation to improve drilling performance  Application programs; Bayesian networks; Costs; Data acquisition; Data streams; Digital storage; Infill drilling; Interface states; Iterative methods; Process control; Real time systems; Software testing; Agile development methodologies; Distribution systems; Drilling performance; Human Machine Interface; Industry applications; Models and algorithms; Real time visualization; Seamless integration; Data integration",Risk management
2256,Agile analytics: Slicing data warehousing user stories for business value,[No abstract available] Agile analytics: Slicing data warehousing user stories for business value  ,Monitoring and control
2260,"17th Americas Conference on Information Systems 2011, AMCIS 2011","The proceedings contain 481 papers. The special focus in this conference is on Developments, Technologies, Knowledge, Benefits and Services of America's Information System. The topics include: Knowledge goals as an essential component of knowledge management; the impact of culture on the enterprise systems adoption in Japan and the YOU.S.; IT governance for e-health applications; developing sustainable IT-related capabilities; smart-board technology integration in teaching; model for the benefit analysis of ICT; a survey of cognitive theories to support data integration; women managers in high tech; a design theory for knowledge transfer in business intelligence; the perceived uniqueness of the IS profession; IT governance in collaborative organizational structures; power interactions in enterprise system assimilation; a typology of positive and negative self-interruptions in voluntary multitasking; business intelligence maturity in Australia; a nursing diagnosis decision aid adoption assessment; a user centric typology of IS requirements; social movements in World of Warcraft; perceptions of sunk cost and habitual IS use; IS architecture characteristics as a measure of IT agility; the effects of ICT pervasiveness on administrative corruption; collaborative learning in software development teams; demand management in the smart grid; software characteristics of B2B electronic intermediaries; model to support Enterprise Resource Planning system selection; maintenance trends in ERP systems; the role of dynamic capabilities in creating business value from IS assets; spatial analysis of the global digital divide; different configurations of flexibility for I/S strategic alignment; towards objectives-based process redesign; model for measuring efficiency of Argentina banks; the effect of CRM system on sales management control; towards a design for IT performance management; Web 3.0 and crowd servicing; inherence of ratios for service identification and evaluation; Information Communication Technology adoption in Moroccan small and medium enterprises; dynamic capabilities and business processes; understanding artifact knowledge in design science; using real options in ERP-systems for improving delivery reliability; inter-organizational integration of smart objects; risk and compliance management for cloud computing services; a comprehensive information model for business change projects; experiential learning in second life; organizational participation in open communities; organizational social media around the GLOBE; structural stability and virtual team performance; the impact of IT governance on organizational performance; analysing knowledge-based growth; designing viable security solutions; factors affecting impact of cloud computing announcements on firm valuation; toward a maturity model for DSS development processes; a meta-model ontology based on scenarios; key influencing factors of information systems quality and success in Jamaican organizations; the state of the art of service description languages; agile software development; revenue streams of cloud-based platforms; IT outsourcing as a source of open innovation; using IS/IT valuation methods in practice; working toward optimal pair programming management and environment; the role of Business Intelligence (BI) in service innovation; management of information systems outsourcing; autonomy and electronic health records; the impact of e-service quality on e-commerce; medical errors and information quality; a model for assessing the performance of virtual teams; knowledge as a contingency factor in virtual organizations; individual knowledge sharing behaviour in organizations; sustainable business transformation; towards a life cycle oriented business intelligence success model; adoption of cloud computing in organizations; conceptualizing knowledge utilization; detecting community influence echelons in Twitter network; team collaboration in virtual worlds; building sustainable collaborative networks; towards an enterprise software component ontology; business modelling for services; a cultural sociology perspective on IT occupational culture; social network sites and digital word of mouth; learning object composition; measuring innovation using business intelligence dashboards; examining SNS adoption through motivational lens; business intelligence in corporate risk management; business and IT aspects of wireless enabled healthcare solutions; developing a dichotomy of information privacy concerns; self-selected identity and social capital in social network sites; developing data marts for healthcare; developing a theory driven view of web based homework; evaluating design solutions using crowds; examining ethical decision making behaviour in e-learning systems; data mining meets decision making; networking skills and hiring managers; a text-based model for identifying online trust relationships and a threat tree for health information security and privacy. 17th Americas Conference on Information Systems 2011, AMCIS 2011  ",Governance
2261,Work systems based fractal architecture of information systems,"Contemporary information systems have to satisfy needs of agile and viable enterprises. They shall include mechanisms of business intelligence, business process management, information technology infrastructure management, and alignment between business and computer systems. The mechanisms for business process handling and computer systems handling are similar, and the mechanisms for their continuous integrated improvement also are similar, therefore the architecture of information systems components that support these processes can have a measure of similarity if considered at a particular level of abstraction. The paper, focusing on aforementioned similarities, uses St. Alter's work systems paradigm for constructing fractal architecture of information systems that can be used for supporting agile and viable organizations. Work systems based fractal architecture of information systems Fractal architecture; Information systems; Work system Administrative data processing; Agile manufacturing systems; Computer architecture; Enterprise resource management; Fractals; Information systems; Management science; Systems engineering; Business Process; Business process management; Fractal architecture; Information technology infrastructure; Level of abstraction; Measure of similarities; Work system; Information management",Governance
2262,CRASP - a strategic methodology perspective for sustainable value Chain management,"Many enterprises (including government and business organisations) are currently showing great concern regarding sustainability development. Strategic sustainability assessment is evolving as a mechanism that attempts to assess systematically the performance impact of decisions made at what is conventionally called, 'levels of strategic decisions'. It is required to provide a comprehensive review of existing and practical approaches of designing and presenting the strategic sustainable development perspective of addressing value chain. This article seeks to provide the demand chain management perspective based CRASP methodology as an adaptive enterprise model developed with agile systems thinking to address the issues and challenges by reflecting a triple bottom line approach to sustainability. These integrated assessment processes are intended to minimize uncertainty through adopting information management and control techniques to e-governance and e-business applications so as to make them as sustainable practices for the regular conduct of governance and business processes. A range of sustainability assessment methodologies are already available [46]. However it is believed that CRASP methodology based assessment offers a more comprehensive and holistic approach as it demands to adopt big data analytics based knowledge management approach and techniques in assessing sustainability measures and indicators towards achieving stakeholders' satisfaction by categorizing them as customers and providers. It provides an opportunity to allow consensus building in enterprise decision making. CRASP - a strategic methodology perspective for sustainable value Chain management CRASP methodology; Enterprise performance impact assessment; Knowledge based sustainability; Sustainable value chain management Competition; Customer satisfaction; Knowledge based systems; Knowledge management; Management science; CRASP methodology; Demand chain management; E-business applications; Enterprise decision-making; Enterprise performance; Knowledge based; Sustainability assessment; Sustainable values; Sustainable development",Strategic alignment
2265,How to make business intelligence agile: The agile BI actions catalog,"The need to develop Business Intelligence (BI) systems which are able to react to unforeseen or volatile requirements in a given time frame results from increasingly complex and dynamic organizational environments. This adaptation capability of BI systems is usually referred to as 'BI agility'. Although a consensus within literature exists about the demand for agile BI systems, a structured overview on how such systems can be developed is missing. Closing this research gap, the paper at hand presents a catalog of Agile BI actions which is based on a comprehensive literature review. It includes 21 assessed actions suitable to increase the agility of BI systems. Therefore the catalog provides benefits to scientists (identification of research areas) as well as practitioners (identification of sophisticated actions for implementation). It consists of the four action categories 'principles', 'process models', 'techniques', and 'technologies'. © 2015 IEEE. How to make business intelligence agile: The agile BI actions catalog Agile bi; Agile business intelligence; Agility; Literature review Information analysis; Agility; Business intelligence systems; Literature reviews; Process model; Time frame; Agile manufacturing systems",Governance
2269,"17th Americas Conference on Information Systems 2011, AMCIS 2011","The proceedings contain 481 papers. The special focus in this conference is on Developments, Technologies, Knowledge, Benefits and Services of America's Information System. The topics include: Knowledge goals as an essential component of knowledge management; the impact of culture on the enterprise systems adoption in Japan and the YOU.S.; IT governance for e-health applications; developing sustainable IT-related capabilities; smart-board technology integration in teaching; model for the benefit analysis of ICT; a survey of cognitive theories to support data integration; women managers in high tech; a design theory for knowledge transfer in business intelligence; the perceived uniqueness of the IS profession; IT governance in collaborative organizational structures; power interactions in enterprise system assimilation; a typology of positive and negative self-interruptions in voluntary multitasking; business intelligence maturity in Australia; a nursing diagnosis decision aid adoption assessment; a user centric typology of IS requirements; social movements in World of Warcraft; perceptions of sunk cost and habitual IS use; IS architecture characteristics as a measure of IT agility; the effects of ICT pervasiveness on administrative corruption; collaborative learning in software development teams; demand management in the smart grid; software characteristics of B2B electronic intermediaries; model to support Enterprise Resource Planning system selection; maintenance trends in ERP systems; the role of dynamic capabilities in creating business value from IS assets; spatial analysis of the global digital divide; different configurations of flexibility for I/S strategic alignment; towards objectives-based process redesign; model for measuring efficiency of Argentina banks; the effect of CRM system on sales management control; towards a design for IT performance management; Web 3.0 and crowd servicing; inherence of ratios for service identification and evaluation; Information Communication Technology adoption in Moroccan small and medium enterprises; dynamic capabilities and business processes; understanding artifact knowledge in design science; using real options in ERP-systems for improving delivery reliability; inter-organizational integration of smart objects; risk and compliance management for cloud computing services; a comprehensive information model for business change projects; experiential learning in second life; organizational participation in open communities; organizational social media around the GLOBE; structural stability and virtual team performance; the impact of IT governance on organizational performance; analysing knowledge-based growth; designing viable security solutions; factors affecting impact of cloud computing announcements on firm valuation; toward a maturity model for DSS development processes; a meta-model ontology based on scenarios; key influencing factors of information systems quality and success in Jamaican organizations; the state of the art of service description languages; agile software development; revenue streams of cloud-based platforms; IT outsourcing as a source of open innovation; using IS/IT valuation methods in practice; working toward optimal pair programming management and environment; the role of Business Intelligence (BI) in service innovation; management of information systems outsourcing; autonomy and electronic health records; the impact of e-service quality on e-commerce; medical errors and information quality; a model for assessing the performance of virtual teams; knowledge as a contingency factor in virtual organizations; individual knowledge sharing behaviour in organizations; sustainable business transformation; towards a life cycle oriented business intelligence success model; adoption of cloud computing in organizations; conceptualizing knowledge utilization; detecting community influence echelons in Twitter network; team collaboration in virtual worlds; building sustainable collaborative networks; towards an enterprise software component ontology; business modelling for services; a cultural sociology perspective on IT occupational culture; social network sites and digital word of mouth; learning object composition; measuring innovation using business intelligence dashboards; examining SNS adoption through motivational lens; business intelligence in corporate risk management; business and IT aspects of wireless enabled healthcare solutions; developing a dichotomy of information privacy concerns; self-selected identity and social capital in social network sites; developing data marts for healthcare; developing a theory driven view of web based homework; evaluating design solutions using crowds; examining ethical decision making behaviour in e-learning systems; data mining meets decision making; networking skills and hiring managers; a text-based model for identifying online trust relationships and a threat tree for health information security and privacy. 17th Americas Conference on Information Systems 2011, AMCIS 2011  ",Governance
2273,Notice of Retraction: Study on prediction model of surface roughness in external cylindrical grinding,"Multiple linear regression is used on establishing a prediction model of surface roughness in cylindrical grinding, according to systematic analysis and testing. The nearly relationship between surface roughness and operating parameters(such as workpiece speed, wheel speed, grinding depth and vertical feed) is more deeply revealed. It is showed that the model has simple structure and well predictive capabilities by the significant testing and examination. The model will provide a reliable basis for intelligent control and virtual manufacturing, and achieve the purpose of precise control of the grinding process. © 2010 IEEE. Notice of Retraction: Study on prediction model of surface roughness in external cylindrical grinding External cylindrical grinding; Multiple linear regression; Surface roughness Agile manufacturing systems; Grinding (machining); Linear regression; Predictive analytics; Well testing; Cylindrical grinding; External cylindrical grinding; Multiple linear regressions; Operating parameters; Predictive capabilities; Simple structures; Systematic analysis; Virtual manufacturing; Surface roughness",Monitoring and control
2278,"Tagungsband Multikonferenz Wirtschaftsinformatik 2014, MKWI 2014","The proceedings contain 179 papers. The topics discussed include: the influence of prior on-premise use on the confirmation and perception of infrastructure-specific system quality - an empirical study; organizational adoption of social networking services - a literature review; extending process virtualization theory: development of a research model and pre-test; do management and IT consulting firms provide different types of innovations for their clients? an activity analysis; an exploration of factors influencing the continuous use of business intelligence systems; towards a price forecast model for the German electricity market based on structured and unstructured data; predicting repayment success in IT-mediated peer-to-peer lending: the value of soft information; towards automated adaptation of disaster response processes - an approach to insert transport activities; agile market models - design and evaluation of efficient structures for a mass-customizable market platform; towards an awareness gap on cybercrime - an empirical analysis of the perceived threat level and implemented IT security measures in companies; and improving comprehensibility of medical information - proof-of-concept for patient information leaflets. Tagungsband Multikonferenz Wirtschaftsinformatik 2014, MKWI 2014  ",Value management
2280,Agile software architecture in advanced data analytics,"Requirements evolve over the development lifecycle of a software project. Agile practices are designed specifically to address this challenge while showing early and continuous progress towards project goals. Applying an agile approach allows stakeholders to adapt the scope and capabilities of a development release to changing market needs. More recently, an agile approach has been recommended for developing the architecture of software systems, enabling the design to support current requirements and early releases while evolving to meet future expectations. Our experience defining emergent software systems to build a product line architecture for advanced data analytics demonstrates the benefits that can be gained from prioritizing work activities and delaying architecture decisions. This paper proposes a process and ontology for agile architecture development. Only the necessary aspects for each evolutionary release are designed and prototyped, as determined by expectations of the identified application domain scenarios. Feedback from implementing the scenarios using the architecture extends our understanding of the requirements and provides the backlog for successive design iterations. © 2014 IEEE. Agile software architecture in advanced data analytics agile development; industrial experiments; industry best practices; product line architectures; software architecture Computer software; Agile architectures; Agile development; Architecture decisions; Best practices; Design iteration; Industrial experiments; Product line architecture; Software systems; Software architecture",Governance
2281,Leveraging the capabilities of service-oriented decision support systems: Putting analytics and big data in cloud,"Using service-oriented decision support systems (DSS in cloud) is one of the major trends for many organizations in hopes of becoming more agile. In this paper, after defining a list of requirements for service-oriented DSS, we propose a conceptual framework for DSS in cloud, and discus about research directions. A unique contribution of this paper is its perspective on how to servitize the product oriented DSS environment, and demonstrate the opportunities and challenges of engineering service oriented DSS in cloud. When we define data, information and analytics as services, we see that traditional measurement mechanisms, which are mainly time and cost driven, do not work well. Organizations need to consider value of service level and quality in addition to the cost and duration of delivered services. DSS in CLOUD enables scale, scope and speed economies. This article contributes new knowledge in service science by tying the information technology strategy perspectives to the database and design science perspectives for a broader audience. Leveraging the capabilities of service-oriented decision support systems: Putting analytics and big data in cloud Analytics-as-a-service; Big data; Cloud computing; Data-as-a-service; Information-as-a-service; Service orientation; Service science Artificial intelligence; Cloud computing; Decision support systems; Design; Information technology; Analytics-as-a-service; Big datum; Data-as-a-service; Information-as-a-service; Service orientation; Service science; Digital storage",Value management
2284,"17th Americas Conference on Information Systems 2011, AMCIS 2011","The proceedings contain 481 papers. The special focus in this conference is on Developments, Technologies, Knowledge, Benefits and Services of America's Information System. The topics include: Knowledge goals as an essential component of knowledge management; the impact of culture on the enterprise systems adoption in Japan and the YOU.S.; IT governance for e-health applications; developing sustainable IT-related capabilities; smart-board technology integration in teaching; model for the benefit analysis of ICT; a survey of cognitive theories to support data integration; women managers in high tech; a design theory for knowledge transfer in business intelligence; the perceived uniqueness of the IS profession; IT governance in collaborative organizational structures; power interactions in enterprise system assimilation; a typology of positive and negative self-interruptions in voluntary multitasking; business intelligence maturity in Australia; a nursing diagnosis decision aid adoption assessment; a user centric typology of IS requirements; social movements in World of Warcraft; perceptions of sunk cost and habitual IS use; IS architecture characteristics as a measure of IT agility; the effects of ICT pervasiveness on administrative corruption; collaborative learning in software development teams; demand management in the smart grid; software characteristics of B2B electronic intermediaries; model to support Enterprise Resource Planning system selection; maintenance trends in ERP systems; the role of dynamic capabilities in creating business value from IS assets; spatial analysis of the global digital divide; different configurations of flexibility for I/S strategic alignment; towards objectives-based process redesign; model for measuring efficiency of Argentina banks; the effect of CRM system on sales management control; towards a design for IT performance management; Web 3.0 and crowd servicing; inherence of ratios for service identification and evaluation; Information Communication Technology adoption in Moroccan small and medium enterprises; dynamic capabilities and business processes; understanding artifact knowledge in design science; using real options in ERP-systems for improving delivery reliability; inter-organizational integration of smart objects; risk and compliance management for cloud computing services; a comprehensive information model for business change projects; experiential learning in second life; organizational participation in open communities; organizational social media around the GLOBE; structural stability and virtual team performance; the impact of IT governance on organizational performance; analysing knowledge-based growth; designing viable security solutions; factors affecting impact of cloud computing announcements on firm valuation; toward a maturity model for DSS development processes; a meta-model ontology based on scenarios; key influencing factors of information systems quality and success in Jamaican organizations; the state of the art of service description languages; agile software development; revenue streams of cloud-based platforms; IT outsourcing as a source of open innovation; using IS/IT valuation methods in practice; working toward optimal pair programming management and environment; the role of Business Intelligence (BI) in service innovation; management of information systems outsourcing; autonomy and electronic health records; the impact of e-service quality on e-commerce; medical errors and information quality; a model for assessing the performance of virtual teams; knowledge as a contingency factor in virtual organizations; individual knowledge sharing behaviour in organizations; sustainable business transformation; towards a life cycle oriented business intelligence success model; adoption of cloud computing in organizations; conceptualizing knowledge utilization; detecting community influence echelons in Twitter network; team collaboration in virtual worlds; building sustainable collaborative networks; towards an enterprise software component ontology; business modelling for services; a cultural sociology perspective on IT occupational culture; social network sites and digital word of mouth; learning object composition; measuring innovation using business intelligence dashboards; examining SNS adoption through motivational lens; business intelligence in corporate risk management; business and IT aspects of wireless enabled healthcare solutions; developing a dichotomy of information privacy concerns; self-selected identity and social capital in social network sites; developing data marts for healthcare; developing a theory driven view of web based homework; evaluating design solutions using crowds; examining ethical decision making behaviour in e-learning systems; data mining meets decision making; networking skills and hiring managers; a text-based model for identifying online trust relationships and a threat tree for health information security and privacy. 17th Americas Conference on Information Systems 2011, AMCIS 2011  ",Governance
2289,Exploit unstructured data using deep analytics to optimise enterprise IT asset management,"Unstructured data pose a huge risk towards supporting critical goals of sustainable reconciliation of inventory due to over spending and audit exposure. They are inherently fragmented, incomplete and without restriction. Hence domain specific explicit information extracted from unstructured data may not be complete or contextually accurate or both for reconciliation. It requires inference to extract implicit information to make the extracted information usefully complete. Domain reconciliation modelled in relational database is static, requires deep domain knowledge prior to modelling and depends on surface commonalities of data labels and values between source and target. These design constraints make relational model-based reconciliation unfit for reconciling entities extracted from unstructured data. In this paper we propose an ontology-based semantic information model and semantic reconciliation mediator to extract valid entity information from unstructured data in knowledge format and reconcile them using pattern-based reconciliation. This agile, superior way of integrating information makes it possible to support unstructured information processing through inference and incorporates effects of temporal events that impact the ownership and usage rights of resources as well. Copyright © 2013 Inderscience Enterprises Ltd. Exploit unstructured data using deep analytics to optimise enterprise IT asset management CMDB; Configuration management database; Information technology infrastructure library; ITIL; ITSM; Ontology; RDF; Resource description framework; Semantic reconciliation; Semantic web; SLM; Software license management; Unstructured data ",Financial management
2292,COVER model pivot view indexing for efficient XML data management,"In order to support corporate business' competition on speed to market for product and service development, generically modeled data structures have been widely used in the development of vertical application software systems, and in storing XML and RDF data for its flexibility, adaptability, and agility. However, generic data models require multiple self-joins on a single table with a large volume of data, causing slow performance for business intelligence (BI) applications. On the other hand, shredded XML data stored in traditional specific data models have faster performance but are not flexible, adaptive, or agile for speed to market. A generic data model named the Class Object Value Element Relationship (COVER) model was developed for storing node-oriented tree data information, and is suitable for automated pivot view generation and distributed data processing. This approach utilizes pivot views with appropriate metadata constructs to expose the search predicate fields for indexing and results in performance gains in data retrieval from branches or leaves across multiple trees for production support or data retrieval to feed business intelligence and data mining. Benchmark experiments for comparing the query performance on the COVER model against self-join and XPath/XQuery approaches using RDBMS were executed and proved that the COVER model outperforms the other two on the same sets of test data queries. © 2010 Fuesane Cheng, Yelena Yesha. COVER model pivot view indexing for efficient XML data management  Computer software; Data handling; Data structures; Indexing (of information); Information management; Metadata; Models; Online systems; XML; Application softwares; Benchmark experiments; Business intelligence; Class objects; Data models; Data retrieval; Distributed data processing; Generic data; Multiple trees; Performance Gain; Product and services; Query performance; RDF data; Self-join; Speed-to-market; Storing XML; Test data; Tree data; Using RDBMS; View generation; XML data; Trees (mathematics)",Governance
2297,Need 4 speed: Leverage new metrics to boost your velocity without compromising on quality,"As Agile becomes the de-facto SDLC practice, it has become evident that additional practices are required to allow enterprises to realize the premise of it. In this experience report, we will share the practices learnt and exercised over the past 3 years that helped us cope with the common challenges of large scale enterprise projects. The concept of ""Application Lifecycle Intelligence"" discussed in this paper covers the key principles of these best practices. It shares the additional analytics and metrics that allowed HP to enable cross time zones collaboration and alignment through transparency and provides insights into quality and risk. The key principal accomplished by extracting data from various practitioners tools and surface it in an actionable format for the various stakeholders. © 2013 IEEE. Need 4 speed: Leverage new metrics to boost your velocity without compromising on quality ALM; Analytics; Enterprise Agile; Scaled Agile ALM; Analytics; Application Lifecycle; Best practices; Experience report; Scaled Agile; Time zones; Industry",Value management
2298,Underpinning sustainability with advanced and visual analytics within the intelligent operations center,"Information superiority is one of the primary issues for Network Enabled Capabilities (NEC) in future crisis management operations that will have to operate in an environment of efficient collaboration and informed decision making in a value network. Exploiting the network-enabled information flows turns out to be the only effective way to meet the challenges and threats we face in this modern, interconnected world. Enhanced inter-agency and inter-company communication and collaboration has been defined as the capability to deliver information superiority whenever required to enable agile and informed decision making to underpin effects-based operations: delivering the right effect, at the right time, to achieve the outcome required. Challenges and threats in our modern world are global and multi-faceted requiring complex responses: governments and corporations buoyed by the realization that the interests of both are mutually engage, are pursuing joint corporate social responsibility to make life and business conduct safe and sustainable. One outcome is increasing openness: organizations increasingly publish data and knowledge in open formats and open spaces and (others) provide tools to gain insight from this open and openly accessible data. Network enablement increases inclusion and participation of people in all domains of private and public life; internet-enabled social networking contributes to data available for analysis and better understanding of human factors. This case study summarizes technology-based social responsibility trends and illustrates how emerging technologies like visual analytics on spatio-temporal data can achieve semantic interoperability and transparency within large amounts of data linked through ontologies and common metadata models. Underpinning sustainability with advanced and visual analytics within the intelligent operations center Corporate social responsibility; Crisis lifecycle management; Intelligent operations center; Smart planet; Visual analytics Behavioral research; Complex networks; Computer aided network analysis; Decision making; Risk management; Semantics; Social aspects; Sustainable development; Visualization; Corporate social responsibilities (CSR); Effects-based operations; Intelligent operations; Inter-company communication; Life-cycle management; Network-enabled capabilities; Semantic interoperability; Visual analytics; Information management",Risk management
2301,"Value driven outcomes (VDO): A pragmatic, modular, and extensible software framework for understanding and improving health care costs and outcomes","Objective: To develop expeditiously a pragmatic, modular, and extensible software framework for understanding and improving healthcare value (costs relative to outcomes). Materials and methods: In 2012, a multidisciplinary team was assembled by the leadership of the University of Utah Health Sciences Center and charged with rapidly developing a pragmatic and actionable analytics framework for understanding and enhancing healthcare value. Based on an analysis of relevant prior work, a value analytics framework known as Value Driven Outcomes (VDO) was developed using an agile methodology. Evaluation consisted of measurement against project objectives, including implementation timeliness, system performance, completeness, accuracy, extensibility, adoption, satisfaction, and the ability to support value improvement. Results: A modular, extensible framework was developed to allocate clinical care costs to individual patient encounters. For example, labor costs in a hospital unit are allocated to patients based on the hours they spent in the unit; actual medication acquisition costs are allocated to patients based on utilization; and radiology costs are allocated based on the minutes required for study performance. Relevant process and outcome measures are also available. A visualization layer facilitates the identification of value improvement opportunities, such as high-volume, high-cost case types with high variability in costs across providers. Initial implementation was completed within 6 months, and all project objectives were fulfilled. The framework has been improved iteratively and is now a foundational tool for delivering high-value care. Conclusions: The framework described can be expeditiously implemented to provide a pragmatic, modular, and extensible approach to understanding and improving healthcare value. © The Author 2014. Value driven outcomes (VDO): A pragmatic, modular, and extensible software framework for understanding and improving health care costs and outcomes Activity-based cost accounting; Care costs; Care outcomes; Care quality; Care value Cost-Benefit Analysis; Health Care Costs; Humans; Software; Treatment Outcome; Utah; accuracy; Article; computer program; cost benefit analysis; health care cost; health care delivery; health care quality; health care system; health care value; hospital subdivisions and components; hospital utilization; outcome assessment; value driven outcome; human; treatment outcome; United States",Risk management
2304,Health informatics visualisation engine: HIVE,"This project was designed to integrate, analyse, synthesise and present essential health and hospital information in a highly accessible, agile and visual form - because pictures are worth a thousand words. We developed a prototype software tool that is; capable of drawing on standardised data files that replicate known industry standard, or are easily derivable from such standards provides the user (analyst, operational manager, financial manger, executive) with a customisable view of the relative outcomes of, and resources used in, care in a number of dimensions- clinical (LOS, number of adverse events, number of drug doses, attending doctor etc) and financial (surgical, pharmacy, nursing etc) - in one setting and identify outliers using advanced statistical modelling techniques. This tool will generate immediate value for a hospitals' endeavour in continuous operational improvement and will be of particular interest to potential customers throughout Australia given the move to nationally provided Activity Based Funding for hospital services. The tool is a useful way to harness the power of ""big data"" through advanced analytics. Copyright © 2014 for the individual papers by the papers' authors. Health informatics visualisation engine: HIVE  Abstracting; Drug dosage; Hospitals; Software prototyping; Health informatics; Hospital information; Hospital service; Industry standards; Operational improvements; Potential customers; Prototype software; Statistical modelling; Big data",Stakeholder management
2307,Work systems paradigm and frames for fractal architecture of information systems,"Contemporary information systems have to satisfy needs of agile and viable enterprises. They shall include mechanisms of business intelligence, business process management, information technology infrastructure management, and alignment between business and computer systems. The mechanisms for business process handling and computer systems handling are similar, and the mechanisms for their continuous integrated improvement also are similar, therefore the architecture of information systems components that support these processes also can have a measure of similarity if considered at a particular level of abstraction. The paper, focusing on aforementioned similarities, uses St. Alter’s work systems paradigm for constructing fractal architecture of information systems that can be used for supporting agile and viable enterprises. The architecture includes predefined frames of processes, a frame for virtual agents, frames of information flows in viable systems, and a frame for information flows in the enterprise architecture that help to derive requirements for introducing continuous changes in information systems. © Springer International Publishing Switzerland 2015. Work systems paradigm and frames for fractal architecture of information systems Continuous change; Fractal systems; Information systems; Viable systems; Work systems Administrative data processing; Agile manufacturing systems; Enterprise resource management; Fractals; Information systems; Management science; Systems engineering; Business process management; Continuous change; Enterprise Architecture; Fractal systems; Information technology infrastructure; Level of abstraction; Measure of similarities; Work system; Information management",Governance
2309,GeoSpatial-temporal analytics to gain insight from linked open data,"Collective endeavours, operating in an environment of efficient collaboration and informed decision making in a value network, bear the only effective way to meet the challenges and threats we face in this modern, interconnected world. Enhanced inter-agency and inter-company communication and collaboration has been defined as the capability to deliver information superiority when required to enable agile and informed decision making to underpin effects-based operations: delivering the right effect, at the right time, to achieve the outcome required. Challenges and threats in our modern world are global and multi-faceted requiring complex responses: governments and corporations buoyed by the realization that the interests of both are mutually engage, are pursuing joint corporate social responsibility to make life and business conduct safe and sustainable. One outcome is increasing openness: organisations increasingly publish data and knowledge in open formats and open spaces and (others) provide tools to gain insight from this open and accessible data. This case study summarizes the technological state-of-the-art and points the way how value networks can benefit from these digital society trends. © 2011 Research Institute for Ope. GeoSpatial-temporal analytics to gain insight from linked open data Corporate Social Responsibility; Linked Open Data; Smarter Planet; Visual Analytics Visualization; Business conduct; Complex response; Corporate social responsibility; Digital society; Effects-based operations; Gain insight; Informed decision; Inter-company communication; Linked Open Data; Open space; Smarter Planet; Value network; Visual analytics; Decision making",Value management
2310,Lean-Agile adaptations in clinical laboratory accredited ISO 15189,"It is introduced Lean techniques in a Clinical Laboratory to improve the operability and the efficiency in continuous processes of analysis, failsafe systems, analysis of areas of value pursuit of zero defects and reduction of waste, and it promote continuous improvement in presented difficulties in adapting to the changing needs of the healthcare environment. Whereas it is necessary to incorporate certification and accreditation, note that the adaptability of the clinical laboratory to the changing needs of physicians in obtaining analytical information is reduced. The application of an agile methodology on analytical systems can provide a line of work that allows the incorporation of planning short work cycles on equips quickly with operational autonomy on the basis of demand and respecting the accreditation requirements and flexibility to ensure adequate performance as the intercomparison of results from the different units analytics, analytical quality and turnaround times. Between 2012 and 2014, a process of analysis and improvement was applied to circuits, a 5 s system, transportation of samples, inventory of reactive and samples, motion of personal and samples, reductions of waiting and delays, overproduction, over processing, and defects of results and reports. At last it seems necessary to apply the Agile methodology to adapt to the evolving necessities in time and the different origins of the samples. It is have used modular systems where the modules of this study are programmed with immunoassay techniques and it has reduced the operative modules depending on the required activity, ensuring the goals of turnaround times, analytic quality, service, health care continuity, and keeping up with the ISO 15189 accreditation requirements. The results of applying the concept of Lean-Agile to a modular system allows us to reduce the associated costs to the seasonal variation of the health care demand and to adapt the system to the changes on the pattern of analytic demand. The savings obtained are two laboratory technicians for a month and those derived from deactivating a complete modular section consisting of three analysts. Average turnaround time (TAT) of 99.5% from the analyzed parameters has been kept equal to the one obtained in common conditions with a p < 0.01. The laboratory has maintained the quality goals measured in analytical variability. Prior to that, it most objectified the inter-comparability of the results among the used channels. © 2015 the authors. Lean-Agile adaptations in clinical laboratory accredited ISO 15189 Accreditation; Agile; Clinical laboratory; ISO 15189; Lean ",Strategic alignment
2312,A workflow for intelligent data-driven analytics software development in oil and gas industry,"A vast amount of oil and gas data has been collected over many years, which can be classified into different groups, including numerical, categorical, historical, real-time, structured, and unstructured. Therefore, analysis and visualization cannot be efficiently accomplished using traditional software platforms. Companies in the oil and gas industry need new technologies and processes to capture and transform data into strategic business intelligence to enhance exploration and production, improve refining and manufacturing efficiency, and optimize global operations while ensuring safety and environmental protection. With the advent of machine learning, data mining, optimization, and cloud computing, it is now possible to manage the large amount of data as never before. Data-driven analytics, in this concept, offers new capabilities to the engineer in exploration and production to learn from data. Analyzing patterns in historical data and identifying the events in real-time data are two methods used for improving operations of activities and assets. The application of data-driven analytics in the oil and gas industry, therefore, potentially provides opportunities for greater insight and efficiency gains. The goal for data-driven analytics software is to provide the strategy required to optimize business outcomes and decision making while helping to minimize risk. The interaction between the users and the information through software tools results in a decision process that helps determine success of a project. The development of a data-driven software platform involves combining a variety of advanced machine learning and data-mining techniques, as well as data management strategies, to provide fit-for- purpose descriptive, predictive, and prescriptive models. Therefore, to be successful, the development team should have a deep understanding of both the underlying data science and the business model as well as software engineering principles. This paper presents a workflow for an intelligent data-driven software platform development. The proposed workflow is composed of data management, data analytics and visualization, and predictive modeling. It covers best practices on database design, object-oriented design patterns and principles, predictive model building and deployment, user-interface development, and test-driven software development in an agile software development structure. Copyright © 2014, Society of Petroleum Engineers. A workflow for intelligent data-driven analytics software development in oil and gas industry  Accident prevention; Data Analytics; Data mining; Data visualization; Decision making; Efficiency; Engineering education; Environmental technology; Gas industry; Information management; Machine learning; Object oriented programming; Object-oriented databases; Petroleum prospecting; Petroleum refining; Predictive analytics; Software testing; User interfaces; Visualization; Agile software development; Data management strategies; Exploration and productions; Manufacturing efficiency; Object oriented design; Software engineering principles; Test-driven software development; User interface development; Software design",Strategic alignment
2314,Going cloud with your big data: A structured approach,"As private and public cloud deployments become more prevalent, it will be critical for end-user organizations to have a clear understanding of big data application requirements, tool capabilities, and best practices for implementation. Every enterprise roadmap for big data in the cloud should include four key steps. They must first identify big data applications where cloud approaches have an advantage that other approaches lack. A more unified framework would need to functionally align the service layers of a big data analytics reference framework with the various layers of a full-blown cloud computing framework. Smarter big data consolidation requires that users preserve, and even expand, where necessary, the distributed, multi-tier, heterogeneous, and agile nature of big data environment by implementing a virtualization capability in middleware, in the access layer, and in the management infrastructure. Just as important is governance of MapReduce and other big data analytic models that execute in the cloud. Going cloud with your big data: A structured approach  ",Governance
2316,"Readiness Integration Center (RIC): Increasing safety, improving maintenance, decreasing cost","The Naval Air System Command (NAVAIR) Naval Aviation Enterprise (NAE) Automated Logistics Environment (ALE) is applying Big Data Analytics and Cloud Computing Technology to support critical elements of Condition Based Maintenance Plus (CBM+) and Reliability Centered Maintenance (RCM) for NAVAIR platforms. The Comprehensive Automated Maintenance Environment - Optimized (CAMEO) Readiness Integration Center (RIC) ALE capability focus is on the V-22 platform with an implementation strategy to apply Collaborative, Agile, Open Source, Big Data Analytics, and Cloud Technology to Collect, Connect, Warehouse, Analyze, and Act to improve platform readiness. The RIC is leveraging NAVAIR NAE ALE capability to Collect, Connect, and Warehouse critical Platform data. NAE collaboration with V-22, E2D, and Triton automated data extracts to support Platform analytic and decision support tool use and development. The RIC sponsors V-22 collaborative, agile, open source development using big data analytics and cloud technology to support the V-22 Readiness Steering Committee and Readiness Teams. The RIC actively supports Agile methodology for analytic and decision support tool development. The RIC use of Open Source tenants includes protections for intellectual property and licensing for use. The RIC development environment is consistent with Big Data and Cloud Technology computing and the RIC infrastructure and selected toolsets have completed proof of concept implementation in the Cloud. The focus of this paper is to describe a ""day in the life of Big Data Analytics"" from data recording and collection through RIC analytics and use by in service support engineering to effect corrective action. © 2014 by the American Helicopter Society International Inc. All rights reserved. NAVAIR SPR approval code 2014-116. Readiness Integration Center (RIC): Increasing safety, improving maintenance, decreasing cost  Decision support systems; Helicopters; Maintenance; Military aircraft; Warehouses; Automated maintenance environments; Cloud computing technologies; Condition based maintenance; Decision support tools; Development environment; Implementation strategies; Open source development; Reliability centered maintenance; Big data",Value management
2317,A classification for business intelligence agility indicators,"Under the label of ""Business Intelligence (BI) agility"" a variety of measures has been proposed, from the adaption of Cloud Computing technologies, over the implementation of ""Self-Service-BI"" approaches, up to the implementation of agile software development methods. Their commonalty is that they all aim at efficiently lowering response times to change requests. The discussion is hampered, however, by the fact that the term ""agile"" remains vague in the context of BI and that it can easily become misguiding: A BI solution that is seemingly ""agile"" from the perspective of a given line of business can easily diminish the agility of the enterprise-wide BI solution. Besides, it is usually neither necessary nor advisable to subsume all sorts of changes in the often complex and vast BI application landscapes under one metric. This contribution tackles this problem. A review of related work and the analysis of multiple interpretative case studies lead to a differentiated and multi-level agility classification scheme for content, functional, and scale related BI agility concepts that are further differentiated with respect to architectural layer and reach. A classification for business intelligence agility indicators Agile-BI.; Agility; Business Intelligence; Data Warehouse Competitive intelligence; Data warehouses; Software engineering; Agile software development; Agile-BI; Agility; Agility indicators; Architectural layers; Classification scheme; Cloud computing technologies; Interpretative case study; Information systems",Governance
2318,AN OPEN SOURCE GEOVISUAL ANALYTICS TOOLBOX for MULTIVARIATE SPATIO-TEMPORAL DATA in ENVIRONMENTAL CHANGE MODELLING,"In environmental change studies, often multiple variables are measured or modelled, and temporal information is essential for the task. These multivariate geographic time-series datasets are often big and difficult to analyse. While many established methods such as PCP (parallel coordinate plots), STC (space-time cubes), scatter-plots and multiple (linked) visualisations help provide more information, we observe that most of the common geovisual analytics suits do not include three-dimensional (3D) visualisations. However, in many environmental studies, we hypothesize that the addition of 3D terrain visualisations along with appropriate data plots and two-dimensional views can help improve the analysts' ability to interpret the spatial relevance better. To test our ideas, we conceptualize, develop, implement and evaluate a geovisual analytics toolbox in a user-centred manner. The conceptualization of the tool is based on concrete user needs that have been identified and collected during informal brainstorming sessions and in a structured focus group session prior to the development. The design process, therefore, is based on a combination of user-centred design with a requirement analysis and agile development. Based on the findings from this phase, the toolbox was designed to have a modular structure and was built on open source geographic information systems (GIS) program Quantum GIS (QGIS), thus benefiting from existing GIS functionality. The modules include a globe view for 3D terrain visualisation (OSGEarth), a scattergram, a time vs. value plot, and a 3D helix visualisation as well as the possibility to view the raw data. The visualisation frame allows real-time linking of these representations. After the design and development stage, a case study was created featuring data from Zermatt valley and the toolbox was evaluated based on expert interviews. Analysts performed multiple spatial and temporal tasks with the case study using the toolbox. The expert interviews were helpful to gain initial insight into the usability of the tools and to highlight further improvements and challenges; revealing certain usability issues and indicating that analysts consider the linked views to be potentially very beneficial and they appreciate seeing the data in its spatial context. AN OPEN SOURCE GEOVISUAL ANALYTICS TOOLBOX for MULTIVARIATE SPATIO-TEMPORAL DATA in ENVIRONMENTAL CHANGE MODELLING Change Modelling; Geovisual Analytics; Geovisualisation; Multivariate Spatio-temporal Data; Open Source ",Capacity management
2320,Modeling software engineering projects as a business: A business intelligence perspective,"Business managers are constantly under pressure to sustain their organizations' competitiveness and to make sound agile decisions in volatile business environments. Business managers make use of business intelligence (BI) tools to assist them make intelligent decisions. Software project managers also need what can be called 'project intelligence' tools in order to deal with the continuously changing and complex software project environment which can be likened to a business environment. Based on this requirement, this research paper proposes that software projects be modeled as a business in order to apply a business intelligence model so as to develop 'project intelligence' tools for software projects. That is, the aim of this research paper is to propose 'project intelligence' tools which are modeled on business intelligence tools. The extension of BI tools to software projects follows from the premise that projects are business constructs through which a business is operated to create a sustainable stakeholder-value. The use of 'project intelligence' tools will enable a software project manager to gain a clear knowledge about factors that may affect project progress and such knowledge will enable quick and informed quality decision making processes which will ensure improved sustainable project performance. © 2013 IEEE. Modeling software engineering projects as a business: A business intelligence perspective business decisions; business intelligence; modeling; project intelligence; tools Competitive intelligence; Management; Managers; Models; Software engineering; Sustainable development; Tools; Business decisions; Business environments; Business managers; Decision making process; Intelligent decisions; Modeling softwares; project intelligence; Sustainable project; Competition",Value management
2321,"17th Americas Conference on Information Systems 2011, AMCIS 2011","The proceedings contain 481 papers. The special focus in this conference is on Developments, Technologies, Knowledge, Benefits, and Services of America's Information System. The topics include: Knowledge goals as an essential component of knowledge management; the impact of culture on the enterprise systems adoption in Japan and the YOU.S.; IT governance for e-health applications; developing sustainable IT-related capabilities; smart-board technology integration in teaching; model for the benefit analysis of ICT; a survey of cognitive theories to support data integration; women managers in high tech; a design theory for knowledge transfer in business intelligence; the perceived uniqueness of the IS profession; IT governance in collaborative organizational structures; power interactions in enterprise system assimilation; a typology of positive and negative self-interruptions in voluntary multitasking; business intelligence maturity in Australia; a nursing diagnosis decision aid adoption assessment; a user centric typology of IS requirements; social movements in World of Warcraft; perceptions of sunk cost and habitual IS use; IS architecture characteristics as a measure of IT agility; the effects of ICT pervasiveness on administrative corruption; collaborative learning in software development teams; demand management in the smart grid; software characteristics of B2B electronic intermediaries; model to support Enterprise Resource Planning system selection; maintenance trends in ERP systems; the role of dynamic capabilities in creating business value from IS assets; spatial analysis of the global digital divide; different configurations of flexibility for I/S strategic alignment; towards objectives-based process redesign; model for measuring efficiency of Argentina banks; the effect of CRM system on sales management control; towards a design for IT performance management; Web 3.0 and crowd servicing; inherence of ratios for service identification and evaluation; Information Communication Technology adoption in Moroccan small and medium enterprises; dynamic capabilities and business processes; understanding artifact knowledge in design science; using real options in ERP-systems for improving delivery reliability; inter-organizational integration of smart objects; risk and compliance management for cloud computing services; a comprehensive information model for business change projects; experiential learning in second life; organizational participation in open communities; organizational social media around the GLOBE; structural stability and virtual team performance; the impact of IT governance on organizational performance; analysing knowledge-based growth; designing viable security solutions; factors affecting impact of cloud computing announcements on firm valuation; toward a maturity model for DSS development processes; a meta-model ontology based on scenarios; key influencing factors of information systems quality and success in Jamaican organizations; the state of the art of service description languages; agile software development; revenue streams of cloud-based platforms; IT outsourcing as a source of open innovation; using IS/IT valuation methods in practice; working toward optimal pair programming management and environment; the role of Business Intelligence (BI) in service innovation; management of information systems outsourcing; autonomy and electronic health records; the impact of e-service quality on e-commerce; medical errors and information quality; a model for assessing the performance of virtual teams; knowledge as a contingency factor in virtual organizations; individual knowledge sharing behaviour in organizations; sustainable business transformation; towards a life cycle oriented business intelligence success model; adoption of cloud computing in organizations; conceptualizing knowledge utilization; detecting community influence echelons in Twitter network; team collaboration in virtual worlds; building sustainable collaborative networks; towards an enterprise software component ontology; business modelling for services; a cultural sociology perspective on IT occupational culture; social network sites and digital word of mouth; learning object composition; measuring innovation using business intelligence dashboards; examining SNS adoption through motivational lens; business intelligence in corporate risk management; business and IT aspects of wireless enabled healthcare solutions; developing a dichotomy of information privacy concerns; self-selected identity and social capital in social network sites; developing data marts for healthcare; developing a theory driven view of web based homework; evaluating design solutions using crowds; examining ethical decision making behaviour in e-learning systems; data mining meets decision making; networking skills and hiring managers; a text-based model for identifying online trust relationships and a threat tree for health information security and privacy. 17th Americas Conference on Information Systems 2011, AMCIS 2011  ",Governance
2327,"17th Americas Conference on Information Systems 2011, AMCIS 2011","The proceedings contain 481 papers. The special focus in this conference is on Developments, Technologies, Knowledge, Benefits and Services of America's Information System. The topics include: Knowledge goals as an essential component of knowledge management; the impact of culture on the enterprise systems adoption in Japan and the YOU.S.; IT governance for e-health applications; developing sustainable IT-related capabilities; smart-board technology integration in teaching; model for the benefit analysis of ICT; a survey of cognitive theories to support data integration; women managers in high tech; a design theory for knowledge transfer in business intelligence; the perceived uniqueness of the IS profession; IT governance in collaborative organizational structures; power interactions in enterprise system assimilation; a typology of positive and negative self-interruptions in voluntary multitasking; business intelligence maturity in Australia; a nursing diagnosis decision aid adoption assessment; a user centric typology of IS requirements; social movements in World of Warcraft; perceptions of sunk cost and habitual IS use; IS architecture characteristics as a measure of IT agility; the effects of ICT pervasiveness on administrative corruption; collaborative learning in software development teams; demand management in the smart grid; software characteristics of B2B electronic intermediaries; model to support Enterprise Resource Planning system selection; maintenance trends in ERP systems; the role of dynamic capabilities in creating business value from IS assets; spatial analysis of the global digital divide; different configurations of flexibility for I/S strategic alignment; towards objectives-based process redesign; model for measuring efficiency of Argentina banks; the effect of CRM system on sales management control; towards a design for IT performance management; Web 3.0 and crowd servicing; inherence of ratios for service identification and evaluation; Information Communication Technology adoption in Moroccan small and medium enterprises; dynamic capabilities and business processes; understanding artifact knowledge in design science; using real options in ERP-systems for improving delivery reliability; inter-organizational integration of smart objects; risk and compliance management for cloud computing services; a comprehensive information model for business change projects; experiential learning in second life; organizational participation in open communities; organizational social media around the GLOBE; structural stability and virtual team performance; the impact of IT governance on organizational performance; analysing knowledge-based growth; designing viable security solutions; factors affecting impact of cloud computing announcements on firm valuation; toward a maturity model for DSS development processes; a meta-model ontology based on scenarios; key influencing factors of information systems quality and success in Jamaican organizations; the state of the art of service description languages; agile software development; revenue streams of cloud-based platforms; IT outsourcing as a source of open innovation; using IS/IT valuation methods in practice; working toward optimal pair programming management and environment; the role of Business Intelligence (BI) in service innovation; management of information systems outsourcing; autonomy and electronic health records; the impact of e-service quality on e-commerce; medical errors and information quality; a model for assessing the performance of virtual teams; knowledge as a contingency factor in virtual organizations; individual knowledge sharing behaviour in organizations; sustainable business transformation; towards a life cycle oriented business intelligence success model; adoption of cloud computing in organizations; conceptualizing knowledge utilization; detecting community influence echelons in Twitter network; team collaboration in virtual worlds; building sustainable collaborative networks; towards an enterprise software component ontology; business modelling for services; a cultural sociology perspective on IT occupational culture; social network sites and digital word of mouth; learning object composition; measuring innovation using business intelligence dashboards; examining SNS adoption through motivational lens; business intelligence in corporate risk management; business and IT aspects of wireless enabled healthcare solutions; developing a dichotomy of information privacy concerns; self-selected identity and social capital in social network sites; developing data marts for healthcare; developing a theory driven view of web based homework; evaluating design solutions using crowds; examining ethical decision making behaviour in e-learning systems; data mining meets decision making; networking skills and hiring managers; a text-based model for identifying online trust relationships and a threat tree for health information security and privacy. 17th Americas Conference on Information Systems 2011, AMCIS 2011  ",Governance
2330,"20th Americas Conference on Information Systems, AMCIS 2014","The proceedings contain 436 papers. The special focus in this conference is on Information Systems. The topics include: A classification of factors that impact the role of the CIO; a comparison of IT governance and control frameworks in cloud computing; a conceptual approach for optimizing distribution logistics using big data; a database-driven model for risk assessment; a framework for developing integrated supply chain information system; a maturity model and web application for environmental management benchmarking; a model of distributed agile team; a model of effective IT governance structures for developing economies; a multi-agent system for healthcare data privacy; a novel indexing method for improving timeliness of high-dimensional data; a systematic review of cloud computing, big data and databases on the cloud; a user satisfaction study of the London congestion charging e-service; achieving business goals with gamification; addressing levels issues in IS qualitative research; affect infusion in a computer based multitasking environment; alignment between business process governance and IT governance; an actor-network perspective on business process management; an agent-based system for medication adherence monitoring and patient care; an application of the knowledge management maturity model; an efficient stochastic update propagation method in data warehousing; an empirical study of consumer behavior in online pay-to-bid auctions; an empirical study of the gratifications of customer resonance on purchase intention; an explorative study of mobile apps in the enterprise; an information system framework and prototype for collaborative and standardized Chinese liquor production; an ontology-based record management systems approach for enhancing decision support; an organizational mining approach based on behavioral process patterns; applying emergent outcome controls to mitigate time pressure in agile software development; augmented reality and print communication; barriers to green IT service management; behavioural affect and cognitive effects of time-pressure and justification requirement in software acquisition; a holistic investigation into factors affecting social media utilisation in the workplace; beyond traditional IT-enabled innovation; a key capability for competitive advantage; BIS adoption determinants in SMEs; boundary spanning and the differentiated effects of IS project deviations; business continuity in network organizations - a literature review; business intelligence and analytics; start-ups in the text analytics software industry; categorising software contexts; causal model for predicting knowledge sharing via ICTs; changing boundaries in virtual (open) innovation work; an organizational learning approach; co-creation in branding through social commerce; communication privacy management in the digital age - effects of general and situational privacy concerns; communities of sentiment around man-made disasters; compensation of IT service management employees; conceptual review of formative assessment in virtual learning environment; conceptualizing business value of IT in healthcare to design sustainable e-health solutions; conditions for participation within synchronous online collaborative learning; consultant strategies and technological affordances; contextual preferences and network-based knowledge sharing in china; contingent role of knowledge self-efficacy distribution on diffusion of knowledge in peer-to-peer networks; data fusion-based decision support architecture for intensive care units; decreasing waiting times with human and equipment resources; design guidelines for a mobile app for wellbeing of emerging adults; designing a hybrid academic workshop; determinants of CIOs reporting relationship; determinants of it job occupations; developing a conceptual model for project knowledge management; developing an engaging IT degree; developing social capital in online communities; development of measurement instrument for sustainable agricultural management; drivers of information quality on blogs; communication preparedness using emergency description information technology; extent of use and overall performance mediated by routinization and infusion; educated people with disabilities in the ICT field; effectiveness and efficiency of blended learning - a literature review; effects of IT-culture conflict and user dissatisfaction on information security policy non-compliance; EHR adoption in healthcare practices; ensuring positive impact of data quality metadata; enterprise architecture as enabler of organizational agility - a municipality case study; enterprise SNS use and profile perceptions; enterprise system lifecycle-wide innovation; evaluating emotions in mobile application descriptions; evaluation of firm level technology usage in developing countries; examining intersubjectivity in social knowledge artifacts; examining the impact of emotions on trust in virtual teams; examining the role of legal climate on individual creativity in virtual worlds; experiential learning with an open-source enterprise system; exploring configurations of affordances; exploring the existence of network governance in the software as a service value network; exploring user satisfaction of the public e-services in the state of Qatar; factors related to social media adoption and use for emergency services operations; features for social CRM technology - an organizational perspective; financial incentives for medication adherence; from strategic to operational collaborations; a qualitative and quantitative analysis of scientific literature; icon design and game app adoption; ICT development and corruption; impact of electronic diabetes registry use on care and outcomes in primary care; impact of online customer reviews and incentives on the product sales at the online retail store; impacts of organizational behavior on it project teams; improving healthcare outreach to a vulnerable community group; improving knowledge-intensive business processes through social media; information security and insider threats in small medical practices; information security in value chains; information systems security training in organizations; recognizing ritualistic behavior by abstracting technical indicators from past cases; inspiring and cultivating female innovators through mobile app development; IT enabled agility in organizational ambidexterity; IT enablers for task organization and innovation support to drive team performance; IT-enabled intangibles and IT capabilities; joint effect of organizational identity and trust on ERP implementation success; critical factors for achieving smart grid value; market reaction to information technology investment announcements in Russian firms; method-based versus software-based design innovation; differences in environment-based voluntariness; news media sentiment of data breaches; online electronic thesis support system at maritime university; a pedagogical approach of promoting information security education; patterns of designer-user interactions in the design innovation process; personality type effects on online credit card payment utilization; personalized design of online communities; predicting fraud from quarterly conference calls; pricing quality attributes of internet domain names; protection motivation driven security learning; prudential risk management of IT sourcing strategies; real-time task attributes and temporal constraints; analyzing requirements cognition in multiple development paradigms; rethinking the concept of organizational readiness; role of justice in information system service recovery process; impression management as motivation to use social networks; seeking efficiency and productivity in health care; sentiment analysis method review in information systems research; smart sustainability through system satisfaction; bridging the gap between online social networks; social determinants of Facebook commerce acceptance; social media communication in European airlines; exploring content guidelines and policy using a grounded theory approach; the use of online social networks to support shopping-oriented decision making; software ecosystem orchestration; visibility feature designs increase tablet use; technology in practice in Brazilian judiciary; testing the group task demands-resources model among IT professionals; the antecedents and impacts of mobile technostress in the workplace; the combined effects of IT and HRM capabilities on competitive performance; the effects of salience, deterrence, and social influence on software piracy; towards a business model framework for E-learning companies; towards understanding and overcoming hurdles in PDMS projects in Germany; trends in the E-learning ecosystem; understanding usages and affordances of enterprise social networks; user interface design and the halo effect; using crowdsourcing tools for implementing open strategy; value proposition of agility in software development - an empirical investigation; virtual team performances in crowdsourcing contests; will insurance brokers use mobile insurance service platform and the antecedents and consequences. 20th Americas Conference on Information Systems, AMCIS 2014  ",Governance
2331,Augmenting in-house big data resources,"Thousands of IBM customers and partners gathered in Las Vegas in November 2013 for the Information On Demand (IOD) conference to dig deeper into the many facets of big data, analytics, and the insight to advance their organizations. Big Data Stampede also enables organizations to be agile and innovative in their approaches to big data initiatives through collaborative and flexible capabilities that can accelerate big data return on investment (ROI). IBM can work alongside organizations to help minimize obstacles and equip them with expert resources, software, and knowledge transfer for deriving the information that can lead to insight for growing their businesses or gaining competitive awareness. Beginning with the Stampede Day session held on November 3, 2013, followed by the dozens of conversations about Big Data Stampede that took place at the Big Data Services booth, IOD attendees were buzzing with Big Data Stampede feedback, observations, and discussions. Augmenting in-house big data resources  Knowledge management; Societies and institutions; Big datum; Knowledge transfer; Las Vegas; On demands; Information technology",Financial management
2332,Information Management: Strategies for Gaining a Competitive Advantage with Data,"Information Management: Gaining a Competitive Advantage with Data is about making smart decisions to make the most of company information. Expert author William McKnight develops the value proposition for information in the enterprise and succinctly outlines the numerous forms of data storage. Information Management will enlighten you, challenge your preconceived notions, and help activate information in the enterprise. Get the big picture on managing data so that your team can make smart decisions by understanding how everything from workload allocation to data stores fits together. The practical, hands-on guidance in this book includes: Part 1: The importance of information management and analytics to business, and how data warehouses are used Part 2: The technologies and data that advance an organization, and extend data warehouses and related functionality Part 3: Big Data and NoSQL, and how technologies like Hadoop enable management of new forms of data Part 4: Pulls it all together, while addressing topics of agile development, modern business intelligence, and organizational change management Read the book cover-to-cover, or keep it within reach for a quick and useful resource. Either way, this book will enable you to master all of the possibilities for data or the broadest view across the enterprise. Balances business and technology, with non-product-specific technical detail Shows how to leverage data to deliver ROI for a business Engaging and approachable, with practical advice on the pros and cons of each domain, so that you learn how information fits together into a complete architecture Provides a path for the data warehouse professional into the new normal of heterogeneity, including NoSQL solutions © 2014 Elsevier Inc. All rights reserved. Information Management: Strategies for Gaining a Competitive Advantage with Data  Big data; Competition; Data warehouses; Digital storage; Engineering education; Agile development; Competitive advantage; Data storage; Data store; Organizational change management; Technical details; Value proposition; Workload allocation; Information management",Strategic alignment
2334,"Proceedings of the European, Mediterranean and Middle Eastern Conference on Information Systems: Global Information Systems Challenges in Management, EMCIS 2010","The proceedings contain 81 papers. The topics discussed include: drivers and barriers to business intelligence adoption: a case of Pakistan; current issues and challenges of supply chain management; a conceptual approach to a service oriented architecture in supply chain management for value bundles; outsourcing logistics for profitability: the case of apparel and clothing industry; buyer-supplier partnership in agile supply chains: a conceptual view; potentials and challenges in multi utility management; drivers and barriers to business intelligence adoption: a case of Pakistan; examining employee attitudes and behaviors towards organizational change using supervisor and peer relations; the impact of organizational antecedents on employee job satisfaction. an empirical evaluation of public sector employees in Pakistan; strategic information systems planning in UAE organization: SISP approaches classification; and strategic alignment and it projects in public sector organization: challenges and solutions. Proceedings of the European, Mediterranean and Middle Eastern Conference on Information Systems: Global Information Systems Challenges in Management, EMCIS 2010  ",Strategic alignment
2336,"Artificial Intelligence for Business Agility - Papers from the AAAI Spring Symposium, Technical Report","The proceedings contain 9 papers. The topics discussed include: SBVR business rules generation from natural language specification; mining of agile business processes; on the collaborative formalization of agile semantics using social network applications; emerging topic detection for business intelligence via predictive analysis of 'Meme' dynamics; estimating sentiment orientation in social media for business informatics; individualization of goods and services: towards a logistics knowledge infrastructure for agile supply chains; semantic web-based integration of heterogeneous web resources; added value of sociofact analysis for business agility; and computer aided strategic planning for eGovernment agility: a global instrument for developing countries. Artificial Intelligence for Business Agility - Papers from the AAAI Spring Symposium, Technical Report  ",Governance
2338,Predicting release quality,"Identifying correlations between in-process development and test metrics is key in anticipating subsequent reliability performance in the field. For several years now at Cisco, our primary measure of field reliability has been Software Defects Per Million Hours (SWDPMH), and this metric has been goaled on a yearly basis for over 100 product families. A key reason SWDPMH is considered to be of critical importance is that we see a high correlation between SWDPMH and Software Customer Satisfaction (SW CSAT) over a wide spectrum of products and feature releases. Therefore it is important to try to anticipate SWDPMH for new releases before the software is released to customers, for several reasons: Early warning that a major feature release is likely to experience substantial quality problems in the field may allow for remediation of the release during, or even prior to, function and system testing Prediction of SWDPMH enables better planning for subsequent maintenance releases and rollout strategies Calculating the tradeoffs between SWDPMH and feature volume provides guidance concerning acceptable feature content, test effort, release cycle timing, and other key parameters affecting feature releases. Our efforts over the past two years have been to enhance our ability to predict SWDPMH in the field. Toward this end, we have developed predictive models, tested the models with a broad range of feature and maintenance releases, and have provided guidance to development, test, and release management teams on how to improve the chances of achieving best-in-class levels of SWDPMH. This work is ongoing, but several models are currently used in a production mode for more than 40 product families, with good results. In this paper we will show correlations with SWDPMH of feature release sequences for 16 product families. We will also show the models' applicability to maintenance release sequences, features, and Business Unit contributions to feature releases. We will also show the models' performance characteristics with agile releases, hybrid agile/waterfall releases, and traditional waterfall releases. © 2014 IEEE. Predicting release quality Customer experience; Reliability modeling; Software products; Software releases; Testing metrics Customer satisfaction; Forecasting; Human resource management; Maintenance; Predictive analytics; Sales; Software testing; Customer experience; Reliability model; Software products; Software release; Testing metrics; Software reliability",Financial management
2339,Correlation aware synchronization for near real time decision support systems,"Many large companies, especially those in financial and insurance service sectors, approach the market with a decentralized management structure, such as by line of business or geographical market segments. However, these companies require access to distributed and possibly heterogeneous data sources for corporate level decision making. In this paper, we focus on challenges of supporting a decision support system (DSS) based on a hybrid approach (i.e. a federation system with replication of frequently accessed remote data sources) for time-sensitive agile business intelligence applications. The response time requirement (and a realistic goal) for such a DSS is near real time (i.e. 2 ∼ 3 minutes to 20 ∼ 30 minutes). The users of a DSS care about not only the response time but also the time stamp of the business operation reports since out-dated reports introduce uncertainty and risks to decision-making. Thus, the information value of a report decreases as time passes. We present a framework of correlation aware synchronization of replicas used in DSS to optimize information values of business reports as a whole. The framework exploits correlation of usage and synchronization latency of replicas in a single query and a workload of queries for an optimal synchronization schedule. We have conducted extensive evaluations based on both TPC-H and synthetic workload. The proposed correlation aware synchronization effectively improves up to 50% of information value comparing with fixed synchronization plans on average. Copyright 2010 ACM. Correlation aware synchronization for near real time decision support systems  Artificial intelligence; Database systems; Decision making; Decision support systems; Decision theory; Insurance; Optimization; Response time (computer systems); Synchronization; Technology; Business intelligence applications; Business operation; Business reports; Corporate level; Decentralized management; Geographical market; Heterogeneous data sources; Hybrid approach; Information value; Near-real time; Optimal synchronization; Remote data sources; Response time; Service sectors; Synthetic workloads; Time stamps; Real time systems",Risk management
2342,40th International Conference on Performance and Capacity 2014 by CMG,The proceedings contain 39 papers and 23 PowerPoint presentations. The topics discussed include: vaulting over career speedbumps; DevOps for non-functional test assurance; performance measurements and post data reduction: using Radview WebLOAD and analytics to load test the georgia tech research institute research portal web site in a Linux environment; entropy-based anomaly detection for SAP z/OS systems; parameter estimation of asymptotically improved super-serial scalability law; challenges in implementing CodeVita - coding competition for students; stack metrics: a taxonomy of metrics supporting the capacity planning stack; adjusting performance testing to the world of agile; technical debt the cost in performance & security; Windows system performance measurement and analysis; a guide to EMC® VNX® performance metrics; and rationale for an IPv6 PDM extension header. 40th International Conference on Performance and Capacity 2014 by CMG  ,Capacity management
2346,Integrating mining operations for improved performance: Applying ISA95 as an enabling framework,"Each step change in technology redefines what ""real-time"" improvements actually means and how businesses can be run. The industry faces the use of different technology, organizational issues, lack of common terminology,(same terms often used for different things by different groups),lack of consistent representation of data, different views of what is important, and different metrics. Today, innovations in the areas of integrated computing, mobility and cloud computing mean that ""real-time ""improvements for the first time actually means real time. People can access up-to-date business data and systems wherever, whenever and however they need to. The flow of business information in exploration, mining and minerals processing has accelerated to real time, where the plant is integrated to sensors, controls, performance KPI's ,and business improvements. Real-time integration offers significant opportunity for intervention on all manufacturing related issues. For the mining executive, it represents an opportunity for agile responsive operations and positive impact on cash flows. This paper describes the use of ISA95 standard to improve information flow between the plant floor, the automation system, the manufacturing execution level and the enterprise resource planning levels. It also shows how measurable performance is generated. Examples used are a Mine to Port Iron Ore application and an induration furnace value generation application using ISA95 standards and Business Intelligence models. Integrating mining operations for improved performance: Applying ISA95 as an enabling framework Best Practices; Business Intelligence ISA95; Real time Automation; Enterprise resource planning; Manufacture; Best practices; Business improvements; Business information; Integrated computing; Manufacturing execution; Organizational issues; Real time; Real time integration; Technology",Financial management
2348,CEUR Workshop Proceedings,The proceedings contain 14 papers. The topics discussed include: participative design of a security risk reference model: an experience in the healthcare sector; insights from a study on decision making in enterprise architecture; initial experiences in developing a reference enterprise architecture for small and medium-sized utilities; adapting an enterprise architecture for business intelligence; from visual language to ontology representation: using OWL for transitivity analysis in 4EM; ontology-based modeling of cloud services: challenges and perspectives; agile design of sustainable networked enterprises; modeling authorization in enterprise-wide contexts; a context modeling method to enhance business service flexibility in organizations; and enterprise architecture modeling for business and IT alignment. CEUR Workshop Proceedings  ,Governance
2350,Shaping the next incarnation of Business intelligence: Towards a flexibly governed network of information integration and analysis capabilities,"The body of knowledge generated by Business Intelligence (BI) research is constantly extended by a stream of heterogeneous technological and organizational innovations. This paper shows how these can be bundled to a new vision for BI that is aligned with new requirements coming from sociotechnical macro trends. The building blocks of the vision come from five research strings that have been extracted from an extensive literature review: BI and Business Process Management, BI across enterprise borders, new approaches of dealing with unstructured data, agile and user-driven BI, and new concepts for BI governance. The macro trend of the diffusion of cyber-physical systems is used to illustrate the argumentation. The realization of this vision comes with an array of open research questions and requires the coordination of research initiatives from a variety of disciplines. Due to the embedded nature of the addressed topics within general research areas of the Information Systems (IS) discipline and the linking pins that come with the underlying Dynamic Capabilities Approach such research provides a contribution to IS. © Springer Fachmedien Wiesbaden 2014. Shaping the next incarnation of Business intelligence: Towards a flexibly governed network of information integration and analysis capabilities Business Intelligence; Data warehousing; Decision and management support; Dynamic capability approach; Literature review; Organizational agility ",Governance
2351,ICircos: Visual analytics for translational bioinformatics,"Translational bioinformatics increasingly involves the discovery of associations between molecular and phenotype information, with the goal of transforming those discoveries into novel methods for diagnosis and treatment. To enable such complex analyses, researchers need approaches that provide the simultaneous representation and interactive analysis of patients, and their molecular and phenotype information. Because few existing visual analytical systems provide appropriate capabilities, we developed a prototypical visual analytical system called iCircos, which enables the simultaneous and interactive exploration of molecular and phenotype information. We discuss our overall method for developing the prototype by integrating user needs and design heuristics from visual analytics, with agile programming in HTML5 and SVG. A demonstration of the prototype to explore molecular and phenotype associations in two disease datasets suggests that iCircos has the potential to accelerate translational discoveries in complex disease datasets. We conclude by discussing insights about designing visual analytical systems for translational bioinformatics, and present our future plans for user testing and adding advanced interactivity to the prototype. Copyright © 2012 ACM. ICircos: Visual analytics for translational bioinformatics Circos; Translational bioinformatics; Visual analytics Diagnosis; Visualization; Agile programming; Analytical systems; Circos; Complex analysis; Complex disease; Data sets; Design heuristics; Interactive analysis; Interactive exploration; Interactivity; Novel methods; User need; User testing; Visual analytics; Bioinformatics",Strategic alignment
2352,Digital information and value,"Digital information changes the ways in which people and organisations interact. This paper examines the nature of this change in the context of the author's Model for Information (MfI). It investigates the relationship between outcomes and value, selection processes and some attributes of information and explores how this relationship changes in the move from analogue to digital information. Selection processes shape the evolution of information ecosystems in which conventions are established for the ways in which information is used. The conventions determine norms for information friction and information quality as well as the sources of information and channels used. Digital information reduces information friction, often dramatically, and changes information quality. The increasing use of analytics in business increasingly delivers predictive or prescriptive digital information. These changes are happening faster than information ecosystem conventions can change. The relationships established in the paper enable an analysis of, and guide changes to, these conventions enabling a more effective use of digital information. © 2015 by the authors; licensee MDPI, Basel, Switzerland. Digital information and value Agile; Analytics; Connection; Digital; Information; Selection Ecology; Ecosystems; Friction; Information analysis; Agile; Analytics; Connection; Digital; Information; Selection; Information use",Capacity management
2356,Harvesting the suggestion box,"Emerging set of software tools are available that can help manufacturers improve products and processes by capturing, analyzing, and applying the valuable data generated by their customer service processes. By capturing service data, manufacturers can address issues early by passing diagnostic data to product development engineers. Measurement instrument manufacturer Instron uses Oracle's Agile PLM software to associate customer service issues with engineering changes and corrective action. The three core capabilities of the software include knowledge base management, natural language search, and advanced analytics and reporting. GE Fanuc Inteligence platform uses the InQuira tool to collect valuable voice of customer data from various sources such as case data capture in each service request. The knowledge management software of InQuira provides the company with content authoring, publishing, and approval workflow processes. Harvesting the suggestion box  ",Risk management
2358,Developing realtime business intelligence systems the agile way,"Real-time Business Intelligence (BI) systems are the buzz of the day. They harness the capabilities of real-time data warehouses to equip the users with current data and enable them to make correct decisions. A typical real time BI system transforms and loads data into a data warehouse as soon it appears in the source systems. This is achieved using a complex network of Database, Web and Application servers coupled with ETL server, source systems and the Intranet. Developing and maintaining these BI systems is a challenging activity where the biggest challenge lies in making the systems work with almost zero latency to make it truly 'real-time'. The paper cites why traditional waterfall methodologies fail to deliver in such projects and how agile methodologies like Scrum can be adopted in their place to ensure success. The paper is supplemented with a live case study of an Infosys Technologies Limited (NASDAQ:INFY) project that uses Scrum and Global Delivery Model (GDM) to successfully manage a complex project of developing and maintaining a BI system right from the requirements elicitation phase. © 2007 IEEE. Developing realtime business intelligence systems the agile way  Agile manufacturing systems; Competitive intelligence; Data warehouses; Decision theory; Intranets; Real-time Business Intelligence (BI) systems; Real-time data warehouses; Source systems; Real time systems",Value management
2363,Information value-driven near real-time decision support systems,"In this paper, we focus on challenges of supporting a decision support system (DSS) based on a hybrid approach (i.e. a federation system with data placement) for agile business intelligence applications. A DSS needs to be designed to handle a workload of potentially complex queries for important decision-making processes. The response time requirement (and a realistic goal) for such a DSS is near real time.The users of a DSS care about not only the response time but also the time stamp of the business operation report since both of them introduce uncertainty and risks to business decision-making. In our proposed DSS, each report is assigned with a business value; denoting its importance to business decision-making. An Information Value (IV) is a business value of a report discounted by time to reflex the uncertainty and risks associated with the computational latency and synchronization latency. We propose a novel Information Value-driven Query Processing (IVQP) framework specific for near real time DSS applications. The framework enables dynamic query plan selection by taking into account of information value and adaptation for online-arrival ad hoc queries. The framework works with single query as well as a workload of queries. The experimental results based on synthetic data and TPC-H show the effectiveness of our approach in achieving optimal information values for the workloads. © 2009 IEEE. Information value-driven near real-time decision support systems  Artificial intelligence; Computer science; Decision support systems; Decision theory; Query processing; Real time systems; Response time (computer systems); Technical presentations; Ad-hoc queries; Business decisions; Business intelligence applications; Business operation; Business value; Complex queries; Data placement; Decision making process; Dynamic Query; Hybrid approach; Information value; Near-real time; Novel information; Real-time decision support systems; Response time; Synthetic data; Time stamps; Decision making",Risk management
2365,"32nd International Convention Proceedings: Digital Economy 6th ALADIN, Information Systems Security, Business Intelligence Systems, Local Government and Student Papers","The proceedings contain 315 papers. The special focus in this conference is on Information and Communication Technology, Electronics and Microelectronics. The topics include: Croatian semiconductor industry cluster; rare-earth-activated nano-structures fabricated by sol-gel route; applications and recent developments in THz research; electrical activation of phosphorus by rapid thermal annealing of doped amorphous silicon films; resonant optical absorption in molecular nanofilms; Raman scattering on porous silicon; modeling of the effect of radicals on plasmas used for etching in micro-electronics; a method of slow-switching interface traps identification in silicon carbide MOS structures; strain and deformation measurement using intrinsic fiber optic low coherence interferometric sensor; modeling of gate leakage current in high-K dielectrics; compact capacitance model for drain-induced barrier-lowering of vertical SONFET; stress effect in ultra-narrow finFET structures; quantum confinement and scaling effects in ultra-thin body double-gate finFETs; optofiber-and-LED device for study of non-phototoxic photosensitizers biodistributions in bio-objects; design and characterization of soil moisture sensor using PCB technology; characterization of temperature sensor using Vt extractor device; an integrated thin film Pt/Ti heater; synthesis of DC power converters without galvanic insulation; microcontroller controlled capacitance decade box; part average analysis in multilayer ceramic manufacturing for automotive industry; improved linearity active resistors using MOS and FGMOS transistors; possibilities of current measurement in CMOS design using current mirrors and comparators; advantages and limitations of the SCW charge pump; tunable gm-C filter using FGMOS based operational transconductance amplifier; a charge-sensitive amplifier associated with APD or PMT for positron emission tomography scanners; efficient and reusable offset cancellation method implemented in CMOS design; kernighan-lin algorithm for n-way circuit partitioning; measuring and modelling of a PCB via structure; analog to digital conversion, synchronization and control and algorithm for hardware realization; AI - look over hardware design - agent with state; fault diagnosis and isolation of the marine diesel engine turbocharger system; UML modeling in design of error detection and correction circuits; the impact of multi-core processor on web server performance; the UK particle physics grid; enabling numerical modeling of mantle convection on the grid; a lightweight specialized meta-scheduler for 3D image rendering applications; the virtualization of computing cluster resources for integration in grid environment; transactional distributed memory management for cluster operating systems; a grid portal for genetic analysis of complex traits; error analysis of quasirandom walks on balls; using sobol sequence in grid environment; towards china's railway freight transportation information grid; fast system matrix generation on a GPU cluster; visualization as a sequence application for grid-based parametric studies; remote graphical visualization of interactive virtual geographical space; a data management and visualization system towards online microscopic imaging; protein data bank graphics generator on grid; real-time evaluation of L-system scene models in online multiplayer games; parallel formulation of 3D implicit function visualization; displaying large amounts of spatial data in GIS; intelligent algorithm for smoke extraction in autonomous forest fire detection; correction of digital images by arbitrary degree Bezier polynomial; wide-view visual systems for flight simulation; visualization of voltage profile and power flow in Croatian power system; network architecture evolution strategy in fixed networks; fireflies synchronization in small overlay networks; capabilities and impacts of EDGE evolution toward seamless wireless networks; adaptable architecture of provisioning system; selecting technology for interactive web application development; objective assessment of speech and audio quality; queue length influence in RED congestion avoidance algorithm; network problems frequency detection using apriori algorithm; radio over fiber technology for wireless access; reliability and scalability of DHCP access model in broadband network; success model of information system implementation; implementation of a GSM-based remotely controlled and monitored machine system; information management to reduce uncertainty in military systems; system for remote supervision and control of power electronics devices; adapting agile practices in globally distributed large scale software development; using static code analysis tools to increase source code maintainability; first time right in AXE using one track and stream line development; collecting of content enterprise knowledge by using metadata; realizacija slozenih procesa uklju_enja kroz sustave za podrsku; problems and possible solutions in offering netphone packages service; specificities of the new complex software based services implementation at mobile operators; dijagnosticiranje i rjesavanje problema you prijenosu audio signala internet protokolom; simulation of DVB-T transmission in matlab; simulators for solving traveling salesman problem variations with various graph search methods; a survey of software quality assessment; identification of persons and business subjects in text documents based on lexical analysis and scoring system; implementation of agent tehnology in web portals for data analysis and consulting; process entropy and informational macrodynamics in a ceramic tile plant; control of single-input-single-output systems with actuator saturation; design space exploration of a multi-core JPEG; 3D optoelectronic method for the steel strip flatness measurement; voltage control of a DC/DC boost converter powered by fuel cell stack; control of a standalone DC voltage source with fuel cell stack; earth fault location in medium voltage distribution networks; review of active vibration control; active control of periodic vibrations based on synchronous averaging of residual signal; energy efficient sensor based control of a greenhouse; electric power distribution protective devices allocation with genetic algorithms; upravljanje jednostavnim procesima putem GSM uredaja; upravljanje sustavom distribucije cementa; upravljanje kvalitetom vazduha you urbano - industrijskim centrima; intelligent control system, data acquisition system and data output system; tagging multimedia stimuli with ontologies; automatic generation of part-whole hierarchies for domain ontologies using web search data; a storage algorithm for a kanerva-like memory model; managing electromagnetic field pollution using genetic algorithm; evolutionary algorithm aided design of multi-switch controller; automatic diagnosis of power transformers based on dissolved gas analysis first level of diagnosis using VAC and VEV inference methods; building an expert system module for world ocean thermocline analysis; precision static and dynamic optical measurement and 3D imaging of reflective surfaces; agent based intelligent forest fire monitoring system; false alarms reduction in forest fire video monitoring system; automatic adjustment of detection parameters in forest fire video monitoring system; vehicle following control considering relative sensor information only; hidden Markov models and convolutional neural network approaches to face recognition tasks solution; digital system of textile fibers identification; exploring string and word kernels on Croatian-English parallel corpus; memetic algorithm for grammatical inference; grapheme-to-phoneme conversion for Croatian speech synthesis; uniform modified method for handwritten text reference line detection; peer assessment system for modern learning settings; semantic metadata for e-learning content; computers in education of children with intellectual and related developmental disorders; avatars and identification in online communication; research regarding introduction of electronic information at the university of jurja dobrile in pula; employment system and needs for ICT labour in the republic of Croatia; parameters estimation according to Engels law in excel; integration of business processes modelling into education; experience with the distance learning bachelor study in the field of finance, banking and investment; combining e-material and data acquisition module to support the educational process; digital repositories and possibilities of their integration into higher education; econometric analysis of the public sector economy in SPSS and excel; integrating protein visualization in the classroom with starbiochem; using web content management systems in university e-commerce courses; computer classroom management and virtualization and visualization in education; teaching, correcting and improving your language competences by means of the internet; an application of excel and VBA in comparison of lattice based option pricing models; implementation of distance learning materials; aesthetic principle in design of distance learning material; capacity testing/planning for successful implementation of LMS/CMS; using multiple-choice tests at university level courses preparation and supporting infrastructure; conceptualisation of learning context in e-learning; how digital immigrants learned to make games for digital natives; document management and exchange system - supporting education process; development of the computer aided system for controlled strikes exercises; adaption in IS integration and enterprise migration; demographic characteristics and internet access of high school teachers and their ICT competencies; advantages and disadvantages of distance learning. 32nd International Convention Proceedings: Digital Economy 6th ALADIN, Information Systems Security, Business Intelligence Systems, Local Government and Student Papers  ",Financial management
2368,"19th Australasian Conference on Information Systems, ACIS 2008","The proceedings contain 104 papers. The special focus in this conference is on information systems. The topics include: electronic commerce technologies adoption by SMEs; success factors for data warehouse and business intelligence systems; transforming research into practice; social networking tools for internal communication in large organizations; mechanisms that impact online auction trust; the role of social networks in technology appropriation over time; the intractable nature of alignment; factor analysis of individual outcomes for teleworkers; mobile network operator strategy; the use of business intelligence systems in Australia; production planning for IT-service providers; an archival analysis of ACIS research papers; measuring ecommerce website success; believable unbelievable internet based information; engaging industry in empirical research; integrating collaboration into the design of complex adaptive systems; practices and principles of IT governance in Australian legal aid organizations; defining identity crimes; ICT risk management in organizations; rethinking the digital divide; polymorphic innovation through unintended consequences; factors influencing the adoption of mobile learning; collaborative tagging of knowledge and learning resources; mindfulness and agile software development; value perception in music information systems; institutional aspects of systems development; pluralism in knowledge management; stakeholder evaluation and selection; strategies for dealing with end-user resistance; agility in information system and towards integrated modelling of business processes and business rules. 19th Australasian Conference on Information Systems, ACIS 2008  ",Governance
2372,Predictive business - fresh initiative or old wine in a new bottle,"Purpose: The purpose of this paper is to present a conceptual analysis of the theoretical and managerial bases and objectives of predictive business. Predictive business refers to operational decision-making and the development of business processes on the basis of business event analysis. It supports the early recognition of business opportunities and threats, better customer intimacy and agile reaction to changes in business environment. An underlying rationale for predictive business is the attainment of competitive advantage through better management of information and knowledge. Design/methodology/approach: The approach to this article is conceptual and theoretical. The literature-based discussion and analysis combines the perspectives of business performance management, business intelligence, and knowledge management to provide a new model of thinking and operation. Findings: For a company predictive business is simultaneously a practical challenge and an epistemic one. It is a practical challenge because predictive business presupposes a change in the company's modes of operation. It is also an epistemic challenge, since it concerns the company's ability to find appropriate balance between knowledge exploitation and knowledge exploration. Research limitations/implications: Further research should be carried out on the functionality of practical applications as well as the attitudinal and technical preparedness of companies to adopt a new mode of operation. As a subject of investigation, the world of business events offer interesting methodological possibilities, since the basis of the work is the gathering and analysis of large quantities of information on operational activities. Originality/value: There has been little research concerning business events in knowledge management context. This article presents a theoretically founded basis for predictive business, combining the concept of analysing business events with previous research in the field of knowledge management. © Emerald Group Publishing Limited. Predictive business - fresh initiative or old wine in a new bottle Business performance; Decision making; Knowledge management; Performance management ",Strategic alignment
2376,Welcome to the Intelligence Age: An examination of intelligence as a complex venture emergent behavior,"Purpose - Aid knowledge management (KM) and business intelligence (BI) practitioners explore and exploit the Intelligence Age complex venture model focusing on intelligence as an emergent behavior. The paper aims to extend the discrete model used by classical system engineering (SE) for a wisdom, knowledge, information, data, and measurement (WKIDM) pyramid to add a wrapper of emergent intelligence to support successful decision making and implementation. Design/methodology/approach - Building on previous theoretical complex venture work, this research explores the value of extending the WKIDM or ""Knowledge Pyramid"" model proposed by classical SE and KM approaches. The resultant IWKIDM model builds on the insights derived from chaos and complexity theories; KM research; observations of several acquisition successes and failures; and doctoral research on agile enterprise decision support. Findings - The paper finds that successful classical SE complicated systems models built with the closed system assumptions of linearity, predictability, and context independence do not scale to the needed open system Intelligence Age solutions. It is necessary to build on a Complex Venture model that guides the engineering solutions that: leverage emergent behavior insights to develop an improved intelligence model for the interaction of complex venture intellectual capital (i.e., self-organizing agents) with the WKIDM pyramid entities and the intelligence products consumer context; and examine WKIDM pyramid levels of abstraction for detachable and complex representations (e.g., explicit versus tacit knowledge). Originality/value - A complex venture conceptual model informs the architecture and systems engineering acquisition practices for new solution category to empower the venture's intellectual capital to produce needed emergent intelligence. Welcome to the Intelligence Age: An examination of intelligence as a complex venture emergent behavior Intellectual capital; Intelligence; Knowledge management; Systems engineering Decision support systems; Engineering research; Knowledge management; Management science; Systems engineering; Complicated systems; Design/methodology/approach; Engineering solutions; Intellectual capital; Intelligence; Intelligence modeling; Intelligence products; Levels of abstraction; Consumer behavior",Strategic alignment
2378,Data quality is everyone's business designing quality into your data warehouse part 1,"In this information age with dramatically growing data volumes, data quality management is proving an avenue for creating sustainable competitive advantage. Combining data from disparate sources provides an opportunity to create new and valuable information. However, it also tends to surface previously existing, but so far unnoticed, data quality issues. To manage these challenges, we propose a data modelling paradigm (Data Vault) and a system development method (Agile), which provide the best alignment among stakeholders. © 2009 Palgrave Macmillan. Data quality is everyone is business designing quality into your data warehouse part 1 Business Intelligence; Data models; Data quality; Data warehousing; System development methods ",Strategic alignment
2380,PLM reaches into the enterprise,"Vendors identified an opportunity to extend its product capabilities well beyond engineering design to encompass many traditional ERP functions even in the times of 2009 economic uncertainty. The decline in spending by manufacturers forced research firm CIMdata Inc. to revise its PLM forecast at mid-year. The next generation of PLM encompasses social networking tools targeting collaboration for the masses, built-in business intelligence for risk management, and industry-specific solutions, and integration with enterprise applications and manufacturing systems. In 2008, Dassault Systèmes outlined its PLM 2.0 strategy and began delivering on it this year in its V6 PLM platform. PTC introduced the concept of social product development as part of its Pro/ENGINEER Wildfire 5.0 3D CAD/CAM/CAE software. Oracle, which acquired PLM purveyor Agile Software a few years ago, has been working on integrating the PLM tools into its enterprise backbone. PLM reaches into the enterprise  ",Capacity management
