{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb4dcd79-925b-4024-808f-8dde237e6d71",
   "metadata": {},
   "source": [
    "## 1. Importación de librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b5c655e5-00cc-43c1-a90f-1b9f7de8da03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\leane\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\leane\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\leane\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Instalación de librerias\n",
    "# librería Natural Language Toolkit, usada para trabajar con textos \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "import re, unicodedata, string\n",
    "import contractions\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, get_scorer_names, f1_score, make_scorer, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c7a260-c584-42a3-9968-7f6b6a214e9a",
   "metadata": {},
   "source": [
    "## 2. Cargar datos iniciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0d7fe19b-944a-4dd1-8e3c-2808ab8324a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargaDeFramework():\n",
    "    frameworkData = pd.read_csv('data/verifiedArticles.csv', sep=',', encoding = 'ANSI')\n",
    "    return frameworkData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e0d5c7bf-cf2b-4d2d-a7ea-a3e5a1243a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargaDeEntrada():    \n",
    "    inputData = pd.read_csv('data/dataToClassify.csv', sep=',', encoding = 'utf-8')\n",
    "    return inputData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17113d2-38eb-4e8a-bf60-b44f3cda853b",
   "metadata": {},
   "source": [
    "## 3. Preparación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c7860a1f-e03b-42ac-be6b-ccf414b23132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminarDuplicados(dataframe):\n",
    "    return dataframe.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ebefbce2-e84f-46d1-9f46-3906e23bcbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation_and_numbers(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        word = word.strip()  \n",
    "        word = re.compile('<.*?>').sub('', word) \n",
    "        word = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', word)  \n",
    "        word = re.sub('\\s+', ' ', word)  \n",
    "        word = re.sub(r'\\[[0-9]*\\]',' ', word) \n",
    "        word = re.sub(r'[^\\w\\s]', '', str(word).lower().strip())\n",
    "        word = re.sub(r'\\d',' ', word) \n",
    "        word = re.sub(r'\\s+',' ', word) \n",
    "        if word != \"\":\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "        \n",
    "    \n",
    "def preprocessing(words):\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation_and_numbers(words)\n",
    "    words = remove_non_ascii(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0249fda8-b020-43ee-8e9f-43b16ba398db",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_verbs(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = lemmatizer.lemmatize(word)\n",
    "        new_words.append(new_word)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5d5cdf41-0cc6-42d2-9f1e-a5a553d11a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizacionLematizacion(dataframe):\n",
    "    dataframe[\"Abstract\"] = dataframe[\"Abstract\"].apply(contractions.fix)\n",
    "    dataframe['Words'] = dataframe['Abstract'].apply(word_tokenize).apply(preprocessing)\n",
    "    dataframe['Words'] = dataframe['Words'].apply(lemmatize_verbs)\n",
    "    dataframe['Words'] = dataframe['Words'].apply(lambda x: ' '.join(map(str, x)))\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "12f9f7e9-2b82-4b5c-a2c7-b910da48658a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminacionDeStopwords(dataframe, dataframeStopwords):\n",
    "    dataframe[\"Words\"] = dataframe[\"Words\"].replace('|'.join(dataframeStopwords), '', regex=True)\n",
    "    dataframe[\"Words\"] = dataframe[\"Words\"].replace(value='', regex=r'\\b[a-z]{1,2}\\b')\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f968f6fa-a2ba-47ea-a364-367959b4400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformacionDeFramework(frameworkData):\n",
    "    frameworkData = frameworkData[frameworkData[\"Category\"].notna()]\n",
    "    frameworkData = frameworkData[[\"Title\", \"Abstract\", \"Category\"]]\n",
    "    frameworkData['Abstract'] = frameworkData['Abstract'] + \" \" + frameworkData['Title']\n",
    "    tokenizacionLematizacion(frameworkData)\n",
    "    frameworkStopwords = [\"project\", \"area\", \"given\", \"level\", \"buzios\", \"world\", \"contain\", \"best\", \"within\", \"field\", \"paper\", \"around\", \"public\", \"ability\", \"making\"\n",
    "                      \"develop\", \"purpose\", \"using\", \"nature\", \"present\", \"author\", \"concept\", \"number\", \"proposed\", \"result\", \"contain\", \"different\", \"several\",\n",
    "                      \"management\", \"portfolio\", \"focus\", \"help\", \"however\", \"term\", \"problem\", \"time\", \"many\", \"system\", \"case\", \"process\", \"make\", \"set\", \"use\", \n",
    "                      \"give\", \"lean\", \"open\", \"well\", \"key\", \"oil\", \"also\", \"new\", \"include\", \"single\", \"face\", \"rapid\", \"long\", \"built\", \"follow\", \"consequently\",\n",
    "                      \"today\", \"achieve\", \"realize\", \"developed\", \"public\", \"constantly\", \"one\", \"identify\", \"give\", \"need\", \"several\", \"often\", \"show\", \"become\",\n",
    "                      \"although\", \"aim\", \"manage\", \"non\", \"site\", \"pre\", \"vital\", \"responibility\", \"applicable\"]\n",
    "    eliminacionDeStopwords(frameworkData, frameworkStopwords)\n",
    "    frameworkData.to_csv('data/transformedFrameworkData.csv')\n",
    "    return frameworkData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "345541b4-d603-495e-9234-d166c60f4046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformacionDeEntrada(inputData):\n",
    "    inputData = inputData[[\"Title\", \"Abstract\", \"Author Keywords\", \"Index Keywords\"]]\n",
    "    inputData['Author Keywords'] = inputData['Author Keywords'].fillna(\"\")\n",
    "    inputData['Index Keywords'] = inputData['Index Keywords'].fillna(\"\")\n",
    "    inputData['Abstract'] = inputData['Abstract'] + \" \" + inputData['Title'] + \" \" + inputData['Author Keywords'] + \" \" +  inputData['Index Keywords']\n",
    "    inputData = inputData.drop(columns = ['Index Keywords', 'Author Keywords'])\n",
    "    tokenizacionLematizacion(inputData)\n",
    "    inputStopwords = [\"project\", \"portfolio\", \"using\", \"develop\", \"approach\", \"system\", \"tool\", \"used\", \"team\", \"current\", \"activity\", \"structure\", \"present\", \"data\",\n",
    "                   \"need\", \"within\", \"open\", \"right\", \"time\", \"paper\", \"proceeding\", \"new\", \"different\", \"towards\", \"case\", \"topic\", \"based\", \"set\", \"use\", \"give\",\n",
    "                   \"make\", \"need\", \"purpose\", \"manage\", \"new\", \"show\", \"aim\"]\n",
    "    eliminacionDeStopwords(inputData, inputStopwords)\n",
    "    inputData.to_csv('data/transformedInputData.csv')\n",
    "    return inputData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e4ba75-6bbb-4207-8489-5ad96fc70f4a",
   "metadata": {},
   "source": [
    "## 4. Preparación de conjunto de datos preentrenados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0d1b7499-704f-4a46-9ad6-ef7ee7bd005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "numFeatures = 0\n",
    "tfidfconverterF = TfidfVectorizer(sublinear_tf=True, ngram_range=(1, 1), encoding='latin-1', min_df=3, max_df=0.4, stop_words=('english'))\n",
    "def crearModeloFramework(frameworkData):\n",
    "    featuresF = tfidfconverterF.fit_transform(frameworkData.Words).toarray()\n",
    "    labels = frameworkData['Category']\n",
    "    smote = SMOTE(random_state=0, k_neighbors=2)\n",
    "    featuresSmote, labelsSmote = smote.fit_resample(featuresF, labels)\n",
    "    numFeatures = featuresSmote.shape[1]\n",
    "    model = OneVsRestClassifier(LinearSVC())\n",
    "    model.fit(featuresSmote, labelsSmote);\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9ad3ed-4065-46b0-bbf6-f45a4412ceb2",
   "metadata": {},
   "source": [
    "## 5. Predicción de etiquetas y actualización de datos entrantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5d277373-2acc-46a5-84aa-7d959f1923df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predecirCategorias(inputData, model):\n",
    "    featuresS = tfidfconverterF.transform(inputData.Words).toarray()\n",
    "    inputData[\"Category\"] = model.predict(featuresS)\n",
    "    return inputData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae678e2-8cf7-4870-a414-712b03875ebe",
   "metadata": {},
   "source": [
    "## 6. Carga de predicción de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0ea898d1-9603-4098-aa44-84118e8dcd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargarPrediccion(inputData):\n",
    "    inputData = inputData.drop(columns = ['Words'])\n",
    "    inputData.to_csv('data/classifiedData.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122c97d-5063-48de-9f7d-56ae59818f36",
   "metadata": {},
   "source": [
    "## 7. Ejecución de pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "dfebff5c-8bf1-4467-9a09-db91ddf42c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leane\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "C:\\Users\\leane\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "C:\\Users\\leane\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "C:\\Users\\leane\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "C:\\Users\\leane\\AppData\\Local\\Temp\\ipykernel_12236\\1551371521.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  inputData['Author Keywords'] = inputData['Author Keywords'].fillna(\"\")\n",
      "C:\\Users\\leane\\AppData\\Local\\Temp\\ipykernel_12236\\1551371521.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  inputData['Index Keywords'] = inputData['Index Keywords'].fillna(\"\")\n",
      "C:\\Users\\leane\\AppData\\Local\\Temp\\ipykernel_12236\\1551371521.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  inputData['Abstract'] = inputData['Abstract'] + \" \" + inputData['Title'] + \" \" + inputData['Author Keywords'] + \" \" +  inputData['Index Keywords']\n"
     ]
    }
   ],
   "source": [
    "frameworkData = (\n",
    "    cargaDeFramework()\n",
    "    .pipe(eliminarDuplicados)\n",
    "    .pipe(transformacionDeFramework)\n",
    ")\n",
    "modeloLinearSVC = crearModeloFramework(frameworkData)\n",
    "inputData = (\n",
    "    cargaDeEntrada()\n",
    "    .pipe(eliminarDuplicados)\n",
    "    .pipe(transformacionDeEntrada)\n",
    "    .pipe(predecirCategorias, model = modeloLinearSVC)\n",
    "    .pipe(cargarPrediccion)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696a4390-fb1d-4847-8148-c83d3964127d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
