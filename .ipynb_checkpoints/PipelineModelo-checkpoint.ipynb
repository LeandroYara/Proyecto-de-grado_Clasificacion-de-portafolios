{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb4dcd79-925b-4024-808f-8dde237e6d71",
   "metadata": {},
   "source": [
    "## 1. Importación de librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b5c655e5-00cc-43c1-a90f-1b9f7de8da03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\leane\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\leane\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\leane\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Instalación de librerias\n",
    "# librería Natural Language Toolkit, usada para trabajar con textos \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "import re, string, unicodedata\n",
    "import contractions\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer, TfidfTransformer\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, roc_curve, confusion_matrix, \\\n",
    "multilabel_confusion_matrix, classification_report\n",
    "from sklearn.multioutput import ClassifierChain, MultiOutputClassifier\n",
    "from sklearn import metrics\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c7a260-c584-42a3-9968-7f6b6a214e9a",
   "metadata": {},
   "source": [
    "## 2. Cargar datos iniciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "0d7fe19b-944a-4dd1-8e3c-2808ab8324a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargaDeFramework():\n",
    "    frameworkData = pd.read_csv('app/data/verifiedArticles.csv', sep=',', encoding = 'ANSI')\n",
    "    return frameworkData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e0d5c7bf-cf2b-4d2d-a7ea-a3e5a1243a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargaDeScopus():    \n",
    "    scopusData = pd.read_csv('app/data/scopusArticles.csv', sep=',', encoding = 'utf-8')\n",
    "    return scopusData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17113d2-38eb-4e8a-bf60-b44f3cda853b",
   "metadata": {},
   "source": [
    "## 3. Preparación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ebefbce2-e84f-46d1-9f46-3906e23bcbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def delete_numbers(words):\n",
    "    \"\"\"Delete all interger occurrences in list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if not word.isdigit():\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "        \n",
    "    \n",
    "def preprocessing(words):\n",
    "    words = to_lowercase(words)\n",
    "    words = delete_numbers(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_non_ascii(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "0249fda8-b020-43ee-8e9f-43b16ba398db",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = lemmatizer.lemmatize(word)\n",
    "        new_words.append(new_word)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "9a8d9beb-d2df-424a-93f7-5b060ba35013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criteriosACategorias(frameworkData):\n",
    "    frameworkData['Criterios'] = frameworkData['Criterios'].replace(['Alineación con la estrategia y objetivos', 'Selección', 'Priorización', 'Optimización'], 'Alineamiento estatégico')\n",
    "    frameworkData['Criterios'] = frameworkData['Criterios'].replace(['Papel de la analitica?', 'Gobierno del portafolio'], 'Manejo de gobernanza')\n",
    "    frameworkData['Criterios'] = frameworkData['Criterios'].replace(['Identificación de riesgos', 'Medición o cuantificación de riesgos'], 'Manejo de riesgo')\n",
    "    frameworkData['Criterios'] = frameworkData['Criterios'].replace(['Criterios para el monitoreo y medición de valor.', 'Criterios para establecer objetivos de valor'], 'Manejo de valor')\n",
    "    frameworkData['Criterios'] = frameworkData['Criterios'].replace(['Métricas para el monitoreo del portafolio de proyectos', 'KPI'], 'Monitoreo y control de portafolio')\n",
    "    frameworkData['Criterios'] = frameworkData['Criterios'].replace(['Gestión de intersados', 'Criterios para la medición de beneficios.', 'Gestión de capacidades', 'Criterios financieros para la selección de proyectos', 'Criterios financieros para la ejecución del portfolio '], 'Otros')\n",
    "    return frameworkData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "5d5cdf41-0cc6-42d2-9f1e-a5a553d11a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizacionLematizacion(dataframe):\n",
    "    dataframe[\"Abstract\"] = dataframe[\"Abstract\"].apply(contractions.fix)\n",
    "    dataframe['Words'] = dataframe['Abstract'].apply(word_tokenize).apply(preprocessing)\n",
    "    dataframe['Words'] = dataframe['Words'].apply(lemmatize_verbs)\n",
    "    dataframe['Words'] = dataframe['Words'].apply(lambda x: ' '.join(map(str, x)))\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "12f9f7e9-2b82-4b5c-a2c7-b910da48658a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminacionDeStopwords(dataframe, dataframeStopwords):\n",
    "    dataframe[\"Words\"] = dataframe[\"Words\"].replace('|'.join(dataframeStopwords), '', regex=True)\n",
    "    dataframe[\"Words\"] = dataframe[\"Words\"].replace(value='', regex=r'\\b(?!big)(?!dat)[a-z]{1,3}\\b')\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "f968f6fa-a2ba-47ea-a364-367959b4400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformacionDeFramework(frameworkData):\n",
    "    frameworkData = frameworkData[frameworkData[\"Criterios\"].notna()]\n",
    "    frameworkData = frameworkData[[\"Title\", \"Abstract\", \"Criterios\"]]\n",
    "    frameworkData['Abstract'] = frameworkData['Abstract'] + \" \" + frameworkData['Title']\n",
    "    criteriosACategorias(frameworkData)\n",
    "    tokenizacionLematizacion(frameworkData)\n",
    "    frameworkStopwords = [\"project\", \"portfolio\", \"help\", \"approach\", \"also\", \"new\", \"used\", \"management\", \"use\", \"time\", \"vrio\", \"aim\", \"could\", \"work\", \"order\", \"purpose\", \"making\", \"make\", \"related\", \"first\", \"towards\", \"role\", \"activity\", \"existing\", \"within\", \"one\", \"need\", \"organization\", \"current\", \"data\", \"paper\"]\n",
    "    eliminacionDeStopwords(frameworkData, frameworkStopwords)\n",
    "    return frameworkData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "345541b4-d603-495e-9234-d166c60f4046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformacionDeScopus(scopusData):\n",
    "    scopusData = scopusData[[\"Title\", \"Abstract\", \"Author Keywords\", \"Index Keywords\"]]\n",
    "    scopusData['Author Keywords'] = scopusData['Author Keywords'].fillna(\"\")\n",
    "    scopusData['Index Keywords'] = scopusData['Index Keywords'].fillna(\"\")\n",
    "    scopusData['Abstract'] = scopusData['Abstract'] + \" \" + scopusData['Title'] + \" \" + scopusData['Author Keywords'] + \" \" +  scopusData['Index Keywords']\n",
    "    scopusData = scopusData.drop(columns = ['Index Keywords', 'Author Keywords'])\n",
    "    tokenizacionLematizacion(scopusData)\n",
    "    scopusStopwords = [\"project\", \"portfolio\", \"develop\", \"approach\", \"system\", \"tool\", \"used\", \"team\", \"current\", \"activity\", \"structure\", \"present\", \"data\", \"need\", \"within\", \"open\", \"right\", \"time\", \"paper\", \"proceeding\", \"new\", \"different\", \"towards\", \"case\", \"topic\", \"based\"]\n",
    "    eliminacionDeStopwords(scopusData, scopusStopwords)\n",
    "    return scopusData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e4ba75-6bbb-4207-8489-5ad96fc70f4a",
   "metadata": {},
   "source": [
    "## 4. Preparación de conjunto de datos preentrenados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "0d1b7499-704f-4a46-9ad6-ef7ee7bd005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crearModeloFramework(frameworkData):\n",
    "    tfidfconverterF = TfidfVectorizer(sublinear_tf=True, ngram_range=(1, 2), encoding='latin-1', min_df=7, stop_words=('english'))\n",
    "    featuresF = tfidfconverterF.fit_transform(frameworkData.Words).toarray()\n",
    "    print(featuresF)\n",
    "    labels = frameworkData['Criterios']\n",
    "    smote = SMOTE(random_state=0, k_neighbors=1)\n",
    "    featuresSmote, labelsSmote = smote.fit_resample(featuresF, labels)\n",
    "    model = LinearSVC()\n",
    "    print(featuresSmote.shape)\n",
    "    model.fit(featuresSmote, labelsSmote);\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9ad3ed-4065-46b0-bbf6-f45a4412ceb2",
   "metadata": {},
   "source": [
    "## 5. Predicción de etiquetas y actualización de datos entrantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "5d277373-2acc-46a5-84aa-7d959f1923df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predecirCategorias(scopusData, model):\n",
    "    tfidfconverterS = TfidfVectorizer(sublinear_tf=True, ngram_range=(1, 2), encoding='latin-1', min_df=380, stop_words=('english'))\n",
    "    featuresS = tfidfconverterS.fit_transform(scopusData.Words).toarray()\n",
    "    print(featuresS.shape)\n",
    "    scopusData[\"Criterios\"] = model.predict(featuresS)\n",
    "    return scopusData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae678e2-8cf7-4870-a414-712b03875ebe",
   "metadata": {},
   "source": [
    "## 6. Carga de predicción de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "0ea898d1-9603-4098-aa44-84118e8dcd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargarPrediccion(scopusData):\n",
    "    scopusData.to_csv('app/data/classifiedScopusData.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122c97d-5063-48de-9f7d-56ae59818f36",
   "metadata": {},
   "source": [
    "## 7. Ejecución de pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "dfebff5c-8bf1-4467-9a09-db91ddf42c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.24649292 0.         ... 0.         0.         0.28098211]\n",
      " [0.         0.         0.15290361 ... 0.         0.12723005 0.        ]\n",
      " [0.17022849 0.18575433 0.         ... 0.21960605 0.         0.        ]\n",
      " ...\n",
      " [0.28360093 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.23324442 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.31206137 0.         ... 0.21789671 0.         0.        ]]\n",
      "(90, 68)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leane\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "C:\\Users\\leane\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "C:\\Users\\leane\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "C:\\Users\\leane\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "C:\\Users\\leane\\AppData\\Local\\Temp\\ipykernel_19864\\2091821024.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  scopusData['Author Keywords'] = scopusData['Author Keywords'].fillna(\"\")\n",
      "C:\\Users\\leane\\AppData\\Local\\Temp\\ipykernel_19864\\2091821024.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  scopusData['Index Keywords'] = scopusData['Index Keywords'].fillna(\"\")\n",
      "C:\\Users\\leane\\AppData\\Local\\Temp\\ipykernel_19864\\2091821024.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  scopusData['Abstract'] = scopusData['Abstract'] + \" \" + scopusData['Title'] + \" \" + scopusData['Author Keywords'] + \" \" +  scopusData['Index Keywords']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2382, 68)\n"
     ]
    }
   ],
   "source": [
    "frameworkData = (\n",
    "    cargaDeFramework()\n",
    "    .pipe(criteriosACategorias)\n",
    "    .pipe(transformacionDeFramework)\n",
    ")\n",
    "modeloLinearSVC = crearModeloFramework(frameworkData)\n",
    "scopusData = (\n",
    "    cargaDeScopus()\n",
    "    .pipe(transformacionDeScopus)\n",
    "    .pipe(predecirCategorias, model = modeloLinearSVC)\n",
    "    .pipe(cargarPrediccion)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
